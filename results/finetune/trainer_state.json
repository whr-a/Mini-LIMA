{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 80.0,
  "eval_steps": 500,
  "global_step": 5000,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.02,
      "grad_norm": 65.96151733398438,
      "learning_rate": 1.0000000000000001e-07,
      "loss": 1.941,
      "step": 1
    },
    {
      "epoch": 0.03,
      "grad_norm": 92.6160888671875,
      "learning_rate": 2.0000000000000002e-07,
      "loss": 1.9761,
      "step": 2
    },
    {
      "epoch": 0.05,
      "grad_norm": 122.7242660522461,
      "learning_rate": 3.0000000000000004e-07,
      "loss": 2.506,
      "step": 3
    },
    {
      "epoch": 0.06,
      "grad_norm": 86.10447692871094,
      "learning_rate": 4.0000000000000003e-07,
      "loss": 2.0703,
      "step": 4
    },
    {
      "epoch": 0.08,
      "grad_norm": 107.36264038085938,
      "learning_rate": 5.000000000000001e-07,
      "loss": 2.2372,
      "step": 5
    },
    {
      "epoch": 0.1,
      "grad_norm": 84.92979431152344,
      "learning_rate": 6.000000000000001e-07,
      "loss": 1.8744,
      "step": 6
    },
    {
      "epoch": 0.11,
      "grad_norm": 83.5097427368164,
      "learning_rate": 7.000000000000001e-07,
      "loss": 2.1792,
      "step": 7
    },
    {
      "epoch": 0.13,
      "grad_norm": 64.43939208984375,
      "learning_rate": 8.000000000000001e-07,
      "loss": 1.8943,
      "step": 8
    },
    {
      "epoch": 0.14,
      "grad_norm": 79.99992370605469,
      "learning_rate": 9.000000000000001e-07,
      "loss": 1.7572,
      "step": 9
    },
    {
      "epoch": 0.16,
      "grad_norm": 86.28009033203125,
      "learning_rate": 1.0000000000000002e-06,
      "loss": 1.8931,
      "step": 10
    },
    {
      "epoch": 0.18,
      "grad_norm": 67.7144546508789,
      "learning_rate": 1.1e-06,
      "loss": 2.1293,
      "step": 11
    },
    {
      "epoch": 0.19,
      "grad_norm": 81.35026550292969,
      "learning_rate": 1.2000000000000002e-06,
      "loss": 2.1668,
      "step": 12
    },
    {
      "epoch": 0.21,
      "grad_norm": 93.82128143310547,
      "learning_rate": 1.3e-06,
      "loss": 2.1435,
      "step": 13
    },
    {
      "epoch": 0.22,
      "grad_norm": 59.812259674072266,
      "learning_rate": 1.4000000000000001e-06,
      "loss": 1.9689,
      "step": 14
    },
    {
      "epoch": 0.24,
      "grad_norm": 49.75564193725586,
      "learning_rate": 1.5e-06,
      "loss": 1.7331,
      "step": 15
    },
    {
      "epoch": 0.26,
      "grad_norm": 48.166358947753906,
      "learning_rate": 1.6000000000000001e-06,
      "loss": 1.8806,
      "step": 16
    },
    {
      "epoch": 0.27,
      "grad_norm": 35.22541046142578,
      "learning_rate": 1.7000000000000002e-06,
      "loss": 1.7544,
      "step": 17
    },
    {
      "epoch": 0.29,
      "grad_norm": 39.54951858520508,
      "learning_rate": 1.8000000000000001e-06,
      "loss": 1.4677,
      "step": 18
    },
    {
      "epoch": 0.3,
      "grad_norm": 36.8757438659668,
      "learning_rate": 1.9000000000000002e-06,
      "loss": 1.5334,
      "step": 19
    },
    {
      "epoch": 0.32,
      "grad_norm": 33.1317024230957,
      "learning_rate": 2.0000000000000003e-06,
      "loss": 1.5183,
      "step": 20
    },
    {
      "epoch": 0.34,
      "grad_norm": 26.476255416870117,
      "learning_rate": 2.1000000000000002e-06,
      "loss": 1.3078,
      "step": 21
    },
    {
      "epoch": 0.35,
      "grad_norm": 29.842918395996094,
      "learning_rate": 2.2e-06,
      "loss": 1.3801,
      "step": 22
    },
    {
      "epoch": 0.37,
      "grad_norm": 28.926708221435547,
      "learning_rate": 2.3000000000000004e-06,
      "loss": 1.4297,
      "step": 23
    },
    {
      "epoch": 0.38,
      "grad_norm": 23.427988052368164,
      "learning_rate": 2.4000000000000003e-06,
      "loss": 1.2487,
      "step": 24
    },
    {
      "epoch": 0.4,
      "grad_norm": 21.93940544128418,
      "learning_rate": 2.5e-06,
      "loss": 1.2667,
      "step": 25
    },
    {
      "epoch": 0.42,
      "grad_norm": 22.23796844482422,
      "learning_rate": 2.6e-06,
      "loss": 1.3535,
      "step": 26
    },
    {
      "epoch": 0.43,
      "grad_norm": 23.390108108520508,
      "learning_rate": 2.7000000000000004e-06,
      "loss": 1.4089,
      "step": 27
    },
    {
      "epoch": 0.45,
      "grad_norm": 29.729145050048828,
      "learning_rate": 2.8000000000000003e-06,
      "loss": 1.4417,
      "step": 28
    },
    {
      "epoch": 0.46,
      "grad_norm": 29.508310317993164,
      "learning_rate": 2.9e-06,
      "loss": 1.426,
      "step": 29
    },
    {
      "epoch": 0.48,
      "grad_norm": 28.63729476928711,
      "learning_rate": 3e-06,
      "loss": 1.3456,
      "step": 30
    },
    {
      "epoch": 0.5,
      "grad_norm": 22.651578903198242,
      "learning_rate": 3.1000000000000004e-06,
      "loss": 1.1065,
      "step": 31
    },
    {
      "epoch": 0.51,
      "grad_norm": 19.812583923339844,
      "learning_rate": 3.2000000000000003e-06,
      "loss": 1.1014,
      "step": 32
    },
    {
      "epoch": 0.53,
      "grad_norm": 20.40202522277832,
      "learning_rate": 3.3000000000000006e-06,
      "loss": 1.1551,
      "step": 33
    },
    {
      "epoch": 0.54,
      "grad_norm": 21.832183837890625,
      "learning_rate": 3.4000000000000005e-06,
      "loss": 1.3755,
      "step": 34
    },
    {
      "epoch": 0.56,
      "grad_norm": 22.37187957763672,
      "learning_rate": 3.5e-06,
      "loss": 1.281,
      "step": 35
    },
    {
      "epoch": 0.58,
      "grad_norm": 21.31250762939453,
      "learning_rate": 3.6000000000000003e-06,
      "loss": 1.2449,
      "step": 36
    },
    {
      "epoch": 0.59,
      "grad_norm": 24.720373153686523,
      "learning_rate": 3.7e-06,
      "loss": 1.2169,
      "step": 37
    },
    {
      "epoch": 0.61,
      "grad_norm": 21.237632751464844,
      "learning_rate": 3.8000000000000005e-06,
      "loss": 1.2328,
      "step": 38
    },
    {
      "epoch": 0.62,
      "grad_norm": 18.681568145751953,
      "learning_rate": 3.900000000000001e-06,
      "loss": 1.3513,
      "step": 39
    },
    {
      "epoch": 0.64,
      "grad_norm": 16.322786331176758,
      "learning_rate": 4.000000000000001e-06,
      "loss": 0.8857,
      "step": 40
    },
    {
      "epoch": 0.66,
      "grad_norm": 19.77005386352539,
      "learning_rate": 4.1e-06,
      "loss": 1.2372,
      "step": 41
    },
    {
      "epoch": 0.67,
      "grad_norm": 24.28969955444336,
      "learning_rate": 4.2000000000000004e-06,
      "loss": 1.6764,
      "step": 42
    },
    {
      "epoch": 0.69,
      "grad_norm": 19.597930908203125,
      "learning_rate": 4.3e-06,
      "loss": 1.3055,
      "step": 43
    },
    {
      "epoch": 0.7,
      "grad_norm": 19.504154205322266,
      "learning_rate": 4.4e-06,
      "loss": 1.1641,
      "step": 44
    },
    {
      "epoch": 0.72,
      "grad_norm": 17.983049392700195,
      "learning_rate": 4.5e-06,
      "loss": 1.0908,
      "step": 45
    },
    {
      "epoch": 0.74,
      "grad_norm": 13.91423225402832,
      "learning_rate": 4.600000000000001e-06,
      "loss": 0.8293,
      "step": 46
    },
    {
      "epoch": 0.75,
      "grad_norm": 30.38557243347168,
      "learning_rate": 4.7e-06,
      "loss": 1.1694,
      "step": 47
    },
    {
      "epoch": 0.77,
      "grad_norm": 17.022214889526367,
      "learning_rate": 4.800000000000001e-06,
      "loss": 0.9712,
      "step": 48
    },
    {
      "epoch": 0.78,
      "grad_norm": 18.73438835144043,
      "learning_rate": 4.9000000000000005e-06,
      "loss": 1.3112,
      "step": 49
    },
    {
      "epoch": 0.8,
      "grad_norm": 16.054073333740234,
      "learning_rate": 5e-06,
      "loss": 0.9087,
      "step": 50
    },
    {
      "epoch": 0.82,
      "grad_norm": 20.195024490356445,
      "learning_rate": 5.1e-06,
      "loss": 1.0505,
      "step": 51
    },
    {
      "epoch": 0.83,
      "grad_norm": 17.591880798339844,
      "learning_rate": 5.2e-06,
      "loss": 1.0985,
      "step": 52
    },
    {
      "epoch": 0.85,
      "grad_norm": 18.51932144165039,
      "learning_rate": 5.300000000000001e-06,
      "loss": 1.1403,
      "step": 53
    },
    {
      "epoch": 0.86,
      "grad_norm": 16.576923370361328,
      "learning_rate": 5.400000000000001e-06,
      "loss": 0.9769,
      "step": 54
    },
    {
      "epoch": 0.88,
      "grad_norm": 19.59630012512207,
      "learning_rate": 5.500000000000001e-06,
      "loss": 1.1469,
      "step": 55
    },
    {
      "epoch": 0.9,
      "grad_norm": 16.56389045715332,
      "learning_rate": 5.600000000000001e-06,
      "loss": 0.9909,
      "step": 56
    },
    {
      "epoch": 0.91,
      "grad_norm": 14.837015151977539,
      "learning_rate": 5.7e-06,
      "loss": 0.8096,
      "step": 57
    },
    {
      "epoch": 0.93,
      "grad_norm": 17.006235122680664,
      "learning_rate": 5.8e-06,
      "loss": 0.8416,
      "step": 58
    },
    {
      "epoch": 0.94,
      "grad_norm": 18.79159927368164,
      "learning_rate": 5.9e-06,
      "loss": 1.0731,
      "step": 59
    },
    {
      "epoch": 0.96,
      "grad_norm": 19.797306060791016,
      "learning_rate": 6e-06,
      "loss": 1.0367,
      "step": 60
    },
    {
      "epoch": 0.98,
      "grad_norm": 15.775390625,
      "learning_rate": 6.1e-06,
      "loss": 0.953,
      "step": 61
    },
    {
      "epoch": 0.99,
      "grad_norm": 19.31832504272461,
      "learning_rate": 6.200000000000001e-06,
      "loss": 1.2055,
      "step": 62
    },
    {
      "epoch": 1.01,
      "grad_norm": 20.436809539794922,
      "learning_rate": 6.300000000000001e-06,
      "loss": 0.9725,
      "step": 63
    },
    {
      "epoch": 1.02,
      "grad_norm": 13.459685325622559,
      "learning_rate": 6.4000000000000006e-06,
      "loss": 0.6556,
      "step": 64
    },
    {
      "epoch": 1.04,
      "grad_norm": 16.7613525390625,
      "learning_rate": 6.5000000000000004e-06,
      "loss": 0.7281,
      "step": 65
    },
    {
      "epoch": 1.06,
      "grad_norm": 12.920466423034668,
      "learning_rate": 6.600000000000001e-06,
      "loss": 0.769,
      "step": 66
    },
    {
      "epoch": 1.07,
      "grad_norm": 15.750144004821777,
      "learning_rate": 6.700000000000001e-06,
      "loss": 0.8342,
      "step": 67
    },
    {
      "epoch": 1.09,
      "grad_norm": 15.52408218383789,
      "learning_rate": 6.800000000000001e-06,
      "loss": 0.8226,
      "step": 68
    },
    {
      "epoch": 1.1,
      "grad_norm": 15.754636764526367,
      "learning_rate": 6.9e-06,
      "loss": 0.8565,
      "step": 69
    },
    {
      "epoch": 1.12,
      "grad_norm": 19.82872772216797,
      "learning_rate": 7e-06,
      "loss": 0.8854,
      "step": 70
    },
    {
      "epoch": 1.14,
      "grad_norm": 14.544764518737793,
      "learning_rate": 7.100000000000001e-06,
      "loss": 0.7036,
      "step": 71
    },
    {
      "epoch": 1.15,
      "grad_norm": 18.160343170166016,
      "learning_rate": 7.2000000000000005e-06,
      "loss": 0.7719,
      "step": 72
    },
    {
      "epoch": 1.17,
      "grad_norm": 16.59869956970215,
      "learning_rate": 7.3e-06,
      "loss": 0.7853,
      "step": 73
    },
    {
      "epoch": 1.18,
      "grad_norm": 15.578808784484863,
      "learning_rate": 7.4e-06,
      "loss": 0.7329,
      "step": 74
    },
    {
      "epoch": 1.2,
      "grad_norm": 14.80730152130127,
      "learning_rate": 7.500000000000001e-06,
      "loss": 0.7483,
      "step": 75
    },
    {
      "epoch": 1.22,
      "grad_norm": 15.91081714630127,
      "learning_rate": 7.600000000000001e-06,
      "loss": 0.7582,
      "step": 76
    },
    {
      "epoch": 1.23,
      "grad_norm": 15.140336036682129,
      "learning_rate": 7.7e-06,
      "loss": 0.7354,
      "step": 77
    },
    {
      "epoch": 1.25,
      "grad_norm": 18.151622772216797,
      "learning_rate": 7.800000000000002e-06,
      "loss": 0.8496,
      "step": 78
    },
    {
      "epoch": 1.26,
      "grad_norm": 16.112098693847656,
      "learning_rate": 7.9e-06,
      "loss": 0.756,
      "step": 79
    },
    {
      "epoch": 1.28,
      "grad_norm": 15.261106491088867,
      "learning_rate": 8.000000000000001e-06,
      "loss": 0.7582,
      "step": 80
    },
    {
      "epoch": 1.3,
      "grad_norm": 18.342132568359375,
      "learning_rate": 8.1e-06,
      "loss": 0.8888,
      "step": 81
    },
    {
      "epoch": 1.31,
      "grad_norm": 15.720168113708496,
      "learning_rate": 8.2e-06,
      "loss": 0.6815,
      "step": 82
    },
    {
      "epoch": 1.33,
      "grad_norm": 16.880887985229492,
      "learning_rate": 8.3e-06,
      "loss": 0.8814,
      "step": 83
    },
    {
      "epoch": 1.34,
      "grad_norm": 14.190397262573242,
      "learning_rate": 8.400000000000001e-06,
      "loss": 0.6563,
      "step": 84
    },
    {
      "epoch": 1.36,
      "grad_norm": 15.9934720993042,
      "learning_rate": 8.5e-06,
      "loss": 0.642,
      "step": 85
    },
    {
      "epoch": 1.38,
      "grad_norm": 13.866466522216797,
      "learning_rate": 8.6e-06,
      "loss": 0.77,
      "step": 86
    },
    {
      "epoch": 1.39,
      "grad_norm": 14.516133308410645,
      "learning_rate": 8.700000000000001e-06,
      "loss": 0.7229,
      "step": 87
    },
    {
      "epoch": 1.41,
      "grad_norm": 17.531503677368164,
      "learning_rate": 8.8e-06,
      "loss": 0.6868,
      "step": 88
    },
    {
      "epoch": 1.42,
      "grad_norm": 13.737227439880371,
      "learning_rate": 8.900000000000001e-06,
      "loss": 0.845,
      "step": 89
    },
    {
      "epoch": 1.44,
      "grad_norm": 15.747355461120605,
      "learning_rate": 9e-06,
      "loss": 0.7454,
      "step": 90
    },
    {
      "epoch": 1.46,
      "grad_norm": 16.766637802124023,
      "learning_rate": 9.100000000000001e-06,
      "loss": 0.638,
      "step": 91
    },
    {
      "epoch": 1.47,
      "grad_norm": 11.824790954589844,
      "learning_rate": 9.200000000000002e-06,
      "loss": 0.6697,
      "step": 92
    },
    {
      "epoch": 1.49,
      "grad_norm": 14.920011520385742,
      "learning_rate": 9.3e-06,
      "loss": 0.7922,
      "step": 93
    },
    {
      "epoch": 1.5,
      "grad_norm": 15.800118446350098,
      "learning_rate": 9.4e-06,
      "loss": 0.6568,
      "step": 94
    },
    {
      "epoch": 1.52,
      "grad_norm": 15.792076110839844,
      "learning_rate": 9.5e-06,
      "loss": 0.8055,
      "step": 95
    },
    {
      "epoch": 1.54,
      "grad_norm": 15.147618293762207,
      "learning_rate": 9.600000000000001e-06,
      "loss": 0.7675,
      "step": 96
    },
    {
      "epoch": 1.55,
      "grad_norm": 12.182133674621582,
      "learning_rate": 9.7e-06,
      "loss": 0.691,
      "step": 97
    },
    {
      "epoch": 1.57,
      "grad_norm": 13.765591621398926,
      "learning_rate": 9.800000000000001e-06,
      "loss": 0.6365,
      "step": 98
    },
    {
      "epoch": 1.58,
      "grad_norm": 15.54188346862793,
      "learning_rate": 9.9e-06,
      "loss": 0.6441,
      "step": 99
    },
    {
      "epoch": 1.6,
      "grad_norm": 14.013427734375,
      "learning_rate": 1e-05,
      "loss": 0.6287,
      "step": 100
    },
    {
      "epoch": 1.62,
      "grad_norm": 15.656842231750488,
      "learning_rate": 9.999998972344433e-06,
      "loss": 0.8011,
      "step": 101
    },
    {
      "epoch": 1.63,
      "grad_norm": 15.289261817932129,
      "learning_rate": 9.999995889378156e-06,
      "loss": 0.6975,
      "step": 102
    },
    {
      "epoch": 1.65,
      "grad_norm": 15.415581703186035,
      "learning_rate": 9.999990751102436e-06,
      "loss": 0.6671,
      "step": 103
    },
    {
      "epoch": 1.66,
      "grad_norm": 20.22511863708496,
      "learning_rate": 9.999983557519382e-06,
      "loss": 0.8874,
      "step": 104
    },
    {
      "epoch": 1.68,
      "grad_norm": 15.799839973449707,
      "learning_rate": 9.999974308631955e-06,
      "loss": 0.6393,
      "step": 105
    },
    {
      "epoch": 1.7,
      "grad_norm": 12.626266479492188,
      "learning_rate": 9.999963004443953e-06,
      "loss": 0.548,
      "step": 106
    },
    {
      "epoch": 1.71,
      "grad_norm": 18.014516830444336,
      "learning_rate": 9.999949644960027e-06,
      "loss": 0.7307,
      "step": 107
    },
    {
      "epoch": 1.73,
      "grad_norm": 13.981911659240723,
      "learning_rate": 9.999934230185666e-06,
      "loss": 0.7103,
      "step": 108
    },
    {
      "epoch": 1.74,
      "grad_norm": 15.055573463439941,
      "learning_rate": 9.999916760127207e-06,
      "loss": 0.6268,
      "step": 109
    },
    {
      "epoch": 1.76,
      "grad_norm": 13.160201072692871,
      "learning_rate": 9.999897234791831e-06,
      "loss": 0.5922,
      "step": 110
    },
    {
      "epoch": 1.78,
      "grad_norm": 14.558294296264648,
      "learning_rate": 9.999875654187566e-06,
      "loss": 0.6928,
      "step": 111
    },
    {
      "epoch": 1.79,
      "grad_norm": 14.656681060791016,
      "learning_rate": 9.99985201832328e-06,
      "loss": 0.7004,
      "step": 112
    },
    {
      "epoch": 1.81,
      "grad_norm": 15.18091869354248,
      "learning_rate": 9.99982632720869e-06,
      "loss": 0.7518,
      "step": 113
    },
    {
      "epoch": 1.82,
      "grad_norm": 14.131463050842285,
      "learning_rate": 9.999798580854356e-06,
      "loss": 0.5344,
      "step": 114
    },
    {
      "epoch": 1.84,
      "grad_norm": 19.40127944946289,
      "learning_rate": 9.999768779271687e-06,
      "loss": 0.9336,
      "step": 115
    },
    {
      "epoch": 1.86,
      "grad_norm": 14.198808670043945,
      "learning_rate": 9.999736922472927e-06,
      "loss": 0.7366,
      "step": 116
    },
    {
      "epoch": 1.87,
      "grad_norm": 13.839877128601074,
      "learning_rate": 9.999703010471177e-06,
      "loss": 0.716,
      "step": 117
    },
    {
      "epoch": 1.89,
      "grad_norm": 15.182780265808105,
      "learning_rate": 9.999667043280375e-06,
      "loss": 0.7529,
      "step": 118
    },
    {
      "epoch": 1.9,
      "grad_norm": 17.32917022705078,
      "learning_rate": 9.999629020915304e-06,
      "loss": 0.9328,
      "step": 119
    },
    {
      "epoch": 1.92,
      "grad_norm": 16.420488357543945,
      "learning_rate": 9.999588943391597e-06,
      "loss": 0.6989,
      "step": 120
    },
    {
      "epoch": 1.94,
      "grad_norm": 13.509124755859375,
      "learning_rate": 9.999546810725726e-06,
      "loss": 0.6776,
      "step": 121
    },
    {
      "epoch": 1.95,
      "grad_norm": 17.08467674255371,
      "learning_rate": 9.99950262293501e-06,
      "loss": 0.6978,
      "step": 122
    },
    {
      "epoch": 1.97,
      "grad_norm": 12.657746315002441,
      "learning_rate": 9.999456380037613e-06,
      "loss": 0.5781,
      "step": 123
    },
    {
      "epoch": 1.98,
      "grad_norm": 12.98134994506836,
      "learning_rate": 9.999408082052544e-06,
      "loss": 0.7507,
      "step": 124
    },
    {
      "epoch": 2.0,
      "grad_norm": 16.223726272583008,
      "learning_rate": 9.999357728999657e-06,
      "loss": 0.7315,
      "step": 125
    },
    {
      "epoch": 2.02,
      "grad_norm": 7.974720001220703,
      "learning_rate": 9.99930532089965e-06,
      "loss": 0.3606,
      "step": 126
    },
    {
      "epoch": 2.03,
      "grad_norm": 10.075377464294434,
      "learning_rate": 9.999250857774066e-06,
      "loss": 0.3255,
      "step": 127
    },
    {
      "epoch": 2.05,
      "grad_norm": 9.8870267868042,
      "learning_rate": 9.999194339645292e-06,
      "loss": 0.4362,
      "step": 128
    },
    {
      "epoch": 2.06,
      "grad_norm": 10.00611686706543,
      "learning_rate": 9.999135766536562e-06,
      "loss": 0.4077,
      "step": 129
    },
    {
      "epoch": 2.08,
      "grad_norm": 10.970564842224121,
      "learning_rate": 9.99907513847195e-06,
      "loss": 0.43,
      "step": 130
    },
    {
      "epoch": 2.1,
      "grad_norm": 10.76160717010498,
      "learning_rate": 9.999012455476382e-06,
      "loss": 0.2913,
      "step": 131
    },
    {
      "epoch": 2.11,
      "grad_norm": 13.02459716796875,
      "learning_rate": 9.998947717575624e-06,
      "loss": 0.4616,
      "step": 132
    },
    {
      "epoch": 2.13,
      "grad_norm": 10.897668838500977,
      "learning_rate": 9.998880924796283e-06,
      "loss": 0.3208,
      "step": 133
    },
    {
      "epoch": 2.14,
      "grad_norm": 9.544211387634277,
      "learning_rate": 9.99881207716582e-06,
      "loss": 0.3102,
      "step": 134
    },
    {
      "epoch": 2.16,
      "grad_norm": 11.04354190826416,
      "learning_rate": 9.998741174712534e-06,
      "loss": 0.263,
      "step": 135
    },
    {
      "epoch": 2.18,
      "grad_norm": 13.572453498840332,
      "learning_rate": 9.998668217465569e-06,
      "loss": 0.4262,
      "step": 136
    },
    {
      "epoch": 2.19,
      "grad_norm": 9.919803619384766,
      "learning_rate": 9.998593205454916e-06,
      "loss": 0.4689,
      "step": 137
    },
    {
      "epoch": 2.21,
      "grad_norm": 11.773658752441406,
      "learning_rate": 9.998516138711411e-06,
      "loss": 0.3727,
      "step": 138
    },
    {
      "epoch": 2.22,
      "grad_norm": 10.88217830657959,
      "learning_rate": 9.998437017266731e-06,
      "loss": 0.4282,
      "step": 139
    },
    {
      "epoch": 2.24,
      "grad_norm": 12.18990707397461,
      "learning_rate": 9.9983558411534e-06,
      "loss": 0.4159,
      "step": 140
    },
    {
      "epoch": 2.26,
      "grad_norm": 11.975643157958984,
      "learning_rate": 9.998272610404789e-06,
      "loss": 0.4181,
      "step": 141
    },
    {
      "epoch": 2.27,
      "grad_norm": 13.813024520874023,
      "learning_rate": 9.998187325055107e-06,
      "loss": 0.3414,
      "step": 142
    },
    {
      "epoch": 2.29,
      "grad_norm": 13.727303504943848,
      "learning_rate": 9.998099985139413e-06,
      "loss": 0.3706,
      "step": 143
    },
    {
      "epoch": 2.3,
      "grad_norm": 14.980402946472168,
      "learning_rate": 9.998010590693612e-06,
      "loss": 0.4237,
      "step": 144
    },
    {
      "epoch": 2.32,
      "grad_norm": 17.598730087280273,
      "learning_rate": 9.997919141754448e-06,
      "loss": 0.5474,
      "step": 145
    },
    {
      "epoch": 2.34,
      "grad_norm": 10.80384349822998,
      "learning_rate": 9.997825638359512e-06,
      "loss": 0.3604,
      "step": 146
    },
    {
      "epoch": 2.35,
      "grad_norm": 13.123688697814941,
      "learning_rate": 9.99773008054724e-06,
      "loss": 0.3885,
      "step": 147
    },
    {
      "epoch": 2.37,
      "grad_norm": 13.948760032653809,
      "learning_rate": 9.997632468356915e-06,
      "loss": 0.3528,
      "step": 148
    },
    {
      "epoch": 2.38,
      "grad_norm": 12.561135292053223,
      "learning_rate": 9.997532801828659e-06,
      "loss": 0.3793,
      "step": 149
    },
    {
      "epoch": 2.4,
      "grad_norm": 8.309569358825684,
      "learning_rate": 9.99743108100344e-06,
      "loss": 0.2324,
      "step": 150
    },
    {
      "epoch": 2.42,
      "grad_norm": 10.433760643005371,
      "learning_rate": 9.997327305923073e-06,
      "loss": 0.441,
      "step": 151
    },
    {
      "epoch": 2.43,
      "grad_norm": 11.388343811035156,
      "learning_rate": 9.997221476630217e-06,
      "loss": 0.4371,
      "step": 152
    },
    {
      "epoch": 2.45,
      "grad_norm": 10.660511016845703,
      "learning_rate": 9.997113593168374e-06,
      "loss": 0.3473,
      "step": 153
    },
    {
      "epoch": 2.46,
      "grad_norm": 7.997110843658447,
      "learning_rate": 9.99700365558189e-06,
      "loss": 0.31,
      "step": 154
    },
    {
      "epoch": 2.48,
      "grad_norm": 12.997166633605957,
      "learning_rate": 9.996891663915955e-06,
      "loss": 0.4911,
      "step": 155
    },
    {
      "epoch": 2.5,
      "grad_norm": 14.795327186584473,
      "learning_rate": 9.996777618216608e-06,
      "loss": 0.5525,
      "step": 156
    },
    {
      "epoch": 2.51,
      "grad_norm": 8.99995231628418,
      "learning_rate": 9.996661518530726e-06,
      "loss": 0.3722,
      "step": 157
    },
    {
      "epoch": 2.53,
      "grad_norm": 9.684342384338379,
      "learning_rate": 9.996543364906033e-06,
      "loss": 0.4251,
      "step": 158
    },
    {
      "epoch": 2.54,
      "grad_norm": 11.041029930114746,
      "learning_rate": 9.996423157391101e-06,
      "loss": 0.4168,
      "step": 159
    },
    {
      "epoch": 2.56,
      "grad_norm": 9.353171348571777,
      "learning_rate": 9.99630089603534e-06,
      "loss": 0.3119,
      "step": 160
    },
    {
      "epoch": 2.58,
      "grad_norm": 10.514168739318848,
      "learning_rate": 9.996176580889005e-06,
      "loss": 0.4081,
      "step": 161
    },
    {
      "epoch": 2.59,
      "grad_norm": 13.912083625793457,
      "learning_rate": 9.996050212003203e-06,
      "loss": 0.5228,
      "step": 162
    },
    {
      "epoch": 2.61,
      "grad_norm": 12.286828994750977,
      "learning_rate": 9.995921789429875e-06,
      "loss": 0.381,
      "step": 163
    },
    {
      "epoch": 2.62,
      "grad_norm": 10.69200611114502,
      "learning_rate": 9.99579131322181e-06,
      "loss": 0.4209,
      "step": 164
    },
    {
      "epoch": 2.64,
      "grad_norm": 10.720061302185059,
      "learning_rate": 9.995658783432645e-06,
      "loss": 0.2725,
      "step": 165
    },
    {
      "epoch": 2.66,
      "grad_norm": 11.183122634887695,
      "learning_rate": 9.995524200116857e-06,
      "loss": 0.421,
      "step": 166
    },
    {
      "epoch": 2.67,
      "grad_norm": 10.458476066589355,
      "learning_rate": 9.995387563329768e-06,
      "loss": 0.3566,
      "step": 167
    },
    {
      "epoch": 2.69,
      "grad_norm": 13.669844627380371,
      "learning_rate": 9.995248873127544e-06,
      "loss": 0.3745,
      "step": 168
    },
    {
      "epoch": 2.7,
      "grad_norm": 10.959264755249023,
      "learning_rate": 9.995108129567193e-06,
      "loss": 0.4194,
      "step": 169
    },
    {
      "epoch": 2.72,
      "grad_norm": 10.298357963562012,
      "learning_rate": 9.994965332706574e-06,
      "loss": 0.3765,
      "step": 170
    },
    {
      "epoch": 2.74,
      "grad_norm": 11.868624687194824,
      "learning_rate": 9.994820482604383e-06,
      "loss": 0.3597,
      "step": 171
    },
    {
      "epoch": 2.75,
      "grad_norm": 11.314136505126953,
      "learning_rate": 9.994673579320162e-06,
      "loss": 0.364,
      "step": 172
    },
    {
      "epoch": 2.77,
      "grad_norm": 14.15630054473877,
      "learning_rate": 9.994524622914297e-06,
      "loss": 0.4409,
      "step": 173
    },
    {
      "epoch": 2.78,
      "grad_norm": 12.031683921813965,
      "learning_rate": 9.994373613448021e-06,
      "loss": 0.4885,
      "step": 174
    },
    {
      "epoch": 2.8,
      "grad_norm": 11.533214569091797,
      "learning_rate": 9.994220550983404e-06,
      "loss": 0.3939,
      "step": 175
    },
    {
      "epoch": 2.82,
      "grad_norm": 12.44566535949707,
      "learning_rate": 9.994065435583368e-06,
      "loss": 0.4022,
      "step": 176
    },
    {
      "epoch": 2.83,
      "grad_norm": 10.38897705078125,
      "learning_rate": 9.993908267311677e-06,
      "loss": 0.3863,
      "step": 177
    },
    {
      "epoch": 2.85,
      "grad_norm": 15.084332466125488,
      "learning_rate": 9.99374904623293e-06,
      "loss": 0.536,
      "step": 178
    },
    {
      "epoch": 2.86,
      "grad_norm": 11.828853607177734,
      "learning_rate": 9.99358777241258e-06,
      "loss": 0.3988,
      "step": 179
    },
    {
      "epoch": 2.88,
      "grad_norm": 16.373689651489258,
      "learning_rate": 9.993424445916923e-06,
      "loss": 0.5544,
      "step": 180
    },
    {
      "epoch": 2.9,
      "grad_norm": 10.410922050476074,
      "learning_rate": 9.993259066813094e-06,
      "loss": 0.3863,
      "step": 181
    },
    {
      "epoch": 2.91,
      "grad_norm": 11.080349922180176,
      "learning_rate": 9.993091635169075e-06,
      "loss": 0.3166,
      "step": 182
    },
    {
      "epoch": 2.93,
      "grad_norm": 7.387266159057617,
      "learning_rate": 9.992922151053688e-06,
      "loss": 0.2688,
      "step": 183
    },
    {
      "epoch": 2.94,
      "grad_norm": 9.711329460144043,
      "learning_rate": 9.992750614536606e-06,
      "loss": 0.3383,
      "step": 184
    },
    {
      "epoch": 2.96,
      "grad_norm": 12.829411506652832,
      "learning_rate": 9.992577025688338e-06,
      "loss": 0.4841,
      "step": 185
    },
    {
      "epoch": 2.98,
      "grad_norm": 15.572443008422852,
      "learning_rate": 9.992401384580241e-06,
      "loss": 0.4471,
      "step": 186
    },
    {
      "epoch": 2.99,
      "grad_norm": 9.121296882629395,
      "learning_rate": 9.992223691284515e-06,
      "loss": 0.3066,
      "step": 187
    },
    {
      "epoch": 3.01,
      "grad_norm": 11.03829574584961,
      "learning_rate": 9.9920439458742e-06,
      "loss": 0.4003,
      "step": 188
    },
    {
      "epoch": 3.02,
      "grad_norm": 7.788311958312988,
      "learning_rate": 9.991862148423187e-06,
      "loss": 0.2716,
      "step": 189
    },
    {
      "epoch": 3.04,
      "grad_norm": 9.088115692138672,
      "learning_rate": 9.991678299006206e-06,
      "loss": 0.2238,
      "step": 190
    },
    {
      "epoch": 3.06,
      "grad_norm": 8.808502197265625,
      "learning_rate": 9.991492397698825e-06,
      "loss": 0.1542,
      "step": 191
    },
    {
      "epoch": 3.07,
      "grad_norm": 11.419720649719238,
      "learning_rate": 9.991304444577465e-06,
      "loss": 0.2518,
      "step": 192
    },
    {
      "epoch": 3.09,
      "grad_norm": 8.986350059509277,
      "learning_rate": 9.991114439719387e-06,
      "loss": 0.2334,
      "step": 193
    },
    {
      "epoch": 3.1,
      "grad_norm": 4.737133502960205,
      "learning_rate": 9.990922383202694e-06,
      "loss": 0.1233,
      "step": 194
    },
    {
      "epoch": 3.12,
      "grad_norm": 4.66371488571167,
      "learning_rate": 9.990728275106332e-06,
      "loss": 0.1439,
      "step": 195
    },
    {
      "epoch": 3.14,
      "grad_norm": 7.9010114669799805,
      "learning_rate": 9.990532115510093e-06,
      "loss": 0.2729,
      "step": 196
    },
    {
      "epoch": 3.15,
      "grad_norm": 23.080110549926758,
      "learning_rate": 9.990333904494609e-06,
      "loss": 0.2495,
      "step": 197
    },
    {
      "epoch": 3.17,
      "grad_norm": 11.975020408630371,
      "learning_rate": 9.990133642141359e-06,
      "loss": 0.3215,
      "step": 198
    },
    {
      "epoch": 3.18,
      "grad_norm": 6.190375328063965,
      "learning_rate": 9.989931328532663e-06,
      "loss": 0.2908,
      "step": 199
    },
    {
      "epoch": 3.2,
      "grad_norm": 9.43266487121582,
      "learning_rate": 9.989726963751683e-06,
      "loss": 0.2847,
      "step": 200
    },
    {
      "epoch": 3.22,
      "grad_norm": 9.032967567443848,
      "learning_rate": 9.989520547882426e-06,
      "loss": 0.1704,
      "step": 201
    },
    {
      "epoch": 3.23,
      "grad_norm": 7.7806267738342285,
      "learning_rate": 9.989312081009744e-06,
      "loss": 0.229,
      "step": 202
    },
    {
      "epoch": 3.25,
      "grad_norm": 5.941298484802246,
      "learning_rate": 9.989101563219327e-06,
      "loss": 0.1071,
      "step": 203
    },
    {
      "epoch": 3.26,
      "grad_norm": 10.230195045471191,
      "learning_rate": 9.988888994597714e-06,
      "loss": 0.2657,
      "step": 204
    },
    {
      "epoch": 3.28,
      "grad_norm": 10.350685119628906,
      "learning_rate": 9.98867437523228e-06,
      "loss": 0.1997,
      "step": 205
    },
    {
      "epoch": 3.3,
      "grad_norm": 9.329275131225586,
      "learning_rate": 9.988457705211251e-06,
      "loss": 0.332,
      "step": 206
    },
    {
      "epoch": 3.31,
      "grad_norm": 5.896895885467529,
      "learning_rate": 9.988238984623691e-06,
      "loss": 0.1417,
      "step": 207
    },
    {
      "epoch": 3.33,
      "grad_norm": 9.309696197509766,
      "learning_rate": 9.988018213559504e-06,
      "loss": 0.2232,
      "step": 208
    },
    {
      "epoch": 3.34,
      "grad_norm": 12.366339683532715,
      "learning_rate": 9.987795392109443e-06,
      "loss": 0.2138,
      "step": 209
    },
    {
      "epoch": 3.36,
      "grad_norm": 8.028887748718262,
      "learning_rate": 9.987570520365105e-06,
      "loss": 0.1862,
      "step": 210
    },
    {
      "epoch": 3.38,
      "grad_norm": 6.331714630126953,
      "learning_rate": 9.987343598418919e-06,
      "loss": 0.2096,
      "step": 211
    },
    {
      "epoch": 3.39,
      "grad_norm": 18.080060958862305,
      "learning_rate": 9.987114626364172e-06,
      "loss": 0.221,
      "step": 212
    },
    {
      "epoch": 3.41,
      "grad_norm": 9.13018798828125,
      "learning_rate": 9.98688360429498e-06,
      "loss": 0.1718,
      "step": 213
    },
    {
      "epoch": 3.42,
      "grad_norm": 10.102595329284668,
      "learning_rate": 9.986650532306308e-06,
      "loss": 0.2042,
      "step": 214
    },
    {
      "epoch": 3.44,
      "grad_norm": 7.7974700927734375,
      "learning_rate": 9.986415410493966e-06,
      "loss": 0.1837,
      "step": 215
    },
    {
      "epoch": 3.46,
      "grad_norm": 7.674556255340576,
      "learning_rate": 9.986178238954602e-06,
      "loss": 0.2304,
      "step": 216
    },
    {
      "epoch": 3.47,
      "grad_norm": 19.142566680908203,
      "learning_rate": 9.985939017785709e-06,
      "loss": 0.2559,
      "step": 217
    },
    {
      "epoch": 3.49,
      "grad_norm": 6.884934425354004,
      "learning_rate": 9.985697747085621e-06,
      "loss": 0.1526,
      "step": 218
    },
    {
      "epoch": 3.5,
      "grad_norm": 8.634044647216797,
      "learning_rate": 9.985454426953513e-06,
      "loss": 0.1946,
      "step": 219
    },
    {
      "epoch": 3.52,
      "grad_norm": 7.903127670288086,
      "learning_rate": 9.98520905748941e-06,
      "loss": 0.2237,
      "step": 220
    },
    {
      "epoch": 3.54,
      "grad_norm": 9.677728652954102,
      "learning_rate": 9.984961638794171e-06,
      "loss": 0.2469,
      "step": 221
    },
    {
      "epoch": 3.55,
      "grad_norm": 5.574035167694092,
      "learning_rate": 9.9847121709695e-06,
      "loss": 0.2375,
      "step": 222
    },
    {
      "epoch": 3.57,
      "grad_norm": 7.4053826332092285,
      "learning_rate": 9.984460654117944e-06,
      "loss": 0.248,
      "step": 223
    },
    {
      "epoch": 3.58,
      "grad_norm": 8.591083526611328,
      "learning_rate": 9.984207088342895e-06,
      "loss": 0.2224,
      "step": 224
    },
    {
      "epoch": 3.6,
      "grad_norm": 10.375297546386719,
      "learning_rate": 9.983951473748579e-06,
      "loss": 0.315,
      "step": 225
    },
    {
      "epoch": 3.62,
      "grad_norm": 13.77047061920166,
      "learning_rate": 9.983693810440073e-06,
      "loss": 0.2307,
      "step": 226
    },
    {
      "epoch": 3.63,
      "grad_norm": 8.366671562194824,
      "learning_rate": 9.983434098523294e-06,
      "loss": 0.3244,
      "step": 227
    },
    {
      "epoch": 3.65,
      "grad_norm": 7.410724639892578,
      "learning_rate": 9.983172338104996e-06,
      "loss": 0.2819,
      "step": 228
    },
    {
      "epoch": 3.66,
      "grad_norm": 7.178343772888184,
      "learning_rate": 9.982908529292782e-06,
      "loss": 0.1359,
      "step": 229
    },
    {
      "epoch": 3.68,
      "grad_norm": 10.413265228271484,
      "learning_rate": 9.982642672195093e-06,
      "loss": 0.2856,
      "step": 230
    },
    {
      "epoch": 3.7,
      "grad_norm": 7.512296199798584,
      "learning_rate": 9.98237476692121e-06,
      "loss": 0.2352,
      "step": 231
    },
    {
      "epoch": 3.71,
      "grad_norm": 9.858445167541504,
      "learning_rate": 9.982104813581263e-06,
      "loss": 0.2423,
      "step": 232
    },
    {
      "epoch": 3.73,
      "grad_norm": 6.88232421875,
      "learning_rate": 9.981832812286217e-06,
      "loss": 0.2317,
      "step": 233
    },
    {
      "epoch": 3.74,
      "grad_norm": 11.825775146484375,
      "learning_rate": 9.98155876314788e-06,
      "loss": 0.233,
      "step": 234
    },
    {
      "epoch": 3.76,
      "grad_norm": 12.305248260498047,
      "learning_rate": 9.98128266627891e-06,
      "loss": 0.2847,
      "step": 235
    },
    {
      "epoch": 3.78,
      "grad_norm": 6.980038166046143,
      "learning_rate": 9.981004521792793e-06,
      "loss": 0.2509,
      "step": 236
    },
    {
      "epoch": 3.79,
      "grad_norm": 10.352520942687988,
      "learning_rate": 9.980724329803865e-06,
      "loss": 0.182,
      "step": 237
    },
    {
      "epoch": 3.81,
      "grad_norm": 8.700453758239746,
      "learning_rate": 9.980442090427305e-06,
      "loss": 0.154,
      "step": 238
    },
    {
      "epoch": 3.82,
      "grad_norm": 8.799627304077148,
      "learning_rate": 9.980157803779127e-06,
      "loss": 0.2896,
      "step": 239
    },
    {
      "epoch": 3.84,
      "grad_norm": 10.756231307983398,
      "learning_rate": 9.979871469976197e-06,
      "loss": 0.2312,
      "step": 240
    },
    {
      "epoch": 3.86,
      "grad_norm": 14.663585662841797,
      "learning_rate": 9.979583089136208e-06,
      "loss": 0.3018,
      "step": 241
    },
    {
      "epoch": 3.87,
      "grad_norm": 10.327275276184082,
      "learning_rate": 9.979292661377709e-06,
      "loss": 0.4067,
      "step": 242
    },
    {
      "epoch": 3.89,
      "grad_norm": 8.149499893188477,
      "learning_rate": 9.97900018682008e-06,
      "loss": 0.1736,
      "step": 243
    },
    {
      "epoch": 3.9,
      "grad_norm": 9.056051254272461,
      "learning_rate": 9.978705665583548e-06,
      "loss": 0.1436,
      "step": 244
    },
    {
      "epoch": 3.92,
      "grad_norm": 17.230497360229492,
      "learning_rate": 9.978409097789178e-06,
      "loss": 0.4734,
      "step": 245
    },
    {
      "epoch": 3.94,
      "grad_norm": 7.934141159057617,
      "learning_rate": 9.97811048355888e-06,
      "loss": 0.279,
      "step": 246
    },
    {
      "epoch": 3.95,
      "grad_norm": 6.772695064544678,
      "learning_rate": 9.9778098230154e-06,
      "loss": 0.3453,
      "step": 247
    },
    {
      "epoch": 3.97,
      "grad_norm": 7.543986797332764,
      "learning_rate": 9.977507116282333e-06,
      "loss": 0.2712,
      "step": 248
    },
    {
      "epoch": 3.98,
      "grad_norm": 10.840376853942871,
      "learning_rate": 9.977202363484105e-06,
      "loss": 0.3803,
      "step": 249
    },
    {
      "epoch": 4.0,
      "grad_norm": 6.706561088562012,
      "learning_rate": 9.976895564745993e-06,
      "loss": 0.178,
      "step": 250
    },
    {
      "epoch": 4.02,
      "grad_norm": 4.275976657867432,
      "learning_rate": 9.976586720194106e-06,
      "loss": 0.1314,
      "step": 251
    },
    {
      "epoch": 4.03,
      "grad_norm": 6.275630474090576,
      "learning_rate": 9.9762758299554e-06,
      "loss": 0.1913,
      "step": 252
    },
    {
      "epoch": 4.05,
      "grad_norm": 4.223176002502441,
      "learning_rate": 9.975962894157673e-06,
      "loss": 0.0833,
      "step": 253
    },
    {
      "epoch": 4.06,
      "grad_norm": 4.297914505004883,
      "learning_rate": 9.975647912929558e-06,
      "loss": 0.1176,
      "step": 254
    },
    {
      "epoch": 4.08,
      "grad_norm": 5.420673370361328,
      "learning_rate": 9.975330886400531e-06,
      "loss": 0.1146,
      "step": 255
    },
    {
      "epoch": 4.1,
      "grad_norm": 3.9519240856170654,
      "learning_rate": 9.975011814700912e-06,
      "loss": 0.1131,
      "step": 256
    },
    {
      "epoch": 4.11,
      "grad_norm": 6.941825866699219,
      "learning_rate": 9.97469069796186e-06,
      "loss": 0.1021,
      "step": 257
    },
    {
      "epoch": 4.13,
      "grad_norm": 5.4645891189575195,
      "learning_rate": 9.974367536315372e-06,
      "loss": 0.1248,
      "step": 258
    },
    {
      "epoch": 4.14,
      "grad_norm": 7.963587760925293,
      "learning_rate": 9.974042329894287e-06,
      "loss": 0.1267,
      "step": 259
    },
    {
      "epoch": 4.16,
      "grad_norm": 2.907930374145508,
      "learning_rate": 9.973715078832288e-06,
      "loss": 0.033,
      "step": 260
    },
    {
      "epoch": 4.18,
      "grad_norm": 9.30092716217041,
      "learning_rate": 9.973385783263893e-06,
      "loss": 0.1382,
      "step": 261
    },
    {
      "epoch": 4.19,
      "grad_norm": 8.638160705566406,
      "learning_rate": 9.973054443324463e-06,
      "loss": 0.1626,
      "step": 262
    },
    {
      "epoch": 4.21,
      "grad_norm": 6.871871471405029,
      "learning_rate": 9.9727210591502e-06,
      "loss": 0.1087,
      "step": 263
    },
    {
      "epoch": 4.22,
      "grad_norm": 6.977451324462891,
      "learning_rate": 9.972385630878147e-06,
      "loss": 0.2773,
      "step": 264
    },
    {
      "epoch": 4.24,
      "grad_norm": 5.230697154998779,
      "learning_rate": 9.972048158646184e-06,
      "loss": 0.1302,
      "step": 265
    },
    {
      "epoch": 4.26,
      "grad_norm": 7.563255786895752,
      "learning_rate": 9.971708642593033e-06,
      "loss": 0.2853,
      "step": 266
    },
    {
      "epoch": 4.27,
      "grad_norm": 7.034541606903076,
      "learning_rate": 9.971367082858256e-06,
      "loss": 0.1894,
      "step": 267
    },
    {
      "epoch": 4.29,
      "grad_norm": 5.914486885070801,
      "learning_rate": 9.971023479582258e-06,
      "loss": 0.1635,
      "step": 268
    },
    {
      "epoch": 4.3,
      "grad_norm": 5.362160682678223,
      "learning_rate": 9.970677832906279e-06,
      "loss": 0.1187,
      "step": 269
    },
    {
      "epoch": 4.32,
      "grad_norm": 12.43779182434082,
      "learning_rate": 9.970330142972403e-06,
      "loss": 0.29,
      "step": 270
    },
    {
      "epoch": 4.34,
      "grad_norm": 9.469731330871582,
      "learning_rate": 9.96998040992355e-06,
      "loss": 0.2312,
      "step": 271
    },
    {
      "epoch": 4.35,
      "grad_norm": 9.86606216430664,
      "learning_rate": 9.969628633903483e-06,
      "loss": 0.1765,
      "step": 272
    },
    {
      "epoch": 4.37,
      "grad_norm": 4.16367769241333,
      "learning_rate": 9.969274815056801e-06,
      "loss": 0.1176,
      "step": 273
    },
    {
      "epoch": 4.38,
      "grad_norm": 4.534965515136719,
      "learning_rate": 9.968918953528953e-06,
      "loss": 0.1905,
      "step": 274
    },
    {
      "epoch": 4.4,
      "grad_norm": 12.178775787353516,
      "learning_rate": 9.968561049466214e-06,
      "loss": 0.1604,
      "step": 275
    },
    {
      "epoch": 4.42,
      "grad_norm": 4.363141059875488,
      "learning_rate": 9.968201103015707e-06,
      "loss": 0.0789,
      "step": 276
    },
    {
      "epoch": 4.43,
      "grad_norm": 6.9772257804870605,
      "learning_rate": 9.96783911432539e-06,
      "loss": 0.1743,
      "step": 277
    },
    {
      "epoch": 4.45,
      "grad_norm": 4.679896354675293,
      "learning_rate": 9.967475083544066e-06,
      "loss": 0.0948,
      "step": 278
    },
    {
      "epoch": 4.46,
      "grad_norm": 6.793182849884033,
      "learning_rate": 9.967109010821374e-06,
      "loss": 0.1354,
      "step": 279
    },
    {
      "epoch": 4.48,
      "grad_norm": 5.939284801483154,
      "learning_rate": 9.966740896307791e-06,
      "loss": 0.1234,
      "step": 280
    },
    {
      "epoch": 4.5,
      "grad_norm": 5.703408718109131,
      "learning_rate": 9.966370740154636e-06,
      "loss": 0.1708,
      "step": 281
    },
    {
      "epoch": 4.51,
      "grad_norm": 4.135433197021484,
      "learning_rate": 9.965998542514066e-06,
      "loss": 0.0964,
      "step": 282
    },
    {
      "epoch": 4.53,
      "grad_norm": 4.929136276245117,
      "learning_rate": 9.965624303539077e-06,
      "loss": 0.1214,
      "step": 283
    },
    {
      "epoch": 4.54,
      "grad_norm": 6.071287155151367,
      "learning_rate": 9.965248023383505e-06,
      "loss": 0.1549,
      "step": 284
    },
    {
      "epoch": 4.56,
      "grad_norm": 6.597996234893799,
      "learning_rate": 9.964869702202023e-06,
      "loss": 0.1581,
      "step": 285
    },
    {
      "epoch": 4.58,
      "grad_norm": 9.8407621383667,
      "learning_rate": 9.964489340150147e-06,
      "loss": 0.1659,
      "step": 286
    },
    {
      "epoch": 4.59,
      "grad_norm": 8.801142692565918,
      "learning_rate": 9.964106937384229e-06,
      "loss": 0.1718,
      "step": 287
    },
    {
      "epoch": 4.61,
      "grad_norm": 9.301398277282715,
      "learning_rate": 9.96372249406146e-06,
      "loss": 0.2116,
      "step": 288
    },
    {
      "epoch": 4.62,
      "grad_norm": 8.079794883728027,
      "learning_rate": 9.963336010339869e-06,
      "loss": 0.1567,
      "step": 289
    },
    {
      "epoch": 4.64,
      "grad_norm": 6.84426736831665,
      "learning_rate": 9.962947486378325e-06,
      "loss": 0.1855,
      "step": 290
    },
    {
      "epoch": 4.66,
      "grad_norm": 5.9994797706604,
      "learning_rate": 9.962556922336538e-06,
      "loss": 0.1812,
      "step": 291
    },
    {
      "epoch": 4.67,
      "grad_norm": 6.418842792510986,
      "learning_rate": 9.962164318375052e-06,
      "loss": 0.1456,
      "step": 292
    },
    {
      "epoch": 4.69,
      "grad_norm": 6.849066257476807,
      "learning_rate": 9.961769674655251e-06,
      "loss": 0.2187,
      "step": 293
    },
    {
      "epoch": 4.7,
      "grad_norm": 7.772678375244141,
      "learning_rate": 9.96137299133936e-06,
      "loss": 0.1602,
      "step": 294
    },
    {
      "epoch": 4.72,
      "grad_norm": 10.75830364227295,
      "learning_rate": 9.96097426859044e-06,
      "loss": 0.2287,
      "step": 295
    },
    {
      "epoch": 4.74,
      "grad_norm": 8.225852966308594,
      "learning_rate": 9.960573506572391e-06,
      "loss": 0.138,
      "step": 296
    },
    {
      "epoch": 4.75,
      "grad_norm": 5.9158830642700195,
      "learning_rate": 9.960170705449948e-06,
      "loss": 0.1524,
      "step": 297
    },
    {
      "epoch": 4.77,
      "grad_norm": 8.54313850402832,
      "learning_rate": 9.959765865388693e-06,
      "loss": 0.2097,
      "step": 298
    },
    {
      "epoch": 4.78,
      "grad_norm": 11.08303451538086,
      "learning_rate": 9.959358986555037e-06,
      "loss": 0.108,
      "step": 299
    },
    {
      "epoch": 4.8,
      "grad_norm": 7.5914082527160645,
      "learning_rate": 9.95895006911623e-06,
      "loss": 0.125,
      "step": 300
    },
    {
      "epoch": 4.82,
      "grad_norm": 7.927587032318115,
      "learning_rate": 9.958539113240368e-06,
      "loss": 0.2437,
      "step": 301
    },
    {
      "epoch": 4.83,
      "grad_norm": 6.027647018432617,
      "learning_rate": 9.958126119096378e-06,
      "loss": 0.1591,
      "step": 302
    },
    {
      "epoch": 4.85,
      "grad_norm": 10.563497543334961,
      "learning_rate": 9.957711086854025e-06,
      "loss": 0.1997,
      "step": 303
    },
    {
      "epoch": 4.86,
      "grad_norm": 4.50126314163208,
      "learning_rate": 9.957294016683912e-06,
      "loss": 0.1611,
      "step": 304
    },
    {
      "epoch": 4.88,
      "grad_norm": 9.566961288452148,
      "learning_rate": 9.956874908757482e-06,
      "loss": 0.2627,
      "step": 305
    },
    {
      "epoch": 4.9,
      "grad_norm": 5.110769748687744,
      "learning_rate": 9.956453763247015e-06,
      "loss": 0.1041,
      "step": 306
    },
    {
      "epoch": 4.91,
      "grad_norm": 4.951406478881836,
      "learning_rate": 9.956030580325628e-06,
      "loss": 0.1386,
      "step": 307
    },
    {
      "epoch": 4.93,
      "grad_norm": 7.143189430236816,
      "learning_rate": 9.955605360167275e-06,
      "loss": 0.1724,
      "step": 308
    },
    {
      "epoch": 4.94,
      "grad_norm": 5.813572883605957,
      "learning_rate": 9.955178102946747e-06,
      "loss": 0.1503,
      "step": 309
    },
    {
      "epoch": 4.96,
      "grad_norm": 6.334144115447998,
      "learning_rate": 9.954748808839675e-06,
      "loss": 0.1584,
      "step": 310
    },
    {
      "epoch": 4.98,
      "grad_norm": 7.497405529022217,
      "learning_rate": 9.954317478022524e-06,
      "loss": 0.1063,
      "step": 311
    },
    {
      "epoch": 4.99,
      "grad_norm": 5.875688076019287,
      "learning_rate": 9.9538841106726e-06,
      "loss": 0.1281,
      "step": 312
    },
    {
      "epoch": 5.01,
      "grad_norm": 10.495598793029785,
      "learning_rate": 9.953448706968043e-06,
      "loss": 0.1645,
      "step": 313
    },
    {
      "epoch": 5.02,
      "grad_norm": 6.1285247802734375,
      "learning_rate": 9.95301126708783e-06,
      "loss": 0.1368,
      "step": 314
    },
    {
      "epoch": 5.04,
      "grad_norm": 4.170087814331055,
      "learning_rate": 9.952571791211776e-06,
      "loss": 0.0878,
      "step": 315
    },
    {
      "epoch": 5.06,
      "grad_norm": 4.862946033477783,
      "learning_rate": 9.952130279520535e-06,
      "loss": 0.123,
      "step": 316
    },
    {
      "epoch": 5.07,
      "grad_norm": 7.938698768615723,
      "learning_rate": 9.951686732195593e-06,
      "loss": 0.1704,
      "step": 317
    },
    {
      "epoch": 5.09,
      "grad_norm": 7.698734283447266,
      "learning_rate": 9.951241149419278e-06,
      "loss": 0.1337,
      "step": 318
    },
    {
      "epoch": 5.1,
      "grad_norm": 3.653722047805786,
      "learning_rate": 9.95079353137475e-06,
      "loss": 0.0837,
      "step": 319
    },
    {
      "epoch": 5.12,
      "grad_norm": 2.8233354091644287,
      "learning_rate": 9.950343878246011e-06,
      "loss": 0.0349,
      "step": 320
    },
    {
      "epoch": 5.14,
      "grad_norm": 5.799461841583252,
      "learning_rate": 9.949892190217892e-06,
      "loss": 0.035,
      "step": 321
    },
    {
      "epoch": 5.15,
      "grad_norm": 4.079384803771973,
      "learning_rate": 9.94943846747607e-06,
      "loss": 0.098,
      "step": 322
    },
    {
      "epoch": 5.17,
      "grad_norm": 7.003876209259033,
      "learning_rate": 9.94898271020705e-06,
      "loss": 0.0778,
      "step": 323
    },
    {
      "epoch": 5.18,
      "grad_norm": 6.081490993499756,
      "learning_rate": 9.948524918598175e-06,
      "loss": 0.0779,
      "step": 324
    },
    {
      "epoch": 5.2,
      "grad_norm": 5.0124831199646,
      "learning_rate": 9.948065092837631e-06,
      "loss": 0.0713,
      "step": 325
    },
    {
      "epoch": 5.22,
      "grad_norm": 4.40268087387085,
      "learning_rate": 9.94760323311443e-06,
      "loss": 0.065,
      "step": 326
    },
    {
      "epoch": 5.23,
      "grad_norm": 8.243903160095215,
      "learning_rate": 9.947139339618428e-06,
      "loss": 0.1144,
      "step": 327
    },
    {
      "epoch": 5.25,
      "grad_norm": 11.620987892150879,
      "learning_rate": 9.946673412540313e-06,
      "loss": 0.1151,
      "step": 328
    },
    {
      "epoch": 5.26,
      "grad_norm": 15.421627044677734,
      "learning_rate": 9.946205452071611e-06,
      "loss": 0.2263,
      "step": 329
    },
    {
      "epoch": 5.28,
      "grad_norm": 7.117086410522461,
      "learning_rate": 9.945735458404681e-06,
      "loss": 0.0784,
      "step": 330
    },
    {
      "epoch": 5.3,
      "grad_norm": 4.062171459197998,
      "learning_rate": 9.945263431732722e-06,
      "loss": 0.0827,
      "step": 331
    },
    {
      "epoch": 5.31,
      "grad_norm": 7.328185558319092,
      "learning_rate": 9.944789372249765e-06,
      "loss": 0.0799,
      "step": 332
    },
    {
      "epoch": 5.33,
      "grad_norm": 5.564020156860352,
      "learning_rate": 9.944313280150678e-06,
      "loss": 0.0908,
      "step": 333
    },
    {
      "epoch": 5.34,
      "grad_norm": 4.301059246063232,
      "learning_rate": 9.943835155631166e-06,
      "loss": 0.0744,
      "step": 334
    },
    {
      "epoch": 5.36,
      "grad_norm": 5.759435653686523,
      "learning_rate": 9.943354998887763e-06,
      "loss": 0.1663,
      "step": 335
    },
    {
      "epoch": 5.38,
      "grad_norm": 6.957027435302734,
      "learning_rate": 9.94287281011785e-06,
      "loss": 0.1603,
      "step": 336
    },
    {
      "epoch": 5.39,
      "grad_norm": 2.8580195903778076,
      "learning_rate": 9.942388589519631e-06,
      "loss": 0.0991,
      "step": 337
    },
    {
      "epoch": 5.41,
      "grad_norm": 9.617596626281738,
      "learning_rate": 9.941902337292156e-06,
      "loss": 0.1027,
      "step": 338
    },
    {
      "epoch": 5.42,
      "grad_norm": 5.349980354309082,
      "learning_rate": 9.9414140536353e-06,
      "loss": 0.1145,
      "step": 339
    },
    {
      "epoch": 5.44,
      "grad_norm": 1.9382424354553223,
      "learning_rate": 9.94092373874978e-06,
      "loss": 0.0504,
      "step": 340
    },
    {
      "epoch": 5.46,
      "grad_norm": 8.152183532714844,
      "learning_rate": 9.940431392837147e-06,
      "loss": 0.0945,
      "step": 341
    },
    {
      "epoch": 5.47,
      "grad_norm": 2.858873128890991,
      "learning_rate": 9.939937016099783e-06,
      "loss": 0.0715,
      "step": 342
    },
    {
      "epoch": 5.49,
      "grad_norm": 7.52351713180542,
      "learning_rate": 9.939440608740912e-06,
      "loss": 0.1505,
      "step": 343
    },
    {
      "epoch": 5.5,
      "grad_norm": 4.6977739334106445,
      "learning_rate": 9.938942170964583e-06,
      "loss": 0.09,
      "step": 344
    },
    {
      "epoch": 5.52,
      "grad_norm": 6.8045244216918945,
      "learning_rate": 9.938441702975689e-06,
      "loss": 0.1429,
      "step": 345
    },
    {
      "epoch": 5.54,
      "grad_norm": 10.389022827148438,
      "learning_rate": 9.937939204979952e-06,
      "loss": 0.1005,
      "step": 346
    },
    {
      "epoch": 5.55,
      "grad_norm": 9.143857955932617,
      "learning_rate": 9.937434677183931e-06,
      "loss": 0.1662,
      "step": 347
    },
    {
      "epoch": 5.57,
      "grad_norm": 4.126138210296631,
      "learning_rate": 9.936928119795017e-06,
      "loss": 0.1117,
      "step": 348
    },
    {
      "epoch": 5.58,
      "grad_norm": 4.428994655609131,
      "learning_rate": 9.936419533021437e-06,
      "loss": 0.0579,
      "step": 349
    },
    {
      "epoch": 5.6,
      "grad_norm": 5.359214782714844,
      "learning_rate": 9.935908917072253e-06,
      "loss": 0.0994,
      "step": 350
    },
    {
      "epoch": 5.62,
      "grad_norm": 5.339041709899902,
      "learning_rate": 9.935396272157357e-06,
      "loss": 0.1415,
      "step": 351
    },
    {
      "epoch": 5.63,
      "grad_norm": 4.028134822845459,
      "learning_rate": 9.934881598487478e-06,
      "loss": 0.0657,
      "step": 352
    },
    {
      "epoch": 5.65,
      "grad_norm": 6.179399490356445,
      "learning_rate": 9.934364896274185e-06,
      "loss": 0.1021,
      "step": 353
    },
    {
      "epoch": 5.66,
      "grad_norm": 20.39905548095703,
      "learning_rate": 9.933846165729867e-06,
      "loss": 0.192,
      "step": 354
    },
    {
      "epoch": 5.68,
      "grad_norm": 4.862467288970947,
      "learning_rate": 9.93332540706776e-06,
      "loss": 0.0446,
      "step": 355
    },
    {
      "epoch": 5.7,
      "grad_norm": 5.249717712402344,
      "learning_rate": 9.932802620501925e-06,
      "loss": 0.0907,
      "step": 356
    },
    {
      "epoch": 5.71,
      "grad_norm": 6.369500637054443,
      "learning_rate": 9.93227780624726e-06,
      "loss": 0.1368,
      "step": 357
    },
    {
      "epoch": 5.73,
      "grad_norm": 2.6739728450775146,
      "learning_rate": 9.9317509645195e-06,
      "loss": 0.0749,
      "step": 358
    },
    {
      "epoch": 5.74,
      "grad_norm": 3.464177370071411,
      "learning_rate": 9.931222095535204e-06,
      "loss": 0.0893,
      "step": 359
    },
    {
      "epoch": 5.76,
      "grad_norm": 4.434343338012695,
      "learning_rate": 9.930691199511775e-06,
      "loss": 0.1289,
      "step": 360
    },
    {
      "epoch": 5.78,
      "grad_norm": 6.552560806274414,
      "learning_rate": 9.930158276667443e-06,
      "loss": 0.156,
      "step": 361
    },
    {
      "epoch": 5.79,
      "grad_norm": 5.811800956726074,
      "learning_rate": 9.92962332722127e-06,
      "loss": 0.127,
      "step": 362
    },
    {
      "epoch": 5.81,
      "grad_norm": 7.275248050689697,
      "learning_rate": 9.929086351393158e-06,
      "loss": 0.2239,
      "step": 363
    },
    {
      "epoch": 5.82,
      "grad_norm": 6.2026872634887695,
      "learning_rate": 9.928547349403832e-06,
      "loss": 0.081,
      "step": 364
    },
    {
      "epoch": 5.84,
      "grad_norm": 7.534078598022461,
      "learning_rate": 9.928006321474859e-06,
      "loss": 0.1979,
      "step": 365
    },
    {
      "epoch": 5.86,
      "grad_norm": 6.997053623199463,
      "learning_rate": 9.927463267828635e-06,
      "loss": 0.1729,
      "step": 366
    },
    {
      "epoch": 5.87,
      "grad_norm": 12.551338195800781,
      "learning_rate": 9.926918188688387e-06,
      "loss": 0.2419,
      "step": 367
    },
    {
      "epoch": 5.89,
      "grad_norm": 4.840639591217041,
      "learning_rate": 9.926371084278178e-06,
      "loss": 0.0663,
      "step": 368
    },
    {
      "epoch": 5.9,
      "grad_norm": 6.477741718292236,
      "learning_rate": 9.925821954822902e-06,
      "loss": 0.1187,
      "step": 369
    },
    {
      "epoch": 5.92,
      "grad_norm": 5.772064208984375,
      "learning_rate": 9.925270800548285e-06,
      "loss": 0.1896,
      "step": 370
    },
    {
      "epoch": 5.94,
      "grad_norm": 6.400912284851074,
      "learning_rate": 9.924717621680885e-06,
      "loss": 0.1108,
      "step": 371
    },
    {
      "epoch": 5.95,
      "grad_norm": 6.5575408935546875,
      "learning_rate": 9.924162418448093e-06,
      "loss": 0.1353,
      "step": 372
    },
    {
      "epoch": 5.97,
      "grad_norm": 5.584460735321045,
      "learning_rate": 9.923605191078134e-06,
      "loss": 0.1211,
      "step": 373
    },
    {
      "epoch": 5.98,
      "grad_norm": 9.004141807556152,
      "learning_rate": 9.923045939800063e-06,
      "loss": 0.1244,
      "step": 374
    },
    {
      "epoch": 6.0,
      "grad_norm": 4.5202412605285645,
      "learning_rate": 9.922484664843763e-06,
      "loss": 0.118,
      "step": 375
    },
    {
      "epoch": 6.02,
      "grad_norm": 3.5862817764282227,
      "learning_rate": 9.921921366439958e-06,
      "loss": 0.0431,
      "step": 376
    },
    {
      "epoch": 6.03,
      "grad_norm": 10.184333801269531,
      "learning_rate": 9.921356044820194e-06,
      "loss": 0.092,
      "step": 377
    },
    {
      "epoch": 6.05,
      "grad_norm": 13.01767635345459,
      "learning_rate": 9.920788700216857e-06,
      "loss": 0.1197,
      "step": 378
    },
    {
      "epoch": 6.06,
      "grad_norm": 4.696653842926025,
      "learning_rate": 9.920219332863161e-06,
      "loss": 0.0767,
      "step": 379
    },
    {
      "epoch": 6.08,
      "grad_norm": 6.891735076904297,
      "learning_rate": 9.91964794299315e-06,
      "loss": 0.1507,
      "step": 380
    },
    {
      "epoch": 6.1,
      "grad_norm": 3.2890286445617676,
      "learning_rate": 9.9190745308417e-06,
      "loss": 0.0675,
      "step": 381
    },
    {
      "epoch": 6.11,
      "grad_norm": 3.4570472240448,
      "learning_rate": 9.91849909664452e-06,
      "loss": 0.0633,
      "step": 382
    },
    {
      "epoch": 6.13,
      "grad_norm": 2.7446720600128174,
      "learning_rate": 9.91792164063815e-06,
      "loss": 0.0543,
      "step": 383
    },
    {
      "epoch": 6.14,
      "grad_norm": 3.397634506225586,
      "learning_rate": 9.917342163059959e-06,
      "loss": 0.0572,
      "step": 384
    },
    {
      "epoch": 6.16,
      "grad_norm": 1.8799220323562622,
      "learning_rate": 9.916760664148148e-06,
      "loss": 0.0249,
      "step": 385
    },
    {
      "epoch": 6.18,
      "grad_norm": 4.03758430480957,
      "learning_rate": 9.916177144141752e-06,
      "loss": 0.0854,
      "step": 386
    },
    {
      "epoch": 6.19,
      "grad_norm": 4.691997051239014,
      "learning_rate": 9.915591603280632e-06,
      "loss": 0.0848,
      "step": 387
    },
    {
      "epoch": 6.21,
      "grad_norm": 2.1662955284118652,
      "learning_rate": 9.915004041805482e-06,
      "loss": 0.0452,
      "step": 388
    },
    {
      "epoch": 6.22,
      "grad_norm": 5.918210506439209,
      "learning_rate": 9.914414459957826e-06,
      "loss": 0.0739,
      "step": 389
    },
    {
      "epoch": 6.24,
      "grad_norm": 5.025871753692627,
      "learning_rate": 9.91382285798002e-06,
      "loss": 0.112,
      "step": 390
    },
    {
      "epoch": 6.26,
      "grad_norm": 7.49147891998291,
      "learning_rate": 9.913229236115248e-06,
      "loss": 0.1174,
      "step": 391
    },
    {
      "epoch": 6.27,
      "grad_norm": 6.390525817871094,
      "learning_rate": 9.912633594607526e-06,
      "loss": 0.0834,
      "step": 392
    },
    {
      "epoch": 6.29,
      "grad_norm": 4.092784881591797,
      "learning_rate": 9.912035933701699e-06,
      "loss": 0.0674,
      "step": 393
    },
    {
      "epoch": 6.3,
      "grad_norm": 6.738653182983398,
      "learning_rate": 9.911436253643445e-06,
      "loss": 0.1069,
      "step": 394
    },
    {
      "epoch": 6.32,
      "grad_norm": 5.807829856872559,
      "learning_rate": 9.910834554679266e-06,
      "loss": 0.0571,
      "step": 395
    },
    {
      "epoch": 6.34,
      "grad_norm": 4.854951858520508,
      "learning_rate": 9.910230837056501e-06,
      "loss": 0.0702,
      "step": 396
    },
    {
      "epoch": 6.35,
      "grad_norm": 5.349706649780273,
      "learning_rate": 9.909625101023315e-06,
      "loss": 0.0443,
      "step": 397
    },
    {
      "epoch": 6.37,
      "grad_norm": 3.1017000675201416,
      "learning_rate": 9.909017346828702e-06,
      "loss": 0.0453,
      "step": 398
    },
    {
      "epoch": 6.38,
      "grad_norm": 5.977466583251953,
      "learning_rate": 9.908407574722489e-06,
      "loss": 0.0566,
      "step": 399
    },
    {
      "epoch": 6.4,
      "grad_norm": 2.3208136558532715,
      "learning_rate": 9.907795784955327e-06,
      "loss": 0.0389,
      "step": 400
    },
    {
      "epoch": 6.42,
      "grad_norm": 2.1285061836242676,
      "learning_rate": 9.907181977778702e-06,
      "loss": 0.0503,
      "step": 401
    },
    {
      "epoch": 6.43,
      "grad_norm": 7.27864933013916,
      "learning_rate": 9.906566153444927e-06,
      "loss": 0.0997,
      "step": 402
    },
    {
      "epoch": 6.45,
      "grad_norm": 5.051735877990723,
      "learning_rate": 9.905948312207143e-06,
      "loss": 0.0805,
      "step": 403
    },
    {
      "epoch": 6.46,
      "grad_norm": 2.5471811294555664,
      "learning_rate": 9.905328454319323e-06,
      "loss": 0.0485,
      "step": 404
    },
    {
      "epoch": 6.48,
      "grad_norm": 2.813758373260498,
      "learning_rate": 9.904706580036265e-06,
      "loss": 0.0871,
      "step": 405
    },
    {
      "epoch": 6.5,
      "grad_norm": 3.2193212509155273,
      "learning_rate": 9.904082689613598e-06,
      "loss": 0.0657,
      "step": 406
    },
    {
      "epoch": 6.51,
      "grad_norm": 4.763813018798828,
      "learning_rate": 9.903456783307781e-06,
      "loss": 0.1095,
      "step": 407
    },
    {
      "epoch": 6.53,
      "grad_norm": 5.902143955230713,
      "learning_rate": 9.902828861376101e-06,
      "loss": 0.095,
      "step": 408
    },
    {
      "epoch": 6.54,
      "grad_norm": 3.9484567642211914,
      "learning_rate": 9.902198924076671e-06,
      "loss": 0.1381,
      "step": 409
    },
    {
      "epoch": 6.56,
      "grad_norm": 7.564910411834717,
      "learning_rate": 9.901566971668437e-06,
      "loss": 0.1018,
      "step": 410
    },
    {
      "epoch": 6.58,
      "grad_norm": 1.1640549898147583,
      "learning_rate": 9.900933004411169e-06,
      "loss": 0.0224,
      "step": 411
    },
    {
      "epoch": 6.59,
      "grad_norm": 5.306463718414307,
      "learning_rate": 9.900297022565467e-06,
      "loss": 0.098,
      "step": 412
    },
    {
      "epoch": 6.61,
      "grad_norm": 3.8980002403259277,
      "learning_rate": 9.899659026392758e-06,
      "loss": 0.1033,
      "step": 413
    },
    {
      "epoch": 6.62,
      "grad_norm": 3.5043957233428955,
      "learning_rate": 9.899019016155302e-06,
      "loss": 0.0883,
      "step": 414
    },
    {
      "epoch": 6.64,
      "grad_norm": 11.31230640411377,
      "learning_rate": 9.898376992116179e-06,
      "loss": 0.1119,
      "step": 415
    },
    {
      "epoch": 6.66,
      "grad_norm": 2.3574259281158447,
      "learning_rate": 9.897732954539303e-06,
      "loss": 0.0632,
      "step": 416
    },
    {
      "epoch": 6.67,
      "grad_norm": 3.5579822063446045,
      "learning_rate": 9.897086903689413e-06,
      "loss": 0.0483,
      "step": 417
    },
    {
      "epoch": 6.69,
      "grad_norm": 4.089546203613281,
      "learning_rate": 9.896438839832074e-06,
      "loss": 0.1104,
      "step": 418
    },
    {
      "epoch": 6.7,
      "grad_norm": 5.702855587005615,
      "learning_rate": 9.895788763233684e-06,
      "loss": 0.0867,
      "step": 419
    },
    {
      "epoch": 6.72,
      "grad_norm": 1.693025827407837,
      "learning_rate": 9.895136674161466e-06,
      "loss": 0.0448,
      "step": 420
    },
    {
      "epoch": 6.74,
      "grad_norm": 9.30104923248291,
      "learning_rate": 9.894482572883464e-06,
      "loss": 0.1048,
      "step": 421
    },
    {
      "epoch": 6.75,
      "grad_norm": 6.010862827301025,
      "learning_rate": 9.89382645966856e-06,
      "loss": 0.1369,
      "step": 422
    },
    {
      "epoch": 6.77,
      "grad_norm": 6.719523906707764,
      "learning_rate": 9.89316833478645e-06,
      "loss": 0.1055,
      "step": 423
    },
    {
      "epoch": 6.78,
      "grad_norm": 3.873335599899292,
      "learning_rate": 9.892508198507671e-06,
      "loss": 0.0509,
      "step": 424
    },
    {
      "epoch": 6.8,
      "grad_norm": 5.933447360992432,
      "learning_rate": 9.891846051103578e-06,
      "loss": 0.0877,
      "step": 425
    },
    {
      "epoch": 6.82,
      "grad_norm": 5.305391311645508,
      "learning_rate": 9.891181892846353e-06,
      "loss": 0.1166,
      "step": 426
    },
    {
      "epoch": 6.83,
      "grad_norm": 6.099409103393555,
      "learning_rate": 9.89051572400901e-06,
      "loss": 0.1307,
      "step": 427
    },
    {
      "epoch": 6.85,
      "grad_norm": 5.055529594421387,
      "learning_rate": 9.889847544865383e-06,
      "loss": 0.0628,
      "step": 428
    },
    {
      "epoch": 6.86,
      "grad_norm": 3.62105655670166,
      "learning_rate": 9.889177355690136e-06,
      "loss": 0.0717,
      "step": 429
    },
    {
      "epoch": 6.88,
      "grad_norm": 5.670097351074219,
      "learning_rate": 9.888505156758758e-06,
      "loss": 0.0864,
      "step": 430
    },
    {
      "epoch": 6.9,
      "grad_norm": 5.809406757354736,
      "learning_rate": 9.887830948347565e-06,
      "loss": 0.0866,
      "step": 431
    },
    {
      "epoch": 6.91,
      "grad_norm": 7.252783298492432,
      "learning_rate": 9.887154730733699e-06,
      "loss": 0.0776,
      "step": 432
    },
    {
      "epoch": 6.93,
      "grad_norm": 4.633249282836914,
      "learning_rate": 9.886476504195129e-06,
      "loss": 0.1077,
      "step": 433
    },
    {
      "epoch": 6.94,
      "grad_norm": 5.701761245727539,
      "learning_rate": 9.885796269010644e-06,
      "loss": 0.0954,
      "step": 434
    },
    {
      "epoch": 6.96,
      "grad_norm": 4.205622673034668,
      "learning_rate": 9.885114025459865e-06,
      "loss": 0.0685,
      "step": 435
    },
    {
      "epoch": 6.98,
      "grad_norm": 3.566925287246704,
      "learning_rate": 9.884429773823238e-06,
      "loss": 0.0808,
      "step": 436
    },
    {
      "epoch": 6.99,
      "grad_norm": 5.092951774597168,
      "learning_rate": 9.883743514382034e-06,
      "loss": 0.1325,
      "step": 437
    },
    {
      "epoch": 7.01,
      "grad_norm": 3.6133484840393066,
      "learning_rate": 9.883055247418343e-06,
      "loss": 0.0465,
      "step": 438
    },
    {
      "epoch": 7.02,
      "grad_norm": 3.1454813480377197,
      "learning_rate": 9.882364973215092e-06,
      "loss": 0.0544,
      "step": 439
    },
    {
      "epoch": 7.04,
      "grad_norm": 4.127089977264404,
      "learning_rate": 9.881672692056022e-06,
      "loss": 0.0441,
      "step": 440
    },
    {
      "epoch": 7.06,
      "grad_norm": 3.037733793258667,
      "learning_rate": 9.880978404225705e-06,
      "loss": 0.0174,
      "step": 441
    },
    {
      "epoch": 7.07,
      "grad_norm": 1.854258418083191,
      "learning_rate": 9.880282110009537e-06,
      "loss": 0.022,
      "step": 442
    },
    {
      "epoch": 7.09,
      "grad_norm": 1.4832100868225098,
      "learning_rate": 9.879583809693737e-06,
      "loss": 0.0554,
      "step": 443
    },
    {
      "epoch": 7.1,
      "grad_norm": 6.706745624542236,
      "learning_rate": 9.878883503565353e-06,
      "loss": 0.0798,
      "step": 444
    },
    {
      "epoch": 7.12,
      "grad_norm": 2.123878002166748,
      "learning_rate": 9.878181191912251e-06,
      "loss": 0.0475,
      "step": 445
    },
    {
      "epoch": 7.14,
      "grad_norm": 1.4434720277786255,
      "learning_rate": 9.877476875023125e-06,
      "loss": 0.0131,
      "step": 446
    },
    {
      "epoch": 7.15,
      "grad_norm": 4.905411243438721,
      "learning_rate": 9.876770553187495e-06,
      "loss": 0.0197,
      "step": 447
    },
    {
      "epoch": 7.17,
      "grad_norm": 3.487309694290161,
      "learning_rate": 9.876062226695703e-06,
      "loss": 0.0363,
      "step": 448
    },
    {
      "epoch": 7.18,
      "grad_norm": 3.914083957672119,
      "learning_rate": 9.875351895838914e-06,
      "loss": 0.0675,
      "step": 449
    },
    {
      "epoch": 7.2,
      "grad_norm": 3.055457592010498,
      "learning_rate": 9.874639560909118e-06,
      "loss": 0.0611,
      "step": 450
    },
    {
      "epoch": 7.22,
      "grad_norm": 8.602633476257324,
      "learning_rate": 9.873925222199131e-06,
      "loss": 0.076,
      "step": 451
    },
    {
      "epoch": 7.23,
      "grad_norm": 6.407105445861816,
      "learning_rate": 9.87320888000259e-06,
      "loss": 0.0437,
      "step": 452
    },
    {
      "epoch": 7.25,
      "grad_norm": 4.919384479522705,
      "learning_rate": 9.872490534613954e-06,
      "loss": 0.0314,
      "step": 453
    },
    {
      "epoch": 7.26,
      "grad_norm": 1.7304812669754028,
      "learning_rate": 9.87177018632851e-06,
      "loss": 0.0187,
      "step": 454
    },
    {
      "epoch": 7.28,
      "grad_norm": 3.2616703510284424,
      "learning_rate": 9.871047835442365e-06,
      "loss": 0.0847,
      "step": 455
    },
    {
      "epoch": 7.3,
      "grad_norm": 5.798643112182617,
      "learning_rate": 9.870323482252451e-06,
      "loss": 0.0532,
      "step": 456
    },
    {
      "epoch": 7.31,
      "grad_norm": 6.517640590667725,
      "learning_rate": 9.86959712705652e-06,
      "loss": 0.1296,
      "step": 457
    },
    {
      "epoch": 7.33,
      "grad_norm": 4.419760227203369,
      "learning_rate": 9.868868770153153e-06,
      "loss": 0.0778,
      "step": 458
    },
    {
      "epoch": 7.34,
      "grad_norm": 3.2686972618103027,
      "learning_rate": 9.868138411841746e-06,
      "loss": 0.0539,
      "step": 459
    },
    {
      "epoch": 7.36,
      "grad_norm": 2.656069755554199,
      "learning_rate": 9.867406052422525e-06,
      "loss": 0.0401,
      "step": 460
    },
    {
      "epoch": 7.38,
      "grad_norm": 4.219679832458496,
      "learning_rate": 9.866671692196533e-06,
      "loss": 0.0684,
      "step": 461
    },
    {
      "epoch": 7.39,
      "grad_norm": 4.0485453605651855,
      "learning_rate": 9.86593533146564e-06,
      "loss": 0.0559,
      "step": 462
    },
    {
      "epoch": 7.41,
      "grad_norm": 4.882740020751953,
      "learning_rate": 9.865196970532532e-06,
      "loss": 0.0471,
      "step": 463
    },
    {
      "epoch": 7.42,
      "grad_norm": 5.227786064147949,
      "learning_rate": 9.864456609700726e-06,
      "loss": 0.0359,
      "step": 464
    },
    {
      "epoch": 7.44,
      "grad_norm": 3.471174478530884,
      "learning_rate": 9.863714249274553e-06,
      "loss": 0.0718,
      "step": 465
    },
    {
      "epoch": 7.46,
      "grad_norm": 1.774751901626587,
      "learning_rate": 9.862969889559172e-06,
      "loss": 0.0204,
      "step": 466
    },
    {
      "epoch": 7.47,
      "grad_norm": 3.9224209785461426,
      "learning_rate": 9.862223530860559e-06,
      "loss": 0.0807,
      "step": 467
    },
    {
      "epoch": 7.49,
      "grad_norm": 2.653256416320801,
      "learning_rate": 9.861475173485516e-06,
      "loss": 0.0278,
      "step": 468
    },
    {
      "epoch": 7.5,
      "grad_norm": 10.624712944030762,
      "learning_rate": 9.860724817741661e-06,
      "loss": 0.1225,
      "step": 469
    },
    {
      "epoch": 7.52,
      "grad_norm": 1.8318651914596558,
      "learning_rate": 9.85997246393744e-06,
      "loss": 0.0295,
      "step": 470
    },
    {
      "epoch": 7.54,
      "grad_norm": 4.13655948638916,
      "learning_rate": 9.859218112382117e-06,
      "loss": 0.0613,
      "step": 471
    },
    {
      "epoch": 7.55,
      "grad_norm": 4.661935329437256,
      "learning_rate": 9.858461763385776e-06,
      "loss": 0.0854,
      "step": 472
    },
    {
      "epoch": 7.57,
      "grad_norm": 2.227023124694824,
      "learning_rate": 9.857703417259325e-06,
      "loss": 0.0646,
      "step": 473
    },
    {
      "epoch": 7.58,
      "grad_norm": 4.176807403564453,
      "learning_rate": 9.85694307431449e-06,
      "loss": 0.056,
      "step": 474
    },
    {
      "epoch": 7.6,
      "grad_norm": 3.6804115772247314,
      "learning_rate": 9.85618073486382e-06,
      "loss": 0.0653,
      "step": 475
    },
    {
      "epoch": 7.62,
      "grad_norm": 2.705221176147461,
      "learning_rate": 9.855416399220683e-06,
      "loss": 0.0248,
      "step": 476
    },
    {
      "epoch": 7.63,
      "grad_norm": 6.7180681228637695,
      "learning_rate": 9.854650067699272e-06,
      "loss": 0.0629,
      "step": 477
    },
    {
      "epoch": 7.65,
      "grad_norm": 4.380806922912598,
      "learning_rate": 9.853881740614591e-06,
      "loss": 0.0841,
      "step": 478
    },
    {
      "epoch": 7.66,
      "grad_norm": 6.589293956756592,
      "learning_rate": 9.853111418282476e-06,
      "loss": 0.1355,
      "step": 479
    },
    {
      "epoch": 7.68,
      "grad_norm": 5.2104315757751465,
      "learning_rate": 9.852339101019574e-06,
      "loss": 0.0832,
      "step": 480
    },
    {
      "epoch": 7.7,
      "grad_norm": 3.3434994220733643,
      "learning_rate": 9.851564789143357e-06,
      "loss": 0.0497,
      "step": 481
    },
    {
      "epoch": 7.71,
      "grad_norm": 4.954225063323975,
      "learning_rate": 9.850788482972114e-06,
      "loss": 0.1217,
      "step": 482
    },
    {
      "epoch": 7.73,
      "grad_norm": 1.3405542373657227,
      "learning_rate": 9.850010182824956e-06,
      "loss": 0.0169,
      "step": 483
    },
    {
      "epoch": 7.74,
      "grad_norm": 3.8712613582611084,
      "learning_rate": 9.849229889021814e-06,
      "loss": 0.0542,
      "step": 484
    },
    {
      "epoch": 7.76,
      "grad_norm": 4.181652069091797,
      "learning_rate": 9.848447601883436e-06,
      "loss": 0.0817,
      "step": 485
    },
    {
      "epoch": 7.78,
      "grad_norm": 3.21855092048645,
      "learning_rate": 9.847663321731388e-06,
      "loss": 0.031,
      "step": 486
    },
    {
      "epoch": 7.79,
      "grad_norm": 4.053468227386475,
      "learning_rate": 9.846877048888064e-06,
      "loss": 0.1079,
      "step": 487
    },
    {
      "epoch": 7.81,
      "grad_norm": 7.7038750648498535,
      "learning_rate": 9.846088783676666e-06,
      "loss": 0.0996,
      "step": 488
    },
    {
      "epoch": 7.82,
      "grad_norm": 7.404571533203125,
      "learning_rate": 9.845298526421224e-06,
      "loss": 0.0999,
      "step": 489
    },
    {
      "epoch": 7.84,
      "grad_norm": 3.7526731491088867,
      "learning_rate": 9.844506277446577e-06,
      "loss": 0.0667,
      "step": 490
    },
    {
      "epoch": 7.86,
      "grad_norm": 8.182846069335938,
      "learning_rate": 9.843712037078394e-06,
      "loss": 0.0695,
      "step": 491
    },
    {
      "epoch": 7.87,
      "grad_norm": 2.314483404159546,
      "learning_rate": 9.842915805643156e-06,
      "loss": 0.0715,
      "step": 492
    },
    {
      "epoch": 7.89,
      "grad_norm": 3.2949445247650146,
      "learning_rate": 9.842117583468165e-06,
      "loss": 0.0463,
      "step": 493
    },
    {
      "epoch": 7.9,
      "grad_norm": 4.701555252075195,
      "learning_rate": 9.841317370881535e-06,
      "loss": 0.0932,
      "step": 494
    },
    {
      "epoch": 7.92,
      "grad_norm": 4.097732067108154,
      "learning_rate": 9.840515168212208e-06,
      "loss": 0.06,
      "step": 495
    },
    {
      "epoch": 7.94,
      "grad_norm": 2.686243772506714,
      "learning_rate": 9.839710975789937e-06,
      "loss": 0.0568,
      "step": 496
    },
    {
      "epoch": 7.95,
      "grad_norm": 3.9466350078582764,
      "learning_rate": 9.838904793945297e-06,
      "loss": 0.1062,
      "step": 497
    },
    {
      "epoch": 7.97,
      "grad_norm": 10.619266510009766,
      "learning_rate": 9.838096623009676e-06,
      "loss": 0.0535,
      "step": 498
    },
    {
      "epoch": 7.98,
      "grad_norm": 2.6348607540130615,
      "learning_rate": 9.837286463315284e-06,
      "loss": 0.0445,
      "step": 499
    },
    {
      "epoch": 8.0,
      "grad_norm": 0.9397540092468262,
      "learning_rate": 9.836474315195148e-06,
      "loss": 0.011,
      "step": 500
    },
    {
      "epoch": 8.02,
      "grad_norm": 0.8513403534889221,
      "learning_rate": 9.83566017898311e-06,
      "loss": 0.0058,
      "step": 501
    },
    {
      "epoch": 8.03,
      "grad_norm": 2.542562961578369,
      "learning_rate": 9.834844055013833e-06,
      "loss": 0.0904,
      "step": 502
    },
    {
      "epoch": 8.05,
      "grad_norm": 3.203601598739624,
      "learning_rate": 9.834025943622792e-06,
      "loss": 0.0225,
      "step": 503
    },
    {
      "epoch": 8.06,
      "grad_norm": 2.6721386909484863,
      "learning_rate": 9.833205845146283e-06,
      "loss": 0.0396,
      "step": 504
    },
    {
      "epoch": 8.08,
      "grad_norm": 1.7109947204589844,
      "learning_rate": 9.832383759921415e-06,
      "loss": 0.0245,
      "step": 505
    },
    {
      "epoch": 8.1,
      "grad_norm": 0.9297162294387817,
      "learning_rate": 9.83155968828612e-06,
      "loss": 0.019,
      "step": 506
    },
    {
      "epoch": 8.11,
      "grad_norm": 2.548727512359619,
      "learning_rate": 9.830733630579142e-06,
      "loss": 0.0267,
      "step": 507
    },
    {
      "epoch": 8.13,
      "grad_norm": 10.393939971923828,
      "learning_rate": 9.829905587140041e-06,
      "loss": 0.0626,
      "step": 508
    },
    {
      "epoch": 8.14,
      "grad_norm": 5.077254295349121,
      "learning_rate": 9.829075558309194e-06,
      "loss": 0.0391,
      "step": 509
    },
    {
      "epoch": 8.16,
      "grad_norm": 1.657354474067688,
      "learning_rate": 9.828243544427795e-06,
      "loss": 0.0292,
      "step": 510
    },
    {
      "epoch": 8.18,
      "grad_norm": 2.128138303756714,
      "learning_rate": 9.827409545837853e-06,
      "loss": 0.0383,
      "step": 511
    },
    {
      "epoch": 8.19,
      "grad_norm": 0.7234435677528381,
      "learning_rate": 9.826573562882195e-06,
      "loss": 0.0073,
      "step": 512
    },
    {
      "epoch": 8.21,
      "grad_norm": 11.060304641723633,
      "learning_rate": 9.825735595904462e-06,
      "loss": 0.093,
      "step": 513
    },
    {
      "epoch": 8.22,
      "grad_norm": 6.52720832824707,
      "learning_rate": 9.824895645249107e-06,
      "loss": 0.0653,
      "step": 514
    },
    {
      "epoch": 8.24,
      "grad_norm": 4.30407190322876,
      "learning_rate": 9.824053711261405e-06,
      "loss": 0.0833,
      "step": 515
    },
    {
      "epoch": 8.26,
      "grad_norm": 1.5221145153045654,
      "learning_rate": 9.823209794287446e-06,
      "loss": 0.0271,
      "step": 516
    },
    {
      "epoch": 8.27,
      "grad_norm": 2.3546736240386963,
      "learning_rate": 9.822363894674125e-06,
      "loss": 0.0335,
      "step": 517
    },
    {
      "epoch": 8.29,
      "grad_norm": 1.3180012702941895,
      "learning_rate": 9.821516012769165e-06,
      "loss": 0.0159,
      "step": 518
    },
    {
      "epoch": 8.3,
      "grad_norm": 2.5859830379486084,
      "learning_rate": 9.820666148921097e-06,
      "loss": 0.0386,
      "step": 519
    },
    {
      "epoch": 8.32,
      "grad_norm": 1.7289156913757324,
      "learning_rate": 9.819814303479268e-06,
      "loss": 0.0243,
      "step": 520
    },
    {
      "epoch": 8.34,
      "grad_norm": 2.3139820098876953,
      "learning_rate": 9.818960476793837e-06,
      "loss": 0.0158,
      "step": 521
    },
    {
      "epoch": 8.35,
      "grad_norm": 3.0290145874023438,
      "learning_rate": 9.818104669215784e-06,
      "loss": 0.0429,
      "step": 522
    },
    {
      "epoch": 8.37,
      "grad_norm": 3.7507410049438477,
      "learning_rate": 9.817246881096898e-06,
      "loss": 0.0781,
      "step": 523
    },
    {
      "epoch": 8.38,
      "grad_norm": 2.9637744426727295,
      "learning_rate": 9.81638711278978e-06,
      "loss": 0.035,
      "step": 524
    },
    {
      "epoch": 8.4,
      "grad_norm": 1.578292727470398,
      "learning_rate": 9.815525364647853e-06,
      "loss": 0.0244,
      "step": 525
    },
    {
      "epoch": 8.42,
      "grad_norm": 2.3218274116516113,
      "learning_rate": 9.814661637025346e-06,
      "loss": 0.0398,
      "step": 526
    },
    {
      "epoch": 8.43,
      "grad_norm": 3.840951442718506,
      "learning_rate": 9.813795930277306e-06,
      "loss": 0.0688,
      "step": 527
    },
    {
      "epoch": 8.45,
      "grad_norm": 0.8071022629737854,
      "learning_rate": 9.812928244759591e-06,
      "loss": 0.0094,
      "step": 528
    },
    {
      "epoch": 8.46,
      "grad_norm": 2.902984142303467,
      "learning_rate": 9.812058580828877e-06,
      "loss": 0.0428,
      "step": 529
    },
    {
      "epoch": 8.48,
      "grad_norm": 2.291184902191162,
      "learning_rate": 9.811186938842645e-06,
      "loss": 0.0245,
      "step": 530
    },
    {
      "epoch": 8.5,
      "grad_norm": 6.7707977294921875,
      "learning_rate": 9.810313319159198e-06,
      "loss": 0.0866,
      "step": 531
    },
    {
      "epoch": 8.51,
      "grad_norm": 2.457592725753784,
      "learning_rate": 9.809437722137647e-06,
      "loss": 0.0339,
      "step": 532
    },
    {
      "epoch": 8.53,
      "grad_norm": 4.405618190765381,
      "learning_rate": 9.808560148137916e-06,
      "loss": 0.0709,
      "step": 533
    },
    {
      "epoch": 8.54,
      "grad_norm": 11.51903247833252,
      "learning_rate": 9.807680597520746e-06,
      "loss": 0.136,
      "step": 534
    },
    {
      "epoch": 8.56,
      "grad_norm": 1.3739029169082642,
      "learning_rate": 9.80679907064768e-06,
      "loss": 0.0176,
      "step": 535
    },
    {
      "epoch": 8.58,
      "grad_norm": 1.442203164100647,
      "learning_rate": 9.805915567881088e-06,
      "loss": 0.0277,
      "step": 536
    },
    {
      "epoch": 8.59,
      "grad_norm": 10.742025375366211,
      "learning_rate": 9.805030089584141e-06,
      "loss": 0.0663,
      "step": 537
    },
    {
      "epoch": 8.61,
      "grad_norm": 2.1123557090759277,
      "learning_rate": 9.804142636120827e-06,
      "loss": 0.0538,
      "step": 538
    },
    {
      "epoch": 8.62,
      "grad_norm": 5.806980609893799,
      "learning_rate": 9.80325320785594e-06,
      "loss": 0.0798,
      "step": 539
    },
    {
      "epoch": 8.64,
      "grad_norm": 5.868502616882324,
      "learning_rate": 9.802361805155097e-06,
      "loss": 0.0918,
      "step": 540
    },
    {
      "epoch": 8.66,
      "grad_norm": 2.461193561553955,
      "learning_rate": 9.801468428384716e-06,
      "loss": 0.0577,
      "step": 541
    },
    {
      "epoch": 8.67,
      "grad_norm": 3.521345615386963,
      "learning_rate": 9.800573077912032e-06,
      "loss": 0.0422,
      "step": 542
    },
    {
      "epoch": 8.69,
      "grad_norm": 2.9910335540771484,
      "learning_rate": 9.799675754105088e-06,
      "loss": 0.0439,
      "step": 543
    },
    {
      "epoch": 8.7,
      "grad_norm": 2.4239046573638916,
      "learning_rate": 9.798776457332742e-06,
      "loss": 0.037,
      "step": 544
    },
    {
      "epoch": 8.72,
      "grad_norm": 1.8934255838394165,
      "learning_rate": 9.797875187964661e-06,
      "loss": 0.0508,
      "step": 545
    },
    {
      "epoch": 8.74,
      "grad_norm": 2.5131852626800537,
      "learning_rate": 9.79697194637132e-06,
      "loss": 0.0573,
      "step": 546
    },
    {
      "epoch": 8.75,
      "grad_norm": 2.087178945541382,
      "learning_rate": 9.79606673292401e-06,
      "loss": 0.0285,
      "step": 547
    },
    {
      "epoch": 8.77,
      "grad_norm": 7.574217319488525,
      "learning_rate": 9.79515954799483e-06,
      "loss": 0.0952,
      "step": 548
    },
    {
      "epoch": 8.78,
      "grad_norm": 2.567269802093506,
      "learning_rate": 9.794250391956687e-06,
      "loss": 0.0516,
      "step": 549
    },
    {
      "epoch": 8.8,
      "grad_norm": 3.600358009338379,
      "learning_rate": 9.793339265183303e-06,
      "loss": 0.0466,
      "step": 550
    },
    {
      "epoch": 8.82,
      "grad_norm": 3.390141010284424,
      "learning_rate": 9.79242616804921e-06,
      "loss": 0.0814,
      "step": 551
    },
    {
      "epoch": 8.83,
      "grad_norm": 5.064462184906006,
      "learning_rate": 9.791511100929743e-06,
      "loss": 0.0745,
      "step": 552
    },
    {
      "epoch": 8.85,
      "grad_norm": 2.2183806896209717,
      "learning_rate": 9.790594064201053e-06,
      "loss": 0.0531,
      "step": 553
    },
    {
      "epoch": 8.86,
      "grad_norm": 10.897894859313965,
      "learning_rate": 9.7896750582401e-06,
      "loss": 0.0753,
      "step": 554
    },
    {
      "epoch": 8.88,
      "grad_norm": 2.8085176944732666,
      "learning_rate": 9.788754083424654e-06,
      "loss": 0.0386,
      "step": 555
    },
    {
      "epoch": 8.9,
      "grad_norm": 3.347554922103882,
      "learning_rate": 9.78783114013329e-06,
      "loss": 0.0256,
      "step": 556
    },
    {
      "epoch": 8.91,
      "grad_norm": 5.00623893737793,
      "learning_rate": 9.786906228745397e-06,
      "loss": 0.0797,
      "step": 557
    },
    {
      "epoch": 8.93,
      "grad_norm": 3.6545701026916504,
      "learning_rate": 9.78597934964117e-06,
      "loss": 0.0719,
      "step": 558
    },
    {
      "epoch": 8.94,
      "grad_norm": 1.32533860206604,
      "learning_rate": 9.785050503201614e-06,
      "loss": 0.0165,
      "step": 559
    },
    {
      "epoch": 8.96,
      "grad_norm": 2.7228806018829346,
      "learning_rate": 9.784119689808545e-06,
      "loss": 0.0343,
      "step": 560
    },
    {
      "epoch": 8.98,
      "grad_norm": 6.117408752441406,
      "learning_rate": 9.783186909844582e-06,
      "loss": 0.0531,
      "step": 561
    },
    {
      "epoch": 8.99,
      "grad_norm": 8.410966873168945,
      "learning_rate": 9.782252163693159e-06,
      "loss": 0.0771,
      "step": 562
    },
    {
      "epoch": 9.01,
      "grad_norm": 2.548884153366089,
      "learning_rate": 9.78131545173851e-06,
      "loss": 0.0391,
      "step": 563
    },
    {
      "epoch": 9.02,
      "grad_norm": 1.2130341529846191,
      "learning_rate": 9.780376774365687e-06,
      "loss": 0.0172,
      "step": 564
    },
    {
      "epoch": 9.04,
      "grad_norm": 2.0062990188598633,
      "learning_rate": 9.779436131960544e-06,
      "loss": 0.0217,
      "step": 565
    },
    {
      "epoch": 9.06,
      "grad_norm": 2.6592793464660645,
      "learning_rate": 9.77849352490974e-06,
      "loss": 0.0375,
      "step": 566
    },
    {
      "epoch": 9.07,
      "grad_norm": 2.782193183898926,
      "learning_rate": 9.777548953600748e-06,
      "loss": 0.0133,
      "step": 567
    },
    {
      "epoch": 9.09,
      "grad_norm": 2.0225257873535156,
      "learning_rate": 9.776602418421846e-06,
      "loss": 0.0166,
      "step": 568
    },
    {
      "epoch": 9.1,
      "grad_norm": 1.6685348749160767,
      "learning_rate": 9.775653919762117e-06,
      "loss": 0.0366,
      "step": 569
    },
    {
      "epoch": 9.12,
      "grad_norm": 2.836167097091675,
      "learning_rate": 9.774703458011453e-06,
      "loss": 0.0294,
      "step": 570
    },
    {
      "epoch": 9.14,
      "grad_norm": 2.2007970809936523,
      "learning_rate": 9.773751033560554e-06,
      "loss": 0.0424,
      "step": 571
    },
    {
      "epoch": 9.15,
      "grad_norm": 1.5528862476348877,
      "learning_rate": 9.772796646800926e-06,
      "loss": 0.01,
      "step": 572
    },
    {
      "epoch": 9.17,
      "grad_norm": 1.4789577722549438,
      "learning_rate": 9.771840298124882e-06,
      "loss": 0.0204,
      "step": 573
    },
    {
      "epoch": 9.18,
      "grad_norm": 2.043762445449829,
      "learning_rate": 9.770881987925538e-06,
      "loss": 0.0308,
      "step": 574
    },
    {
      "epoch": 9.2,
      "grad_norm": 3.5389585494995117,
      "learning_rate": 9.76992171659682e-06,
      "loss": 0.0404,
      "step": 575
    },
    {
      "epoch": 9.22,
      "grad_norm": 3.371370315551758,
      "learning_rate": 9.768959484533461e-06,
      "loss": 0.0719,
      "step": 576
    },
    {
      "epoch": 9.23,
      "grad_norm": 2.916163682937622,
      "learning_rate": 9.767995292130998e-06,
      "loss": 0.0183,
      "step": 577
    },
    {
      "epoch": 9.25,
      "grad_norm": 4.281394004821777,
      "learning_rate": 9.767029139785772e-06,
      "loss": 0.0482,
      "step": 578
    },
    {
      "epoch": 9.26,
      "grad_norm": 1.0092856884002686,
      "learning_rate": 9.766061027894932e-06,
      "loss": 0.0101,
      "step": 579
    },
    {
      "epoch": 9.28,
      "grad_norm": 2.840874195098877,
      "learning_rate": 9.765090956856437e-06,
      "loss": 0.0759,
      "step": 580
    },
    {
      "epoch": 9.3,
      "grad_norm": 0.6366624236106873,
      "learning_rate": 9.76411892706904e-06,
      "loss": 0.0106,
      "step": 581
    },
    {
      "epoch": 9.31,
      "grad_norm": 1.6814610958099365,
      "learning_rate": 9.76314493893231e-06,
      "loss": 0.0091,
      "step": 582
    },
    {
      "epoch": 9.33,
      "grad_norm": 3.524733781814575,
      "learning_rate": 9.762168992846615e-06,
      "loss": 0.0471,
      "step": 583
    },
    {
      "epoch": 9.34,
      "grad_norm": 3.8583078384399414,
      "learning_rate": 9.76119108921313e-06,
      "loss": 0.0197,
      "step": 584
    },
    {
      "epoch": 9.36,
      "grad_norm": 2.0957870483398438,
      "learning_rate": 9.760211228433834e-06,
      "loss": 0.0193,
      "step": 585
    },
    {
      "epoch": 9.38,
      "grad_norm": 2.3696658611297607,
      "learning_rate": 9.759229410911511e-06,
      "loss": 0.0427,
      "step": 586
    },
    {
      "epoch": 9.39,
      "grad_norm": 1.6836861371994019,
      "learning_rate": 9.758245637049748e-06,
      "loss": 0.0256,
      "step": 587
    },
    {
      "epoch": 9.41,
      "grad_norm": 2.177395820617676,
      "learning_rate": 9.757259907252938e-06,
      "loss": 0.0443,
      "step": 588
    },
    {
      "epoch": 9.42,
      "grad_norm": 3.2103934288024902,
      "learning_rate": 9.756272221926278e-06,
      "loss": 0.0656,
      "step": 589
    },
    {
      "epoch": 9.44,
      "grad_norm": 3.1974000930786133,
      "learning_rate": 9.755282581475769e-06,
      "loss": 0.0296,
      "step": 590
    },
    {
      "epoch": 9.46,
      "grad_norm": 1.7987439632415771,
      "learning_rate": 9.754290986308212e-06,
      "loss": 0.0165,
      "step": 591
    },
    {
      "epoch": 9.47,
      "grad_norm": 2.4906423091888428,
      "learning_rate": 9.753297436831217e-06,
      "loss": 0.0141,
      "step": 592
    },
    {
      "epoch": 9.49,
      "grad_norm": 7.716856956481934,
      "learning_rate": 9.752301933453192e-06,
      "loss": 0.1032,
      "step": 593
    },
    {
      "epoch": 9.5,
      "grad_norm": 4.133382797241211,
      "learning_rate": 9.751304476583354e-06,
      "loss": 0.046,
      "step": 594
    },
    {
      "epoch": 9.52,
      "grad_norm": 2.3016562461853027,
      "learning_rate": 9.750305066631717e-06,
      "loss": 0.0318,
      "step": 595
    },
    {
      "epoch": 9.54,
      "grad_norm": 8.612323760986328,
      "learning_rate": 9.749303704009103e-06,
      "loss": 0.0266,
      "step": 596
    },
    {
      "epoch": 9.55,
      "grad_norm": 9.621041297912598,
      "learning_rate": 9.748300389127132e-06,
      "loss": 0.0469,
      "step": 597
    },
    {
      "epoch": 9.57,
      "grad_norm": 2.558444023132324,
      "learning_rate": 9.74729512239823e-06,
      "loss": 0.0261,
      "step": 598
    },
    {
      "epoch": 9.58,
      "grad_norm": 2.6578586101531982,
      "learning_rate": 9.746287904235625e-06,
      "loss": 0.0654,
      "step": 599
    },
    {
      "epoch": 9.6,
      "grad_norm": 2.10679030418396,
      "learning_rate": 9.745278735053345e-06,
      "loss": 0.0254,
      "step": 600
    },
    {
      "epoch": 9.62,
      "grad_norm": 3.073206901550293,
      "learning_rate": 9.74426761526622e-06,
      "loss": 0.0444,
      "step": 601
    },
    {
      "epoch": 9.63,
      "grad_norm": 3.608260154724121,
      "learning_rate": 9.743254545289889e-06,
      "loss": 0.0814,
      "step": 602
    },
    {
      "epoch": 9.65,
      "grad_norm": 1.3865798711776733,
      "learning_rate": 9.74223952554078e-06,
      "loss": 0.0126,
      "step": 603
    },
    {
      "epoch": 9.66,
      "grad_norm": 1.6299582719802856,
      "learning_rate": 9.741222556436132e-06,
      "loss": 0.0399,
      "step": 604
    },
    {
      "epoch": 9.68,
      "grad_norm": 2.7466466426849365,
      "learning_rate": 9.740203638393984e-06,
      "loss": 0.0861,
      "step": 605
    },
    {
      "epoch": 9.7,
      "grad_norm": 1.5725712776184082,
      "learning_rate": 9.739182771833173e-06,
      "loss": 0.021,
      "step": 606
    },
    {
      "epoch": 9.71,
      "grad_norm": 3.709528684616089,
      "learning_rate": 9.738159957173338e-06,
      "loss": 0.0681,
      "step": 607
    },
    {
      "epoch": 9.73,
      "grad_norm": 3.526073932647705,
      "learning_rate": 9.737135194834923e-06,
      "loss": 0.0334,
      "step": 608
    },
    {
      "epoch": 9.74,
      "grad_norm": 2.1937694549560547,
      "learning_rate": 9.736108485239165e-06,
      "loss": 0.0323,
      "step": 609
    },
    {
      "epoch": 9.76,
      "grad_norm": 1.8719559907913208,
      "learning_rate": 9.735079828808107e-06,
      "loss": 0.0277,
      "step": 610
    },
    {
      "epoch": 9.78,
      "grad_norm": 1.7288413047790527,
      "learning_rate": 9.734049225964591e-06,
      "loss": 0.0104,
      "step": 611
    },
    {
      "epoch": 9.79,
      "grad_norm": 1.650159478187561,
      "learning_rate": 9.73301667713226e-06,
      "loss": 0.0244,
      "step": 612
    },
    {
      "epoch": 9.81,
      "grad_norm": 0.9251985549926758,
      "learning_rate": 9.731982182735553e-06,
      "loss": 0.0094,
      "step": 613
    },
    {
      "epoch": 9.82,
      "grad_norm": 0.9738061428070068,
      "learning_rate": 9.730945743199715e-06,
      "loss": 0.0069,
      "step": 614
    },
    {
      "epoch": 9.84,
      "grad_norm": 3.6086342334747314,
      "learning_rate": 9.729907358950785e-06,
      "loss": 0.0737,
      "step": 615
    },
    {
      "epoch": 9.86,
      "grad_norm": 2.1486802101135254,
      "learning_rate": 9.728867030415604e-06,
      "loss": 0.0253,
      "step": 616
    },
    {
      "epoch": 9.87,
      "grad_norm": 9.991865158081055,
      "learning_rate": 9.727824758021812e-06,
      "loss": 0.097,
      "step": 617
    },
    {
      "epoch": 9.89,
      "grad_norm": 6.748747825622559,
      "learning_rate": 9.726780542197845e-06,
      "loss": 0.0302,
      "step": 618
    },
    {
      "epoch": 9.9,
      "grad_norm": 2.6265203952789307,
      "learning_rate": 9.725734383372945e-06,
      "loss": 0.0413,
      "step": 619
    },
    {
      "epoch": 9.92,
      "grad_norm": 3.3372743129730225,
      "learning_rate": 9.724686281977146e-06,
      "loss": 0.0601,
      "step": 620
    },
    {
      "epoch": 9.94,
      "grad_norm": 2.6911351680755615,
      "learning_rate": 9.723636238441285e-06,
      "loss": 0.0294,
      "step": 621
    },
    {
      "epoch": 9.95,
      "grad_norm": 1.2667934894561768,
      "learning_rate": 9.722584253196993e-06,
      "loss": 0.015,
      "step": 622
    },
    {
      "epoch": 9.97,
      "grad_norm": 5.3026533126831055,
      "learning_rate": 9.721530326676703e-06,
      "loss": 0.1222,
      "step": 623
    },
    {
      "epoch": 9.98,
      "grad_norm": 4.044942855834961,
      "learning_rate": 9.720474459313641e-06,
      "loss": 0.0452,
      "step": 624
    },
    {
      "epoch": 10.0,
      "grad_norm": 2.934788703918457,
      "learning_rate": 9.719416651541839e-06,
      "loss": 0.0694,
      "step": 625
    },
    {
      "epoch": 10.02,
      "grad_norm": 2.284575939178467,
      "learning_rate": 9.718356903796118e-06,
      "loss": 0.0312,
      "step": 626
    },
    {
      "epoch": 10.03,
      "grad_norm": 1.487818956375122,
      "learning_rate": 9.717295216512103e-06,
      "loss": 0.0131,
      "step": 627
    },
    {
      "epoch": 10.05,
      "grad_norm": 0.6358056664466858,
      "learning_rate": 9.716231590126211e-06,
      "loss": 0.0066,
      "step": 628
    },
    {
      "epoch": 10.06,
      "grad_norm": 1.6392982006072998,
      "learning_rate": 9.715166025075662e-06,
      "loss": 0.0098,
      "step": 629
    },
    {
      "epoch": 10.08,
      "grad_norm": 2.6732468605041504,
      "learning_rate": 9.714098521798466e-06,
      "loss": 0.0596,
      "step": 630
    },
    {
      "epoch": 10.1,
      "grad_norm": 3.720059871673584,
      "learning_rate": 9.713029080733434e-06,
      "loss": 0.0238,
      "step": 631
    },
    {
      "epoch": 10.11,
      "grad_norm": 1.9984185695648193,
      "learning_rate": 9.711957702320176e-06,
      "loss": 0.0295,
      "step": 632
    },
    {
      "epoch": 10.13,
      "grad_norm": 1.7736663818359375,
      "learning_rate": 9.71088438699909e-06,
      "loss": 0.0219,
      "step": 633
    },
    {
      "epoch": 10.14,
      "grad_norm": 0.24648860096931458,
      "learning_rate": 9.70980913521138e-06,
      "loss": 0.0031,
      "step": 634
    },
    {
      "epoch": 10.16,
      "grad_norm": 1.302254557609558,
      "learning_rate": 9.708731947399039e-06,
      "loss": 0.0124,
      "step": 635
    },
    {
      "epoch": 10.18,
      "grad_norm": 5.050171852111816,
      "learning_rate": 9.707652824004858e-06,
      "loss": 0.0549,
      "step": 636
    },
    {
      "epoch": 10.19,
      "grad_norm": 1.6004945039749146,
      "learning_rate": 9.706571765472426e-06,
      "loss": 0.0186,
      "step": 637
    },
    {
      "epoch": 10.21,
      "grad_norm": 1.5039416551589966,
      "learning_rate": 9.705488772246124e-06,
      "loss": 0.0239,
      "step": 638
    },
    {
      "epoch": 10.22,
      "grad_norm": 4.210566520690918,
      "learning_rate": 9.704403844771128e-06,
      "loss": 0.0457,
      "step": 639
    },
    {
      "epoch": 10.24,
      "grad_norm": 1.088931679725647,
      "learning_rate": 9.703316983493414e-06,
      "loss": 0.0096,
      "step": 640
    },
    {
      "epoch": 10.26,
      "grad_norm": 1.4735653400421143,
      "learning_rate": 9.702228188859746e-06,
      "loss": 0.0061,
      "step": 641
    },
    {
      "epoch": 10.27,
      "grad_norm": 1.0822199583053589,
      "learning_rate": 9.70113746131769e-06,
      "loss": 0.0187,
      "step": 642
    },
    {
      "epoch": 10.29,
      "grad_norm": 1.368896722793579,
      "learning_rate": 9.7000448013156e-06,
      "loss": 0.0276,
      "step": 643
    },
    {
      "epoch": 10.3,
      "grad_norm": 0.6902735233306885,
      "learning_rate": 9.698950209302629e-06,
      "loss": 0.0059,
      "step": 644
    },
    {
      "epoch": 10.32,
      "grad_norm": 2.124427318572998,
      "learning_rate": 9.697853685728721e-06,
      "loss": 0.0122,
      "step": 645
    },
    {
      "epoch": 10.34,
      "grad_norm": 1.4188179969787598,
      "learning_rate": 9.696755231044618e-06,
      "loss": 0.0143,
      "step": 646
    },
    {
      "epoch": 10.35,
      "grad_norm": 1.1000336408615112,
      "learning_rate": 9.695654845701852e-06,
      "loss": 0.0103,
      "step": 647
    },
    {
      "epoch": 10.37,
      "grad_norm": 2.2077558040618896,
      "learning_rate": 9.694552530152747e-06,
      "loss": 0.0225,
      "step": 648
    },
    {
      "epoch": 10.38,
      "grad_norm": 1.5709669589996338,
      "learning_rate": 9.693448284850427e-06,
      "loss": 0.0155,
      "step": 649
    },
    {
      "epoch": 10.4,
      "grad_norm": 2.498774528503418,
      "learning_rate": 9.692342110248802e-06,
      "loss": 0.0337,
      "step": 650
    },
    {
      "epoch": 10.42,
      "grad_norm": 1.9403984546661377,
      "learning_rate": 9.691234006802584e-06,
      "loss": 0.0302,
      "step": 651
    },
    {
      "epoch": 10.43,
      "grad_norm": 4.426129341125488,
      "learning_rate": 9.690123974967267e-06,
      "loss": 0.0421,
      "step": 652
    },
    {
      "epoch": 10.45,
      "grad_norm": 2.3342196941375732,
      "learning_rate": 9.689012015199147e-06,
      "loss": 0.0268,
      "step": 653
    },
    {
      "epoch": 10.46,
      "grad_norm": 3.7218003273010254,
      "learning_rate": 9.687898127955305e-06,
      "loss": 0.0442,
      "step": 654
    },
    {
      "epoch": 10.48,
      "grad_norm": 2.277082920074463,
      "learning_rate": 9.686782313693622e-06,
      "loss": 0.0471,
      "step": 655
    },
    {
      "epoch": 10.5,
      "grad_norm": 1.4309149980545044,
      "learning_rate": 9.68566457287276e-06,
      "loss": 0.0146,
      "step": 656
    },
    {
      "epoch": 10.51,
      "grad_norm": 0.9951757192611694,
      "learning_rate": 9.684544905952192e-06,
      "loss": 0.0057,
      "step": 657
    },
    {
      "epoch": 10.53,
      "grad_norm": 0.9182383418083191,
      "learning_rate": 9.68342331339216e-06,
      "loss": 0.0078,
      "step": 658
    },
    {
      "epoch": 10.54,
      "grad_norm": 4.826396942138672,
      "learning_rate": 9.682299795653714e-06,
      "loss": 0.0474,
      "step": 659
    },
    {
      "epoch": 10.56,
      "grad_norm": 1.6990453004837036,
      "learning_rate": 9.681174353198687e-06,
      "loss": 0.0178,
      "step": 660
    },
    {
      "epoch": 10.58,
      "grad_norm": 1.9271491765975952,
      "learning_rate": 9.680046986489707e-06,
      "loss": 0.0282,
      "step": 661
    },
    {
      "epoch": 10.59,
      "grad_norm": 0.5068991184234619,
      "learning_rate": 9.678917695990191e-06,
      "loss": 0.0041,
      "step": 662
    },
    {
      "epoch": 10.61,
      "grad_norm": 1.7946776151657104,
      "learning_rate": 9.67778648216435e-06,
      "loss": 0.0181,
      "step": 663
    },
    {
      "epoch": 10.62,
      "grad_norm": 0.590260922908783,
      "learning_rate": 9.67665334547718e-06,
      "loss": 0.0072,
      "step": 664
    },
    {
      "epoch": 10.64,
      "grad_norm": 2.2070655822753906,
      "learning_rate": 9.675518286394474e-06,
      "loss": 0.0425,
      "step": 665
    },
    {
      "epoch": 10.66,
      "grad_norm": 4.743477821350098,
      "learning_rate": 9.674381305382808e-06,
      "loss": 0.0574,
      "step": 666
    },
    {
      "epoch": 10.67,
      "grad_norm": 1.7090089321136475,
      "learning_rate": 9.673242402909555e-06,
      "loss": 0.0091,
      "step": 667
    },
    {
      "epoch": 10.69,
      "grad_norm": 2.2445294857025146,
      "learning_rate": 9.672101579442875e-06,
      "loss": 0.022,
      "step": 668
    },
    {
      "epoch": 10.7,
      "grad_norm": 1.4941127300262451,
      "learning_rate": 9.670958835451716e-06,
      "loss": 0.0225,
      "step": 669
    },
    {
      "epoch": 10.72,
      "grad_norm": 4.2405900955200195,
      "learning_rate": 9.669814171405818e-06,
      "loss": 0.042,
      "step": 670
    },
    {
      "epoch": 10.74,
      "grad_norm": 2.415656805038452,
      "learning_rate": 9.668667587775706e-06,
      "loss": 0.0374,
      "step": 671
    },
    {
      "epoch": 10.75,
      "grad_norm": 4.536567687988281,
      "learning_rate": 9.667519085032701e-06,
      "loss": 0.0319,
      "step": 672
    },
    {
      "epoch": 10.77,
      "grad_norm": 5.16109561920166,
      "learning_rate": 9.666368663648908e-06,
      "loss": 0.0327,
      "step": 673
    },
    {
      "epoch": 10.78,
      "grad_norm": 1.714712381362915,
      "learning_rate": 9.665216324097222e-06,
      "loss": 0.0273,
      "step": 674
    },
    {
      "epoch": 10.8,
      "grad_norm": 6.113115310668945,
      "learning_rate": 9.664062066851325e-06,
      "loss": 0.0593,
      "step": 675
    },
    {
      "epoch": 10.82,
      "grad_norm": 5.160564422607422,
      "learning_rate": 9.66290589238569e-06,
      "loss": 0.0993,
      "step": 676
    },
    {
      "epoch": 10.83,
      "grad_norm": 2.8801498413085938,
      "learning_rate": 9.661747801175576e-06,
      "loss": 0.0352,
      "step": 677
    },
    {
      "epoch": 10.85,
      "grad_norm": 1.9799875020980835,
      "learning_rate": 9.660587793697029e-06,
      "loss": 0.0229,
      "step": 678
    },
    {
      "epoch": 10.86,
      "grad_norm": 2.521879196166992,
      "learning_rate": 9.659425870426885e-06,
      "loss": 0.0121,
      "step": 679
    },
    {
      "epoch": 10.88,
      "grad_norm": 3.4796459674835205,
      "learning_rate": 9.658262031842772e-06,
      "loss": 0.0375,
      "step": 680
    },
    {
      "epoch": 10.9,
      "grad_norm": 1.9961950778961182,
      "learning_rate": 9.657096278423093e-06,
      "loss": 0.0566,
      "step": 681
    },
    {
      "epoch": 10.91,
      "grad_norm": 5.96663236618042,
      "learning_rate": 9.655928610647048e-06,
      "loss": 0.054,
      "step": 682
    },
    {
      "epoch": 10.93,
      "grad_norm": 3.7912487983703613,
      "learning_rate": 9.65475902899462e-06,
      "loss": 0.0504,
      "step": 683
    },
    {
      "epoch": 10.94,
      "grad_norm": 1.4608007669448853,
      "learning_rate": 9.653587533946583e-06,
      "loss": 0.016,
      "step": 684
    },
    {
      "epoch": 10.96,
      "grad_norm": 5.072758674621582,
      "learning_rate": 9.65241412598449e-06,
      "loss": 0.0409,
      "step": 685
    },
    {
      "epoch": 10.98,
      "grad_norm": 1.5390039682388306,
      "learning_rate": 9.651238805590689e-06,
      "loss": 0.0136,
      "step": 686
    },
    {
      "epoch": 10.99,
      "grad_norm": 2.556567907333374,
      "learning_rate": 9.650061573248305e-06,
      "loss": 0.0279,
      "step": 687
    },
    {
      "epoch": 11.01,
      "grad_norm": 3.3631505966186523,
      "learning_rate": 9.648882429441258e-06,
      "loss": 0.0583,
      "step": 688
    },
    {
      "epoch": 11.02,
      "grad_norm": 0.5612444877624512,
      "learning_rate": 9.647701374654248e-06,
      "loss": 0.0034,
      "step": 689
    },
    {
      "epoch": 11.04,
      "grad_norm": 1.312469720840454,
      "learning_rate": 9.64651840937276e-06,
      "loss": 0.0156,
      "step": 690
    },
    {
      "epoch": 11.06,
      "grad_norm": 1.1022824048995972,
      "learning_rate": 9.645333534083072e-06,
      "loss": 0.0093,
      "step": 691
    },
    {
      "epoch": 11.07,
      "grad_norm": 1.26968252658844,
      "learning_rate": 9.644146749272234e-06,
      "loss": 0.0107,
      "step": 692
    },
    {
      "epoch": 11.09,
      "grad_norm": 2.428990364074707,
      "learning_rate": 9.642958055428094e-06,
      "loss": 0.026,
      "step": 693
    },
    {
      "epoch": 11.1,
      "grad_norm": 1.294784426689148,
      "learning_rate": 9.641767453039276e-06,
      "loss": 0.009,
      "step": 694
    },
    {
      "epoch": 11.12,
      "grad_norm": 0.9413118362426758,
      "learning_rate": 9.640574942595195e-06,
      "loss": 0.0077,
      "step": 695
    },
    {
      "epoch": 11.14,
      "grad_norm": 2.9242031574249268,
      "learning_rate": 9.639380524586044e-06,
      "loss": 0.0271,
      "step": 696
    },
    {
      "epoch": 11.15,
      "grad_norm": 2.3101723194122314,
      "learning_rate": 9.638184199502805e-06,
      "loss": 0.0269,
      "step": 697
    },
    {
      "epoch": 11.17,
      "grad_norm": 3.468813180923462,
      "learning_rate": 9.63698596783724e-06,
      "loss": 0.038,
      "step": 698
    },
    {
      "epoch": 11.18,
      "grad_norm": 3.2920913696289062,
      "learning_rate": 9.635785830081898e-06,
      "loss": 0.0397,
      "step": 699
    },
    {
      "epoch": 11.2,
      "grad_norm": 12.727725982666016,
      "learning_rate": 9.63458378673011e-06,
      "loss": 0.0837,
      "step": 700
    },
    {
      "epoch": 11.22,
      "grad_norm": 1.9776990413665771,
      "learning_rate": 9.633379838275991e-06,
      "loss": 0.0372,
      "step": 701
    },
    {
      "epoch": 11.23,
      "grad_norm": 2.4494683742523193,
      "learning_rate": 9.632173985214438e-06,
      "loss": 0.0294,
      "step": 702
    },
    {
      "epoch": 11.25,
      "grad_norm": 2.1979148387908936,
      "learning_rate": 9.630966228041132e-06,
      "loss": 0.0299,
      "step": 703
    },
    {
      "epoch": 11.26,
      "grad_norm": 1.3027193546295166,
      "learning_rate": 9.629756567252539e-06,
      "loss": 0.0155,
      "step": 704
    },
    {
      "epoch": 11.28,
      "grad_norm": 3.283012628555298,
      "learning_rate": 9.6285450033459e-06,
      "loss": 0.018,
      "step": 705
    },
    {
      "epoch": 11.3,
      "grad_norm": 1.2519598007202148,
      "learning_rate": 9.627331536819246e-06,
      "loss": 0.0209,
      "step": 706
    },
    {
      "epoch": 11.31,
      "grad_norm": 2.174823045730591,
      "learning_rate": 9.626116168171386e-06,
      "loss": 0.0215,
      "step": 707
    },
    {
      "epoch": 11.33,
      "grad_norm": 0.9943961501121521,
      "learning_rate": 9.624898897901915e-06,
      "loss": 0.0061,
      "step": 708
    },
    {
      "epoch": 11.34,
      "grad_norm": 2.6207261085510254,
      "learning_rate": 9.623679726511204e-06,
      "loss": 0.0454,
      "step": 709
    },
    {
      "epoch": 11.36,
      "grad_norm": 1.1253185272216797,
      "learning_rate": 9.622458654500408e-06,
      "loss": 0.0202,
      "step": 710
    },
    {
      "epoch": 11.38,
      "grad_norm": 2.235111951828003,
      "learning_rate": 9.621235682371466e-06,
      "loss": 0.0287,
      "step": 711
    },
    {
      "epoch": 11.39,
      "grad_norm": 1.3778750896453857,
      "learning_rate": 9.620010810627093e-06,
      "loss": 0.0181,
      "step": 712
    },
    {
      "epoch": 11.41,
      "grad_norm": 1.0799400806427002,
      "learning_rate": 9.61878403977079e-06,
      "loss": 0.0065,
      "step": 713
    },
    {
      "epoch": 11.42,
      "grad_norm": 6.459620952606201,
      "learning_rate": 9.617555370306834e-06,
      "loss": 0.0331,
      "step": 714
    },
    {
      "epoch": 11.44,
      "grad_norm": 0.7371445298194885,
      "learning_rate": 9.616324802740287e-06,
      "loss": 0.0102,
      "step": 715
    },
    {
      "epoch": 11.46,
      "grad_norm": 1.6909699440002441,
      "learning_rate": 9.615092337576987e-06,
      "loss": 0.0304,
      "step": 716
    },
    {
      "epoch": 11.47,
      "grad_norm": 1.246473789215088,
      "learning_rate": 9.613857975323553e-06,
      "loss": 0.018,
      "step": 717
    },
    {
      "epoch": 11.49,
      "grad_norm": 1.9956047534942627,
      "learning_rate": 9.612621716487388e-06,
      "loss": 0.0431,
      "step": 718
    },
    {
      "epoch": 11.5,
      "grad_norm": 2.5614800453186035,
      "learning_rate": 9.611383561576668e-06,
      "loss": 0.031,
      "step": 719
    },
    {
      "epoch": 11.52,
      "grad_norm": 1.4122668504714966,
      "learning_rate": 9.610143511100354e-06,
      "loss": 0.0153,
      "step": 720
    },
    {
      "epoch": 11.54,
      "grad_norm": 0.4733893871307373,
      "learning_rate": 9.608901565568181e-06,
      "loss": 0.0046,
      "step": 721
    },
    {
      "epoch": 11.55,
      "grad_norm": 2.4161603450775146,
      "learning_rate": 9.607657725490669e-06,
      "loss": 0.0058,
      "step": 722
    },
    {
      "epoch": 11.57,
      "grad_norm": 6.091847896575928,
      "learning_rate": 9.606411991379113e-06,
      "loss": 0.0548,
      "step": 723
    },
    {
      "epoch": 11.58,
      "grad_norm": 2.0913658142089844,
      "learning_rate": 9.605164363745588e-06,
      "loss": 0.0098,
      "step": 724
    },
    {
      "epoch": 11.6,
      "grad_norm": 5.022281646728516,
      "learning_rate": 9.603914843102941e-06,
      "loss": 0.0533,
      "step": 725
    },
    {
      "epoch": 11.62,
      "grad_norm": 0.6439379453659058,
      "learning_rate": 9.602663429964811e-06,
      "loss": 0.0051,
      "step": 726
    },
    {
      "epoch": 11.63,
      "grad_norm": 2.0426647663116455,
      "learning_rate": 9.6014101248456e-06,
      "loss": 0.0106,
      "step": 727
    },
    {
      "epoch": 11.65,
      "grad_norm": 5.363095760345459,
      "learning_rate": 9.600154928260499e-06,
      "loss": 0.0655,
      "step": 728
    },
    {
      "epoch": 11.66,
      "grad_norm": 1.0119034051895142,
      "learning_rate": 9.59889784072547e-06,
      "loss": 0.0093,
      "step": 729
    },
    {
      "epoch": 11.68,
      "grad_norm": 1.9503475427627563,
      "learning_rate": 9.597638862757255e-06,
      "loss": 0.0193,
      "step": 730
    },
    {
      "epoch": 11.7,
      "grad_norm": 11.045741081237793,
      "learning_rate": 9.596377994873369e-06,
      "loss": 0.0949,
      "step": 731
    },
    {
      "epoch": 11.71,
      "grad_norm": 1.9263883829116821,
      "learning_rate": 9.595115237592112e-06,
      "loss": 0.0168,
      "step": 732
    },
    {
      "epoch": 11.73,
      "grad_norm": 2.273021697998047,
      "learning_rate": 9.593850591432552e-06,
      "loss": 0.0334,
      "step": 733
    },
    {
      "epoch": 11.74,
      "grad_norm": 10.070587158203125,
      "learning_rate": 9.592584056914539e-06,
      "loss": 0.064,
      "step": 734
    },
    {
      "epoch": 11.76,
      "grad_norm": 1.1343857049942017,
      "learning_rate": 9.591315634558698e-06,
      "loss": 0.0162,
      "step": 735
    },
    {
      "epoch": 11.78,
      "grad_norm": 2.3805153369903564,
      "learning_rate": 9.590045324886429e-06,
      "loss": 0.0427,
      "step": 736
    },
    {
      "epoch": 11.79,
      "grad_norm": 1.6966608762741089,
      "learning_rate": 9.588773128419907e-06,
      "loss": 0.0151,
      "step": 737
    },
    {
      "epoch": 11.81,
      "grad_norm": 3.304238796234131,
      "learning_rate": 9.587499045682084e-06,
      "loss": 0.0648,
      "step": 738
    },
    {
      "epoch": 11.82,
      "grad_norm": 2.1830270290374756,
      "learning_rate": 9.58622307719669e-06,
      "loss": 0.0382,
      "step": 739
    },
    {
      "epoch": 11.84,
      "grad_norm": 1.1398420333862305,
      "learning_rate": 9.584945223488227e-06,
      "loss": 0.0235,
      "step": 740
    },
    {
      "epoch": 11.86,
      "grad_norm": 1.1458464860916138,
      "learning_rate": 9.58366548508197e-06,
      "loss": 0.0159,
      "step": 741
    },
    {
      "epoch": 11.87,
      "grad_norm": 2.805856466293335,
      "learning_rate": 9.582383862503972e-06,
      "loss": 0.0426,
      "step": 742
    },
    {
      "epoch": 11.89,
      "grad_norm": 1.776235818862915,
      "learning_rate": 9.581100356281059e-06,
      "loss": 0.0094,
      "step": 743
    },
    {
      "epoch": 11.9,
      "grad_norm": 1.399289608001709,
      "learning_rate": 9.579814966940833e-06,
      "loss": 0.0106,
      "step": 744
    },
    {
      "epoch": 11.92,
      "grad_norm": 1.5787516832351685,
      "learning_rate": 9.57852769501167e-06,
      "loss": 0.0241,
      "step": 745
    },
    {
      "epoch": 11.94,
      "grad_norm": 2.0155608654022217,
      "learning_rate": 9.577238541022718e-06,
      "loss": 0.0287,
      "step": 746
    },
    {
      "epoch": 11.95,
      "grad_norm": 3.8723137378692627,
      "learning_rate": 9.575947505503897e-06,
      "loss": 0.0409,
      "step": 747
    },
    {
      "epoch": 11.97,
      "grad_norm": 3.5427675247192383,
      "learning_rate": 9.574654588985907e-06,
      "loss": 0.0561,
      "step": 748
    },
    {
      "epoch": 11.98,
      "grad_norm": 1.771302580833435,
      "learning_rate": 9.573359792000214e-06,
      "loss": 0.0186,
      "step": 749
    },
    {
      "epoch": 12.0,
      "grad_norm": 2.3561389446258545,
      "learning_rate": 9.572063115079063e-06,
      "loss": 0.0277,
      "step": 750
    },
    {
      "epoch": 12.02,
      "grad_norm": 0.6079907417297363,
      "learning_rate": 9.570764558755466e-06,
      "loss": 0.0064,
      "step": 751
    },
    {
      "epoch": 12.03,
      "grad_norm": 0.3330226540565491,
      "learning_rate": 9.569464123563212e-06,
      "loss": 0.0037,
      "step": 752
    },
    {
      "epoch": 12.05,
      "grad_norm": 3.1529123783111572,
      "learning_rate": 9.568161810036861e-06,
      "loss": 0.0196,
      "step": 753
    },
    {
      "epoch": 12.06,
      "grad_norm": 1.7979955673217773,
      "learning_rate": 9.566857618711744e-06,
      "loss": 0.02,
      "step": 754
    },
    {
      "epoch": 12.08,
      "grad_norm": 1.707484245300293,
      "learning_rate": 9.565551550123967e-06,
      "loss": 0.0218,
      "step": 755
    },
    {
      "epoch": 12.1,
      "grad_norm": 0.5742951035499573,
      "learning_rate": 9.564243604810401e-06,
      "loss": 0.0054,
      "step": 756
    },
    {
      "epoch": 12.11,
      "grad_norm": 2.0355384349823,
      "learning_rate": 9.562933783308698e-06,
      "loss": 0.0114,
      "step": 757
    },
    {
      "epoch": 12.13,
      "grad_norm": 0.7267528772354126,
      "learning_rate": 9.561622086157273e-06,
      "loss": 0.0111,
      "step": 758
    },
    {
      "epoch": 12.14,
      "grad_norm": 0.7750834822654724,
      "learning_rate": 9.560308513895315e-06,
      "loss": 0.0123,
      "step": 759
    },
    {
      "epoch": 12.16,
      "grad_norm": 0.3280489146709442,
      "learning_rate": 9.558993067062785e-06,
      "loss": 0.0027,
      "step": 760
    },
    {
      "epoch": 12.18,
      "grad_norm": 1.0619823932647705,
      "learning_rate": 9.557675746200414e-06,
      "loss": 0.0086,
      "step": 761
    },
    {
      "epoch": 12.19,
      "grad_norm": 1.8607282638549805,
      "learning_rate": 9.556356551849702e-06,
      "loss": 0.0172,
      "step": 762
    },
    {
      "epoch": 12.21,
      "grad_norm": 1.635956883430481,
      "learning_rate": 9.55503548455292e-06,
      "loss": 0.0124,
      "step": 763
    },
    {
      "epoch": 12.22,
      "grad_norm": 3.1051247119903564,
      "learning_rate": 9.553712544853109e-06,
      "loss": 0.0357,
      "step": 764
    },
    {
      "epoch": 12.24,
      "grad_norm": 2.5919876098632812,
      "learning_rate": 9.552387733294081e-06,
      "loss": 0.0614,
      "step": 765
    },
    {
      "epoch": 12.26,
      "grad_norm": 1.157294750213623,
      "learning_rate": 9.551061050420413e-06,
      "loss": 0.0084,
      "step": 766
    },
    {
      "epoch": 12.27,
      "grad_norm": 1.1037923097610474,
      "learning_rate": 9.549732496777455e-06,
      "loss": 0.0129,
      "step": 767
    },
    {
      "epoch": 12.29,
      "grad_norm": 0.5243232250213623,
      "learning_rate": 9.548402072911328e-06,
      "loss": 0.0031,
      "step": 768
    },
    {
      "epoch": 12.3,
      "grad_norm": 2.3186841011047363,
      "learning_rate": 9.547069779368915e-06,
      "loss": 0.0357,
      "step": 769
    },
    {
      "epoch": 12.32,
      "grad_norm": 0.834151029586792,
      "learning_rate": 9.545735616697875e-06,
      "loss": 0.0069,
      "step": 770
    },
    {
      "epoch": 12.34,
      "grad_norm": 8.850337028503418,
      "learning_rate": 9.54439958544663e-06,
      "loss": 0.0919,
      "step": 771
    },
    {
      "epoch": 12.35,
      "grad_norm": 0.8040649890899658,
      "learning_rate": 9.543061686164374e-06,
      "loss": 0.0102,
      "step": 772
    },
    {
      "epoch": 12.37,
      "grad_norm": 2.0959136486053467,
      "learning_rate": 9.541721919401063e-06,
      "loss": 0.0324,
      "step": 773
    },
    {
      "epoch": 12.38,
      "grad_norm": 2.1061596870422363,
      "learning_rate": 9.540380285707426e-06,
      "loss": 0.0204,
      "step": 774
    },
    {
      "epoch": 12.4,
      "grad_norm": 2.454941511154175,
      "learning_rate": 9.539036785634961e-06,
      "loss": 0.0351,
      "step": 775
    },
    {
      "epoch": 12.42,
      "grad_norm": 1.9465163946151733,
      "learning_rate": 9.537691419735929e-06,
      "loss": 0.026,
      "step": 776
    },
    {
      "epoch": 12.43,
      "grad_norm": 0.3444218635559082,
      "learning_rate": 9.536344188563355e-06,
      "loss": 0.0027,
      "step": 777
    },
    {
      "epoch": 12.45,
      "grad_norm": 2.3123488426208496,
      "learning_rate": 9.534995092671038e-06,
      "loss": 0.0419,
      "step": 778
    },
    {
      "epoch": 12.46,
      "grad_norm": 1.800607442855835,
      "learning_rate": 9.533644132613542e-06,
      "loss": 0.0142,
      "step": 779
    },
    {
      "epoch": 12.48,
      "grad_norm": 0.9058864712715149,
      "learning_rate": 9.532291308946191e-06,
      "loss": 0.0081,
      "step": 780
    },
    {
      "epoch": 12.5,
      "grad_norm": 1.4724301099777222,
      "learning_rate": 9.530936622225084e-06,
      "loss": 0.0225,
      "step": 781
    },
    {
      "epoch": 12.51,
      "grad_norm": 0.4957478642463684,
      "learning_rate": 9.52958007300708e-06,
      "loss": 0.0063,
      "step": 782
    },
    {
      "epoch": 12.53,
      "grad_norm": 1.5582242012023926,
      "learning_rate": 9.528221661849806e-06,
      "loss": 0.0137,
      "step": 783
    },
    {
      "epoch": 12.54,
      "grad_norm": 0.3700534999370575,
      "learning_rate": 9.526861389311652e-06,
      "loss": 0.0035,
      "step": 784
    },
    {
      "epoch": 12.56,
      "grad_norm": 2.596723794937134,
      "learning_rate": 9.525499255951775e-06,
      "loss": 0.0268,
      "step": 785
    },
    {
      "epoch": 12.58,
      "grad_norm": 1.2375686168670654,
      "learning_rate": 9.524135262330098e-06,
      "loss": 0.0207,
      "step": 786
    },
    {
      "epoch": 12.59,
      "grad_norm": 1.8371200561523438,
      "learning_rate": 9.522769409007306e-06,
      "loss": 0.0261,
      "step": 787
    },
    {
      "epoch": 12.61,
      "grad_norm": 7.795184135437012,
      "learning_rate": 9.52140169654485e-06,
      "loss": 0.046,
      "step": 788
    },
    {
      "epoch": 12.62,
      "grad_norm": 0.8883206844329834,
      "learning_rate": 9.520032125504942e-06,
      "loss": 0.009,
      "step": 789
    },
    {
      "epoch": 12.64,
      "grad_norm": 1.4030040502548218,
      "learning_rate": 9.518660696450567e-06,
      "loss": 0.024,
      "step": 790
    },
    {
      "epoch": 12.66,
      "grad_norm": 0.7910358309745789,
      "learning_rate": 9.517287409945463e-06,
      "loss": 0.0088,
      "step": 791
    },
    {
      "epoch": 12.67,
      "grad_norm": 0.6336977481842041,
      "learning_rate": 9.51591226655414e-06,
      "loss": 0.0052,
      "step": 792
    },
    {
      "epoch": 12.69,
      "grad_norm": 1.7860902547836304,
      "learning_rate": 9.514535266841863e-06,
      "loss": 0.0118,
      "step": 793
    },
    {
      "epoch": 12.7,
      "grad_norm": 3.1116018295288086,
      "learning_rate": 9.513156411374667e-06,
      "loss": 0.0623,
      "step": 794
    },
    {
      "epoch": 12.72,
      "grad_norm": 4.910407543182373,
      "learning_rate": 9.511775700719347e-06,
      "loss": 0.0131,
      "step": 795
    },
    {
      "epoch": 12.74,
      "grad_norm": 8.644158363342285,
      "learning_rate": 9.51039313544346e-06,
      "loss": 0.0387,
      "step": 796
    },
    {
      "epoch": 12.75,
      "grad_norm": 0.8248209357261658,
      "learning_rate": 9.509008716115328e-06,
      "loss": 0.0052,
      "step": 797
    },
    {
      "epoch": 12.77,
      "grad_norm": 2.4717705249786377,
      "learning_rate": 9.507622443304036e-06,
      "loss": 0.0192,
      "step": 798
    },
    {
      "epoch": 12.78,
      "grad_norm": 1.5635442733764648,
      "learning_rate": 9.506234317579422e-06,
      "loss": 0.0278,
      "step": 799
    },
    {
      "epoch": 12.8,
      "grad_norm": 5.632394790649414,
      "learning_rate": 9.504844339512096e-06,
      "loss": 0.0386,
      "step": 800
    },
    {
      "epoch": 12.82,
      "grad_norm": 0.5710117220878601,
      "learning_rate": 9.503452509673426e-06,
      "loss": 0.0031,
      "step": 801
    },
    {
      "epoch": 12.83,
      "grad_norm": 0.5835042595863342,
      "learning_rate": 9.50205882863554e-06,
      "loss": 0.001,
      "step": 802
    },
    {
      "epoch": 12.85,
      "grad_norm": 2.3193118572235107,
      "learning_rate": 9.500663296971324e-06,
      "loss": 0.0358,
      "step": 803
    },
    {
      "epoch": 12.86,
      "grad_norm": 2.155752182006836,
      "learning_rate": 9.499265915254434e-06,
      "loss": 0.051,
      "step": 804
    },
    {
      "epoch": 12.88,
      "grad_norm": 1.4673024415969849,
      "learning_rate": 9.497866684059278e-06,
      "loss": 0.0065,
      "step": 805
    },
    {
      "epoch": 12.9,
      "grad_norm": 1.459961175918579,
      "learning_rate": 9.496465603961028e-06,
      "loss": 0.0134,
      "step": 806
    },
    {
      "epoch": 12.91,
      "grad_norm": 1.6017365455627441,
      "learning_rate": 9.495062675535614e-06,
      "loss": 0.0157,
      "step": 807
    },
    {
      "epoch": 12.93,
      "grad_norm": 1.677079677581787,
      "learning_rate": 9.493657899359727e-06,
      "loss": 0.0151,
      "step": 808
    },
    {
      "epoch": 12.94,
      "grad_norm": 0.9544321298599243,
      "learning_rate": 9.492251276010817e-06,
      "loss": 0.0124,
      "step": 809
    },
    {
      "epoch": 12.96,
      "grad_norm": 1.5691708326339722,
      "learning_rate": 9.490842806067095e-06,
      "loss": 0.0109,
      "step": 810
    },
    {
      "epoch": 12.98,
      "grad_norm": 2.362659215927124,
      "learning_rate": 9.489432490107531e-06,
      "loss": 0.0355,
      "step": 811
    },
    {
      "epoch": 12.99,
      "grad_norm": 4.071302890777588,
      "learning_rate": 9.488020328711851e-06,
      "loss": 0.1053,
      "step": 812
    },
    {
      "epoch": 13.01,
      "grad_norm": 1.685452938079834,
      "learning_rate": 9.48660632246054e-06,
      "loss": 0.0292,
      "step": 813
    },
    {
      "epoch": 13.02,
      "grad_norm": 1.042799472808838,
      "learning_rate": 9.485190471934845e-06,
      "loss": 0.0104,
      "step": 814
    },
    {
      "epoch": 13.04,
      "grad_norm": 1.0856311321258545,
      "learning_rate": 9.483772777716767e-06,
      "loss": 0.0196,
      "step": 815
    },
    {
      "epoch": 13.06,
      "grad_norm": 0.9794950485229492,
      "learning_rate": 9.482353240389066e-06,
      "loss": 0.0122,
      "step": 816
    },
    {
      "epoch": 13.07,
      "grad_norm": 0.8929558396339417,
      "learning_rate": 9.480931860535261e-06,
      "loss": 0.006,
      "step": 817
    },
    {
      "epoch": 13.09,
      "grad_norm": 1.4075260162353516,
      "learning_rate": 9.479508638739629e-06,
      "loss": 0.0149,
      "step": 818
    },
    {
      "epoch": 13.1,
      "grad_norm": 1.4281761646270752,
      "learning_rate": 9.4780835755872e-06,
      "loss": 0.0115,
      "step": 819
    },
    {
      "epoch": 13.12,
      "grad_norm": 3.5087499618530273,
      "learning_rate": 9.476656671663766e-06,
      "loss": 0.0119,
      "step": 820
    },
    {
      "epoch": 13.14,
      "grad_norm": 2.3535916805267334,
      "learning_rate": 9.475227927555873e-06,
      "loss": 0.0321,
      "step": 821
    },
    {
      "epoch": 13.15,
      "grad_norm": 2.854072093963623,
      "learning_rate": 9.473797343850822e-06,
      "loss": 0.0585,
      "step": 822
    },
    {
      "epoch": 13.17,
      "grad_norm": 0.4737025201320648,
      "learning_rate": 9.472364921136674e-06,
      "loss": 0.0036,
      "step": 823
    },
    {
      "epoch": 13.18,
      "grad_norm": 0.895517110824585,
      "learning_rate": 9.470930660002241e-06,
      "loss": 0.0045,
      "step": 824
    },
    {
      "epoch": 13.2,
      "grad_norm": 1.5147939920425415,
      "learning_rate": 9.469494561037097e-06,
      "loss": 0.0151,
      "step": 825
    },
    {
      "epoch": 13.22,
      "grad_norm": 0.6965582370758057,
      "learning_rate": 9.468056624831568e-06,
      "loss": 0.0031,
      "step": 826
    },
    {
      "epoch": 13.23,
      "grad_norm": 0.9179592728614807,
      "learning_rate": 9.466616851976734e-06,
      "loss": 0.0079,
      "step": 827
    },
    {
      "epoch": 13.25,
      "grad_norm": 0.9399427175521851,
      "learning_rate": 9.465175243064428e-06,
      "loss": 0.0063,
      "step": 828
    },
    {
      "epoch": 13.26,
      "grad_norm": 5.843096733093262,
      "learning_rate": 9.463731798687246e-06,
      "loss": 0.0307,
      "step": 829
    },
    {
      "epoch": 13.28,
      "grad_norm": 2.493544340133667,
      "learning_rate": 9.462286519438531e-06,
      "loss": 0.03,
      "step": 830
    },
    {
      "epoch": 13.3,
      "grad_norm": 0.8986279964447021,
      "learning_rate": 9.46083940591238e-06,
      "loss": 0.0089,
      "step": 831
    },
    {
      "epoch": 13.31,
      "grad_norm": 0.7553679347038269,
      "learning_rate": 9.459390458703654e-06,
      "loss": 0.0105,
      "step": 832
    },
    {
      "epoch": 13.33,
      "grad_norm": 0.7776663303375244,
      "learning_rate": 9.457939678407955e-06,
      "loss": 0.0082,
      "step": 833
    },
    {
      "epoch": 13.34,
      "grad_norm": 2.185588836669922,
      "learning_rate": 9.456487065621646e-06,
      "loss": 0.0338,
      "step": 834
    },
    {
      "epoch": 13.36,
      "grad_norm": 5.582080364227295,
      "learning_rate": 9.45503262094184e-06,
      "loss": 0.0318,
      "step": 835
    },
    {
      "epoch": 13.38,
      "grad_norm": 2.586591958999634,
      "learning_rate": 9.453576344966404e-06,
      "loss": 0.049,
      "step": 836
    },
    {
      "epoch": 13.39,
      "grad_norm": 0.7867441177368164,
      "learning_rate": 9.452118238293961e-06,
      "loss": 0.0065,
      "step": 837
    },
    {
      "epoch": 13.41,
      "grad_norm": 0.8846791386604309,
      "learning_rate": 9.45065830152388e-06,
      "loss": 0.0063,
      "step": 838
    },
    {
      "epoch": 13.42,
      "grad_norm": 0.2650858163833618,
      "learning_rate": 9.449196535256286e-06,
      "loss": 0.0014,
      "step": 839
    },
    {
      "epoch": 13.44,
      "grad_norm": 0.27747446298599243,
      "learning_rate": 9.44773294009206e-06,
      "loss": 0.0023,
      "step": 840
    },
    {
      "epoch": 13.46,
      "grad_norm": 2.5302441120147705,
      "learning_rate": 9.446267516632826e-06,
      "loss": 0.0193,
      "step": 841
    },
    {
      "epoch": 13.47,
      "grad_norm": 2.173412799835205,
      "learning_rate": 9.444800265480968e-06,
      "loss": 0.0106,
      "step": 842
    },
    {
      "epoch": 13.49,
      "grad_norm": 1.570107102394104,
      "learning_rate": 9.443331187239613e-06,
      "loss": 0.0169,
      "step": 843
    },
    {
      "epoch": 13.5,
      "grad_norm": 0.7987038493156433,
      "learning_rate": 9.441860282512648e-06,
      "loss": 0.0072,
      "step": 844
    },
    {
      "epoch": 13.52,
      "grad_norm": 1.7606303691864014,
      "learning_rate": 9.440387551904705e-06,
      "loss": 0.0133,
      "step": 845
    },
    {
      "epoch": 13.54,
      "grad_norm": 1.8308292627334595,
      "learning_rate": 9.438912996021165e-06,
      "loss": 0.0231,
      "step": 846
    },
    {
      "epoch": 13.55,
      "grad_norm": 2.125998020172119,
      "learning_rate": 9.437436615468167e-06,
      "loss": 0.008,
      "step": 847
    },
    {
      "epoch": 13.57,
      "grad_norm": 2.0202882289886475,
      "learning_rate": 9.435958410852593e-06,
      "loss": 0.0228,
      "step": 848
    },
    {
      "epoch": 13.58,
      "grad_norm": 3.2643496990203857,
      "learning_rate": 9.434478382782075e-06,
      "loss": 0.0356,
      "step": 849
    },
    {
      "epoch": 13.6,
      "grad_norm": 7.941174507141113,
      "learning_rate": 9.432996531865001e-06,
      "loss": 0.0308,
      "step": 850
    },
    {
      "epoch": 13.62,
      "grad_norm": 2.3203084468841553,
      "learning_rate": 9.431512858710501e-06,
      "loss": 0.0244,
      "step": 851
    },
    {
      "epoch": 13.63,
      "grad_norm": 1.270585298538208,
      "learning_rate": 9.430027363928458e-06,
      "loss": 0.0088,
      "step": 852
    },
    {
      "epoch": 13.65,
      "grad_norm": 1.8045446872711182,
      "learning_rate": 9.428540048129502e-06,
      "loss": 0.0309,
      "step": 853
    },
    {
      "epoch": 13.66,
      "grad_norm": 2.4720065593719482,
      "learning_rate": 9.427050911925014e-06,
      "loss": 0.0336,
      "step": 854
    },
    {
      "epoch": 13.68,
      "grad_norm": 2.284787178039551,
      "learning_rate": 9.425559955927118e-06,
      "loss": 0.0398,
      "step": 855
    },
    {
      "epoch": 13.7,
      "grad_norm": 1.824144721031189,
      "learning_rate": 9.424067180748692e-06,
      "loss": 0.0154,
      "step": 856
    },
    {
      "epoch": 13.71,
      "grad_norm": 1.9807416200637817,
      "learning_rate": 9.422572587003362e-06,
      "loss": 0.0076,
      "step": 857
    },
    {
      "epoch": 13.73,
      "grad_norm": 1.349348545074463,
      "learning_rate": 9.421076175305496e-06,
      "loss": 0.0203,
      "step": 858
    },
    {
      "epoch": 13.74,
      "grad_norm": 6.034634590148926,
      "learning_rate": 9.419577946270213e-06,
      "loss": 0.0787,
      "step": 859
    },
    {
      "epoch": 13.76,
      "grad_norm": 0.8115677833557129,
      "learning_rate": 9.418077900513377e-06,
      "loss": 0.0051,
      "step": 860
    },
    {
      "epoch": 13.78,
      "grad_norm": 0.4141666293144226,
      "learning_rate": 9.416576038651602e-06,
      "loss": 0.0039,
      "step": 861
    },
    {
      "epoch": 13.79,
      "grad_norm": 0.7167579531669617,
      "learning_rate": 9.415072361302247e-06,
      "loss": 0.0074,
      "step": 862
    },
    {
      "epoch": 13.81,
      "grad_norm": 1.2613707780838013,
      "learning_rate": 9.413566869083417e-06,
      "loss": 0.0088,
      "step": 863
    },
    {
      "epoch": 13.82,
      "grad_norm": 1.2790770530700684,
      "learning_rate": 9.41205956261396e-06,
      "loss": 0.018,
      "step": 864
    },
    {
      "epoch": 13.84,
      "grad_norm": 0.3742436170578003,
      "learning_rate": 9.410550442513475e-06,
      "loss": 0.0017,
      "step": 865
    },
    {
      "epoch": 13.86,
      "grad_norm": 2.387887716293335,
      "learning_rate": 9.409039509402305e-06,
      "loss": 0.0329,
      "step": 866
    },
    {
      "epoch": 13.87,
      "grad_norm": 3.0302810668945312,
      "learning_rate": 9.407526763901538e-06,
      "loss": 0.0359,
      "step": 867
    },
    {
      "epoch": 13.89,
      "grad_norm": 0.3207687437534332,
      "learning_rate": 9.406012206633004e-06,
      "loss": 0.0047,
      "step": 868
    },
    {
      "epoch": 13.9,
      "grad_norm": 1.4536691904067993,
      "learning_rate": 9.40449583821928e-06,
      "loss": 0.0195,
      "step": 869
    },
    {
      "epoch": 13.92,
      "grad_norm": 4.523533344268799,
      "learning_rate": 9.40297765928369e-06,
      "loss": 0.0184,
      "step": 870
    },
    {
      "epoch": 13.94,
      "grad_norm": 0.786756157875061,
      "learning_rate": 9.4014576704503e-06,
      "loss": 0.0093,
      "step": 871
    },
    {
      "epoch": 13.95,
      "grad_norm": 1.9535151720046997,
      "learning_rate": 9.39993587234392e-06,
      "loss": 0.0064,
      "step": 872
    },
    {
      "epoch": 13.97,
      "grad_norm": 1.1609727144241333,
      "learning_rate": 9.398412265590102e-06,
      "loss": 0.0107,
      "step": 873
    },
    {
      "epoch": 13.98,
      "grad_norm": 1.2540913820266724,
      "learning_rate": 9.396886850815144e-06,
      "loss": 0.0193,
      "step": 874
    },
    {
      "epoch": 14.0,
      "grad_norm": 1.7323399782180786,
      "learning_rate": 9.395359628646087e-06,
      "loss": 0.036,
      "step": 875
    },
    {
      "epoch": 14.02,
      "grad_norm": 0.47575607895851135,
      "learning_rate": 9.393830599710714e-06,
      "loss": 0.0063,
      "step": 876
    },
    {
      "epoch": 14.03,
      "grad_norm": 0.6378353238105774,
      "learning_rate": 9.39229976463755e-06,
      "loss": 0.0089,
      "step": 877
    },
    {
      "epoch": 14.05,
      "grad_norm": 4.1432204246521,
      "learning_rate": 9.390767124055866e-06,
      "loss": 0.0175,
      "step": 878
    },
    {
      "epoch": 14.06,
      "grad_norm": 0.5498522520065308,
      "learning_rate": 9.38923267859567e-06,
      "loss": 0.0053,
      "step": 879
    },
    {
      "epoch": 14.08,
      "grad_norm": 0.320039302110672,
      "learning_rate": 9.387696428887715e-06,
      "loss": 0.0025,
      "step": 880
    },
    {
      "epoch": 14.1,
      "grad_norm": 0.6754450798034668,
      "learning_rate": 9.386158375563497e-06,
      "loss": 0.0072,
      "step": 881
    },
    {
      "epoch": 14.11,
      "grad_norm": 0.2650596499443054,
      "learning_rate": 9.384618519255252e-06,
      "loss": 0.0028,
      "step": 882
    },
    {
      "epoch": 14.13,
      "grad_norm": 0.5443189740180969,
      "learning_rate": 9.383076860595952e-06,
      "loss": 0.0029,
      "step": 883
    },
    {
      "epoch": 14.14,
      "grad_norm": 0.851101279258728,
      "learning_rate": 9.381533400219319e-06,
      "loss": 0.0088,
      "step": 884
    },
    {
      "epoch": 14.16,
      "grad_norm": 0.3747986853122711,
      "learning_rate": 9.37998813875981e-06,
      "loss": 0.003,
      "step": 885
    },
    {
      "epoch": 14.18,
      "grad_norm": 1.5206420421600342,
      "learning_rate": 9.378441076852624e-06,
      "loss": 0.0287,
      "step": 886
    },
    {
      "epoch": 14.19,
      "grad_norm": 0.2616053521633148,
      "learning_rate": 9.3768922151337e-06,
      "loss": 0.002,
      "step": 887
    },
    {
      "epoch": 14.21,
      "grad_norm": 6.033935070037842,
      "learning_rate": 9.375341554239716e-06,
      "loss": 0.0661,
      "step": 888
    },
    {
      "epoch": 14.22,
      "grad_norm": 0.3531147241592407,
      "learning_rate": 9.37378909480809e-06,
      "loss": 0.0022,
      "step": 889
    },
    {
      "epoch": 14.24,
      "grad_norm": 1.0851588249206543,
      "learning_rate": 9.372234837476979e-06,
      "loss": 0.0127,
      "step": 890
    },
    {
      "epoch": 14.26,
      "grad_norm": 1.6890183687210083,
      "learning_rate": 9.37067878288528e-06,
      "loss": 0.0294,
      "step": 891
    },
    {
      "epoch": 14.27,
      "grad_norm": 1.2201261520385742,
      "learning_rate": 9.369120931672631e-06,
      "loss": 0.0083,
      "step": 892
    },
    {
      "epoch": 14.29,
      "grad_norm": 2.9142627716064453,
      "learning_rate": 9.3675612844794e-06,
      "loss": 0.0205,
      "step": 893
    },
    {
      "epoch": 14.3,
      "grad_norm": 0.5436248183250427,
      "learning_rate": 9.365999841946703e-06,
      "loss": 0.0045,
      "step": 894
    },
    {
      "epoch": 14.32,
      "grad_norm": 1.2297749519348145,
      "learning_rate": 9.364436604716389e-06,
      "loss": 0.0144,
      "step": 895
    },
    {
      "epoch": 14.34,
      "grad_norm": 0.9316636323928833,
      "learning_rate": 9.362871573431046e-06,
      "loss": 0.0059,
      "step": 896
    },
    {
      "epoch": 14.35,
      "grad_norm": 1.1050807237625122,
      "learning_rate": 9.361304748734e-06,
      "loss": 0.004,
      "step": 897
    },
    {
      "epoch": 14.37,
      "grad_norm": 1.5277700424194336,
      "learning_rate": 9.359736131269312e-06,
      "loss": 0.0296,
      "step": 898
    },
    {
      "epoch": 14.38,
      "grad_norm": 1.5083496570587158,
      "learning_rate": 9.358165721681782e-06,
      "loss": 0.0258,
      "step": 899
    },
    {
      "epoch": 14.4,
      "grad_norm": 1.0635364055633545,
      "learning_rate": 9.356593520616948e-06,
      "loss": 0.0137,
      "step": 900
    },
    {
      "epoch": 14.42,
      "grad_norm": 0.6802024841308594,
      "learning_rate": 9.355019528721079e-06,
      "loss": 0.0097,
      "step": 901
    },
    {
      "epoch": 14.43,
      "grad_norm": 2.1965432167053223,
      "learning_rate": 9.353443746641185e-06,
      "loss": 0.038,
      "step": 902
    },
    {
      "epoch": 14.45,
      "grad_norm": 2.5188207626342773,
      "learning_rate": 9.351866175025011e-06,
      "loss": 0.0189,
      "step": 903
    },
    {
      "epoch": 14.46,
      "grad_norm": 1.8355638980865479,
      "learning_rate": 9.350286814521037e-06,
      "loss": 0.0195,
      "step": 904
    },
    {
      "epoch": 14.48,
      "grad_norm": 1.8334685564041138,
      "learning_rate": 9.348705665778479e-06,
      "loss": 0.011,
      "step": 905
    },
    {
      "epoch": 14.5,
      "grad_norm": 1.194510579109192,
      "learning_rate": 9.347122729447284e-06,
      "loss": 0.0168,
      "step": 906
    },
    {
      "epoch": 14.51,
      "grad_norm": 2.2884135246276855,
      "learning_rate": 9.345538006178143e-06,
      "loss": 0.0093,
      "step": 907
    },
    {
      "epoch": 14.53,
      "grad_norm": 0.29001420736312866,
      "learning_rate": 9.343951496622473e-06,
      "loss": 0.006,
      "step": 908
    },
    {
      "epoch": 14.54,
      "grad_norm": 3.6596791744232178,
      "learning_rate": 9.342363201432425e-06,
      "loss": 0.0664,
      "step": 909
    },
    {
      "epoch": 14.56,
      "grad_norm": 4.518401622772217,
      "learning_rate": 9.340773121260893e-06,
      "loss": 0.0077,
      "step": 910
    },
    {
      "epoch": 14.58,
      "grad_norm": 1.5940196514129639,
      "learning_rate": 9.339181256761495e-06,
      "loss": 0.0188,
      "step": 911
    },
    {
      "epoch": 14.59,
      "grad_norm": 1.3310811519622803,
      "learning_rate": 9.337587608588588e-06,
      "loss": 0.018,
      "step": 912
    },
    {
      "epoch": 14.61,
      "grad_norm": 0.9711111783981323,
      "learning_rate": 9.335992177397261e-06,
      "loss": 0.0088,
      "step": 913
    },
    {
      "epoch": 14.62,
      "grad_norm": 1.819186806678772,
      "learning_rate": 9.334394963843334e-06,
      "loss": 0.0233,
      "step": 914
    },
    {
      "epoch": 14.64,
      "grad_norm": 1.4442732334136963,
      "learning_rate": 9.33279596858336e-06,
      "loss": 0.0352,
      "step": 915
    },
    {
      "epoch": 14.66,
      "grad_norm": 1.7796382904052734,
      "learning_rate": 9.33119519227463e-06,
      "loss": 0.0257,
      "step": 916
    },
    {
      "epoch": 14.67,
      "grad_norm": 1.3981467485427856,
      "learning_rate": 9.32959263557516e-06,
      "loss": 0.0286,
      "step": 917
    },
    {
      "epoch": 14.69,
      "grad_norm": 0.7538534998893738,
      "learning_rate": 9.327988299143697e-06,
      "loss": 0.0092,
      "step": 918
    },
    {
      "epoch": 14.7,
      "grad_norm": 1.7101678848266602,
      "learning_rate": 9.326382183639731e-06,
      "loss": 0.0319,
      "step": 919
    },
    {
      "epoch": 14.72,
      "grad_norm": 1.566427230834961,
      "learning_rate": 9.324774289723469e-06,
      "loss": 0.0236,
      "step": 920
    },
    {
      "epoch": 14.74,
      "grad_norm": 0.3803955316543579,
      "learning_rate": 9.323164618055858e-06,
      "loss": 0.003,
      "step": 921
    },
    {
      "epoch": 14.75,
      "grad_norm": 1.9702237844467163,
      "learning_rate": 9.321553169298571e-06,
      "loss": 0.0422,
      "step": 922
    },
    {
      "epoch": 14.77,
      "grad_norm": 1.4259850978851318,
      "learning_rate": 9.319939944114018e-06,
      "loss": 0.0147,
      "step": 923
    },
    {
      "epoch": 14.78,
      "grad_norm": 0.2756137549877167,
      "learning_rate": 9.318324943165331e-06,
      "loss": 0.002,
      "step": 924
    },
    {
      "epoch": 14.8,
      "grad_norm": 2.2793285846710205,
      "learning_rate": 9.316708167116377e-06,
      "loss": 0.0397,
      "step": 925
    },
    {
      "epoch": 14.82,
      "grad_norm": 1.4848726987838745,
      "learning_rate": 9.315089616631752e-06,
      "loss": 0.0203,
      "step": 926
    },
    {
      "epoch": 14.83,
      "grad_norm": 2.7229554653167725,
      "learning_rate": 9.313469292376781e-06,
      "loss": 0.0395,
      "step": 927
    },
    {
      "epoch": 14.85,
      "grad_norm": 6.0734968185424805,
      "learning_rate": 9.311847195017518e-06,
      "loss": 0.0156,
      "step": 928
    },
    {
      "epoch": 14.86,
      "grad_norm": 1.7953673601150513,
      "learning_rate": 9.310223325220747e-06,
      "loss": 0.0168,
      "step": 929
    },
    {
      "epoch": 14.88,
      "grad_norm": 1.593231439590454,
      "learning_rate": 9.308597683653976e-06,
      "loss": 0.0362,
      "step": 930
    },
    {
      "epoch": 14.9,
      "grad_norm": 2.504340171813965,
      "learning_rate": 9.306970270985449e-06,
      "loss": 0.0455,
      "step": 931
    },
    {
      "epoch": 14.91,
      "grad_norm": 2.5128841400146484,
      "learning_rate": 9.30534108788413e-06,
      "loss": 0.0161,
      "step": 932
    },
    {
      "epoch": 14.93,
      "grad_norm": 2.9533941745758057,
      "learning_rate": 9.30371013501972e-06,
      "loss": 0.0714,
      "step": 933
    },
    {
      "epoch": 14.94,
      "grad_norm": 0.12106013298034668,
      "learning_rate": 9.302077413062636e-06,
      "loss": 0.0009,
      "step": 934
    },
    {
      "epoch": 14.96,
      "grad_norm": 4.718937873840332,
      "learning_rate": 9.300442922684033e-06,
      "loss": 0.0085,
      "step": 935
    },
    {
      "epoch": 14.98,
      "grad_norm": 1.3156030178070068,
      "learning_rate": 9.298806664555783e-06,
      "loss": 0.0251,
      "step": 936
    },
    {
      "epoch": 14.99,
      "grad_norm": 0.7134014368057251,
      "learning_rate": 9.297168639350498e-06,
      "loss": 0.0081,
      "step": 937
    },
    {
      "epoch": 15.01,
      "grad_norm": 0.4737856686115265,
      "learning_rate": 9.295528847741501e-06,
      "loss": 0.0036,
      "step": 938
    },
    {
      "epoch": 15.02,
      "grad_norm": 2.0899157524108887,
      "learning_rate": 9.293887290402853e-06,
      "loss": 0.0308,
      "step": 939
    },
    {
      "epoch": 15.04,
      "grad_norm": 0.5794583559036255,
      "learning_rate": 9.292243968009332e-06,
      "loss": 0.0054,
      "step": 940
    },
    {
      "epoch": 15.06,
      "grad_norm": 1.3292195796966553,
      "learning_rate": 9.290598881236448e-06,
      "loss": 0.0276,
      "step": 941
    },
    {
      "epoch": 15.07,
      "grad_norm": 3.73732328414917,
      "learning_rate": 9.288952030760434e-06,
      "loss": 0.0245,
      "step": 942
    },
    {
      "epoch": 15.09,
      "grad_norm": 0.13564202189445496,
      "learning_rate": 9.287303417258248e-06,
      "loss": 0.001,
      "step": 943
    },
    {
      "epoch": 15.1,
      "grad_norm": 0.23728124797344208,
      "learning_rate": 9.285653041407575e-06,
      "loss": 0.003,
      "step": 944
    },
    {
      "epoch": 15.12,
      "grad_norm": 1.9837119579315186,
      "learning_rate": 9.284000903886818e-06,
      "loss": 0.0117,
      "step": 945
    },
    {
      "epoch": 15.14,
      "grad_norm": 0.9948961138725281,
      "learning_rate": 9.282347005375111e-06,
      "loss": 0.0116,
      "step": 946
    },
    {
      "epoch": 15.15,
      "grad_norm": 1.5511993169784546,
      "learning_rate": 9.280691346552308e-06,
      "loss": 0.0329,
      "step": 947
    },
    {
      "epoch": 15.17,
      "grad_norm": 0.701704204082489,
      "learning_rate": 9.27903392809899e-06,
      "loss": 0.0085,
      "step": 948
    },
    {
      "epoch": 15.18,
      "grad_norm": 1.7725701332092285,
      "learning_rate": 9.277374750696455e-06,
      "loss": 0.008,
      "step": 949
    },
    {
      "epoch": 15.2,
      "grad_norm": 7.921988487243652,
      "learning_rate": 9.275713815026732e-06,
      "loss": 0.0498,
      "step": 950
    },
    {
      "epoch": 15.22,
      "grad_norm": 1.340278148651123,
      "learning_rate": 9.274051121772568e-06,
      "loss": 0.0113,
      "step": 951
    },
    {
      "epoch": 15.23,
      "grad_norm": 0.675719141960144,
      "learning_rate": 9.272386671617431e-06,
      "loss": 0.0039,
      "step": 952
    },
    {
      "epoch": 15.25,
      "grad_norm": 2.705252170562744,
      "learning_rate": 9.270720465245515e-06,
      "loss": 0.0135,
      "step": 953
    },
    {
      "epoch": 15.26,
      "grad_norm": 0.4206753373146057,
      "learning_rate": 9.269052503341737e-06,
      "loss": 0.0042,
      "step": 954
    },
    {
      "epoch": 15.28,
      "grad_norm": 1.3464508056640625,
      "learning_rate": 9.26738278659173e-06,
      "loss": 0.014,
      "step": 955
    },
    {
      "epoch": 15.3,
      "grad_norm": 0.9225562810897827,
      "learning_rate": 9.265711315681853e-06,
      "loss": 0.0112,
      "step": 956
    },
    {
      "epoch": 15.31,
      "grad_norm": 3.3397514820098877,
      "learning_rate": 9.264038091299185e-06,
      "loss": 0.0126,
      "step": 957
    },
    {
      "epoch": 15.33,
      "grad_norm": 0.3162444829940796,
      "learning_rate": 9.262363114131523e-06,
      "loss": 0.0024,
      "step": 958
    },
    {
      "epoch": 15.34,
      "grad_norm": 1.03169846534729,
      "learning_rate": 9.260686384867388e-06,
      "loss": 0.0074,
      "step": 959
    },
    {
      "epoch": 15.36,
      "grad_norm": 2.3111660480499268,
      "learning_rate": 9.259007904196023e-06,
      "loss": 0.0504,
      "step": 960
    },
    {
      "epoch": 15.38,
      "grad_norm": 0.8345363140106201,
      "learning_rate": 9.257327672807383e-06,
      "loss": 0.0022,
      "step": 961
    },
    {
      "epoch": 15.39,
      "grad_norm": 0.45969101786613464,
      "learning_rate": 9.25564569139215e-06,
      "loss": 0.0071,
      "step": 962
    },
    {
      "epoch": 15.41,
      "grad_norm": 0.9940335750579834,
      "learning_rate": 9.253961960641724e-06,
      "loss": 0.0038,
      "step": 963
    },
    {
      "epoch": 15.42,
      "grad_norm": 0.5513010621070862,
      "learning_rate": 9.25227648124822e-06,
      "loss": 0.0079,
      "step": 964
    },
    {
      "epoch": 15.44,
      "grad_norm": 2.1082751750946045,
      "learning_rate": 9.250589253904481e-06,
      "loss": 0.0047,
      "step": 965
    },
    {
      "epoch": 15.46,
      "grad_norm": 1.435079574584961,
      "learning_rate": 9.248900279304056e-06,
      "loss": 0.0123,
      "step": 966
    },
    {
      "epoch": 15.47,
      "grad_norm": 1.0699031352996826,
      "learning_rate": 9.247209558141222e-06,
      "loss": 0.0083,
      "step": 967
    },
    {
      "epoch": 15.49,
      "grad_norm": 1.296701192855835,
      "learning_rate": 9.24551709111097e-06,
      "loss": 0.0166,
      "step": 968
    },
    {
      "epoch": 15.5,
      "grad_norm": 1.4805370569229126,
      "learning_rate": 9.243822878909007e-06,
      "loss": 0.0147,
      "step": 969
    },
    {
      "epoch": 15.52,
      "grad_norm": 0.24033713340759277,
      "learning_rate": 9.242126922231763e-06,
      "loss": 0.0013,
      "step": 970
    },
    {
      "epoch": 15.54,
      "grad_norm": 1.3409818410873413,
      "learning_rate": 9.240429221776382e-06,
      "loss": 0.0194,
      "step": 971
    },
    {
      "epoch": 15.55,
      "grad_norm": 1.539067268371582,
      "learning_rate": 9.23872977824072e-06,
      "loss": 0.0126,
      "step": 972
    },
    {
      "epoch": 15.57,
      "grad_norm": 0.9105733036994934,
      "learning_rate": 9.237028592323358e-06,
      "loss": 0.0051,
      "step": 973
    },
    {
      "epoch": 15.58,
      "grad_norm": 0.5284721255302429,
      "learning_rate": 9.23532566472359e-06,
      "loss": 0.0017,
      "step": 974
    },
    {
      "epoch": 15.6,
      "grad_norm": 1.9790081977844238,
      "learning_rate": 9.233620996141421e-06,
      "loss": 0.0272,
      "step": 975
    },
    {
      "epoch": 15.62,
      "grad_norm": 2.4536540508270264,
      "learning_rate": 9.231914587277579e-06,
      "loss": 0.0101,
      "step": 976
    },
    {
      "epoch": 15.63,
      "grad_norm": 1.3353092670440674,
      "learning_rate": 9.230206438833506e-06,
      "loss": 0.0039,
      "step": 977
    },
    {
      "epoch": 15.65,
      "grad_norm": 2.615837574005127,
      "learning_rate": 9.228496551511352e-06,
      "loss": 0.0178,
      "step": 978
    },
    {
      "epoch": 15.66,
      "grad_norm": 0.7857709527015686,
      "learning_rate": 9.226784926013991e-06,
      "loss": 0.0064,
      "step": 979
    },
    {
      "epoch": 15.68,
      "grad_norm": 0.865062415599823,
      "learning_rate": 9.225071563045007e-06,
      "loss": 0.0072,
      "step": 980
    },
    {
      "epoch": 15.7,
      "grad_norm": 6.2422590255737305,
      "learning_rate": 9.223356463308698e-06,
      "loss": 0.0095,
      "step": 981
    },
    {
      "epoch": 15.71,
      "grad_norm": 2.6998424530029297,
      "learning_rate": 9.221639627510076e-06,
      "loss": 0.0237,
      "step": 982
    },
    {
      "epoch": 15.73,
      "grad_norm": 3.1176135540008545,
      "learning_rate": 9.219921056354871e-06,
      "loss": 0.0152,
      "step": 983
    },
    {
      "epoch": 15.74,
      "grad_norm": 0.9108282923698425,
      "learning_rate": 9.218200750549517e-06,
      "loss": 0.0145,
      "step": 984
    },
    {
      "epoch": 15.76,
      "grad_norm": 0.4536166787147522,
      "learning_rate": 9.216478710801171e-06,
      "loss": 0.0052,
      "step": 985
    },
    {
      "epoch": 15.78,
      "grad_norm": 1.5123050212860107,
      "learning_rate": 9.214754937817697e-06,
      "loss": 0.0089,
      "step": 986
    },
    {
      "epoch": 15.79,
      "grad_norm": 4.8354058265686035,
      "learning_rate": 9.213029432307673e-06,
      "loss": 0.0357,
      "step": 987
    },
    {
      "epoch": 15.81,
      "grad_norm": 2.5627613067626953,
      "learning_rate": 9.211302194980391e-06,
      "loss": 0.0372,
      "step": 988
    },
    {
      "epoch": 15.82,
      "grad_norm": 3.39286208152771,
      "learning_rate": 9.209573226545852e-06,
      "loss": 0.0252,
      "step": 989
    },
    {
      "epoch": 15.84,
      "grad_norm": 9.455162048339844,
      "learning_rate": 9.207842527714767e-06,
      "loss": 0.0442,
      "step": 990
    },
    {
      "epoch": 15.86,
      "grad_norm": 3.5557773113250732,
      "learning_rate": 9.206110099198563e-06,
      "loss": 0.0299,
      "step": 991
    },
    {
      "epoch": 15.87,
      "grad_norm": 2.0365939140319824,
      "learning_rate": 9.204375941709377e-06,
      "loss": 0.0219,
      "step": 992
    },
    {
      "epoch": 15.89,
      "grad_norm": 1.13972008228302,
      "learning_rate": 9.202640055960053e-06,
      "loss": 0.0125,
      "step": 993
    },
    {
      "epoch": 15.9,
      "grad_norm": 1.5395395755767822,
      "learning_rate": 9.20090244266415e-06,
      "loss": 0.0212,
      "step": 994
    },
    {
      "epoch": 15.92,
      "grad_norm": 0.48710140585899353,
      "learning_rate": 9.199163102535937e-06,
      "loss": 0.0026,
      "step": 995
    },
    {
      "epoch": 15.94,
      "grad_norm": 2.773149251937866,
      "learning_rate": 9.197422036290386e-06,
      "loss": 0.024,
      "step": 996
    },
    {
      "epoch": 15.95,
      "grad_norm": 1.1964126825332642,
      "learning_rate": 9.195679244643188e-06,
      "loss": 0.0187,
      "step": 997
    },
    {
      "epoch": 15.97,
      "grad_norm": 0.536987841129303,
      "learning_rate": 9.193934728310737e-06,
      "loss": 0.0052,
      "step": 998
    },
    {
      "epoch": 15.98,
      "grad_norm": 0.349479079246521,
      "learning_rate": 9.192188488010139e-06,
      "loss": 0.0043,
      "step": 999
    },
    {
      "epoch": 16.0,
      "grad_norm": 0.5028808116912842,
      "learning_rate": 9.190440524459203e-06,
      "loss": 0.0043,
      "step": 1000
    },
    {
      "epoch": 16.02,
      "grad_norm": 0.23334208130836487,
      "learning_rate": 9.188690838376457e-06,
      "loss": 0.0014,
      "step": 1001
    },
    {
      "epoch": 16.03,
      "grad_norm": 1.242781639099121,
      "learning_rate": 9.186939430481128e-06,
      "loss": 0.0096,
      "step": 1002
    },
    {
      "epoch": 16.05,
      "grad_norm": 3.538210391998291,
      "learning_rate": 9.185186301493153e-06,
      "loss": 0.0063,
      "step": 1003
    },
    {
      "epoch": 16.06,
      "grad_norm": 0.8795598745346069,
      "learning_rate": 9.183431452133177e-06,
      "loss": 0.012,
      "step": 1004
    },
    {
      "epoch": 16.08,
      "grad_norm": 1.852497935295105,
      "learning_rate": 9.181674883122554e-06,
      "loss": 0.0279,
      "step": 1005
    },
    {
      "epoch": 16.1,
      "grad_norm": 1.7845423221588135,
      "learning_rate": 9.179916595183342e-06,
      "loss": 0.0133,
      "step": 1006
    },
    {
      "epoch": 16.11,
      "grad_norm": 0.8455705642700195,
      "learning_rate": 9.178156589038307e-06,
      "loss": 0.0112,
      "step": 1007
    },
    {
      "epoch": 16.13,
      "grad_norm": 0.9205543398857117,
      "learning_rate": 9.176394865410922e-06,
      "loss": 0.0109,
      "step": 1008
    },
    {
      "epoch": 16.14,
      "grad_norm": 9.351475715637207,
      "learning_rate": 9.174631425025363e-06,
      "loss": 0.0417,
      "step": 1009
    },
    {
      "epoch": 16.16,
      "grad_norm": 0.08612806349992752,
      "learning_rate": 9.172866268606514e-06,
      "loss": 0.0009,
      "step": 1010
    },
    {
      "epoch": 16.18,
      "grad_norm": 0.08063888549804688,
      "learning_rate": 9.171099396879966e-06,
      "loss": 0.001,
      "step": 1011
    },
    {
      "epoch": 16.19,
      "grad_norm": 0.4007175862789154,
      "learning_rate": 9.169330810572012e-06,
      "loss": 0.0028,
      "step": 1012
    },
    {
      "epoch": 16.21,
      "grad_norm": 0.827393651008606,
      "learning_rate": 9.167560510409649e-06,
      "loss": 0.0072,
      "step": 1013
    },
    {
      "epoch": 16.22,
      "grad_norm": 1.2216647863388062,
      "learning_rate": 9.165788497120587e-06,
      "loss": 0.0206,
      "step": 1014
    },
    {
      "epoch": 16.24,
      "grad_norm": 0.19559632241725922,
      "learning_rate": 9.164014771433228e-06,
      "loss": 0.0018,
      "step": 1015
    },
    {
      "epoch": 16.26,
      "grad_norm": 3.5686306953430176,
      "learning_rate": 9.162239334076684e-06,
      "loss": 0.0446,
      "step": 1016
    },
    {
      "epoch": 16.27,
      "grad_norm": 0.4494299292564392,
      "learning_rate": 9.16046218578077e-06,
      "loss": 0.0033,
      "step": 1017
    },
    {
      "epoch": 16.29,
      "grad_norm": 0.9305359721183777,
      "learning_rate": 9.158683327276008e-06,
      "loss": 0.0157,
      "step": 1018
    },
    {
      "epoch": 16.3,
      "grad_norm": 1.4995273351669312,
      "learning_rate": 9.156902759293616e-06,
      "loss": 0.0142,
      "step": 1019
    },
    {
      "epoch": 16.32,
      "grad_norm": 1.418704867362976,
      "learning_rate": 9.15512048256552e-06,
      "loss": 0.0223,
      "step": 1020
    },
    {
      "epoch": 16.34,
      "grad_norm": 2.452268600463867,
      "learning_rate": 9.153336497824348e-06,
      "loss": 0.0151,
      "step": 1021
    },
    {
      "epoch": 16.35,
      "grad_norm": 1.1749018430709839,
      "learning_rate": 9.151550805803424e-06,
      "loss": 0.0158,
      "step": 1022
    },
    {
      "epoch": 16.37,
      "grad_norm": 0.22307825088500977,
      "learning_rate": 9.149763407236785e-06,
      "loss": 0.0009,
      "step": 1023
    },
    {
      "epoch": 16.38,
      "grad_norm": 1.1407535076141357,
      "learning_rate": 9.147974302859158e-06,
      "loss": 0.0042,
      "step": 1024
    },
    {
      "epoch": 16.4,
      "grad_norm": 0.6756918430328369,
      "learning_rate": 9.146183493405976e-06,
      "loss": 0.0057,
      "step": 1025
    },
    {
      "epoch": 16.42,
      "grad_norm": 1.4610663652420044,
      "learning_rate": 9.144390979613376e-06,
      "loss": 0.017,
      "step": 1026
    },
    {
      "epoch": 16.43,
      "grad_norm": 1.741824984550476,
      "learning_rate": 9.142596762218192e-06,
      "loss": 0.0292,
      "step": 1027
    },
    {
      "epoch": 16.45,
      "grad_norm": 0.11484470218420029,
      "learning_rate": 9.140800841957958e-06,
      "loss": 0.001,
      "step": 1028
    },
    {
      "epoch": 16.46,
      "grad_norm": 0.9143111109733582,
      "learning_rate": 9.139003219570911e-06,
      "loss": 0.0141,
      "step": 1029
    },
    {
      "epoch": 16.48,
      "grad_norm": 0.3649626672267914,
      "learning_rate": 9.137203895795983e-06,
      "loss": 0.0017,
      "step": 1030
    },
    {
      "epoch": 16.5,
      "grad_norm": 0.18787603080272675,
      "learning_rate": 9.13540287137281e-06,
      "loss": 0.0021,
      "step": 1031
    },
    {
      "epoch": 16.51,
      "grad_norm": 1.061951756477356,
      "learning_rate": 9.133600147041723e-06,
      "loss": 0.017,
      "step": 1032
    },
    {
      "epoch": 16.53,
      "grad_norm": 1.1175307035446167,
      "learning_rate": 9.131795723543757e-06,
      "loss": 0.0167,
      "step": 1033
    },
    {
      "epoch": 16.54,
      "grad_norm": 0.8014785051345825,
      "learning_rate": 9.12998960162064e-06,
      "loss": 0.0073,
      "step": 1034
    },
    {
      "epoch": 16.56,
      "grad_norm": 0.4240367114543915,
      "learning_rate": 9.128181782014801e-06,
      "loss": 0.0029,
      "step": 1035
    },
    {
      "epoch": 16.58,
      "grad_norm": 3.3089592456817627,
      "learning_rate": 9.126372265469368e-06,
      "loss": 0.0034,
      "step": 1036
    },
    {
      "epoch": 16.59,
      "grad_norm": 0.40693214535713196,
      "learning_rate": 9.12456105272816e-06,
      "loss": 0.0039,
      "step": 1037
    },
    {
      "epoch": 16.61,
      "grad_norm": 2.3018617630004883,
      "learning_rate": 9.122748144535704e-06,
      "loss": 0.0455,
      "step": 1038
    },
    {
      "epoch": 16.62,
      "grad_norm": 2.208984851837158,
      "learning_rate": 9.120933541637215e-06,
      "loss": 0.0364,
      "step": 1039
    },
    {
      "epoch": 16.64,
      "grad_norm": 1.367403507232666,
      "learning_rate": 9.119117244778609e-06,
      "loss": 0.0115,
      "step": 1040
    },
    {
      "epoch": 16.66,
      "grad_norm": 2.4919018745422363,
      "learning_rate": 9.117299254706496e-06,
      "loss": 0.0167,
      "step": 1041
    },
    {
      "epoch": 16.67,
      "grad_norm": 1.4957479238510132,
      "learning_rate": 9.115479572168183e-06,
      "loss": 0.0155,
      "step": 1042
    },
    {
      "epoch": 16.69,
      "grad_norm": 2.0238289833068848,
      "learning_rate": 9.113658197911673e-06,
      "loss": 0.0316,
      "step": 1043
    },
    {
      "epoch": 16.7,
      "grad_norm": 1.126781940460205,
      "learning_rate": 9.111835132685665e-06,
      "loss": 0.0159,
      "step": 1044
    },
    {
      "epoch": 16.72,
      "grad_norm": 0.7073091268539429,
      "learning_rate": 9.110010377239552e-06,
      "loss": 0.0037,
      "step": 1045
    },
    {
      "epoch": 16.74,
      "grad_norm": 0.9284394383430481,
      "learning_rate": 9.10818393232342e-06,
      "loss": 0.0084,
      "step": 1046
    },
    {
      "epoch": 16.75,
      "grad_norm": 0.9759916067123413,
      "learning_rate": 9.106355798688052e-06,
      "loss": 0.0153,
      "step": 1047
    },
    {
      "epoch": 16.77,
      "grad_norm": 2.25189471244812,
      "learning_rate": 9.104525977084928e-06,
      "loss": 0.0178,
      "step": 1048
    },
    {
      "epoch": 16.78,
      "grad_norm": 2.2545273303985596,
      "learning_rate": 9.102694468266216e-06,
      "loss": 0.0463,
      "step": 1049
    },
    {
      "epoch": 16.8,
      "grad_norm": 1.1511677503585815,
      "learning_rate": 9.10086127298478e-06,
      "loss": 0.0087,
      "step": 1050
    },
    {
      "epoch": 16.82,
      "grad_norm": 1.9137078523635864,
      "learning_rate": 9.09902639199418e-06,
      "loss": 0.0231,
      "step": 1051
    },
    {
      "epoch": 16.83,
      "grad_norm": 0.3439532518386841,
      "learning_rate": 9.09718982604866e-06,
      "loss": 0.0021,
      "step": 1052
    },
    {
      "epoch": 16.85,
      "grad_norm": 9.471895217895508,
      "learning_rate": 9.095351575903168e-06,
      "loss": 0.0439,
      "step": 1053
    },
    {
      "epoch": 16.86,
      "grad_norm": 2.03503680229187,
      "learning_rate": 9.09351164231334e-06,
      "loss": 0.0292,
      "step": 1054
    },
    {
      "epoch": 16.88,
      "grad_norm": 0.29225221276283264,
      "learning_rate": 9.0916700260355e-06,
      "loss": 0.0029,
      "step": 1055
    },
    {
      "epoch": 16.9,
      "grad_norm": 0.3869774043560028,
      "learning_rate": 9.08982672782667e-06,
      "loss": 0.0017,
      "step": 1056
    },
    {
      "epoch": 16.91,
      "grad_norm": 0.9376232624053955,
      "learning_rate": 9.087981748444556e-06,
      "loss": 0.0096,
      "step": 1057
    },
    {
      "epoch": 16.93,
      "grad_norm": 1.7189509868621826,
      "learning_rate": 9.086135088647563e-06,
      "loss": 0.0167,
      "step": 1058
    },
    {
      "epoch": 16.94,
      "grad_norm": 1.4138273000717163,
      "learning_rate": 9.084286749194783e-06,
      "loss": 0.0048,
      "step": 1059
    },
    {
      "epoch": 16.96,
      "grad_norm": 2.0085620880126953,
      "learning_rate": 9.082436730845993e-06,
      "loss": 0.0458,
      "step": 1060
    },
    {
      "epoch": 16.98,
      "grad_norm": 0.8881579637527466,
      "learning_rate": 9.080585034361675e-06,
      "loss": 0.0148,
      "step": 1061
    },
    {
      "epoch": 16.99,
      "grad_norm": 0.30392885208129883,
      "learning_rate": 9.078731660502983e-06,
      "loss": 0.0021,
      "step": 1062
    },
    {
      "epoch": 17.01,
      "grad_norm": 0.20498080551624298,
      "learning_rate": 9.076876610031776e-06,
      "loss": 0.0024,
      "step": 1063
    },
    {
      "epoch": 17.02,
      "grad_norm": 1.0776851177215576,
      "learning_rate": 9.07501988371059e-06,
      "loss": 0.0146,
      "step": 1064
    },
    {
      "epoch": 17.04,
      "grad_norm": 1.177596926689148,
      "learning_rate": 9.073161482302656e-06,
      "loss": 0.0045,
      "step": 1065
    },
    {
      "epoch": 17.06,
      "grad_norm": 0.13385258615016937,
      "learning_rate": 9.071301406571893e-06,
      "loss": 0.001,
      "step": 1066
    },
    {
      "epoch": 17.07,
      "grad_norm": 0.9974989295005798,
      "learning_rate": 9.06943965728291e-06,
      "loss": 0.0103,
      "step": 1067
    },
    {
      "epoch": 17.09,
      "grad_norm": 0.41776761412620544,
      "learning_rate": 9.067576235200999e-06,
      "loss": 0.0026,
      "step": 1068
    },
    {
      "epoch": 17.1,
      "grad_norm": 1.4333282709121704,
      "learning_rate": 9.065711141092144e-06,
      "loss": 0.0046,
      "step": 1069
    },
    {
      "epoch": 17.12,
      "grad_norm": 2.1126139163970947,
      "learning_rate": 9.063844375723014e-06,
      "loss": 0.0405,
      "step": 1070
    },
    {
      "epoch": 17.14,
      "grad_norm": 1.418983817100525,
      "learning_rate": 9.061975939860966e-06,
      "loss": 0.0142,
      "step": 1071
    },
    {
      "epoch": 17.15,
      "grad_norm": 0.2548023462295532,
      "learning_rate": 9.060105834274044e-06,
      "loss": 0.004,
      "step": 1072
    },
    {
      "epoch": 17.17,
      "grad_norm": 1.0900533199310303,
      "learning_rate": 9.058234059730977e-06,
      "loss": 0.0073,
      "step": 1073
    },
    {
      "epoch": 17.18,
      "grad_norm": 0.19399651885032654,
      "learning_rate": 9.05636061700118e-06,
      "loss": 0.0026,
      "step": 1074
    },
    {
      "epoch": 17.2,
      "grad_norm": 1.4290109872817993,
      "learning_rate": 9.054485506854756e-06,
      "loss": 0.018,
      "step": 1075
    },
    {
      "epoch": 17.22,
      "grad_norm": 1.5326536893844604,
      "learning_rate": 9.05260873006249e-06,
      "loss": 0.0061,
      "step": 1076
    },
    {
      "epoch": 17.23,
      "grad_norm": 1.0657017230987549,
      "learning_rate": 9.050730287395858e-06,
      "loss": 0.0059,
      "step": 1077
    },
    {
      "epoch": 17.25,
      "grad_norm": 4.001807689666748,
      "learning_rate": 9.048850179627013e-06,
      "loss": 0.0084,
      "step": 1078
    },
    {
      "epoch": 17.26,
      "grad_norm": 2.4036130905151367,
      "learning_rate": 9.046968407528797e-06,
      "loss": 0.0426,
      "step": 1079
    },
    {
      "epoch": 17.28,
      "grad_norm": 2.6156609058380127,
      "learning_rate": 9.045084971874738e-06,
      "loss": 0.0333,
      "step": 1080
    },
    {
      "epoch": 17.3,
      "grad_norm": 0.41684553027153015,
      "learning_rate": 9.04319987343904e-06,
      "loss": 0.0011,
      "step": 1081
    },
    {
      "epoch": 17.31,
      "grad_norm": 0.28653043508529663,
      "learning_rate": 9.041313112996604e-06,
      "loss": 0.0009,
      "step": 1082
    },
    {
      "epoch": 17.33,
      "grad_norm": 0.20434701442718506,
      "learning_rate": 9.039424691322998e-06,
      "loss": 0.0025,
      "step": 1083
    },
    {
      "epoch": 17.34,
      "grad_norm": 1.5237797498703003,
      "learning_rate": 9.037534609194482e-06,
      "loss": 0.0246,
      "step": 1084
    },
    {
      "epoch": 17.36,
      "grad_norm": 0.8436790704727173,
      "learning_rate": 9.035642867388003e-06,
      "loss": 0.0162,
      "step": 1085
    },
    {
      "epoch": 17.38,
      "grad_norm": 0.39237701892852783,
      "learning_rate": 9.033749466681178e-06,
      "loss": 0.0014,
      "step": 1086
    },
    {
      "epoch": 17.39,
      "grad_norm": 0.7343915700912476,
      "learning_rate": 9.031854407852317e-06,
      "loss": 0.0052,
      "step": 1087
    },
    {
      "epoch": 17.41,
      "grad_norm": 5.276886940002441,
      "learning_rate": 9.029957691680404e-06,
      "loss": 0.0256,
      "step": 1088
    },
    {
      "epoch": 17.42,
      "grad_norm": 0.04181262478232384,
      "learning_rate": 9.02805931894511e-06,
      "loss": 0.0005,
      "step": 1089
    },
    {
      "epoch": 17.44,
      "grad_norm": 1.3325403928756714,
      "learning_rate": 9.026159290426782e-06,
      "loss": 0.0202,
      "step": 1090
    },
    {
      "epoch": 17.46,
      "grad_norm": 0.3515317440032959,
      "learning_rate": 9.024257606906452e-06,
      "loss": 0.0046,
      "step": 1091
    },
    {
      "epoch": 17.47,
      "grad_norm": 1.3367464542388916,
      "learning_rate": 9.022354269165828e-06,
      "loss": 0.0254,
      "step": 1092
    },
    {
      "epoch": 17.49,
      "grad_norm": 1.0290104150772095,
      "learning_rate": 9.020449277987302e-06,
      "loss": 0.0076,
      "step": 1093
    },
    {
      "epoch": 17.5,
      "grad_norm": 1.2548720836639404,
      "learning_rate": 9.018542634153944e-06,
      "loss": 0.015,
      "step": 1094
    },
    {
      "epoch": 17.52,
      "grad_norm": 1.9364339113235474,
      "learning_rate": 9.016634338449504e-06,
      "loss": 0.0329,
      "step": 1095
    },
    {
      "epoch": 17.54,
      "grad_norm": 1.229857325553894,
      "learning_rate": 9.014724391658407e-06,
      "loss": 0.0094,
      "step": 1096
    },
    {
      "epoch": 17.55,
      "grad_norm": 0.5788080096244812,
      "learning_rate": 9.012812794565762e-06,
      "loss": 0.0044,
      "step": 1097
    },
    {
      "epoch": 17.57,
      "grad_norm": 3.2801756858825684,
      "learning_rate": 9.010899547957354e-06,
      "loss": 0.0055,
      "step": 1098
    },
    {
      "epoch": 17.58,
      "grad_norm": 1.502677083015442,
      "learning_rate": 9.008984652619649e-06,
      "loss": 0.035,
      "step": 1099
    },
    {
      "epoch": 17.6,
      "grad_norm": 1.1091415882110596,
      "learning_rate": 9.007068109339783e-06,
      "loss": 0.0201,
      "step": 1100
    },
    {
      "epoch": 17.62,
      "grad_norm": 0.45499297976493835,
      "learning_rate": 9.00514991890558e-06,
      "loss": 0.0044,
      "step": 1101
    },
    {
      "epoch": 17.63,
      "grad_norm": 1.1431163549423218,
      "learning_rate": 9.003230082105532e-06,
      "loss": 0.011,
      "step": 1102
    },
    {
      "epoch": 17.65,
      "grad_norm": 1.7378535270690918,
      "learning_rate": 9.001308599728813e-06,
      "loss": 0.0306,
      "step": 1103
    },
    {
      "epoch": 17.66,
      "grad_norm": 0.3583339750766754,
      "learning_rate": 8.999385472565271e-06,
      "loss": 0.0017,
      "step": 1104
    },
    {
      "epoch": 17.68,
      "grad_norm": 0.14395040273666382,
      "learning_rate": 8.997460701405431e-06,
      "loss": 0.0011,
      "step": 1105
    },
    {
      "epoch": 17.7,
      "grad_norm": 1.0589030981063843,
      "learning_rate": 8.995534287040494e-06,
      "loss": 0.0129,
      "step": 1106
    },
    {
      "epoch": 17.71,
      "grad_norm": 0.9928015470504761,
      "learning_rate": 8.993606230262335e-06,
      "loss": 0.0054,
      "step": 1107
    },
    {
      "epoch": 17.73,
      "grad_norm": 0.3201795220375061,
      "learning_rate": 8.991676531863507e-06,
      "loss": 0.0019,
      "step": 1108
    },
    {
      "epoch": 17.74,
      "grad_norm": 0.2827489674091339,
      "learning_rate": 8.989745192637237e-06,
      "loss": 0.0011,
      "step": 1109
    },
    {
      "epoch": 17.76,
      "grad_norm": 2.529055118560791,
      "learning_rate": 8.987812213377423e-06,
      "loss": 0.0033,
      "step": 1110
    },
    {
      "epoch": 17.78,
      "grad_norm": 0.2905406653881073,
      "learning_rate": 8.985877594878642e-06,
      "loss": 0.0025,
      "step": 1111
    },
    {
      "epoch": 17.79,
      "grad_norm": 1.2575256824493408,
      "learning_rate": 8.98394133793614e-06,
      "loss": 0.0138,
      "step": 1112
    },
    {
      "epoch": 17.81,
      "grad_norm": 1.805618405342102,
      "learning_rate": 8.982003443345841e-06,
      "loss": 0.0208,
      "step": 1113
    },
    {
      "epoch": 17.82,
      "grad_norm": 3.797236204147339,
      "learning_rate": 8.98006391190434e-06,
      "loss": 0.0077,
      "step": 1114
    },
    {
      "epoch": 17.84,
      "grad_norm": 0.7788085341453552,
      "learning_rate": 8.978122744408905e-06,
      "loss": 0.0016,
      "step": 1115
    },
    {
      "epoch": 17.86,
      "grad_norm": 5.090478420257568,
      "learning_rate": 8.976179941657478e-06,
      "loss": 0.0205,
      "step": 1116
    },
    {
      "epoch": 17.87,
      "grad_norm": 3.7617831230163574,
      "learning_rate": 8.97423550444867e-06,
      "loss": 0.0366,
      "step": 1117
    },
    {
      "epoch": 17.89,
      "grad_norm": 0.3201581835746765,
      "learning_rate": 8.972289433581765e-06,
      "loss": 0.0032,
      "step": 1118
    },
    {
      "epoch": 17.9,
      "grad_norm": 1.0075178146362305,
      "learning_rate": 8.97034172985672e-06,
      "loss": 0.0057,
      "step": 1119
    },
    {
      "epoch": 17.92,
      "grad_norm": 0.18560434877872467,
      "learning_rate": 8.968392394074164e-06,
      "loss": 0.0011,
      "step": 1120
    },
    {
      "epoch": 17.94,
      "grad_norm": 1.0988361835479736,
      "learning_rate": 8.966441427035392e-06,
      "loss": 0.0187,
      "step": 1121
    },
    {
      "epoch": 17.95,
      "grad_norm": 1.8105908632278442,
      "learning_rate": 8.964488829542377e-06,
      "loss": 0.029,
      "step": 1122
    },
    {
      "epoch": 17.97,
      "grad_norm": 0.37482795119285583,
      "learning_rate": 8.962534602397756e-06,
      "loss": 0.0021,
      "step": 1123
    },
    {
      "epoch": 17.98,
      "grad_norm": 1.796521544456482,
      "learning_rate": 8.960578746404837e-06,
      "loss": 0.0194,
      "step": 1124
    },
    {
      "epoch": 18.0,
      "grad_norm": 18.813383102416992,
      "learning_rate": 8.9586212623676e-06,
      "loss": 0.1672,
      "step": 1125
    },
    {
      "epoch": 18.02,
      "grad_norm": 0.6181356310844421,
      "learning_rate": 8.95666215109069e-06,
      "loss": 0.0061,
      "step": 1126
    },
    {
      "epoch": 18.03,
      "grad_norm": 0.08522050082683563,
      "learning_rate": 8.95470141337943e-06,
      "loss": 0.0012,
      "step": 1127
    },
    {
      "epoch": 18.05,
      "grad_norm": 2.319288969039917,
      "learning_rate": 8.9527390500398e-06,
      "loss": 0.0154,
      "step": 1128
    },
    {
      "epoch": 18.06,
      "grad_norm": 0.9634143114089966,
      "learning_rate": 8.950775061878453e-06,
      "loss": 0.0089,
      "step": 1129
    },
    {
      "epoch": 18.08,
      "grad_norm": 0.09827161580324173,
      "learning_rate": 8.948809449702712e-06,
      "loss": 0.001,
      "step": 1130
    },
    {
      "epoch": 18.1,
      "grad_norm": 0.22268348932266235,
      "learning_rate": 8.946842214320566e-06,
      "loss": 0.0011,
      "step": 1131
    },
    {
      "epoch": 18.11,
      "grad_norm": 2.1747028827667236,
      "learning_rate": 8.944873356540671e-06,
      "loss": 0.0411,
      "step": 1132
    },
    {
      "epoch": 18.13,
      "grad_norm": 1.197014570236206,
      "learning_rate": 8.94290287717235e-06,
      "loss": 0.0137,
      "step": 1133
    },
    {
      "epoch": 18.14,
      "grad_norm": 0.6529809236526489,
      "learning_rate": 8.940930777025594e-06,
      "loss": 0.0033,
      "step": 1134
    },
    {
      "epoch": 18.16,
      "grad_norm": 2.308014392852783,
      "learning_rate": 8.938957056911057e-06,
      "loss": 0.015,
      "step": 1135
    },
    {
      "epoch": 18.18,
      "grad_norm": 0.6934324502944946,
      "learning_rate": 8.936981717640061e-06,
      "loss": 0.0046,
      "step": 1136
    },
    {
      "epoch": 18.19,
      "grad_norm": 0.24204829335212708,
      "learning_rate": 8.935004760024592e-06,
      "loss": 0.0018,
      "step": 1137
    },
    {
      "epoch": 18.21,
      "grad_norm": 1.1639269590377808,
      "learning_rate": 8.933026184877308e-06,
      "loss": 0.0131,
      "step": 1138
    },
    {
      "epoch": 18.22,
      "grad_norm": 1.3095917701721191,
      "learning_rate": 8.93104599301152e-06,
      "loss": 0.0037,
      "step": 1139
    },
    {
      "epoch": 18.24,
      "grad_norm": 1.5576715469360352,
      "learning_rate": 8.929064185241214e-06,
      "loss": 0.0262,
      "step": 1140
    },
    {
      "epoch": 18.26,
      "grad_norm": 6.208588600158691,
      "learning_rate": 8.927080762381033e-06,
      "loss": 0.0172,
      "step": 1141
    },
    {
      "epoch": 18.27,
      "grad_norm": 0.7184385657310486,
      "learning_rate": 8.925095725246291e-06,
      "loss": 0.0062,
      "step": 1142
    },
    {
      "epoch": 18.29,
      "grad_norm": 1.7904037237167358,
      "learning_rate": 8.92310907465296e-06,
      "loss": 0.0354,
      "step": 1143
    },
    {
      "epoch": 18.3,
      "grad_norm": 1.172080636024475,
      "learning_rate": 8.921120811417678e-06,
      "loss": 0.0128,
      "step": 1144
    },
    {
      "epoch": 18.32,
      "grad_norm": 1.3160159587860107,
      "learning_rate": 8.919130936357743e-06,
      "loss": 0.0113,
      "step": 1145
    },
    {
      "epoch": 18.34,
      "grad_norm": 1.400455117225647,
      "learning_rate": 8.917139450291119e-06,
      "loss": 0.0177,
      "step": 1146
    },
    {
      "epoch": 18.35,
      "grad_norm": 0.8268936276435852,
      "learning_rate": 8.91514635403643e-06,
      "loss": 0.0102,
      "step": 1147
    },
    {
      "epoch": 18.37,
      "grad_norm": 0.9733471274375916,
      "learning_rate": 8.913151648412963e-06,
      "loss": 0.0119,
      "step": 1148
    },
    {
      "epoch": 18.38,
      "grad_norm": 3.2026827335357666,
      "learning_rate": 8.911155334240667e-06,
      "loss": 0.004,
      "step": 1149
    },
    {
      "epoch": 18.4,
      "grad_norm": 4.9615607261657715,
      "learning_rate": 8.90915741234015e-06,
      "loss": 0.0206,
      "step": 1150
    },
    {
      "epoch": 18.42,
      "grad_norm": 1.145661473274231,
      "learning_rate": 8.907157883532682e-06,
      "loss": 0.0094,
      "step": 1151
    },
    {
      "epoch": 18.43,
      "grad_norm": 1.812587857246399,
      "learning_rate": 8.905156748640194e-06,
      "loss": 0.0057,
      "step": 1152
    },
    {
      "epoch": 18.45,
      "grad_norm": 0.7507002949714661,
      "learning_rate": 8.903154008485278e-06,
      "loss": 0.0076,
      "step": 1153
    },
    {
      "epoch": 18.46,
      "grad_norm": 5.954523086547852,
      "learning_rate": 8.901149663891184e-06,
      "loss": 0.0719,
      "step": 1154
    },
    {
      "epoch": 18.48,
      "grad_norm": 0.938245415687561,
      "learning_rate": 8.899143715681822e-06,
      "loss": 0.0033,
      "step": 1155
    },
    {
      "epoch": 18.5,
      "grad_norm": 1.102414846420288,
      "learning_rate": 8.897136164681763e-06,
      "loss": 0.0094,
      "step": 1156
    },
    {
      "epoch": 18.51,
      "grad_norm": 0.17822319269180298,
      "learning_rate": 8.895127011716234e-06,
      "loss": 0.0013,
      "step": 1157
    },
    {
      "epoch": 18.53,
      "grad_norm": 0.9582459926605225,
      "learning_rate": 8.893116257611122e-06,
      "loss": 0.0163,
      "step": 1158
    },
    {
      "epoch": 18.54,
      "grad_norm": 1.5839859247207642,
      "learning_rate": 8.891103903192972e-06,
      "loss": 0.0113,
      "step": 1159
    },
    {
      "epoch": 18.56,
      "grad_norm": 1.0896960496902466,
      "learning_rate": 8.889089949288986e-06,
      "loss": 0.0034,
      "step": 1160
    },
    {
      "epoch": 18.58,
      "grad_norm": 1.751421332359314,
      "learning_rate": 8.88707439672703e-06,
      "loss": 0.0281,
      "step": 1161
    },
    {
      "epoch": 18.59,
      "grad_norm": 0.7456004619598389,
      "learning_rate": 8.885057246335614e-06,
      "loss": 0.0117,
      "step": 1162
    },
    {
      "epoch": 18.61,
      "grad_norm": 0.5783898234367371,
      "learning_rate": 8.883038498943916e-06,
      "loss": 0.0019,
      "step": 1163
    },
    {
      "epoch": 18.62,
      "grad_norm": 0.6890028119087219,
      "learning_rate": 8.881018155381766e-06,
      "loss": 0.0019,
      "step": 1164
    },
    {
      "epoch": 18.64,
      "grad_norm": 1.3190313577651978,
      "learning_rate": 8.878996216479651e-06,
      "loss": 0.0254,
      "step": 1165
    },
    {
      "epoch": 18.66,
      "grad_norm": 1.5839214324951172,
      "learning_rate": 8.876972683068714e-06,
      "loss": 0.0056,
      "step": 1166
    },
    {
      "epoch": 18.67,
      "grad_norm": 1.936997652053833,
      "learning_rate": 8.874947555980753e-06,
      "loss": 0.0331,
      "step": 1167
    },
    {
      "epoch": 18.69,
      "grad_norm": 0.6026593446731567,
      "learning_rate": 8.872920836048222e-06,
      "loss": 0.0061,
      "step": 1168
    },
    {
      "epoch": 18.7,
      "grad_norm": 1.396553635597229,
      "learning_rate": 8.87089252410423e-06,
      "loss": 0.0136,
      "step": 1169
    },
    {
      "epoch": 18.72,
      "grad_norm": 1.2837761640548706,
      "learning_rate": 8.868862620982534e-06,
      "loss": 0.0084,
      "step": 1170
    },
    {
      "epoch": 18.74,
      "grad_norm": 0.6356934905052185,
      "learning_rate": 8.866831127517557e-06,
      "loss": 0.0045,
      "step": 1171
    },
    {
      "epoch": 18.75,
      "grad_norm": 0.23841975629329681,
      "learning_rate": 8.864798044544365e-06,
      "loss": 0.0016,
      "step": 1172
    },
    {
      "epoch": 18.77,
      "grad_norm": 0.1113000437617302,
      "learning_rate": 8.862763372898684e-06,
      "loss": 0.0008,
      "step": 1173
    },
    {
      "epoch": 18.78,
      "grad_norm": 1.1051359176635742,
      "learning_rate": 8.860727113416889e-06,
      "loss": 0.0112,
      "step": 1174
    },
    {
      "epoch": 18.8,
      "grad_norm": 3.595212936401367,
      "learning_rate": 8.85868926693601e-06,
      "loss": 0.0416,
      "step": 1175
    },
    {
      "epoch": 18.82,
      "grad_norm": 0.8167740106582642,
      "learning_rate": 8.85664983429373e-06,
      "loss": 0.0079,
      "step": 1176
    },
    {
      "epoch": 18.83,
      "grad_norm": 1.991145372390747,
      "learning_rate": 8.854608816328379e-06,
      "loss": 0.0192,
      "step": 1177
    },
    {
      "epoch": 18.85,
      "grad_norm": 0.8636705875396729,
      "learning_rate": 8.852566213878947e-06,
      "loss": 0.0113,
      "step": 1178
    },
    {
      "epoch": 18.86,
      "grad_norm": 0.9093413949012756,
      "learning_rate": 8.850522027785067e-06,
      "loss": 0.0065,
      "step": 1179
    },
    {
      "epoch": 18.88,
      "grad_norm": 0.0737474337220192,
      "learning_rate": 8.84847625888703e-06,
      "loss": 0.0008,
      "step": 1180
    },
    {
      "epoch": 18.9,
      "grad_norm": 4.107824802398682,
      "learning_rate": 8.846428908025773e-06,
      "loss": 0.0127,
      "step": 1181
    },
    {
      "epoch": 18.91,
      "grad_norm": 0.8852227330207825,
      "learning_rate": 8.844379976042882e-06,
      "loss": 0.0096,
      "step": 1182
    },
    {
      "epoch": 18.93,
      "grad_norm": 0.9210038781166077,
      "learning_rate": 8.842329463780601e-06,
      "loss": 0.003,
      "step": 1183
    },
    {
      "epoch": 18.94,
      "grad_norm": 0.16398607194423676,
      "learning_rate": 8.840277372081812e-06,
      "loss": 0.0012,
      "step": 1184
    },
    {
      "epoch": 18.96,
      "grad_norm": 1.0065957307815552,
      "learning_rate": 8.838223701790057e-06,
      "loss": 0.0176,
      "step": 1185
    },
    {
      "epoch": 18.98,
      "grad_norm": 3.734785556793213,
      "learning_rate": 8.83616845374952e-06,
      "loss": 0.0093,
      "step": 1186
    },
    {
      "epoch": 18.99,
      "grad_norm": 1.6990375518798828,
      "learning_rate": 8.834111628805036e-06,
      "loss": 0.0485,
      "step": 1187
    },
    {
      "epoch": 19.01,
      "grad_norm": 2.186166286468506,
      "learning_rate": 8.832053227802089e-06,
      "loss": 0.0178,
      "step": 1188
    },
    {
      "epoch": 19.02,
      "grad_norm": 0.36594435572624207,
      "learning_rate": 8.829993251586808e-06,
      "loss": 0.001,
      "step": 1189
    },
    {
      "epoch": 19.04,
      "grad_norm": 1.2831494808197021,
      "learning_rate": 8.827931701005974e-06,
      "loss": 0.019,
      "step": 1190
    },
    {
      "epoch": 19.06,
      "grad_norm": 0.7565898895263672,
      "learning_rate": 8.825868576907012e-06,
      "loss": 0.0074,
      "step": 1191
    },
    {
      "epoch": 19.07,
      "grad_norm": 0.6685026288032532,
      "learning_rate": 8.823803880137993e-06,
      "loss": 0.0065,
      "step": 1192
    },
    {
      "epoch": 19.09,
      "grad_norm": 0.5318118929862976,
      "learning_rate": 8.821737611547636e-06,
      "loss": 0.0024,
      "step": 1193
    },
    {
      "epoch": 19.1,
      "grad_norm": 4.649519920349121,
      "learning_rate": 8.819669771985308e-06,
      "loss": 0.0154,
      "step": 1194
    },
    {
      "epoch": 19.12,
      "grad_norm": 0.9358391761779785,
      "learning_rate": 8.817600362301018e-06,
      "loss": 0.0038,
      "step": 1195
    },
    {
      "epoch": 19.14,
      "grad_norm": 2.932873010635376,
      "learning_rate": 8.815529383345421e-06,
      "loss": 0.016,
      "step": 1196
    },
    {
      "epoch": 19.15,
      "grad_norm": 4.44010591506958,
      "learning_rate": 8.81345683596982e-06,
      "loss": 0.0278,
      "step": 1197
    },
    {
      "epoch": 19.17,
      "grad_norm": 0.787818193435669,
      "learning_rate": 8.81138272102616e-06,
      "loss": 0.0051,
      "step": 1198
    },
    {
      "epoch": 19.18,
      "grad_norm": 0.6283307671546936,
      "learning_rate": 8.809307039367035e-06,
      "loss": 0.0058,
      "step": 1199
    },
    {
      "epoch": 19.2,
      "grad_norm": 1.128288745880127,
      "learning_rate": 8.807229791845673e-06,
      "loss": 0.012,
      "step": 1200
    },
    {
      "epoch": 19.22,
      "grad_norm": 0.5622731447219849,
      "learning_rate": 8.805150979315956e-06,
      "loss": 0.0058,
      "step": 1201
    },
    {
      "epoch": 19.23,
      "grad_norm": 0.0688987746834755,
      "learning_rate": 8.803070602632405e-06,
      "loss": 0.0008,
      "step": 1202
    },
    {
      "epoch": 19.25,
      "grad_norm": 5.403507709503174,
      "learning_rate": 8.800988662650183e-06,
      "loss": 0.0239,
      "step": 1203
    },
    {
      "epoch": 19.26,
      "grad_norm": 0.8467326164245605,
      "learning_rate": 8.7989051602251e-06,
      "loss": 0.0115,
      "step": 1204
    },
    {
      "epoch": 19.28,
      "grad_norm": 1.5015095472335815,
      "learning_rate": 8.7968200962136e-06,
      "loss": 0.0243,
      "step": 1205
    },
    {
      "epoch": 19.3,
      "grad_norm": 0.8025253415107727,
      "learning_rate": 8.794733471472778e-06,
      "loss": 0.01,
      "step": 1206
    },
    {
      "epoch": 19.31,
      "grad_norm": 1.3549014329910278,
      "learning_rate": 8.792645286860367e-06,
      "loss": 0.0058,
      "step": 1207
    },
    {
      "epoch": 19.33,
      "grad_norm": 0.45427027344703674,
      "learning_rate": 8.790555543234739e-06,
      "loss": 0.0026,
      "step": 1208
    },
    {
      "epoch": 19.34,
      "grad_norm": 0.09070687741041183,
      "learning_rate": 8.788464241454906e-06,
      "loss": 0.0009,
      "step": 1209
    },
    {
      "epoch": 19.36,
      "grad_norm": 3.77469801902771,
      "learning_rate": 8.786371382380527e-06,
      "loss": 0.0247,
      "step": 1210
    },
    {
      "epoch": 19.38,
      "grad_norm": 0.6912159323692322,
      "learning_rate": 8.784276966871898e-06,
      "loss": 0.0049,
      "step": 1211
    },
    {
      "epoch": 19.39,
      "grad_norm": 0.13790683448314667,
      "learning_rate": 8.782180995789953e-06,
      "loss": 0.0008,
      "step": 1212
    },
    {
      "epoch": 19.41,
      "grad_norm": 0.06197298318147659,
      "learning_rate": 8.780083469996264e-06,
      "loss": 0.0007,
      "step": 1213
    },
    {
      "epoch": 19.42,
      "grad_norm": 5.743110656738281,
      "learning_rate": 8.777984390353048e-06,
      "loss": 0.0356,
      "step": 1214
    },
    {
      "epoch": 19.44,
      "grad_norm": 1.1707731485366821,
      "learning_rate": 8.775883757723156e-06,
      "loss": 0.0105,
      "step": 1215
    },
    {
      "epoch": 19.46,
      "grad_norm": 0.694736659526825,
      "learning_rate": 8.773781572970079e-06,
      "loss": 0.0041,
      "step": 1216
    },
    {
      "epoch": 19.47,
      "grad_norm": 1.1009728908538818,
      "learning_rate": 8.771677836957944e-06,
      "loss": 0.0125,
      "step": 1217
    },
    {
      "epoch": 19.49,
      "grad_norm": 0.5497755408287048,
      "learning_rate": 8.769572550551522e-06,
      "loss": 0.0029,
      "step": 1218
    },
    {
      "epoch": 19.5,
      "grad_norm": 2.5015757083892822,
      "learning_rate": 8.767465714616212e-06,
      "loss": 0.011,
      "step": 1219
    },
    {
      "epoch": 19.52,
      "grad_norm": 1.4625557661056519,
      "learning_rate": 8.765357330018056e-06,
      "loss": 0.0256,
      "step": 1220
    },
    {
      "epoch": 19.54,
      "grad_norm": 5.689533233642578,
      "learning_rate": 8.763247397623731e-06,
      "loss": 0.0154,
      "step": 1221
    },
    {
      "epoch": 19.55,
      "grad_norm": 1.2857389450073242,
      "learning_rate": 8.761135918300552e-06,
      "loss": 0.0144,
      "step": 1222
    },
    {
      "epoch": 19.57,
      "grad_norm": 0.13203072547912598,
      "learning_rate": 8.759022892916468e-06,
      "loss": 0.0014,
      "step": 1223
    },
    {
      "epoch": 19.58,
      "grad_norm": 0.24860194325447083,
      "learning_rate": 8.756908322340063e-06,
      "loss": 0.001,
      "step": 1224
    },
    {
      "epoch": 19.6,
      "grad_norm": 0.14107729494571686,
      "learning_rate": 8.754792207440557e-06,
      "loss": 0.0011,
      "step": 1225
    },
    {
      "epoch": 19.62,
      "grad_norm": 1.3666846752166748,
      "learning_rate": 8.752674549087806e-06,
      "loss": 0.0181,
      "step": 1226
    },
    {
      "epoch": 19.63,
      "grad_norm": 0.9150940775871277,
      "learning_rate": 8.750555348152299e-06,
      "loss": 0.0093,
      "step": 1227
    },
    {
      "epoch": 19.65,
      "grad_norm": 1.5714261531829834,
      "learning_rate": 8.748434605505159e-06,
      "loss": 0.0265,
      "step": 1228
    },
    {
      "epoch": 19.66,
      "grad_norm": 1.10048246383667,
      "learning_rate": 8.746312322018143e-06,
      "loss": 0.0155,
      "step": 1229
    },
    {
      "epoch": 19.68,
      "grad_norm": 1.2656524181365967,
      "learning_rate": 8.74418849856364e-06,
      "loss": 0.0083,
      "step": 1230
    },
    {
      "epoch": 19.7,
      "grad_norm": 1.0095958709716797,
      "learning_rate": 8.74206313601468e-06,
      "loss": 0.0049,
      "step": 1231
    },
    {
      "epoch": 19.71,
      "grad_norm": 1.959859848022461,
      "learning_rate": 8.739936235244913e-06,
      "loss": 0.0423,
      "step": 1232
    },
    {
      "epoch": 19.73,
      "grad_norm": 1.6421892642974854,
      "learning_rate": 8.73780779712863e-06,
      "loss": 0.0213,
      "step": 1233
    },
    {
      "epoch": 19.74,
      "grad_norm": 11.661835670471191,
      "learning_rate": 8.73567782254075e-06,
      "loss": 0.0771,
      "step": 1234
    },
    {
      "epoch": 19.76,
      "grad_norm": 0.18670305609703064,
      "learning_rate": 8.733546312356826e-06,
      "loss": 0.0018,
      "step": 1235
    },
    {
      "epoch": 19.78,
      "grad_norm": 0.3547627031803131,
      "learning_rate": 8.73141326745304e-06,
      "loss": 0.0047,
      "step": 1236
    },
    {
      "epoch": 19.79,
      "grad_norm": 0.30135148763656616,
      "learning_rate": 8.72927868870621e-06,
      "loss": 0.0029,
      "step": 1237
    },
    {
      "epoch": 19.81,
      "grad_norm": 1.62018620967865,
      "learning_rate": 8.727142576993776e-06,
      "loss": 0.0223,
      "step": 1238
    },
    {
      "epoch": 19.82,
      "grad_norm": 0.4040265381336212,
      "learning_rate": 8.725004933193818e-06,
      "loss": 0.0018,
      "step": 1239
    },
    {
      "epoch": 19.84,
      "grad_norm": 0.3072858154773712,
      "learning_rate": 8.722865758185036e-06,
      "loss": 0.002,
      "step": 1240
    },
    {
      "epoch": 19.86,
      "grad_norm": 1.491309404373169,
      "learning_rate": 8.720725052846766e-06,
      "loss": 0.017,
      "step": 1241
    },
    {
      "epoch": 19.87,
      "grad_norm": 1.1291424036026,
      "learning_rate": 8.718582818058971e-06,
      "loss": 0.015,
      "step": 1242
    },
    {
      "epoch": 19.89,
      "grad_norm": 1.0830450057983398,
      "learning_rate": 8.716439054702242e-06,
      "loss": 0.0051,
      "step": 1243
    },
    {
      "epoch": 19.9,
      "grad_norm": 2.3074774742126465,
      "learning_rate": 8.7142937636578e-06,
      "loss": 0.0367,
      "step": 1244
    },
    {
      "epoch": 19.92,
      "grad_norm": 1.0086182355880737,
      "learning_rate": 8.712146945807494e-06,
      "loss": 0.0167,
      "step": 1245
    },
    {
      "epoch": 19.94,
      "grad_norm": 2.1324357986450195,
      "learning_rate": 8.709998602033796e-06,
      "loss": 0.0311,
      "step": 1246
    },
    {
      "epoch": 19.95,
      "grad_norm": 6.626575946807861,
      "learning_rate": 8.707848733219815e-06,
      "loss": 0.0145,
      "step": 1247
    },
    {
      "epoch": 19.97,
      "grad_norm": 0.22447963058948517,
      "learning_rate": 8.705697340249275e-06,
      "loss": 0.0017,
      "step": 1248
    },
    {
      "epoch": 19.98,
      "grad_norm": 0.9575444459915161,
      "learning_rate": 8.703544424006536e-06,
      "loss": 0.0181,
      "step": 1249
    },
    {
      "epoch": 20.0,
      "grad_norm": 3.7132108211517334,
      "learning_rate": 8.701389985376578e-06,
      "loss": 0.0283,
      "step": 1250
    },
    {
      "epoch": 20.02,
      "grad_norm": 0.6840149760246277,
      "learning_rate": 8.699234025245012e-06,
      "loss": 0.012,
      "step": 1251
    },
    {
      "epoch": 20.03,
      "grad_norm": 3.116739511489868,
      "learning_rate": 8.69707654449807e-06,
      "loss": 0.0056,
      "step": 1252
    },
    {
      "epoch": 20.05,
      "grad_norm": 0.7790525555610657,
      "learning_rate": 8.694917544022611e-06,
      "loss": 0.0054,
      "step": 1253
    },
    {
      "epoch": 20.06,
      "grad_norm": 0.0937148928642273,
      "learning_rate": 8.692757024706121e-06,
      "loss": 0.0011,
      "step": 1254
    },
    {
      "epoch": 20.08,
      "grad_norm": 1.2987388372421265,
      "learning_rate": 8.690594987436705e-06,
      "loss": 0.0162,
      "step": 1255
    },
    {
      "epoch": 20.1,
      "grad_norm": 0.0569668710231781,
      "learning_rate": 8.688431433103094e-06,
      "loss": 0.0006,
      "step": 1256
    },
    {
      "epoch": 20.11,
      "grad_norm": 1.1617579460144043,
      "learning_rate": 8.686266362594646e-06,
      "loss": 0.0085,
      "step": 1257
    },
    {
      "epoch": 20.13,
      "grad_norm": 0.7319672703742981,
      "learning_rate": 8.684099776801339e-06,
      "loss": 0.0057,
      "step": 1258
    },
    {
      "epoch": 20.14,
      "grad_norm": 0.7421292662620544,
      "learning_rate": 8.681931676613774e-06,
      "loss": 0.0109,
      "step": 1259
    },
    {
      "epoch": 20.16,
      "grad_norm": 1.0193151235580444,
      "learning_rate": 8.679762062923176e-06,
      "loss": 0.01,
      "step": 1260
    },
    {
      "epoch": 20.18,
      "grad_norm": 0.036086950451135635,
      "learning_rate": 8.67759093662139e-06,
      "loss": 0.0006,
      "step": 1261
    },
    {
      "epoch": 20.19,
      "grad_norm": 0.619775652885437,
      "learning_rate": 8.675418298600884e-06,
      "loss": 0.0017,
      "step": 1262
    },
    {
      "epoch": 20.21,
      "grad_norm": 1.0351325273513794,
      "learning_rate": 8.67324414975475e-06,
      "loss": 0.0154,
      "step": 1263
    },
    {
      "epoch": 20.22,
      "grad_norm": 0.42659708857536316,
      "learning_rate": 8.671068490976695e-06,
      "loss": 0.005,
      "step": 1264
    },
    {
      "epoch": 20.24,
      "grad_norm": 1.0089861154556274,
      "learning_rate": 8.668891323161053e-06,
      "loss": 0.0078,
      "step": 1265
    },
    {
      "epoch": 20.26,
      "grad_norm": 1.9900267124176025,
      "learning_rate": 8.666712647202775e-06,
      "loss": 0.0436,
      "step": 1266
    },
    {
      "epoch": 20.27,
      "grad_norm": 2.0807154178619385,
      "learning_rate": 8.664532463997429e-06,
      "loss": 0.0159,
      "step": 1267
    },
    {
      "epoch": 20.29,
      "grad_norm": 0.20453032851219177,
      "learning_rate": 8.66235077444121e-06,
      "loss": 0.0013,
      "step": 1268
    },
    {
      "epoch": 20.3,
      "grad_norm": 0.6879249215126038,
      "learning_rate": 8.660167579430926e-06,
      "loss": 0.0078,
      "step": 1269
    },
    {
      "epoch": 20.32,
      "grad_norm": 1.818360447883606,
      "learning_rate": 8.657982879864007e-06,
      "loss": 0.0167,
      "step": 1270
    },
    {
      "epoch": 20.34,
      "grad_norm": 0.2510460913181305,
      "learning_rate": 8.655796676638502e-06,
      "loss": 0.0014,
      "step": 1271
    },
    {
      "epoch": 20.35,
      "grad_norm": 0.6334635615348816,
      "learning_rate": 8.653608970653072e-06,
      "loss": 0.0101,
      "step": 1272
    },
    {
      "epoch": 20.37,
      "grad_norm": 1.1164911985397339,
      "learning_rate": 8.651419762807005e-06,
      "loss": 0.018,
      "step": 1273
    },
    {
      "epoch": 20.38,
      "grad_norm": 0.18451657891273499,
      "learning_rate": 8.649229054000198e-06,
      "loss": 0.0018,
      "step": 1274
    },
    {
      "epoch": 20.4,
      "grad_norm": 1.010335087776184,
      "learning_rate": 8.647036845133171e-06,
      "loss": 0.0029,
      "step": 1275
    },
    {
      "epoch": 20.42,
      "grad_norm": 0.34271132946014404,
      "learning_rate": 8.644843137107058e-06,
      "loss": 0.0016,
      "step": 1276
    },
    {
      "epoch": 20.43,
      "grad_norm": 2.4715335369110107,
      "learning_rate": 8.64264793082361e-06,
      "loss": 0.0521,
      "step": 1277
    },
    {
      "epoch": 20.45,
      "grad_norm": 0.8612470626831055,
      "learning_rate": 8.640451227185191e-06,
      "loss": 0.0135,
      "step": 1278
    },
    {
      "epoch": 20.46,
      "grad_norm": 0.4107007086277008,
      "learning_rate": 8.638253027094785e-06,
      "loss": 0.0051,
      "step": 1279
    },
    {
      "epoch": 20.48,
      "grad_norm": 0.3313935101032257,
      "learning_rate": 8.636053331455986e-06,
      "loss": 0.0024,
      "step": 1280
    },
    {
      "epoch": 20.5,
      "grad_norm": 0.7664854526519775,
      "learning_rate": 8.633852141173012e-06,
      "loss": 0.0122,
      "step": 1281
    },
    {
      "epoch": 20.51,
      "grad_norm": 0.8990470767021179,
      "learning_rate": 8.631649457150684e-06,
      "loss": 0.006,
      "step": 1282
    },
    {
      "epoch": 20.53,
      "grad_norm": 0.7655798196792603,
      "learning_rate": 8.629445280294445e-06,
      "loss": 0.0078,
      "step": 1283
    },
    {
      "epoch": 20.54,
      "grad_norm": 0.8466750979423523,
      "learning_rate": 8.627239611510343e-06,
      "loss": 0.0065,
      "step": 1284
    },
    {
      "epoch": 20.56,
      "grad_norm": 1.9698162078857422,
      "learning_rate": 8.625032451705053e-06,
      "loss": 0.0076,
      "step": 1285
    },
    {
      "epoch": 20.58,
      "grad_norm": 1.3467196226119995,
      "learning_rate": 8.622823801785851e-06,
      "loss": 0.0197,
      "step": 1286
    },
    {
      "epoch": 20.59,
      "grad_norm": 0.4710582196712494,
      "learning_rate": 8.62061366266063e-06,
      "loss": 0.0032,
      "step": 1287
    },
    {
      "epoch": 20.61,
      "grad_norm": 1.780446171760559,
      "learning_rate": 8.618402035237895e-06,
      "loss": 0.0243,
      "step": 1288
    },
    {
      "epoch": 20.62,
      "grad_norm": 1.2707116603851318,
      "learning_rate": 8.61618892042676e-06,
      "loss": 0.0297,
      "step": 1289
    },
    {
      "epoch": 20.64,
      "grad_norm": 0.592056155204773,
      "learning_rate": 8.613974319136959e-06,
      "loss": 0.0021,
      "step": 1290
    },
    {
      "epoch": 20.66,
      "grad_norm": 0.28153419494628906,
      "learning_rate": 8.611758232278824e-06,
      "loss": 0.0019,
      "step": 1291
    },
    {
      "epoch": 20.67,
      "grad_norm": 1.3447130918502808,
      "learning_rate": 8.60954066076331e-06,
      "loss": 0.0138,
      "step": 1292
    },
    {
      "epoch": 20.69,
      "grad_norm": 0.27689582109451294,
      "learning_rate": 8.607321605501974e-06,
      "loss": 0.0027,
      "step": 1293
    },
    {
      "epoch": 20.7,
      "grad_norm": 0.6574634909629822,
      "learning_rate": 8.605101067406986e-06,
      "loss": 0.0092,
      "step": 1294
    },
    {
      "epoch": 20.72,
      "grad_norm": 2.180104970932007,
      "learning_rate": 8.602879047391127e-06,
      "loss": 0.049,
      "step": 1295
    },
    {
      "epoch": 20.74,
      "grad_norm": 1.0782041549682617,
      "learning_rate": 8.600655546367782e-06,
      "loss": 0.0113,
      "step": 1296
    },
    {
      "epoch": 20.75,
      "grad_norm": 6.607219696044922,
      "learning_rate": 8.598430565250953e-06,
      "loss": 0.0216,
      "step": 1297
    },
    {
      "epoch": 20.77,
      "grad_norm": 1.3781818151474,
      "learning_rate": 8.596204104955241e-06,
      "loss": 0.0175,
      "step": 1298
    },
    {
      "epoch": 20.78,
      "grad_norm": 4.948002815246582,
      "learning_rate": 8.593976166395864e-06,
      "loss": 0.01,
      "step": 1299
    },
    {
      "epoch": 20.8,
      "grad_norm": 0.9950366616249084,
      "learning_rate": 8.591746750488639e-06,
      "loss": 0.013,
      "step": 1300
    },
    {
      "epoch": 20.82,
      "grad_norm": 0.07659564912319183,
      "learning_rate": 8.589515858149998e-06,
      "loss": 0.0009,
      "step": 1301
    },
    {
      "epoch": 20.83,
      "grad_norm": 1.1616746187210083,
      "learning_rate": 8.587283490296976e-06,
      "loss": 0.0178,
      "step": 1302
    },
    {
      "epoch": 20.85,
      "grad_norm": 2.61403226852417,
      "learning_rate": 8.585049647847214e-06,
      "loss": 0.0083,
      "step": 1303
    },
    {
      "epoch": 20.86,
      "grad_norm": 2.241227865219116,
      "learning_rate": 8.582814331718961e-06,
      "loss": 0.0177,
      "step": 1304
    },
    {
      "epoch": 20.88,
      "grad_norm": 0.2570261061191559,
      "learning_rate": 8.580577542831072e-06,
      "loss": 0.0011,
      "step": 1305
    },
    {
      "epoch": 20.9,
      "grad_norm": 0.5989307761192322,
      "learning_rate": 8.578339282103006e-06,
      "loss": 0.0045,
      "step": 1306
    },
    {
      "epoch": 20.91,
      "grad_norm": 10.197319030761719,
      "learning_rate": 8.576099550454825e-06,
      "loss": 0.0246,
      "step": 1307
    },
    {
      "epoch": 20.93,
      "grad_norm": 1.4741278886795044,
      "learning_rate": 8.5738583488072e-06,
      "loss": 0.0182,
      "step": 1308
    },
    {
      "epoch": 20.94,
      "grad_norm": 1.1446458101272583,
      "learning_rate": 8.571615678081405e-06,
      "loss": 0.0136,
      "step": 1309
    },
    {
      "epoch": 20.96,
      "grad_norm": 4.790345668792725,
      "learning_rate": 8.569371539199316e-06,
      "loss": 0.0125,
      "step": 1310
    },
    {
      "epoch": 20.98,
      "grad_norm": 1.372393250465393,
      "learning_rate": 8.567125933083415e-06,
      "loss": 0.0108,
      "step": 1311
    },
    {
      "epoch": 20.99,
      "grad_norm": 1.1155823469161987,
      "learning_rate": 8.564878860656784e-06,
      "loss": 0.0101,
      "step": 1312
    },
    {
      "epoch": 21.01,
      "grad_norm": 4.901855945587158,
      "learning_rate": 8.56263032284311e-06,
      "loss": 0.0091,
      "step": 1313
    },
    {
      "epoch": 21.02,
      "grad_norm": 0.6070632338523865,
      "learning_rate": 8.560380320566685e-06,
      "loss": 0.0019,
      "step": 1314
    },
    {
      "epoch": 21.04,
      "grad_norm": 0.9420875310897827,
      "learning_rate": 8.558128854752397e-06,
      "loss": 0.0057,
      "step": 1315
    },
    {
      "epoch": 21.06,
      "grad_norm": 0.08444690704345703,
      "learning_rate": 8.555875926325738e-06,
      "loss": 0.0006,
      "step": 1316
    },
    {
      "epoch": 21.07,
      "grad_norm": 1.0788698196411133,
      "learning_rate": 8.553621536212801e-06,
      "loss": 0.0132,
      "step": 1317
    },
    {
      "epoch": 21.09,
      "grad_norm": 0.998012125492096,
      "learning_rate": 8.551365685340285e-06,
      "loss": 0.0165,
      "step": 1318
    },
    {
      "epoch": 21.1,
      "grad_norm": 0.31393179297447205,
      "learning_rate": 8.549108374635482e-06,
      "loss": 0.0028,
      "step": 1319
    },
    {
      "epoch": 21.12,
      "grad_norm": 0.07058592140674591,
      "learning_rate": 8.54684960502629e-06,
      "loss": 0.0006,
      "step": 1320
    },
    {
      "epoch": 21.14,
      "grad_norm": 0.05540892109274864,
      "learning_rate": 8.5445893774412e-06,
      "loss": 0.0006,
      "step": 1321
    },
    {
      "epoch": 21.15,
      "grad_norm": 0.1094982922077179,
      "learning_rate": 8.542327692809307e-06,
      "loss": 0.0012,
      "step": 1322
    },
    {
      "epoch": 21.17,
      "grad_norm": 1.1314893960952759,
      "learning_rate": 8.540064552060304e-06,
      "loss": 0.017,
      "step": 1323
    },
    {
      "epoch": 21.18,
      "grad_norm": 0.579784095287323,
      "learning_rate": 8.537799956124486e-06,
      "loss": 0.0034,
      "step": 1324
    },
    {
      "epoch": 21.2,
      "grad_norm": 0.40732014179229736,
      "learning_rate": 8.535533905932739e-06,
      "loss": 0.0011,
      "step": 1325
    },
    {
      "epoch": 21.22,
      "grad_norm": 0.058598656207323074,
      "learning_rate": 8.533266402416551e-06,
      "loss": 0.0008,
      "step": 1326
    },
    {
      "epoch": 21.23,
      "grad_norm": 0.45696964859962463,
      "learning_rate": 8.530997446508011e-06,
      "loss": 0.0033,
      "step": 1327
    },
    {
      "epoch": 21.25,
      "grad_norm": 1.1195653676986694,
      "learning_rate": 8.528727039139796e-06,
      "loss": 0.0175,
      "step": 1328
    },
    {
      "epoch": 21.26,
      "grad_norm": 0.986454427242279,
      "learning_rate": 8.526455181245188e-06,
      "loss": 0.0105,
      "step": 1329
    },
    {
      "epoch": 21.28,
      "grad_norm": 0.07550422847270966,
      "learning_rate": 8.52418187375806e-06,
      "loss": 0.0008,
      "step": 1330
    },
    {
      "epoch": 21.3,
      "grad_norm": 0.8403381705284119,
      "learning_rate": 8.521907117612883e-06,
      "loss": 0.0035,
      "step": 1331
    },
    {
      "epoch": 21.31,
      "grad_norm": 1.0069164037704468,
      "learning_rate": 8.519630913744726e-06,
      "loss": 0.0176,
      "step": 1332
    },
    {
      "epoch": 21.33,
      "grad_norm": 0.3500046133995056,
      "learning_rate": 8.517353263089247e-06,
      "loss": 0.0044,
      "step": 1333
    },
    {
      "epoch": 21.34,
      "grad_norm": 0.17645059525966644,
      "learning_rate": 8.515074166582703e-06,
      "loss": 0.0013,
      "step": 1334
    },
    {
      "epoch": 21.36,
      "grad_norm": 0.11416930705308914,
      "learning_rate": 8.512793625161947e-06,
      "loss": 0.0009,
      "step": 1335
    },
    {
      "epoch": 21.38,
      "grad_norm": 0.7886137962341309,
      "learning_rate": 8.51051163976442e-06,
      "loss": 0.0025,
      "step": 1336
    },
    {
      "epoch": 21.39,
      "grad_norm": 0.6699998378753662,
      "learning_rate": 8.508228211328164e-06,
      "loss": 0.0097,
      "step": 1337
    },
    {
      "epoch": 21.41,
      "grad_norm": 6.128129005432129,
      "learning_rate": 8.505943340791804e-06,
      "loss": 0.0163,
      "step": 1338
    },
    {
      "epoch": 21.42,
      "grad_norm": 0.4984460175037384,
      "learning_rate": 8.503657029094569e-06,
      "loss": 0.0025,
      "step": 1339
    },
    {
      "epoch": 21.44,
      "grad_norm": 0.33249565958976746,
      "learning_rate": 8.501369277176275e-06,
      "loss": 0.0019,
      "step": 1340
    },
    {
      "epoch": 21.46,
      "grad_norm": 0.5707900524139404,
      "learning_rate": 8.499080085977329e-06,
      "loss": 0.0084,
      "step": 1341
    },
    {
      "epoch": 21.47,
      "grad_norm": 0.7001329660415649,
      "learning_rate": 8.496789456438731e-06,
      "loss": 0.0116,
      "step": 1342
    },
    {
      "epoch": 21.49,
      "grad_norm": 0.36732998490333557,
      "learning_rate": 8.494497389502075e-06,
      "loss": 0.001,
      "step": 1343
    },
    {
      "epoch": 21.5,
      "grad_norm": 0.5126339197158813,
      "learning_rate": 8.492203886109538e-06,
      "loss": 0.0045,
      "step": 1344
    },
    {
      "epoch": 21.52,
      "grad_norm": 10.34594440460205,
      "learning_rate": 8.489908947203897e-06,
      "loss": 0.0171,
      "step": 1345
    },
    {
      "epoch": 21.54,
      "grad_norm": 0.05265961214900017,
      "learning_rate": 8.487612573728513e-06,
      "loss": 0.0006,
      "step": 1346
    },
    {
      "epoch": 21.55,
      "grad_norm": 0.4663985073566437,
      "learning_rate": 8.485314766627337e-06,
      "loss": 0.0029,
      "step": 1347
    },
    {
      "epoch": 21.57,
      "grad_norm": 0.05087748542428017,
      "learning_rate": 8.483015526844914e-06,
      "loss": 0.0007,
      "step": 1348
    },
    {
      "epoch": 21.58,
      "grad_norm": 1.0724035501480103,
      "learning_rate": 8.480714855326372e-06,
      "loss": 0.0187,
      "step": 1349
    },
    {
      "epoch": 21.6,
      "grad_norm": 0.12906812131404877,
      "learning_rate": 8.478412753017433e-06,
      "loss": 0.0006,
      "step": 1350
    },
    {
      "epoch": 21.62,
      "grad_norm": 1.0948070287704468,
      "learning_rate": 8.476109220864401e-06,
      "loss": 0.0114,
      "step": 1351
    },
    {
      "epoch": 21.63,
      "grad_norm": 2.791593313217163,
      "learning_rate": 8.473804259814173e-06,
      "loss": 0.0403,
      "step": 1352
    },
    {
      "epoch": 21.65,
      "grad_norm": 1.3474396467208862,
      "learning_rate": 8.47149787081423e-06,
      "loss": 0.0129,
      "step": 1353
    },
    {
      "epoch": 21.66,
      "grad_norm": 1.3937840461730957,
      "learning_rate": 8.469190054812642e-06,
      "loss": 0.028,
      "step": 1354
    },
    {
      "epoch": 21.68,
      "grad_norm": 1.2444548606872559,
      "learning_rate": 8.466880812758064e-06,
      "loss": 0.0128,
      "step": 1355
    },
    {
      "epoch": 21.7,
      "grad_norm": 0.6487327814102173,
      "learning_rate": 8.464570145599742e-06,
      "loss": 0.0016,
      "step": 1356
    },
    {
      "epoch": 21.71,
      "grad_norm": 0.22125639021396637,
      "learning_rate": 8.4622580542875e-06,
      "loss": 0.0014,
      "step": 1357
    },
    {
      "epoch": 21.73,
      "grad_norm": 0.4400305151939392,
      "learning_rate": 8.459944539771755e-06,
      "loss": 0.0012,
      "step": 1358
    },
    {
      "epoch": 21.74,
      "grad_norm": 0.936139702796936,
      "learning_rate": 8.457629603003502e-06,
      "loss": 0.0022,
      "step": 1359
    },
    {
      "epoch": 21.76,
      "grad_norm": 1.778730034828186,
      "learning_rate": 8.455313244934324e-06,
      "loss": 0.022,
      "step": 1360
    },
    {
      "epoch": 21.78,
      "grad_norm": 15.01415729522705,
      "learning_rate": 8.45299546651639e-06,
      "loss": 0.0372,
      "step": 1361
    },
    {
      "epoch": 21.79,
      "grad_norm": 1.2577314376831055,
      "learning_rate": 8.45067626870245e-06,
      "loss": 0.0179,
      "step": 1362
    },
    {
      "epoch": 21.81,
      "grad_norm": 1.449792504310608,
      "learning_rate": 8.448355652445843e-06,
      "loss": 0.0136,
      "step": 1363
    },
    {
      "epoch": 21.82,
      "grad_norm": 0.8687264919281006,
      "learning_rate": 8.44603361870048e-06,
      "loss": 0.004,
      "step": 1364
    },
    {
      "epoch": 21.84,
      "grad_norm": 0.8155516982078552,
      "learning_rate": 8.443710168420866e-06,
      "loss": 0.0121,
      "step": 1365
    },
    {
      "epoch": 21.86,
      "grad_norm": 0.12771932780742645,
      "learning_rate": 8.44138530256208e-06,
      "loss": 0.001,
      "step": 1366
    },
    {
      "epoch": 21.87,
      "grad_norm": 1.6901705265045166,
      "learning_rate": 8.439059022079789e-06,
      "loss": 0.0206,
      "step": 1367
    },
    {
      "epoch": 21.89,
      "grad_norm": 1.1885002851486206,
      "learning_rate": 8.43673132793024e-06,
      "loss": 0.0171,
      "step": 1368
    },
    {
      "epoch": 21.9,
      "grad_norm": 2.3596436977386475,
      "learning_rate": 8.434402221070258e-06,
      "loss": 0.0082,
      "step": 1369
    },
    {
      "epoch": 21.92,
      "grad_norm": 0.05609651282429695,
      "learning_rate": 8.432071702457253e-06,
      "loss": 0.0008,
      "step": 1370
    },
    {
      "epoch": 21.94,
      "grad_norm": 1.7092636823654175,
      "learning_rate": 8.42973977304921e-06,
      "loss": 0.0214,
      "step": 1371
    },
    {
      "epoch": 21.95,
      "grad_norm": 1.6809252500534058,
      "learning_rate": 8.4274064338047e-06,
      "loss": 0.0031,
      "step": 1372
    },
    {
      "epoch": 21.97,
      "grad_norm": 0.7240549921989441,
      "learning_rate": 8.425071685682868e-06,
      "loss": 0.0072,
      "step": 1373
    },
    {
      "epoch": 21.98,
      "grad_norm": 2.9565820693969727,
      "learning_rate": 8.422735529643445e-06,
      "loss": 0.0031,
      "step": 1374
    },
    {
      "epoch": 22.0,
      "grad_norm": 1.8125542402267456,
      "learning_rate": 8.420397966646732e-06,
      "loss": 0.0205,
      "step": 1375
    },
    {
      "epoch": 22.02,
      "grad_norm": 4.760380268096924,
      "learning_rate": 8.418058997653613e-06,
      "loss": 0.0251,
      "step": 1376
    },
    {
      "epoch": 22.03,
      "grad_norm": 0.061363235116004944,
      "learning_rate": 8.415718623625553e-06,
      "loss": 0.0006,
      "step": 1377
    },
    {
      "epoch": 22.05,
      "grad_norm": 7.31688117980957,
      "learning_rate": 8.41337684552459e-06,
      "loss": 0.009,
      "step": 1378
    },
    {
      "epoch": 22.06,
      "grad_norm": 0.1626138836145401,
      "learning_rate": 8.411033664313341e-06,
      "loss": 0.0016,
      "step": 1379
    },
    {
      "epoch": 22.08,
      "grad_norm": 0.29839473962783813,
      "learning_rate": 8.408689080954997e-06,
      "loss": 0.002,
      "step": 1380
    },
    {
      "epoch": 22.1,
      "grad_norm": 5.40683650970459,
      "learning_rate": 8.406343096413333e-06,
      "loss": 0.0055,
      "step": 1381
    },
    {
      "epoch": 22.11,
      "grad_norm": 0.06848254799842834,
      "learning_rate": 8.403995711652687e-06,
      "loss": 0.0008,
      "step": 1382
    },
    {
      "epoch": 22.13,
      "grad_norm": 0.5543511509895325,
      "learning_rate": 8.401646927637985e-06,
      "loss": 0.0021,
      "step": 1383
    },
    {
      "epoch": 22.14,
      "grad_norm": 0.3053779602050781,
      "learning_rate": 8.399296745334723e-06,
      "loss": 0.0015,
      "step": 1384
    },
    {
      "epoch": 22.16,
      "grad_norm": 0.7135550379753113,
      "learning_rate": 8.396945165708971e-06,
      "loss": 0.0036,
      "step": 1385
    },
    {
      "epoch": 22.18,
      "grad_norm": 1.2787153720855713,
      "learning_rate": 8.394592189727375e-06,
      "loss": 0.0195,
      "step": 1386
    },
    {
      "epoch": 22.19,
      "grad_norm": 0.04461019113659859,
      "learning_rate": 8.392237818357157e-06,
      "loss": 0.0007,
      "step": 1387
    },
    {
      "epoch": 22.21,
      "grad_norm": 6.934388160705566,
      "learning_rate": 8.389882052566106e-06,
      "loss": 0.0729,
      "step": 1388
    },
    {
      "epoch": 22.22,
      "grad_norm": 0.07573741674423218,
      "learning_rate": 8.38752489332259e-06,
      "loss": 0.0008,
      "step": 1389
    },
    {
      "epoch": 22.24,
      "grad_norm": 0.36267203092575073,
      "learning_rate": 8.38516634159555e-06,
      "loss": 0.0008,
      "step": 1390
    },
    {
      "epoch": 22.26,
      "grad_norm": 2.2764737606048584,
      "learning_rate": 8.382806398354493e-06,
      "loss": 0.013,
      "step": 1391
    },
    {
      "epoch": 22.27,
      "grad_norm": 0.7993055582046509,
      "learning_rate": 8.380445064569506e-06,
      "loss": 0.0096,
      "step": 1392
    },
    {
      "epoch": 22.29,
      "grad_norm": 0.8432145118713379,
      "learning_rate": 8.378082341211245e-06,
      "loss": 0.0018,
      "step": 1393
    },
    {
      "epoch": 22.3,
      "grad_norm": 0.8723902702331543,
      "learning_rate": 8.375718229250935e-06,
      "loss": 0.0122,
      "step": 1394
    },
    {
      "epoch": 22.32,
      "grad_norm": 0.40363821387290955,
      "learning_rate": 8.373352729660373e-06,
      "loss": 0.002,
      "step": 1395
    },
    {
      "epoch": 22.34,
      "grad_norm": 0.09610789269208908,
      "learning_rate": 8.370985843411924e-06,
      "loss": 0.0012,
      "step": 1396
    },
    {
      "epoch": 22.35,
      "grad_norm": 0.18189366161823273,
      "learning_rate": 8.368617571478533e-06,
      "loss": 0.0017,
      "step": 1397
    },
    {
      "epoch": 22.37,
      "grad_norm": 4.219527244567871,
      "learning_rate": 8.366247914833698e-06,
      "loss": 0.0741,
      "step": 1398
    },
    {
      "epoch": 22.38,
      "grad_norm": 0.535189151763916,
      "learning_rate": 8.3638768744515e-06,
      "loss": 0.0016,
      "step": 1399
    },
    {
      "epoch": 22.4,
      "grad_norm": 1.0718141794204712,
      "learning_rate": 8.361504451306585e-06,
      "loss": 0.0141,
      "step": 1400
    },
    {
      "epoch": 22.42,
      "grad_norm": 1.6234652996063232,
      "learning_rate": 8.359130646374164e-06,
      "loss": 0.0333,
      "step": 1401
    },
    {
      "epoch": 22.43,
      "grad_norm": 0.3280230164527893,
      "learning_rate": 8.35675546063002e-06,
      "loss": 0.0021,
      "step": 1402
    },
    {
      "epoch": 22.45,
      "grad_norm": 0.1023169457912445,
      "learning_rate": 8.354378895050502e-06,
      "loss": 0.0012,
      "step": 1403
    },
    {
      "epoch": 22.46,
      "grad_norm": 0.17849089205265045,
      "learning_rate": 8.352000950612526e-06,
      "loss": 0.0016,
      "step": 1404
    },
    {
      "epoch": 22.48,
      "grad_norm": 1.262760043144226,
      "learning_rate": 8.349621628293578e-06,
      "loss": 0.0201,
      "step": 1405
    },
    {
      "epoch": 22.5,
      "grad_norm": 0.4971104860305786,
      "learning_rate": 8.347240929071701e-06,
      "loss": 0.001,
      "step": 1406
    },
    {
      "epoch": 22.51,
      "grad_norm": 10.709864616394043,
      "learning_rate": 8.344858853925517e-06,
      "loss": 0.0635,
      "step": 1407
    },
    {
      "epoch": 22.53,
      "grad_norm": 0.08736246824264526,
      "learning_rate": 8.342475403834203e-06,
      "loss": 0.0012,
      "step": 1408
    },
    {
      "epoch": 22.54,
      "grad_norm": 0.12173714488744736,
      "learning_rate": 8.340090579777507e-06,
      "loss": 0.0006,
      "step": 1409
    },
    {
      "epoch": 22.56,
      "grad_norm": 1.0453431606292725,
      "learning_rate": 8.337704382735741e-06,
      "loss": 0.0194,
      "step": 1410
    },
    {
      "epoch": 22.58,
      "grad_norm": 0.03648211807012558,
      "learning_rate": 8.335316813689778e-06,
      "loss": 0.0006,
      "step": 1411
    },
    {
      "epoch": 22.59,
      "grad_norm": 1.1094448566436768,
      "learning_rate": 8.332927873621059e-06,
      "loss": 0.0138,
      "step": 1412
    },
    {
      "epoch": 22.61,
      "grad_norm": 0.8672231435775757,
      "learning_rate": 8.330537563511588e-06,
      "loss": 0.0046,
      "step": 1413
    },
    {
      "epoch": 22.62,
      "grad_norm": 1.928665280342102,
      "learning_rate": 8.32814588434393e-06,
      "loss": 0.0215,
      "step": 1414
    },
    {
      "epoch": 22.64,
      "grad_norm": 0.712665319442749,
      "learning_rate": 8.325752837101213e-06,
      "loss": 0.0052,
      "step": 1415
    },
    {
      "epoch": 22.66,
      "grad_norm": 2.148350238800049,
      "learning_rate": 8.32335842276713e-06,
      "loss": 0.019,
      "step": 1416
    },
    {
      "epoch": 22.67,
      "grad_norm": 1.5148435831069946,
      "learning_rate": 8.320962642325931e-06,
      "loss": 0.0248,
      "step": 1417
    },
    {
      "epoch": 22.69,
      "grad_norm": 1.808647632598877,
      "learning_rate": 8.318565496762436e-06,
      "loss": 0.0248,
      "step": 1418
    },
    {
      "epoch": 22.7,
      "grad_norm": 1.0477848052978516,
      "learning_rate": 8.31616698706202e-06,
      "loss": 0.0108,
      "step": 1419
    },
    {
      "epoch": 22.72,
      "grad_norm": 1.5795584917068481,
      "learning_rate": 8.313767114210615e-06,
      "loss": 0.0166,
      "step": 1420
    },
    {
      "epoch": 22.74,
      "grad_norm": 0.1262214183807373,
      "learning_rate": 8.311365879194725e-06,
      "loss": 0.0014,
      "step": 1421
    },
    {
      "epoch": 22.75,
      "grad_norm": 1.4461158514022827,
      "learning_rate": 8.308963283001399e-06,
      "loss": 0.0218,
      "step": 1422
    },
    {
      "epoch": 22.77,
      "grad_norm": 0.3101532459259033,
      "learning_rate": 8.30655932661826e-06,
      "loss": 0.002,
      "step": 1423
    },
    {
      "epoch": 22.78,
      "grad_norm": 0.07941704988479614,
      "learning_rate": 8.30415401103348e-06,
      "loss": 0.0005,
      "step": 1424
    },
    {
      "epoch": 22.8,
      "grad_norm": 0.4584568738937378,
      "learning_rate": 8.301747337235798e-06,
      "loss": 0.0027,
      "step": 1425
    },
    {
      "epoch": 22.82,
      "grad_norm": 0.44748231768608093,
      "learning_rate": 8.299339306214501e-06,
      "loss": 0.0054,
      "step": 1426
    },
    {
      "epoch": 22.83,
      "grad_norm": 0.17128001153469086,
      "learning_rate": 8.296929918959444e-06,
      "loss": 0.0006,
      "step": 1427
    },
    {
      "epoch": 22.85,
      "grad_norm": 1.230000376701355,
      "learning_rate": 8.29451917646103e-06,
      "loss": 0.0066,
      "step": 1428
    },
    {
      "epoch": 22.86,
      "grad_norm": 1.0981208086013794,
      "learning_rate": 8.292107079710231e-06,
      "loss": 0.0165,
      "step": 1429
    },
    {
      "epoch": 22.88,
      "grad_norm": 0.12379787862300873,
      "learning_rate": 8.289693629698564e-06,
      "loss": 0.0016,
      "step": 1430
    },
    {
      "epoch": 22.9,
      "grad_norm": 0.38706693053245544,
      "learning_rate": 8.287278827418108e-06,
      "loss": 0.0027,
      "step": 1431
    },
    {
      "epoch": 22.91,
      "grad_norm": 0.09795940667390823,
      "learning_rate": 8.284862673861498e-06,
      "loss": 0.001,
      "step": 1432
    },
    {
      "epoch": 22.93,
      "grad_norm": 0.5553276538848877,
      "learning_rate": 8.282445170021922e-06,
      "loss": 0.0073,
      "step": 1433
    },
    {
      "epoch": 22.94,
      "grad_norm": 0.042269498109817505,
      "learning_rate": 8.280026316893126e-06,
      "loss": 0.0004,
      "step": 1434
    },
    {
      "epoch": 22.96,
      "grad_norm": 0.39423587918281555,
      "learning_rate": 8.27760611546941e-06,
      "loss": 0.0019,
      "step": 1435
    },
    {
      "epoch": 22.98,
      "grad_norm": 1.930793285369873,
      "learning_rate": 8.275184566745625e-06,
      "loss": 0.0231,
      "step": 1436
    },
    {
      "epoch": 22.99,
      "grad_norm": 1.6418087482452393,
      "learning_rate": 8.272761671717178e-06,
      "loss": 0.0206,
      "step": 1437
    },
    {
      "epoch": 23.01,
      "grad_norm": 1.766120195388794,
      "learning_rate": 8.270337431380032e-06,
      "loss": 0.0359,
      "step": 1438
    },
    {
      "epoch": 23.02,
      "grad_norm": 0.05575231462717056,
      "learning_rate": 8.267911846730698e-06,
      "loss": 0.0007,
      "step": 1439
    },
    {
      "epoch": 23.04,
      "grad_norm": 1.0956244468688965,
      "learning_rate": 8.265484918766243e-06,
      "loss": 0.0189,
      "step": 1440
    },
    {
      "epoch": 23.06,
      "grad_norm": 0.08073350042104721,
      "learning_rate": 8.26305664848429e-06,
      "loss": 0.0009,
      "step": 1441
    },
    {
      "epoch": 23.07,
      "grad_norm": 0.48269376158714294,
      "learning_rate": 8.260627036883002e-06,
      "loss": 0.0012,
      "step": 1442
    },
    {
      "epoch": 23.09,
      "grad_norm": 1.4218002557754517,
      "learning_rate": 8.258196084961103e-06,
      "loss": 0.0024,
      "step": 1443
    },
    {
      "epoch": 23.1,
      "grad_norm": 5.938902854919434,
      "learning_rate": 8.255763793717868e-06,
      "loss": 0.0474,
      "step": 1444
    },
    {
      "epoch": 23.12,
      "grad_norm": 0.02306552045047283,
      "learning_rate": 8.253330164153118e-06,
      "loss": 0.0004,
      "step": 1445
    },
    {
      "epoch": 23.14,
      "grad_norm": 0.5796533823013306,
      "learning_rate": 8.250895197267227e-06,
      "loss": 0.0027,
      "step": 1446
    },
    {
      "epoch": 23.15,
      "grad_norm": 0.05894874036312103,
      "learning_rate": 8.248458894061117e-06,
      "loss": 0.0007,
      "step": 1447
    },
    {
      "epoch": 23.17,
      "grad_norm": 0.41886577010154724,
      "learning_rate": 8.24602125553626e-06,
      "loss": 0.006,
      "step": 1448
    },
    {
      "epoch": 23.18,
      "grad_norm": 1.5752562284469604,
      "learning_rate": 8.24358228269468e-06,
      "loss": 0.0118,
      "step": 1449
    },
    {
      "epoch": 23.2,
      "grad_norm": 9.390212059020996,
      "learning_rate": 8.241141976538944e-06,
      "loss": 0.0304,
      "step": 1450
    },
    {
      "epoch": 23.22,
      "grad_norm": 0.2864284813404083,
      "learning_rate": 8.238700338072167e-06,
      "loss": 0.0015,
      "step": 1451
    },
    {
      "epoch": 23.23,
      "grad_norm": 2.2707488536834717,
      "learning_rate": 8.236257368298022e-06,
      "loss": 0.0458,
      "step": 1452
    },
    {
      "epoch": 23.25,
      "grad_norm": 0.7054921388626099,
      "learning_rate": 8.233813068220712e-06,
      "loss": 0.0043,
      "step": 1453
    },
    {
      "epoch": 23.26,
      "grad_norm": 2.0117027759552,
      "learning_rate": 8.231367438845005e-06,
      "loss": 0.0133,
      "step": 1454
    },
    {
      "epoch": 23.28,
      "grad_norm": 0.736692488193512,
      "learning_rate": 8.228920481176202e-06,
      "loss": 0.008,
      "step": 1455
    },
    {
      "epoch": 23.3,
      "grad_norm": 5.565258502960205,
      "learning_rate": 8.226472196220156e-06,
      "loss": 0.0063,
      "step": 1456
    },
    {
      "epoch": 23.31,
      "grad_norm": 0.09619937092065811,
      "learning_rate": 8.224022584983267e-06,
      "loss": 0.0006,
      "step": 1457
    },
    {
      "epoch": 23.33,
      "grad_norm": 0.3007771074771881,
      "learning_rate": 8.221571648472473e-06,
      "loss": 0.0018,
      "step": 1458
    },
    {
      "epoch": 23.34,
      "grad_norm": 0.07716875523328781,
      "learning_rate": 8.219119387695263e-06,
      "loss": 0.001,
      "step": 1459
    },
    {
      "epoch": 23.36,
      "grad_norm": 0.5765562057495117,
      "learning_rate": 8.216665803659671e-06,
      "loss": 0.0046,
      "step": 1460
    },
    {
      "epoch": 23.38,
      "grad_norm": 0.8979700207710266,
      "learning_rate": 8.214210897374271e-06,
      "loss": 0.0081,
      "step": 1461
    },
    {
      "epoch": 23.39,
      "grad_norm": 0.04538774862885475,
      "learning_rate": 8.211754669848182e-06,
      "loss": 0.0005,
      "step": 1462
    },
    {
      "epoch": 23.41,
      "grad_norm": 0.8608867526054382,
      "learning_rate": 8.209297122091066e-06,
      "loss": 0.011,
      "step": 1463
    },
    {
      "epoch": 23.42,
      "grad_norm": 0.6981846690177917,
      "learning_rate": 8.206838255113132e-06,
      "loss": 0.0059,
      "step": 1464
    },
    {
      "epoch": 23.44,
      "grad_norm": 0.3841575086116791,
      "learning_rate": 8.204378069925121e-06,
      "loss": 0.003,
      "step": 1465
    },
    {
      "epoch": 23.46,
      "grad_norm": 0.1133350357413292,
      "learning_rate": 8.201916567538327e-06,
      "loss": 0.0007,
      "step": 1466
    },
    {
      "epoch": 23.47,
      "grad_norm": 0.19732975959777832,
      "learning_rate": 8.199453748964579e-06,
      "loss": 0.0014,
      "step": 1467
    },
    {
      "epoch": 23.49,
      "grad_norm": 0.05425919219851494,
      "learning_rate": 8.196989615216248e-06,
      "loss": 0.0011,
      "step": 1468
    },
    {
      "epoch": 23.5,
      "grad_norm": 0.06950365006923676,
      "learning_rate": 8.194524167306247e-06,
      "loss": 0.0007,
      "step": 1469
    },
    {
      "epoch": 23.52,
      "grad_norm": 1.5309017896652222,
      "learning_rate": 8.192057406248028e-06,
      "loss": 0.0183,
      "step": 1470
    },
    {
      "epoch": 23.54,
      "grad_norm": 0.10425883531570435,
      "learning_rate": 8.189589333055586e-06,
      "loss": 0.0009,
      "step": 1471
    },
    {
      "epoch": 23.55,
      "grad_norm": 0.7321273684501648,
      "learning_rate": 8.18711994874345e-06,
      "loss": 0.0021,
      "step": 1472
    },
    {
      "epoch": 23.57,
      "grad_norm": 0.1715707629919052,
      "learning_rate": 8.18464925432669e-06,
      "loss": 0.0014,
      "step": 1473
    },
    {
      "epoch": 23.58,
      "grad_norm": 1.2451605796813965,
      "learning_rate": 8.182177250820918e-06,
      "loss": 0.0111,
      "step": 1474
    },
    {
      "epoch": 23.6,
      "grad_norm": 1.904952049255371,
      "learning_rate": 8.179703939242276e-06,
      "loss": 0.0057,
      "step": 1475
    },
    {
      "epoch": 23.62,
      "grad_norm": 1.833131194114685,
      "learning_rate": 8.177229320607455e-06,
      "loss": 0.0222,
      "step": 1476
    },
    {
      "epoch": 23.63,
      "grad_norm": 1.08124840259552,
      "learning_rate": 8.174753395933674e-06,
      "loss": 0.0116,
      "step": 1477
    },
    {
      "epoch": 23.65,
      "grad_norm": 1.2224459648132324,
      "learning_rate": 8.172276166238694e-06,
      "loss": 0.0253,
      "step": 1478
    },
    {
      "epoch": 23.66,
      "grad_norm": 0.7406772971153259,
      "learning_rate": 8.16979763254081e-06,
      "loss": 0.0096,
      "step": 1479
    },
    {
      "epoch": 23.68,
      "grad_norm": 0.03358576446771622,
      "learning_rate": 8.16731779585885e-06,
      "loss": 0.0005,
      "step": 1480
    },
    {
      "epoch": 23.7,
      "grad_norm": 0.7574333548545837,
      "learning_rate": 8.164836657212186e-06,
      "loss": 0.0061,
      "step": 1481
    },
    {
      "epoch": 23.71,
      "grad_norm": 0.8904780149459839,
      "learning_rate": 8.162354217620719e-06,
      "loss": 0.0122,
      "step": 1482
    },
    {
      "epoch": 23.73,
      "grad_norm": 0.6592087149620056,
      "learning_rate": 8.159870478104885e-06,
      "loss": 0.0044,
      "step": 1483
    },
    {
      "epoch": 23.74,
      "grad_norm": 0.8506538271903992,
      "learning_rate": 8.157385439685656e-06,
      "loss": 0.0075,
      "step": 1484
    },
    {
      "epoch": 23.76,
      "grad_norm": 6.2891411781311035,
      "learning_rate": 8.154899103384536e-06,
      "loss": 0.0131,
      "step": 1485
    },
    {
      "epoch": 23.78,
      "grad_norm": 0.30173590779304504,
      "learning_rate": 8.15241147022357e-06,
      "loss": 0.0014,
      "step": 1486
    },
    {
      "epoch": 23.79,
      "grad_norm": 1.18161940574646,
      "learning_rate": 8.149922541225322e-06,
      "loss": 0.008,
      "step": 1487
    },
    {
      "epoch": 23.81,
      "grad_norm": 0.5280227065086365,
      "learning_rate": 8.147432317412902e-06,
      "loss": 0.0017,
      "step": 1488
    },
    {
      "epoch": 23.82,
      "grad_norm": 1.7631382942199707,
      "learning_rate": 8.144940799809944e-06,
      "loss": 0.0167,
      "step": 1489
    },
    {
      "epoch": 23.84,
      "grad_norm": 3.4244544506073,
      "learning_rate": 8.142447989440618e-06,
      "loss": 0.0145,
      "step": 1490
    },
    {
      "epoch": 23.86,
      "grad_norm": 1.5340360403060913,
      "learning_rate": 8.139953887329626e-06,
      "loss": 0.015,
      "step": 1491
    },
    {
      "epoch": 23.87,
      "grad_norm": 0.09789630025625229,
      "learning_rate": 8.137458494502195e-06,
      "loss": 0.0009,
      "step": 1492
    },
    {
      "epoch": 23.89,
      "grad_norm": 1.0756993293762207,
      "learning_rate": 8.13496181198409e-06,
      "loss": 0.0042,
      "step": 1493
    },
    {
      "epoch": 23.9,
      "grad_norm": 2.4846532344818115,
      "learning_rate": 8.132463840801603e-06,
      "loss": 0.0091,
      "step": 1494
    },
    {
      "epoch": 23.92,
      "grad_norm": 0.08840779960155487,
      "learning_rate": 8.129964581981554e-06,
      "loss": 0.0005,
      "step": 1495
    },
    {
      "epoch": 23.94,
      "grad_norm": 1.0977507829666138,
      "learning_rate": 8.127464036551294e-06,
      "loss": 0.0153,
      "step": 1496
    },
    {
      "epoch": 23.95,
      "grad_norm": 0.07216960936784744,
      "learning_rate": 8.124962205538703e-06,
      "loss": 0.0006,
      "step": 1497
    },
    {
      "epoch": 23.97,
      "grad_norm": 1.011715054512024,
      "learning_rate": 8.12245908997219e-06,
      "loss": 0.017,
      "step": 1498
    },
    {
      "epoch": 23.98,
      "grad_norm": 1.0800162553787231,
      "learning_rate": 8.11995469088069e-06,
      "loss": 0.0182,
      "step": 1499
    },
    {
      "epoch": 24.0,
      "grad_norm": 1.8781729936599731,
      "learning_rate": 8.117449009293668e-06,
      "loss": 0.0112,
      "step": 1500
    },
    {
      "epoch": 24.02,
      "grad_norm": 0.7881783246994019,
      "learning_rate": 8.114942046241115e-06,
      "loss": 0.0088,
      "step": 1501
    },
    {
      "epoch": 24.03,
      "grad_norm": 0.4582022428512573,
      "learning_rate": 8.112433802753547e-06,
      "loss": 0.0014,
      "step": 1502
    },
    {
      "epoch": 24.05,
      "grad_norm": 0.03610575199127197,
      "learning_rate": 8.10992427986201e-06,
      "loss": 0.0006,
      "step": 1503
    },
    {
      "epoch": 24.06,
      "grad_norm": 0.15076911449432373,
      "learning_rate": 8.107413478598076e-06,
      "loss": 0.0011,
      "step": 1504
    },
    {
      "epoch": 24.08,
      "grad_norm": 1.083283543586731,
      "learning_rate": 8.104901399993837e-06,
      "loss": 0.0103,
      "step": 1505
    },
    {
      "epoch": 24.1,
      "grad_norm": 0.06178690865635872,
      "learning_rate": 8.102388045081915e-06,
      "loss": 0.0008,
      "step": 1506
    },
    {
      "epoch": 24.11,
      "grad_norm": 0.1292390078306198,
      "learning_rate": 8.099873414895453e-06,
      "loss": 0.0014,
      "step": 1507
    },
    {
      "epoch": 24.13,
      "grad_norm": 7.588351726531982,
      "learning_rate": 8.097357510468125e-06,
      "loss": 0.0174,
      "step": 1508
    },
    {
      "epoch": 24.14,
      "grad_norm": 1.0270071029663086,
      "learning_rate": 8.094840332834123e-06,
      "loss": 0.0185,
      "step": 1509
    },
    {
      "epoch": 24.16,
      "grad_norm": 0.2063382863998413,
      "learning_rate": 8.092321883028157e-06,
      "loss": 0.0008,
      "step": 1510
    },
    {
      "epoch": 24.18,
      "grad_norm": 0.18804189562797546,
      "learning_rate": 8.089802162085478e-06,
      "loss": 0.0016,
      "step": 1511
    },
    {
      "epoch": 24.19,
      "grad_norm": 0.04287507012486458,
      "learning_rate": 8.087281171041838e-06,
      "loss": 0.0005,
      "step": 1512
    },
    {
      "epoch": 24.21,
      "grad_norm": 0.05212974175810814,
      "learning_rate": 8.084758910933527e-06,
      "loss": 0.0006,
      "step": 1513
    },
    {
      "epoch": 24.22,
      "grad_norm": 0.05371742323040962,
      "learning_rate": 8.08223538279735e-06,
      "loss": 0.0009,
      "step": 1514
    },
    {
      "epoch": 24.24,
      "grad_norm": 0.5422487258911133,
      "learning_rate": 8.079710587670633e-06,
      "loss": 0.0041,
      "step": 1515
    },
    {
      "epoch": 24.26,
      "grad_norm": 0.07089779525995255,
      "learning_rate": 8.077184526591224e-06,
      "loss": 0.0007,
      "step": 1516
    },
    {
      "epoch": 24.27,
      "grad_norm": 0.38264426589012146,
      "learning_rate": 8.074657200597492e-06,
      "loss": 0.002,
      "step": 1517
    },
    {
      "epoch": 24.29,
      "grad_norm": 4.044142246246338,
      "learning_rate": 8.072128610728324e-06,
      "loss": 0.0304,
      "step": 1518
    },
    {
      "epoch": 24.3,
      "grad_norm": 1.7172735929489136,
      "learning_rate": 8.06959875802313e-06,
      "loss": 0.0123,
      "step": 1519
    },
    {
      "epoch": 24.32,
      "grad_norm": 1.3049547672271729,
      "learning_rate": 8.067067643521834e-06,
      "loss": 0.0296,
      "step": 1520
    },
    {
      "epoch": 24.34,
      "grad_norm": 1.2288603782653809,
      "learning_rate": 8.064535268264883e-06,
      "loss": 0.018,
      "step": 1521
    },
    {
      "epoch": 24.35,
      "grad_norm": 0.8564816117286682,
      "learning_rate": 8.062001633293242e-06,
      "loss": 0.008,
      "step": 1522
    },
    {
      "epoch": 24.37,
      "grad_norm": 0.046479981392621994,
      "learning_rate": 8.05946673964839e-06,
      "loss": 0.0004,
      "step": 1523
    },
    {
      "epoch": 24.38,
      "grad_norm": 0.033331889659166336,
      "learning_rate": 8.056930588372329e-06,
      "loss": 0.0008,
      "step": 1524
    },
    {
      "epoch": 24.4,
      "grad_norm": 0.6551024317741394,
      "learning_rate": 8.054393180507572e-06,
      "loss": 0.0048,
      "step": 1525
    },
    {
      "epoch": 24.42,
      "grad_norm": 0.6955334544181824,
      "learning_rate": 8.051854517097154e-06,
      "loss": 0.0038,
      "step": 1526
    },
    {
      "epoch": 24.43,
      "grad_norm": 1.559824824333191,
      "learning_rate": 8.049314599184623e-06,
      "loss": 0.006,
      "step": 1527
    },
    {
      "epoch": 24.45,
      "grad_norm": 0.16654594242572784,
      "learning_rate": 8.046773427814043e-06,
      "loss": 0.0011,
      "step": 1528
    },
    {
      "epoch": 24.46,
      "grad_norm": 1.0674562454223633,
      "learning_rate": 8.044231004029993e-06,
      "loss": 0.011,
      "step": 1529
    },
    {
      "epoch": 24.48,
      "grad_norm": 0.8231514692306519,
      "learning_rate": 8.041687328877566e-06,
      "loss": 0.0133,
      "step": 1530
    },
    {
      "epoch": 24.5,
      "grad_norm": 0.8256330490112305,
      "learning_rate": 8.039142403402374e-06,
      "loss": 0.0067,
      "step": 1531
    },
    {
      "epoch": 24.51,
      "grad_norm": 0.040822189301252365,
      "learning_rate": 8.03659622865054e-06,
      "loss": 0.0005,
      "step": 1532
    },
    {
      "epoch": 24.53,
      "grad_norm": 0.26834991574287415,
      "learning_rate": 8.034048805668697e-06,
      "loss": 0.0018,
      "step": 1533
    },
    {
      "epoch": 24.54,
      "grad_norm": 2.6114516258239746,
      "learning_rate": 8.031500135503995e-06,
      "loss": 0.0356,
      "step": 1534
    },
    {
      "epoch": 24.56,
      "grad_norm": 1.493125557899475,
      "learning_rate": 8.0289502192041e-06,
      "loss": 0.0178,
      "step": 1535
    },
    {
      "epoch": 24.58,
      "grad_norm": 1.2572534084320068,
      "learning_rate": 8.026399057817182e-06,
      "loss": 0.0155,
      "step": 1536
    },
    {
      "epoch": 24.59,
      "grad_norm": 0.04011926427483559,
      "learning_rate": 8.023846652391928e-06,
      "loss": 0.0005,
      "step": 1537
    },
    {
      "epoch": 24.61,
      "grad_norm": 0.7481503486633301,
      "learning_rate": 8.021293003977538e-06,
      "loss": 0.0055,
      "step": 1538
    },
    {
      "epoch": 24.62,
      "grad_norm": 1.8489320278167725,
      "learning_rate": 8.018738113623714e-06,
      "loss": 0.0311,
      "step": 1539
    },
    {
      "epoch": 24.64,
      "grad_norm": 1.3080251216888428,
      "learning_rate": 8.016181982380682e-06,
      "loss": 0.0184,
      "step": 1540
    },
    {
      "epoch": 24.66,
      "grad_norm": 0.16325969994068146,
      "learning_rate": 8.01362461129917e-06,
      "loss": 0.001,
      "step": 1541
    },
    {
      "epoch": 24.67,
      "grad_norm": 0.10699523240327835,
      "learning_rate": 8.011066001430412e-06,
      "loss": 0.0007,
      "step": 1542
    },
    {
      "epoch": 24.69,
      "grad_norm": 4.239792346954346,
      "learning_rate": 8.008506153826158e-06,
      "loss": 0.0488,
      "step": 1543
    },
    {
      "epoch": 24.7,
      "grad_norm": 0.8700094223022461,
      "learning_rate": 8.005945069538668e-06,
      "loss": 0.0051,
      "step": 1544
    },
    {
      "epoch": 24.72,
      "grad_norm": 0.10304064303636551,
      "learning_rate": 8.003382749620704e-06,
      "loss": 0.0006,
      "step": 1545
    },
    {
      "epoch": 24.74,
      "grad_norm": 0.460961252450943,
      "learning_rate": 8.000819195125537e-06,
      "loss": 0.0086,
      "step": 1546
    },
    {
      "epoch": 24.75,
      "grad_norm": 1.1560901403427124,
      "learning_rate": 7.998254407106952e-06,
      "loss": 0.0242,
      "step": 1547
    },
    {
      "epoch": 24.77,
      "grad_norm": 0.4648929834365845,
      "learning_rate": 7.995688386619233e-06,
      "loss": 0.0027,
      "step": 1548
    },
    {
      "epoch": 24.78,
      "grad_norm": 3.939918279647827,
      "learning_rate": 7.993121134717177e-06,
      "loss": 0.0299,
      "step": 1549
    },
    {
      "epoch": 24.8,
      "grad_norm": 1.7081252336502075,
      "learning_rate": 7.99055265245608e-06,
      "loss": 0.0295,
      "step": 1550
    },
    {
      "epoch": 24.82,
      "grad_norm": 0.1253833770751953,
      "learning_rate": 7.987982940891751e-06,
      "loss": 0.0012,
      "step": 1551
    },
    {
      "epoch": 24.83,
      "grad_norm": 0.46304038166999817,
      "learning_rate": 7.985412001080503e-06,
      "loss": 0.0034,
      "step": 1552
    },
    {
      "epoch": 24.85,
      "grad_norm": 0.1001957580447197,
      "learning_rate": 7.98283983407915e-06,
      "loss": 0.0008,
      "step": 1553
    },
    {
      "epoch": 24.86,
      "grad_norm": 0.05260187387466431,
      "learning_rate": 7.980266440945013e-06,
      "loss": 0.0007,
      "step": 1554
    },
    {
      "epoch": 24.88,
      "grad_norm": 1.197145700454712,
      "learning_rate": 7.977691822735914e-06,
      "loss": 0.0163,
      "step": 1555
    },
    {
      "epoch": 24.9,
      "grad_norm": 0.035064324736595154,
      "learning_rate": 7.975115980510187e-06,
      "loss": 0.0005,
      "step": 1556
    },
    {
      "epoch": 24.91,
      "grad_norm": 0.8434139490127563,
      "learning_rate": 7.972538915326658e-06,
      "loss": 0.0137,
      "step": 1557
    },
    {
      "epoch": 24.93,
      "grad_norm": 0.8607823848724365,
      "learning_rate": 7.969960628244664e-06,
      "loss": 0.0021,
      "step": 1558
    },
    {
      "epoch": 24.94,
      "grad_norm": 4.802082061767578,
      "learning_rate": 7.967381120324043e-06,
      "loss": 0.0109,
      "step": 1559
    },
    {
      "epoch": 24.96,
      "grad_norm": 0.08649466931819916,
      "learning_rate": 7.96480039262513e-06,
      "loss": 0.0006,
      "step": 1560
    },
    {
      "epoch": 24.98,
      "grad_norm": 3.554959774017334,
      "learning_rate": 7.962218446208765e-06,
      "loss": 0.0117,
      "step": 1561
    },
    {
      "epoch": 24.99,
      "grad_norm": 0.764958918094635,
      "learning_rate": 7.959635282136292e-06,
      "loss": 0.0133,
      "step": 1562
    },
    {
      "epoch": 25.01,
      "grad_norm": 0.9422463774681091,
      "learning_rate": 7.957050901469544e-06,
      "loss": 0.0137,
      "step": 1563
    },
    {
      "epoch": 25.02,
      "grad_norm": 0.179290771484375,
      "learning_rate": 7.954465305270875e-06,
      "loss": 0.0016,
      "step": 1564
    },
    {
      "epoch": 25.04,
      "grad_norm": 1.9386759996414185,
      "learning_rate": 7.951878494603116e-06,
      "loss": 0.0146,
      "step": 1565
    },
    {
      "epoch": 25.06,
      "grad_norm": 0.02956044301390648,
      "learning_rate": 7.949290470529609e-06,
      "loss": 0.0006,
      "step": 1566
    },
    {
      "epoch": 25.07,
      "grad_norm": 0.027481958270072937,
      "learning_rate": 7.946701234114195e-06,
      "loss": 0.0003,
      "step": 1567
    },
    {
      "epoch": 25.09,
      "grad_norm": 0.30593064427375793,
      "learning_rate": 7.94411078642121e-06,
      "loss": 0.002,
      "step": 1568
    },
    {
      "epoch": 25.1,
      "grad_norm": 3.630953311920166,
      "learning_rate": 7.94151912851549e-06,
      "loss": 0.0276,
      "step": 1569
    },
    {
      "epoch": 25.12,
      "grad_norm": 0.05372767150402069,
      "learning_rate": 7.938926261462366e-06,
      "loss": 0.0008,
      "step": 1570
    },
    {
      "epoch": 25.14,
      "grad_norm": 0.033191874623298645,
      "learning_rate": 7.936332186327672e-06,
      "loss": 0.0005,
      "step": 1571
    },
    {
      "epoch": 25.15,
      "grad_norm": 0.5454537868499756,
      "learning_rate": 7.933736904177727e-06,
      "loss": 0.0057,
      "step": 1572
    },
    {
      "epoch": 25.17,
      "grad_norm": 0.2861986458301544,
      "learning_rate": 7.931140416079359e-06,
      "loss": 0.0038,
      "step": 1573
    },
    {
      "epoch": 25.18,
      "grad_norm": 0.907167375087738,
      "learning_rate": 7.928542723099886e-06,
      "loss": 0.0107,
      "step": 1574
    },
    {
      "epoch": 25.2,
      "grad_norm": 0.09564879536628723,
      "learning_rate": 7.925943826307119e-06,
      "loss": 0.0009,
      "step": 1575
    },
    {
      "epoch": 25.22,
      "grad_norm": 0.4071919322013855,
      "learning_rate": 7.923343726769368e-06,
      "loss": 0.0034,
      "step": 1576
    },
    {
      "epoch": 25.23,
      "grad_norm": 0.5633785128593445,
      "learning_rate": 7.920742425555436e-06,
      "loss": 0.0098,
      "step": 1577
    },
    {
      "epoch": 25.25,
      "grad_norm": 5.287714004516602,
      "learning_rate": 7.918139923734618e-06,
      "loss": 0.0241,
      "step": 1578
    },
    {
      "epoch": 25.26,
      "grad_norm": 1.0149683952331543,
      "learning_rate": 7.915536222376705e-06,
      "loss": 0.0141,
      "step": 1579
    },
    {
      "epoch": 25.28,
      "grad_norm": 0.8680214285850525,
      "learning_rate": 7.912931322551981e-06,
      "loss": 0.0083,
      "step": 1580
    },
    {
      "epoch": 25.3,
      "grad_norm": 0.9673311114311218,
      "learning_rate": 7.910325225331221e-06,
      "loss": 0.0152,
      "step": 1581
    },
    {
      "epoch": 25.31,
      "grad_norm": 2.250152111053467,
      "learning_rate": 7.907717931785694e-06,
      "loss": 0.0112,
      "step": 1582
    },
    {
      "epoch": 25.33,
      "grad_norm": 0.0839349552989006,
      "learning_rate": 7.905109442987159e-06,
      "loss": 0.0009,
      "step": 1583
    },
    {
      "epoch": 25.34,
      "grad_norm": 0.7542096376419067,
      "learning_rate": 7.902499760007867e-06,
      "loss": 0.0077,
      "step": 1584
    },
    {
      "epoch": 25.36,
      "grad_norm": 0.7042699456214905,
      "learning_rate": 7.89988888392056e-06,
      "loss": 0.0043,
      "step": 1585
    },
    {
      "epoch": 25.38,
      "grad_norm": 1.480542540550232,
      "learning_rate": 7.897276815798476e-06,
      "loss": 0.0152,
      "step": 1586
    },
    {
      "epoch": 25.39,
      "grad_norm": 0.04733388125896454,
      "learning_rate": 7.894663556715328e-06,
      "loss": 0.0005,
      "step": 1587
    },
    {
      "epoch": 25.41,
      "grad_norm": 1.5381529331207275,
      "learning_rate": 7.892049107745334e-06,
      "loss": 0.0139,
      "step": 1588
    },
    {
      "epoch": 25.42,
      "grad_norm": 0.0899926945567131,
      "learning_rate": 7.889433469963195e-06,
      "loss": 0.0011,
      "step": 1589
    },
    {
      "epoch": 25.44,
      "grad_norm": 0.7973585724830627,
      "learning_rate": 7.886816644444099e-06,
      "loss": 0.011,
      "step": 1590
    },
    {
      "epoch": 25.46,
      "grad_norm": 0.7132837176322937,
      "learning_rate": 7.884198632263725e-06,
      "loss": 0.0101,
      "step": 1591
    },
    {
      "epoch": 25.47,
      "grad_norm": 0.9336450695991516,
      "learning_rate": 7.881579434498239e-06,
      "loss": 0.0048,
      "step": 1592
    },
    {
      "epoch": 25.49,
      "grad_norm": 0.7829710245132446,
      "learning_rate": 7.878959052224294e-06,
      "loss": 0.0094,
      "step": 1593
    },
    {
      "epoch": 25.5,
      "grad_norm": 1.436684012413025,
      "learning_rate": 7.87633748651903e-06,
      "loss": 0.0147,
      "step": 1594
    },
    {
      "epoch": 25.52,
      "grad_norm": 0.027908217161893845,
      "learning_rate": 7.873714738460075e-06,
      "loss": 0.0006,
      "step": 1595
    },
    {
      "epoch": 25.54,
      "grad_norm": 0.06080025061964989,
      "learning_rate": 7.87109080912554e-06,
      "loss": 0.0005,
      "step": 1596
    },
    {
      "epoch": 25.55,
      "grad_norm": 1.3142197132110596,
      "learning_rate": 7.868465699594025e-06,
      "loss": 0.0212,
      "step": 1597
    },
    {
      "epoch": 25.57,
      "grad_norm": 1.3610825538635254,
      "learning_rate": 7.865839410944613e-06,
      "loss": 0.0023,
      "step": 1598
    },
    {
      "epoch": 25.58,
      "grad_norm": 0.14669202268123627,
      "learning_rate": 7.86321194425687e-06,
      "loss": 0.0004,
      "step": 1599
    },
    {
      "epoch": 25.6,
      "grad_norm": 0.2937353551387787,
      "learning_rate": 7.860583300610849e-06,
      "loss": 0.0013,
      "step": 1600
    },
    {
      "epoch": 25.62,
      "grad_norm": 1.358446717262268,
      "learning_rate": 7.857953481087089e-06,
      "loss": 0.0164,
      "step": 1601
    },
    {
      "epoch": 25.63,
      "grad_norm": 1.1968085765838623,
      "learning_rate": 7.855322486766605e-06,
      "loss": 0.0161,
      "step": 1602
    },
    {
      "epoch": 25.65,
      "grad_norm": 0.041488952934741974,
      "learning_rate": 7.852690318730903e-06,
      "loss": 0.0006,
      "step": 1603
    },
    {
      "epoch": 25.66,
      "grad_norm": 0.5485746264457703,
      "learning_rate": 7.850056978061966e-06,
      "loss": 0.0066,
      "step": 1604
    },
    {
      "epoch": 25.68,
      "grad_norm": 0.91071617603302,
      "learning_rate": 7.84742246584226e-06,
      "loss": 0.0122,
      "step": 1605
    },
    {
      "epoch": 25.7,
      "grad_norm": 0.030651405453681946,
      "learning_rate": 7.844786783154737e-06,
      "loss": 0.0005,
      "step": 1606
    },
    {
      "epoch": 25.71,
      "grad_norm": 4.281207084655762,
      "learning_rate": 7.842149931082824e-06,
      "loss": 0.0149,
      "step": 1607
    },
    {
      "epoch": 25.73,
      "grad_norm": 1.315946102142334,
      "learning_rate": 7.839511910710431e-06,
      "loss": 0.0095,
      "step": 1608
    },
    {
      "epoch": 25.74,
      "grad_norm": 0.0412859246134758,
      "learning_rate": 7.83687272312195e-06,
      "loss": 0.0005,
      "step": 1609
    },
    {
      "epoch": 25.76,
      "grad_norm": 1.0999616384506226,
      "learning_rate": 7.83423236940225e-06,
      "loss": 0.0164,
      "step": 1610
    },
    {
      "epoch": 25.78,
      "grad_norm": 1.6934057474136353,
      "learning_rate": 7.831590850636682e-06,
      "loss": 0.0204,
      "step": 1611
    },
    {
      "epoch": 25.79,
      "grad_norm": 0.060261320322752,
      "learning_rate": 7.828948167911073e-06,
      "loss": 0.0006,
      "step": 1612
    },
    {
      "epoch": 25.81,
      "grad_norm": 0.8422393202781677,
      "learning_rate": 7.826304322311731e-06,
      "loss": 0.0149,
      "step": 1613
    },
    {
      "epoch": 25.82,
      "grad_norm": 3.2195303440093994,
      "learning_rate": 7.823659314925443e-06,
      "loss": 0.0018,
      "step": 1614
    },
    {
      "epoch": 25.84,
      "grad_norm": 0.1024371013045311,
      "learning_rate": 7.821013146839467e-06,
      "loss": 0.0006,
      "step": 1615
    },
    {
      "epoch": 25.86,
      "grad_norm": 1.206626296043396,
      "learning_rate": 7.818365819141545e-06,
      "loss": 0.0232,
      "step": 1616
    },
    {
      "epoch": 25.87,
      "grad_norm": 2.679884672164917,
      "learning_rate": 7.815717332919897e-06,
      "loss": 0.0038,
      "step": 1617
    },
    {
      "epoch": 25.89,
      "grad_norm": 0.12135057896375656,
      "learning_rate": 7.81306768926321e-06,
      "loss": 0.0006,
      "step": 1618
    },
    {
      "epoch": 25.9,
      "grad_norm": 0.034800417721271515,
      "learning_rate": 7.810416889260653e-06,
      "loss": 0.0004,
      "step": 1619
    },
    {
      "epoch": 25.92,
      "grad_norm": 0.07387367635965347,
      "learning_rate": 7.807764934001875e-06,
      "loss": 0.0006,
      "step": 1620
    },
    {
      "epoch": 25.94,
      "grad_norm": 0.671897292137146,
      "learning_rate": 7.80511182457699e-06,
      "loss": 0.0041,
      "step": 1621
    },
    {
      "epoch": 25.95,
      "grad_norm": 0.03433603793382645,
      "learning_rate": 7.80245756207659e-06,
      "loss": 0.0004,
      "step": 1622
    },
    {
      "epoch": 25.97,
      "grad_norm": 0.08393082022666931,
      "learning_rate": 7.799802147591747e-06,
      "loss": 0.0015,
      "step": 1623
    },
    {
      "epoch": 25.98,
      "grad_norm": 0.022794535383582115,
      "learning_rate": 7.797145582213998e-06,
      "loss": 0.0003,
      "step": 1624
    },
    {
      "epoch": 26.0,
      "grad_norm": 0.920438289642334,
      "learning_rate": 7.794487867035358e-06,
      "loss": 0.0126,
      "step": 1625
    },
    {
      "epoch": 26.02,
      "grad_norm": 0.7570119500160217,
      "learning_rate": 7.791829003148313e-06,
      "loss": 0.0009,
      "step": 1626
    },
    {
      "epoch": 26.03,
      "grad_norm": 0.036941178143024445,
      "learning_rate": 7.789168991645821e-06,
      "loss": 0.0004,
      "step": 1627
    },
    {
      "epoch": 26.05,
      "grad_norm": 0.09159447252750397,
      "learning_rate": 7.786507833621314e-06,
      "loss": 0.0005,
      "step": 1628
    },
    {
      "epoch": 26.06,
      "grad_norm": 0.20165807008743286,
      "learning_rate": 7.78384553016869e-06,
      "loss": 0.0008,
      "step": 1629
    },
    {
      "epoch": 26.08,
      "grad_norm": 0.04582185670733452,
      "learning_rate": 7.781182082382325e-06,
      "loss": 0.0007,
      "step": 1630
    },
    {
      "epoch": 26.1,
      "grad_norm": 0.795978844165802,
      "learning_rate": 7.77851749135706e-06,
      "loss": 0.001,
      "step": 1631
    },
    {
      "epoch": 26.11,
      "grad_norm": 0.3497416079044342,
      "learning_rate": 7.775851758188209e-06,
      "loss": 0.0026,
      "step": 1632
    },
    {
      "epoch": 26.13,
      "grad_norm": 0.15406477451324463,
      "learning_rate": 7.773184883971553e-06,
      "loss": 0.0006,
      "step": 1633
    },
    {
      "epoch": 26.14,
      "grad_norm": 0.022956056520342827,
      "learning_rate": 7.770516869803341e-06,
      "loss": 0.0005,
      "step": 1634
    },
    {
      "epoch": 26.16,
      "grad_norm": 0.7402378916740417,
      "learning_rate": 7.767847716780297e-06,
      "loss": 0.0126,
      "step": 1635
    },
    {
      "epoch": 26.18,
      "grad_norm": 0.716486930847168,
      "learning_rate": 7.765177425999609e-06,
      "loss": 0.0062,
      "step": 1636
    },
    {
      "epoch": 26.19,
      "grad_norm": 1.7132242918014526,
      "learning_rate": 7.76250599855893e-06,
      "loss": 0.0236,
      "step": 1637
    },
    {
      "epoch": 26.21,
      "grad_norm": 2.5315542221069336,
      "learning_rate": 7.759833435556381e-06,
      "loss": 0.0296,
      "step": 1638
    },
    {
      "epoch": 26.22,
      "grad_norm": 0.9416122436523438,
      "learning_rate": 7.757159738090558e-06,
      "loss": 0.0112,
      "step": 1639
    },
    {
      "epoch": 26.24,
      "grad_norm": 1.0671517848968506,
      "learning_rate": 7.754484907260513e-06,
      "loss": 0.0094,
      "step": 1640
    },
    {
      "epoch": 26.26,
      "grad_norm": 0.025093577802181244,
      "learning_rate": 7.751808944165769e-06,
      "loss": 0.0004,
      "step": 1641
    },
    {
      "epoch": 26.27,
      "grad_norm": 0.26864728331565857,
      "learning_rate": 7.749131849906313e-06,
      "loss": 0.0005,
      "step": 1642
    },
    {
      "epoch": 26.29,
      "grad_norm": 0.021869122982025146,
      "learning_rate": 7.746453625582596e-06,
      "loss": 0.0004,
      "step": 1643
    },
    {
      "epoch": 26.3,
      "grad_norm": 0.6925774216651917,
      "learning_rate": 7.743774272295538e-06,
      "loss": 0.0097,
      "step": 1644
    },
    {
      "epoch": 26.32,
      "grad_norm": 0.8310799598693848,
      "learning_rate": 7.741093791146517e-06,
      "loss": 0.0114,
      "step": 1645
    },
    {
      "epoch": 26.34,
      "grad_norm": 0.1253201812505722,
      "learning_rate": 7.738412183237379e-06,
      "loss": 0.0013,
      "step": 1646
    },
    {
      "epoch": 26.35,
      "grad_norm": 0.03213302046060562,
      "learning_rate": 7.73572944967043e-06,
      "loss": 0.0004,
      "step": 1647
    },
    {
      "epoch": 26.37,
      "grad_norm": 1.1788609027862549,
      "learning_rate": 7.733045591548442e-06,
      "loss": 0.0062,
      "step": 1648
    },
    {
      "epoch": 26.38,
      "grad_norm": 0.9093437194824219,
      "learning_rate": 7.73036060997465e-06,
      "loss": 0.0098,
      "step": 1649
    },
    {
      "epoch": 26.4,
      "grad_norm": 0.4561013877391815,
      "learning_rate": 7.727674506052744e-06,
      "loss": 0.0012,
      "step": 1650
    },
    {
      "epoch": 26.42,
      "grad_norm": 0.282758891582489,
      "learning_rate": 7.724987280886882e-06,
      "loss": 0.0018,
      "step": 1651
    },
    {
      "epoch": 26.43,
      "grad_norm": 0.08026561886072159,
      "learning_rate": 7.72229893558168e-06,
      "loss": 0.0014,
      "step": 1652
    },
    {
      "epoch": 26.45,
      "grad_norm": 0.016184644773602486,
      "learning_rate": 7.719609471242217e-06,
      "loss": 0.0003,
      "step": 1653
    },
    {
      "epoch": 26.46,
      "grad_norm": 0.19651249051094055,
      "learning_rate": 7.716918888974029e-06,
      "loss": 0.0009,
      "step": 1654
    },
    {
      "epoch": 26.48,
      "grad_norm": 0.4691280722618103,
      "learning_rate": 7.714227189883112e-06,
      "loss": 0.0063,
      "step": 1655
    },
    {
      "epoch": 26.5,
      "grad_norm": 0.3448043167591095,
      "learning_rate": 7.711534375075923e-06,
      "loss": 0.0041,
      "step": 1656
    },
    {
      "epoch": 26.51,
      "grad_norm": 0.1706351339817047,
      "learning_rate": 7.708840445659376e-06,
      "loss": 0.0016,
      "step": 1657
    },
    {
      "epoch": 26.53,
      "grad_norm": 1.2636969089508057,
      "learning_rate": 7.706145402740843e-06,
      "loss": 0.021,
      "step": 1658
    },
    {
      "epoch": 26.54,
      "grad_norm": 0.30189281702041626,
      "learning_rate": 7.703449247428157e-06,
      "loss": 0.0021,
      "step": 1659
    },
    {
      "epoch": 26.56,
      "grad_norm": 0.04903094843029976,
      "learning_rate": 7.700751980829601e-06,
      "loss": 0.0005,
      "step": 1660
    },
    {
      "epoch": 26.58,
      "grad_norm": 1.330519437789917,
      "learning_rate": 7.698053604053923e-06,
      "loss": 0.0198,
      "step": 1661
    },
    {
      "epoch": 26.59,
      "grad_norm": 0.08337680250406265,
      "learning_rate": 7.695354118210322e-06,
      "loss": 0.0007,
      "step": 1662
    },
    {
      "epoch": 26.61,
      "grad_norm": 0.7609817981719971,
      "learning_rate": 7.692653524408458e-06,
      "loss": 0.0094,
      "step": 1663
    },
    {
      "epoch": 26.62,
      "grad_norm": 0.8716785311698914,
      "learning_rate": 7.689951823758439e-06,
      "loss": 0.0081,
      "step": 1664
    },
    {
      "epoch": 26.64,
      "grad_norm": 0.14769771695137024,
      "learning_rate": 7.687249017370832e-06,
      "loss": 0.0014,
      "step": 1665
    },
    {
      "epoch": 26.66,
      "grad_norm": 0.7992679476737976,
      "learning_rate": 7.684545106356663e-06,
      "loss": 0.0052,
      "step": 1666
    },
    {
      "epoch": 26.67,
      "grad_norm": 0.20114992558956146,
      "learning_rate": 7.681840091827404e-06,
      "loss": 0.0014,
      "step": 1667
    },
    {
      "epoch": 26.69,
      "grad_norm": 0.24961569905281067,
      "learning_rate": 7.679133974894984e-06,
      "loss": 0.0016,
      "step": 1668
    },
    {
      "epoch": 26.7,
      "grad_norm": 1.4956387281417847,
      "learning_rate": 7.676426756671787e-06,
      "loss": 0.0258,
      "step": 1669
    },
    {
      "epoch": 26.72,
      "grad_norm": 0.08543536067008972,
      "learning_rate": 7.673718438270649e-06,
      "loss": 0.0007,
      "step": 1670
    },
    {
      "epoch": 26.74,
      "grad_norm": 0.29206788539886475,
      "learning_rate": 7.671009020804855e-06,
      "loss": 0.002,
      "step": 1671
    },
    {
      "epoch": 26.75,
      "grad_norm": 1.8204911947250366,
      "learning_rate": 7.668298505388146e-06,
      "loss": 0.006,
      "step": 1672
    },
    {
      "epoch": 26.77,
      "grad_norm": 7.473407745361328,
      "learning_rate": 7.665586893134711e-06,
      "loss": 0.0115,
      "step": 1673
    },
    {
      "epoch": 26.78,
      "grad_norm": 1.0078849792480469,
      "learning_rate": 7.662874185159193e-06,
      "loss": 0.0174,
      "step": 1674
    },
    {
      "epoch": 26.8,
      "grad_norm": 0.027389515191316605,
      "learning_rate": 7.660160382576683e-06,
      "loss": 0.0003,
      "step": 1675
    },
    {
      "epoch": 26.82,
      "grad_norm": 0.6552343964576721,
      "learning_rate": 7.657445486502723e-06,
      "loss": 0.0129,
      "step": 1676
    },
    {
      "epoch": 26.83,
      "grad_norm": 0.040078576654195786,
      "learning_rate": 7.654729498053305e-06,
      "loss": 0.0007,
      "step": 1677
    },
    {
      "epoch": 26.85,
      "grad_norm": 0.7046259641647339,
      "learning_rate": 7.652012418344867e-06,
      "loss": 0.0062,
      "step": 1678
    },
    {
      "epoch": 26.86,
      "grad_norm": 0.022067323327064514,
      "learning_rate": 7.6492942484943e-06,
      "loss": 0.0003,
      "step": 1679
    },
    {
      "epoch": 26.88,
      "grad_norm": 0.02536238171160221,
      "learning_rate": 7.646574989618938e-06,
      "loss": 0.0005,
      "step": 1680
    },
    {
      "epoch": 26.9,
      "grad_norm": 1.156436800956726,
      "learning_rate": 7.643854642836569e-06,
      "loss": 0.0106,
      "step": 1681
    },
    {
      "epoch": 26.91,
      "grad_norm": 0.7306748628616333,
      "learning_rate": 7.641133209265423e-06,
      "loss": 0.0073,
      "step": 1682
    },
    {
      "epoch": 26.93,
      "grad_norm": 0.021603666245937347,
      "learning_rate": 7.638410690024178e-06,
      "loss": 0.0002,
      "step": 1683
    },
    {
      "epoch": 26.94,
      "grad_norm": 0.7727325558662415,
      "learning_rate": 7.635687086231962e-06,
      "loss": 0.0128,
      "step": 1684
    },
    {
      "epoch": 26.96,
      "grad_norm": 0.38387539982795715,
      "learning_rate": 7.632962399008342e-06,
      "loss": 0.0008,
      "step": 1685
    },
    {
      "epoch": 26.98,
      "grad_norm": 0.5809688568115234,
      "learning_rate": 7.630236629473336e-06,
      "loss": 0.0093,
      "step": 1686
    },
    {
      "epoch": 26.99,
      "grad_norm": 1.104070782661438,
      "learning_rate": 7.627509778747404e-06,
      "loss": 0.0139,
      "step": 1687
    },
    {
      "epoch": 27.01,
      "grad_norm": 1.5307929515838623,
      "learning_rate": 7.624781847951453e-06,
      "loss": 0.0184,
      "step": 1688
    },
    {
      "epoch": 27.02,
      "grad_norm": 0.02646462246775627,
      "learning_rate": 7.622052838206829e-06,
      "loss": 0.0005,
      "step": 1689
    },
    {
      "epoch": 27.04,
      "grad_norm": 0.058978207409381866,
      "learning_rate": 7.619322750635327e-06,
      "loss": 0.0007,
      "step": 1690
    },
    {
      "epoch": 27.06,
      "grad_norm": 0.23879790306091309,
      "learning_rate": 7.616591586359186e-06,
      "loss": 0.001,
      "step": 1691
    },
    {
      "epoch": 27.07,
      "grad_norm": 5.285439491271973,
      "learning_rate": 7.613859346501078e-06,
      "loss": 0.0097,
      "step": 1692
    },
    {
      "epoch": 27.09,
      "grad_norm": 1.069305419921875,
      "learning_rate": 7.611126032184127e-06,
      "loss": 0.0145,
      "step": 1693
    },
    {
      "epoch": 27.1,
      "grad_norm": 0.9720348715782166,
      "learning_rate": 7.608391644531894e-06,
      "loss": 0.0165,
      "step": 1694
    },
    {
      "epoch": 27.12,
      "grad_norm": 0.4603033661842346,
      "learning_rate": 7.605656184668385e-06,
      "loss": 0.005,
      "step": 1695
    },
    {
      "epoch": 27.14,
      "grad_norm": 1.0642744302749634,
      "learning_rate": 7.602919653718044e-06,
      "loss": 0.0141,
      "step": 1696
    },
    {
      "epoch": 27.15,
      "grad_norm": 0.03232905641198158,
      "learning_rate": 7.600182052805752e-06,
      "loss": 0.0004,
      "step": 1697
    },
    {
      "epoch": 27.17,
      "grad_norm": 0.053263623267412186,
      "learning_rate": 7.597443383056837e-06,
      "loss": 0.0007,
      "step": 1698
    },
    {
      "epoch": 27.18,
      "grad_norm": 0.017883380874991417,
      "learning_rate": 7.59470364559706e-06,
      "loss": 0.0004,
      "step": 1699
    },
    {
      "epoch": 27.2,
      "grad_norm": 0.08493026345968246,
      "learning_rate": 7.591962841552627e-06,
      "loss": 0.0007,
      "step": 1700
    },
    {
      "epoch": 27.22,
      "grad_norm": 0.9424521923065186,
      "learning_rate": 7.589220972050174e-06,
      "loss": 0.0083,
      "step": 1701
    },
    {
      "epoch": 27.23,
      "grad_norm": 1.0961027145385742,
      "learning_rate": 7.586478038216785e-06,
      "loss": 0.0017,
      "step": 1702
    },
    {
      "epoch": 27.25,
      "grad_norm": 1.7932136058807373,
      "learning_rate": 7.583734041179973e-06,
      "loss": 0.0117,
      "step": 1703
    },
    {
      "epoch": 27.26,
      "grad_norm": 0.05686769261956215,
      "learning_rate": 7.580988982067694e-06,
      "loss": 0.0007,
      "step": 1704
    },
    {
      "epoch": 27.28,
      "grad_norm": 0.024583866819739342,
      "learning_rate": 7.578242862008336e-06,
      "loss": 0.0005,
      "step": 1705
    },
    {
      "epoch": 27.3,
      "grad_norm": 0.014799893833696842,
      "learning_rate": 7.575495682130726e-06,
      "loss": 0.0002,
      "step": 1706
    },
    {
      "epoch": 27.31,
      "grad_norm": 0.8292698860168457,
      "learning_rate": 7.572747443564125e-06,
      "loss": 0.0101,
      "step": 1707
    },
    {
      "epoch": 27.33,
      "grad_norm": 2.066401720046997,
      "learning_rate": 7.569998147438233e-06,
      "loss": 0.0063,
      "step": 1708
    },
    {
      "epoch": 27.34,
      "grad_norm": 0.8633266091346741,
      "learning_rate": 7.56724779488318e-06,
      "loss": 0.0154,
      "step": 1709
    },
    {
      "epoch": 27.36,
      "grad_norm": 1.2141706943511963,
      "learning_rate": 7.564496387029532e-06,
      "loss": 0.018,
      "step": 1710
    },
    {
      "epoch": 27.38,
      "grad_norm": 1.2934095859527588,
      "learning_rate": 7.561743925008287e-06,
      "loss": 0.0178,
      "step": 1711
    },
    {
      "epoch": 27.39,
      "grad_norm": 0.023518748581409454,
      "learning_rate": 7.558990409950881e-06,
      "loss": 0.0006,
      "step": 1712
    },
    {
      "epoch": 27.41,
      "grad_norm": 0.2783324718475342,
      "learning_rate": 7.55623584298918e-06,
      "loss": 0.0021,
      "step": 1713
    },
    {
      "epoch": 27.42,
      "grad_norm": 0.02105274610221386,
      "learning_rate": 7.553480225255481e-06,
      "loss": 0.0003,
      "step": 1714
    },
    {
      "epoch": 27.44,
      "grad_norm": 0.04457173869013786,
      "learning_rate": 7.550723557882514e-06,
      "loss": 0.0005,
      "step": 1715
    },
    {
      "epoch": 27.46,
      "grad_norm": 0.674683690071106,
      "learning_rate": 7.5479658420034415e-06,
      "loss": 0.0039,
      "step": 1716
    },
    {
      "epoch": 27.47,
      "grad_norm": 0.16821669042110443,
      "learning_rate": 7.545207078751858e-06,
      "loss": 0.0012,
      "step": 1717
    },
    {
      "epoch": 27.49,
      "grad_norm": 0.18582411110401154,
      "learning_rate": 7.542447269261783e-06,
      "loss": 0.0012,
      "step": 1718
    },
    {
      "epoch": 27.5,
      "grad_norm": 0.0820365697145462,
      "learning_rate": 7.539686414667674e-06,
      "loss": 0.0005,
      "step": 1719
    },
    {
      "epoch": 27.52,
      "grad_norm": 0.645645260810852,
      "learning_rate": 7.536924516104411e-06,
      "loss": 0.011,
      "step": 1720
    },
    {
      "epoch": 27.54,
      "grad_norm": 0.015999430790543556,
      "learning_rate": 7.5341615747073085e-06,
      "loss": 0.0002,
      "step": 1721
    },
    {
      "epoch": 27.55,
      "grad_norm": 0.0214612428098917,
      "learning_rate": 7.5313975916121065e-06,
      "loss": 0.0002,
      "step": 1722
    },
    {
      "epoch": 27.57,
      "grad_norm": 0.2041676789522171,
      "learning_rate": 7.528632567954973e-06,
      "loss": 0.0009,
      "step": 1723
    },
    {
      "epoch": 27.58,
      "grad_norm": 0.019593048840761185,
      "learning_rate": 7.5258665048725065e-06,
      "loss": 0.0004,
      "step": 1724
    },
    {
      "epoch": 27.6,
      "grad_norm": 0.13274641335010529,
      "learning_rate": 7.52309940350173e-06,
      "loss": 0.0006,
      "step": 1725
    },
    {
      "epoch": 27.62,
      "grad_norm": 1.0976722240447998,
      "learning_rate": 7.520331264980094e-06,
      "loss": 0.0126,
      "step": 1726
    },
    {
      "epoch": 27.63,
      "grad_norm": 0.36791542172431946,
      "learning_rate": 7.517562090445476e-06,
      "loss": 0.0029,
      "step": 1727
    },
    {
      "epoch": 27.65,
      "grad_norm": 0.6807746291160583,
      "learning_rate": 7.514791881036179e-06,
      "loss": 0.0059,
      "step": 1728
    },
    {
      "epoch": 27.66,
      "grad_norm": 0.106105737388134,
      "learning_rate": 7.512020637890931e-06,
      "loss": 0.0006,
      "step": 1729
    },
    {
      "epoch": 27.68,
      "grad_norm": 4.83043909072876,
      "learning_rate": 7.509248362148889e-06,
      "loss": 0.0153,
      "step": 1730
    },
    {
      "epoch": 27.7,
      "grad_norm": 1.3242063522338867,
      "learning_rate": 7.506475054949626e-06,
      "loss": 0.0133,
      "step": 1731
    },
    {
      "epoch": 27.71,
      "grad_norm": 1.967522144317627,
      "learning_rate": 7.503700717433145e-06,
      "loss": 0.0276,
      "step": 1732
    },
    {
      "epoch": 27.73,
      "grad_norm": 0.04886934533715248,
      "learning_rate": 7.5009253507398734e-06,
      "loss": 0.0009,
      "step": 1733
    },
    {
      "epoch": 27.74,
      "grad_norm": 0.09009650349617004,
      "learning_rate": 7.498148956010658e-06,
      "loss": 0.0005,
      "step": 1734
    },
    {
      "epoch": 27.76,
      "grad_norm": 1.6309319734573364,
      "learning_rate": 7.49537153438677e-06,
      "loss": 0.0327,
      "step": 1735
    },
    {
      "epoch": 27.78,
      "grad_norm": 0.7501580715179443,
      "learning_rate": 7.492593087009903e-06,
      "loss": 0.01,
      "step": 1736
    },
    {
      "epoch": 27.79,
      "grad_norm": 1.3123971223831177,
      "learning_rate": 7.489813615022171e-06,
      "loss": 0.0061,
      "step": 1737
    },
    {
      "epoch": 27.81,
      "grad_norm": 0.078004390001297,
      "learning_rate": 7.48703311956611e-06,
      "loss": 0.0006,
      "step": 1738
    },
    {
      "epoch": 27.82,
      "grad_norm": 0.6727762222290039,
      "learning_rate": 7.4842516017846775e-06,
      "loss": 0.0073,
      "step": 1739
    },
    {
      "epoch": 27.84,
      "grad_norm": 0.08667425811290741,
      "learning_rate": 7.481469062821252e-06,
      "loss": 0.0009,
      "step": 1740
    },
    {
      "epoch": 27.86,
      "grad_norm": 0.027175607159733772,
      "learning_rate": 7.478685503819625e-06,
      "loss": 0.0006,
      "step": 1741
    },
    {
      "epoch": 27.87,
      "grad_norm": 1.1486724615097046,
      "learning_rate": 7.475900925924015e-06,
      "loss": 0.01,
      "step": 1742
    },
    {
      "epoch": 27.89,
      "grad_norm": 0.050588686019182205,
      "learning_rate": 7.473115330279059e-06,
      "loss": 0.0005,
      "step": 1743
    },
    {
      "epoch": 27.9,
      "grad_norm": 0.5581552386283875,
      "learning_rate": 7.470328718029809e-06,
      "loss": 0.001,
      "step": 1744
    },
    {
      "epoch": 27.92,
      "grad_norm": 0.016905205324292183,
      "learning_rate": 7.467541090321735e-06,
      "loss": 0.0003,
      "step": 1745
    },
    {
      "epoch": 27.94,
      "grad_norm": 0.678132176399231,
      "learning_rate": 7.464752448300726e-06,
      "loss": 0.0013,
      "step": 1746
    },
    {
      "epoch": 27.95,
      "grad_norm": 1.1692701578140259,
      "learning_rate": 7.4619627931130864e-06,
      "loss": 0.0257,
      "step": 1747
    },
    {
      "epoch": 27.97,
      "grad_norm": 0.07613839954137802,
      "learning_rate": 7.45917212590554e-06,
      "loss": 0.0004,
      "step": 1748
    },
    {
      "epoch": 27.98,
      "grad_norm": 0.06203724443912506,
      "learning_rate": 7.456380447825223e-06,
      "loss": 0.0006,
      "step": 1749
    },
    {
      "epoch": 28.0,
      "grad_norm": 0.9243241548538208,
      "learning_rate": 7.453587760019691e-06,
      "loss": 0.0052,
      "step": 1750
    },
    {
      "epoch": 28.02,
      "grad_norm": 0.014843885786831379,
      "learning_rate": 7.45079406363691e-06,
      "loss": 0.0003,
      "step": 1751
    },
    {
      "epoch": 28.03,
      "grad_norm": 0.04473727568984032,
      "learning_rate": 7.447999359825263e-06,
      "loss": 0.0006,
      "step": 1752
    },
    {
      "epoch": 28.05,
      "grad_norm": 0.5758614540100098,
      "learning_rate": 7.445203649733549e-06,
      "loss": 0.0056,
      "step": 1753
    },
    {
      "epoch": 28.06,
      "grad_norm": 0.04696370288729668,
      "learning_rate": 7.442406934510977e-06,
      "loss": 0.0005,
      "step": 1754
    },
    {
      "epoch": 28.08,
      "grad_norm": 1.4216233491897583,
      "learning_rate": 7.439609215307173e-06,
      "loss": 0.0015,
      "step": 1755
    },
    {
      "epoch": 28.1,
      "grad_norm": 1.013550043106079,
      "learning_rate": 7.436810493272174e-06,
      "loss": 0.0167,
      "step": 1756
    },
    {
      "epoch": 28.11,
      "grad_norm": 0.016798174008727074,
      "learning_rate": 7.434010769556426e-06,
      "loss": 0.0003,
      "step": 1757
    },
    {
      "epoch": 28.13,
      "grad_norm": 2.0682430267333984,
      "learning_rate": 7.431210045310792e-06,
      "loss": 0.006,
      "step": 1758
    },
    {
      "epoch": 28.14,
      "grad_norm": 0.06234314665198326,
      "learning_rate": 7.428408321686542e-06,
      "loss": 0.0008,
      "step": 1759
    },
    {
      "epoch": 28.16,
      "grad_norm": 0.020496828481554985,
      "learning_rate": 7.42560559983536e-06,
      "loss": 0.0004,
      "step": 1760
    },
    {
      "epoch": 28.18,
      "grad_norm": 0.21130892634391785,
      "learning_rate": 7.42280188090934e-06,
      "loss": 0.0008,
      "step": 1761
    },
    {
      "epoch": 28.19,
      "grad_norm": 0.7067210078239441,
      "learning_rate": 7.419997166060985e-06,
      "loss": 0.0085,
      "step": 1762
    },
    {
      "epoch": 28.21,
      "grad_norm": 0.4298425316810608,
      "learning_rate": 7.417191456443204e-06,
      "loss": 0.0042,
      "step": 1763
    },
    {
      "epoch": 28.22,
      "grad_norm": 0.015298576094210148,
      "learning_rate": 7.414384753209323e-06,
      "loss": 0.0003,
      "step": 1764
    },
    {
      "epoch": 28.24,
      "grad_norm": 0.3486662805080414,
      "learning_rate": 7.411577057513066e-06,
      "loss": 0.0035,
      "step": 1765
    },
    {
      "epoch": 28.26,
      "grad_norm": 0.021042997017502785,
      "learning_rate": 7.408768370508577e-06,
      "loss": 0.0003,
      "step": 1766
    },
    {
      "epoch": 28.27,
      "grad_norm": 0.797940731048584,
      "learning_rate": 7.405958693350397e-06,
      "loss": 0.0117,
      "step": 1767
    },
    {
      "epoch": 28.29,
      "grad_norm": 0.4614511728286743,
      "learning_rate": 7.403148027193479e-06,
      "loss": 0.001,
      "step": 1768
    },
    {
      "epoch": 28.3,
      "grad_norm": 0.02490217238664627,
      "learning_rate": 7.400336373193182e-06,
      "loss": 0.0003,
      "step": 1769
    },
    {
      "epoch": 28.32,
      "grad_norm": 0.7308905124664307,
      "learning_rate": 7.39752373250527e-06,
      "loss": 0.0088,
      "step": 1770
    },
    {
      "epoch": 28.34,
      "grad_norm": 0.013606766238808632,
      "learning_rate": 7.394710106285915e-06,
      "loss": 0.0003,
      "step": 1771
    },
    {
      "epoch": 28.35,
      "grad_norm": 0.5091965794563293,
      "learning_rate": 7.39189549569169e-06,
      "loss": 0.005,
      "step": 1772
    },
    {
      "epoch": 28.37,
      "grad_norm": 3.0308656692504883,
      "learning_rate": 7.389079901879579e-06,
      "loss": 0.0083,
      "step": 1773
    },
    {
      "epoch": 28.38,
      "grad_norm": 0.6445861458778381,
      "learning_rate": 7.386263326006962e-06,
      "loss": 0.005,
      "step": 1774
    },
    {
      "epoch": 28.4,
      "grad_norm": 1.5265469551086426,
      "learning_rate": 7.383445769231628e-06,
      "loss": 0.0213,
      "step": 1775
    },
    {
      "epoch": 28.42,
      "grad_norm": 0.020945027470588684,
      "learning_rate": 7.380627232711769e-06,
      "loss": 0.0005,
      "step": 1776
    },
    {
      "epoch": 28.43,
      "grad_norm": 0.2521709203720093,
      "learning_rate": 7.377807717605979e-06,
      "loss": 0.0018,
      "step": 1777
    },
    {
      "epoch": 28.45,
      "grad_norm": 1.2975717782974243,
      "learning_rate": 7.374987225073256e-06,
      "loss": 0.0174,
      "step": 1778
    },
    {
      "epoch": 28.46,
      "grad_norm": 0.8152418732643127,
      "learning_rate": 7.372165756272994e-06,
      "loss": 0.0135,
      "step": 1779
    },
    {
      "epoch": 28.48,
      "grad_norm": 0.019331082701683044,
      "learning_rate": 7.369343312364994e-06,
      "loss": 0.0003,
      "step": 1780
    },
    {
      "epoch": 28.5,
      "grad_norm": 0.020084455609321594,
      "learning_rate": 7.3665198945094565e-06,
      "loss": 0.0003,
      "step": 1781
    },
    {
      "epoch": 28.51,
      "grad_norm": 0.13190887868404388,
      "learning_rate": 7.363695503866981e-06,
      "loss": 0.0011,
      "step": 1782
    },
    {
      "epoch": 28.53,
      "grad_norm": 0.7839522957801819,
      "learning_rate": 7.36087014159857e-06,
      "loss": 0.002,
      "step": 1783
    },
    {
      "epoch": 28.54,
      "grad_norm": 0.016180239617824554,
      "learning_rate": 7.358043808865621e-06,
      "loss": 0.0003,
      "step": 1784
    },
    {
      "epoch": 28.56,
      "grad_norm": 0.21114911139011383,
      "learning_rate": 7.355216506829933e-06,
      "loss": 0.0006,
      "step": 1785
    },
    {
      "epoch": 28.58,
      "grad_norm": 0.0196328517049551,
      "learning_rate": 7.352388236653704e-06,
      "loss": 0.0003,
      "step": 1786
    },
    {
      "epoch": 28.59,
      "grad_norm": 1.1453611850738525,
      "learning_rate": 7.3495589994995274e-06,
      "loss": 0.0103,
      "step": 1787
    },
    {
      "epoch": 28.61,
      "grad_norm": 0.1496434509754181,
      "learning_rate": 7.346728796530398e-06,
      "loss": 0.0018,
      "step": 1788
    },
    {
      "epoch": 28.62,
      "grad_norm": 0.12245181947946548,
      "learning_rate": 7.343897628909703e-06,
      "loss": 0.001,
      "step": 1789
    },
    {
      "epoch": 28.64,
      "grad_norm": 2.72152042388916,
      "learning_rate": 7.34106549780123e-06,
      "loss": 0.0141,
      "step": 1790
    },
    {
      "epoch": 28.66,
      "grad_norm": 0.8445964455604553,
      "learning_rate": 7.338232404369161e-06,
      "loss": 0.0127,
      "step": 1791
    },
    {
      "epoch": 28.67,
      "grad_norm": 0.024125393480062485,
      "learning_rate": 7.3353983497780725e-06,
      "loss": 0.0004,
      "step": 1792
    },
    {
      "epoch": 28.69,
      "grad_norm": 0.019978119060397148,
      "learning_rate": 7.332563335192938e-06,
      "loss": 0.0004,
      "step": 1793
    },
    {
      "epoch": 28.7,
      "grad_norm": 0.3752891421318054,
      "learning_rate": 7.3297273617791246e-06,
      "loss": 0.0024,
      "step": 1794
    },
    {
      "epoch": 28.72,
      "grad_norm": 0.6122468709945679,
      "learning_rate": 7.326890430702396e-06,
      "loss": 0.0045,
      "step": 1795
    },
    {
      "epoch": 28.74,
      "grad_norm": 0.06920573860406876,
      "learning_rate": 7.324052543128904e-06,
      "loss": 0.0007,
      "step": 1796
    },
    {
      "epoch": 28.75,
      "grad_norm": 0.9083302021026611,
      "learning_rate": 7.3212137002252e-06,
      "loss": 0.013,
      "step": 1797
    },
    {
      "epoch": 28.77,
      "grad_norm": 0.02960544265806675,
      "learning_rate": 7.318373903158222e-06,
      "loss": 0.0003,
      "step": 1798
    },
    {
      "epoch": 28.78,
      "grad_norm": 0.9237602353096008,
      "learning_rate": 7.3155331530953064e-06,
      "loss": 0.0123,
      "step": 1799
    },
    {
      "epoch": 28.8,
      "grad_norm": 0.9918788075447083,
      "learning_rate": 7.312691451204178e-06,
      "loss": 0.0192,
      "step": 1800
    },
    {
      "epoch": 28.82,
      "grad_norm": 0.6727921962738037,
      "learning_rate": 7.30984879865295e-06,
      "loss": 0.0118,
      "step": 1801
    },
    {
      "epoch": 28.83,
      "grad_norm": 5.905250072479248,
      "learning_rate": 7.307005196610132e-06,
      "loss": 0.0159,
      "step": 1802
    },
    {
      "epoch": 28.85,
      "grad_norm": 0.878153920173645,
      "learning_rate": 7.30416064624462e-06,
      "loss": 0.0108,
      "step": 1803
    },
    {
      "epoch": 28.86,
      "grad_norm": 0.01440215203911066,
      "learning_rate": 7.301315148725704e-06,
      "loss": 0.0003,
      "step": 1804
    },
    {
      "epoch": 28.88,
      "grad_norm": 1.1040215492248535,
      "learning_rate": 7.2984687052230585e-06,
      "loss": 0.0007,
      "step": 1805
    },
    {
      "epoch": 28.9,
      "grad_norm": 0.046042006462812424,
      "learning_rate": 7.295621316906748e-06,
      "loss": 0.0002,
      "step": 1806
    },
    {
      "epoch": 28.91,
      "grad_norm": 1.237108826637268,
      "learning_rate": 7.292772984947227e-06,
      "loss": 0.0261,
      "step": 1807
    },
    {
      "epoch": 28.93,
      "grad_norm": 1.26105535030365,
      "learning_rate": 7.289923710515338e-06,
      "loss": 0.0154,
      "step": 1808
    },
    {
      "epoch": 28.94,
      "grad_norm": 0.32260242104530334,
      "learning_rate": 7.287073494782309e-06,
      "loss": 0.0007,
      "step": 1809
    },
    {
      "epoch": 28.96,
      "grad_norm": 0.02608475461602211,
      "learning_rate": 7.284222338919758e-06,
      "loss": 0.0004,
      "step": 1810
    },
    {
      "epoch": 28.98,
      "grad_norm": 0.07215116173028946,
      "learning_rate": 7.281370244099686e-06,
      "loss": 0.0005,
      "step": 1811
    },
    {
      "epoch": 28.99,
      "grad_norm": 1.0744272470474243,
      "learning_rate": 7.278517211494481e-06,
      "loss": 0.0099,
      "step": 1812
    },
    {
      "epoch": 29.01,
      "grad_norm": 0.028070436790585518,
      "learning_rate": 7.275663242276918e-06,
      "loss": 0.0004,
      "step": 1813
    },
    {
      "epoch": 29.02,
      "grad_norm": 0.01625681482255459,
      "learning_rate": 7.2728083376201545e-06,
      "loss": 0.0004,
      "step": 1814
    },
    {
      "epoch": 29.04,
      "grad_norm": 0.7421656250953674,
      "learning_rate": 7.269952498697734e-06,
      "loss": 0.0094,
      "step": 1815
    },
    {
      "epoch": 29.06,
      "grad_norm": 0.022485265508294106,
      "learning_rate": 7.267095726683587e-06,
      "loss": 0.0005,
      "step": 1816
    },
    {
      "epoch": 29.07,
      "grad_norm": 0.052288707345724106,
      "learning_rate": 7.264238022752021e-06,
      "loss": 0.0006,
      "step": 1817
    },
    {
      "epoch": 29.09,
      "grad_norm": 0.42286914587020874,
      "learning_rate": 7.261379388077733e-06,
      "loss": 0.0032,
      "step": 1818
    },
    {
      "epoch": 29.1,
      "grad_norm": 0.03011932782828808,
      "learning_rate": 7.258519823835797e-06,
      "loss": 0.0004,
      "step": 1819
    },
    {
      "epoch": 29.12,
      "grad_norm": 0.15348531305789948,
      "learning_rate": 7.255659331201673e-06,
      "loss": 0.001,
      "step": 1820
    },
    {
      "epoch": 29.14,
      "grad_norm": 0.12404439598321915,
      "learning_rate": 7.252797911351203e-06,
      "loss": 0.0022,
      "step": 1821
    },
    {
      "epoch": 29.15,
      "grad_norm": 0.43497681617736816,
      "learning_rate": 7.249935565460606e-06,
      "loss": 0.0026,
      "step": 1822
    },
    {
      "epoch": 29.17,
      "grad_norm": 0.02395167388021946,
      "learning_rate": 7.247072294706487e-06,
      "loss": 0.0006,
      "step": 1823
    },
    {
      "epoch": 29.18,
      "grad_norm": 0.5726220607757568,
      "learning_rate": 7.244208100265826e-06,
      "loss": 0.006,
      "step": 1824
    },
    {
      "epoch": 29.2,
      "grad_norm": 0.017496220767498016,
      "learning_rate": 7.241342983315985e-06,
      "loss": 0.0003,
      "step": 1825
    },
    {
      "epoch": 29.22,
      "grad_norm": 0.23934641480445862,
      "learning_rate": 7.238476945034708e-06,
      "loss": 0.0016,
      "step": 1826
    },
    {
      "epoch": 29.23,
      "grad_norm": 0.85317063331604,
      "learning_rate": 7.235609986600114e-06,
      "loss": 0.01,
      "step": 1827
    },
    {
      "epoch": 29.25,
      "grad_norm": 0.015585456043481827,
      "learning_rate": 7.2327421091907006e-06,
      "loss": 0.0003,
      "step": 1828
    },
    {
      "epoch": 29.26,
      "grad_norm": 0.6430169343948364,
      "learning_rate": 7.2298733139853425e-06,
      "loss": 0.0108,
      "step": 1829
    },
    {
      "epoch": 29.28,
      "grad_norm": 0.027767758816480637,
      "learning_rate": 7.227003602163296e-06,
      "loss": 0.0003,
      "step": 1830
    },
    {
      "epoch": 29.3,
      "grad_norm": 0.013704836368560791,
      "learning_rate": 7.22413297490419e-06,
      "loss": 0.0002,
      "step": 1831
    },
    {
      "epoch": 29.31,
      "grad_norm": 0.8118894696235657,
      "learning_rate": 7.221261433388032e-06,
      "loss": 0.0108,
      "step": 1832
    },
    {
      "epoch": 29.33,
      "grad_norm": 0.01381542719900608,
      "learning_rate": 7.218388978795201e-06,
      "loss": 0.0003,
      "step": 1833
    },
    {
      "epoch": 29.34,
      "grad_norm": 3.689690351486206,
      "learning_rate": 7.2155156123064575e-06,
      "loss": 0.0051,
      "step": 1834
    },
    {
      "epoch": 29.36,
      "grad_norm": 0.7594605684280396,
      "learning_rate": 7.212641335102932e-06,
      "loss": 0.0061,
      "step": 1835
    },
    {
      "epoch": 29.38,
      "grad_norm": 0.01685464009642601,
      "learning_rate": 7.2097661483661355e-06,
      "loss": 0.0004,
      "step": 1836
    },
    {
      "epoch": 29.39,
      "grad_norm": 1.2082849740982056,
      "learning_rate": 7.206890053277943e-06,
      "loss": 0.0107,
      "step": 1837
    },
    {
      "epoch": 29.41,
      "grad_norm": 0.7496945858001709,
      "learning_rate": 7.204013051020612e-06,
      "loss": 0.0108,
      "step": 1838
    },
    {
      "epoch": 29.42,
      "grad_norm": 0.06975653022527695,
      "learning_rate": 7.201135142776768e-06,
      "loss": 0.0004,
      "step": 1839
    },
    {
      "epoch": 29.44,
      "grad_norm": 1.9515496492385864,
      "learning_rate": 7.198256329729412e-06,
      "loss": 0.0093,
      "step": 1840
    },
    {
      "epoch": 29.46,
      "grad_norm": 0.9065999388694763,
      "learning_rate": 7.1953766130619125e-06,
      "loss": 0.0141,
      "step": 1841
    },
    {
      "epoch": 29.47,
      "grad_norm": 0.02455953136086464,
      "learning_rate": 7.192495993958015e-06,
      "loss": 0.0003,
      "step": 1842
    },
    {
      "epoch": 29.49,
      "grad_norm": 0.04565669968724251,
      "learning_rate": 7.189614473601832e-06,
      "loss": 0.0003,
      "step": 1843
    },
    {
      "epoch": 29.5,
      "grad_norm": 0.17364159226417542,
      "learning_rate": 7.186732053177848e-06,
      "loss": 0.0006,
      "step": 1844
    },
    {
      "epoch": 29.52,
      "grad_norm": 0.026202647015452385,
      "learning_rate": 7.183848733870917e-06,
      "loss": 0.0003,
      "step": 1845
    },
    {
      "epoch": 29.54,
      "grad_norm": 0.5965250730514526,
      "learning_rate": 7.180964516866263e-06,
      "loss": 0.0068,
      "step": 1846
    },
    {
      "epoch": 29.55,
      "grad_norm": 0.04204169660806656,
      "learning_rate": 7.178079403349478e-06,
      "loss": 0.0005,
      "step": 1847
    },
    {
      "epoch": 29.57,
      "grad_norm": 0.899124801158905,
      "learning_rate": 7.175193394506523e-06,
      "loss": 0.0122,
      "step": 1848
    },
    {
      "epoch": 29.58,
      "grad_norm": 0.03813905268907547,
      "learning_rate": 7.17230649152373e-06,
      "loss": 0.0008,
      "step": 1849
    },
    {
      "epoch": 29.6,
      "grad_norm": 0.30996957421302795,
      "learning_rate": 7.169418695587791e-06,
      "loss": 0.0018,
      "step": 1850
    },
    {
      "epoch": 29.62,
      "grad_norm": 1.5883221626281738,
      "learning_rate": 7.166530007885774e-06,
      "loss": 0.022,
      "step": 1851
    },
    {
      "epoch": 29.63,
      "grad_norm": 0.5118577480316162,
      "learning_rate": 7.163640429605106e-06,
      "loss": 0.003,
      "step": 1852
    },
    {
      "epoch": 29.65,
      "grad_norm": 0.7551109194755554,
      "learning_rate": 7.160749961933586e-06,
      "loss": 0.0011,
      "step": 1853
    },
    {
      "epoch": 29.66,
      "grad_norm": 0.43917784094810486,
      "learning_rate": 7.157858606059377e-06,
      "loss": 0.003,
      "step": 1854
    },
    {
      "epoch": 29.68,
      "grad_norm": 0.035083942115306854,
      "learning_rate": 7.154966363171003e-06,
      "loss": 0.0004,
      "step": 1855
    },
    {
      "epoch": 29.7,
      "grad_norm": 1.57957124710083,
      "learning_rate": 7.152073234457358e-06,
      "loss": 0.0277,
      "step": 1856
    },
    {
      "epoch": 29.71,
      "grad_norm": 2.0750107765197754,
      "learning_rate": 7.149179221107695e-06,
      "loss": 0.0028,
      "step": 1857
    },
    {
      "epoch": 29.73,
      "grad_norm": 0.03642610087990761,
      "learning_rate": 7.146284324311638e-06,
      "loss": 0.0004,
      "step": 1858
    },
    {
      "epoch": 29.74,
      "grad_norm": 0.18260739743709564,
      "learning_rate": 7.143388545259167e-06,
      "loss": 0.0012,
      "step": 1859
    },
    {
      "epoch": 29.76,
      "grad_norm": 2.286621332168579,
      "learning_rate": 7.140491885140629e-06,
      "loss": 0.0319,
      "step": 1860
    },
    {
      "epoch": 29.78,
      "grad_norm": 0.01919909380376339,
      "learning_rate": 7.137594345146729e-06,
      "loss": 0.0003,
      "step": 1861
    },
    {
      "epoch": 29.79,
      "grad_norm": 0.25894054770469666,
      "learning_rate": 7.1346959264685374e-06,
      "loss": 0.0005,
      "step": 1862
    },
    {
      "epoch": 29.81,
      "grad_norm": 0.13360866904258728,
      "learning_rate": 7.131796630297485e-06,
      "loss": 0.0019,
      "step": 1863
    },
    {
      "epoch": 29.82,
      "grad_norm": 2.960092067718506,
      "learning_rate": 7.128896457825364e-06,
      "loss": 0.021,
      "step": 1864
    },
    {
      "epoch": 29.84,
      "grad_norm": 0.04309279844164848,
      "learning_rate": 7.125995410244324e-06,
      "loss": 0.0006,
      "step": 1865
    },
    {
      "epoch": 29.86,
      "grad_norm": 0.15028834342956543,
      "learning_rate": 7.123093488746877e-06,
      "loss": 0.0019,
      "step": 1866
    },
    {
      "epoch": 29.87,
      "grad_norm": 0.7556943297386169,
      "learning_rate": 7.120190694525893e-06,
      "loss": 0.013,
      "step": 1867
    },
    {
      "epoch": 29.89,
      "grad_norm": 0.530223548412323,
      "learning_rate": 7.117287028774601e-06,
      "loss": 0.0011,
      "step": 1868
    },
    {
      "epoch": 29.9,
      "grad_norm": 1.284000039100647,
      "learning_rate": 7.114382492686588e-06,
      "loss": 0.034,
      "step": 1869
    },
    {
      "epoch": 29.92,
      "grad_norm": 0.04679659754037857,
      "learning_rate": 7.1114770874558e-06,
      "loss": 0.0005,
      "step": 1870
    },
    {
      "epoch": 29.94,
      "grad_norm": 0.1348830759525299,
      "learning_rate": 7.108570814276539e-06,
      "loss": 0.0019,
      "step": 1871
    },
    {
      "epoch": 29.95,
      "grad_norm": 0.6672670245170593,
      "learning_rate": 7.105663674343462e-06,
      "loss": 0.0092,
      "step": 1872
    },
    {
      "epoch": 29.97,
      "grad_norm": 0.17077741026878357,
      "learning_rate": 7.102755668851589e-06,
      "loss": 0.0008,
      "step": 1873
    },
    {
      "epoch": 29.98,
      "grad_norm": 1.209753155708313,
      "learning_rate": 7.099846798996287e-06,
      "loss": 0.0116,
      "step": 1874
    },
    {
      "epoch": 30.0,
      "grad_norm": 0.7508177161216736,
      "learning_rate": 7.096937065973285e-06,
      "loss": 0.0149,
      "step": 1875
    },
    {
      "epoch": 30.02,
      "grad_norm": 0.020278658717870712,
      "learning_rate": 7.094026470978663e-06,
      "loss": 0.0003,
      "step": 1876
    },
    {
      "epoch": 30.03,
      "grad_norm": 0.03131580352783203,
      "learning_rate": 7.091115015208856e-06,
      "loss": 0.0004,
      "step": 1877
    },
    {
      "epoch": 30.05,
      "grad_norm": 0.02144644595682621,
      "learning_rate": 7.088202699860656e-06,
      "loss": 0.0004,
      "step": 1878
    },
    {
      "epoch": 30.06,
      "grad_norm": 0.8454729318618774,
      "learning_rate": 7.085289526131204e-06,
      "loss": 0.0104,
      "step": 1879
    },
    {
      "epoch": 30.08,
      "grad_norm": 0.5294380784034729,
      "learning_rate": 7.082375495217996e-06,
      "loss": 0.0052,
      "step": 1880
    },
    {
      "epoch": 30.1,
      "grad_norm": 0.6456961631774902,
      "learning_rate": 7.07946060831888e-06,
      "loss": 0.0062,
      "step": 1881
    },
    {
      "epoch": 30.11,
      "grad_norm": 0.028034808114171028,
      "learning_rate": 7.076544866632058e-06,
      "loss": 0.0004,
      "step": 1882
    },
    {
      "epoch": 30.13,
      "grad_norm": 0.8573852777481079,
      "learning_rate": 7.073628271356077e-06,
      "loss": 0.0126,
      "step": 1883
    },
    {
      "epoch": 30.14,
      "grad_norm": 1.6372177600860596,
      "learning_rate": 7.070710823689841e-06,
      "loss": 0.0144,
      "step": 1884
    },
    {
      "epoch": 30.16,
      "grad_norm": 0.44492587447166443,
      "learning_rate": 7.067792524832604e-06,
      "loss": 0.003,
      "step": 1885
    },
    {
      "epoch": 30.18,
      "grad_norm": 0.017004074528813362,
      "learning_rate": 7.064873375983967e-06,
      "loss": 0.0003,
      "step": 1886
    },
    {
      "epoch": 30.19,
      "grad_norm": 0.08647865802049637,
      "learning_rate": 7.061953378343883e-06,
      "loss": 0.0005,
      "step": 1887
    },
    {
      "epoch": 30.21,
      "grad_norm": 0.016004052013158798,
      "learning_rate": 7.059032533112652e-06,
      "loss": 0.0003,
      "step": 1888
    },
    {
      "epoch": 30.22,
      "grad_norm": 0.24353377521038055,
      "learning_rate": 7.056110841490922e-06,
      "loss": 0.0015,
      "step": 1889
    },
    {
      "epoch": 30.24,
      "grad_norm": 0.7108328938484192,
      "learning_rate": 7.053188304679691e-06,
      "loss": 0.0097,
      "step": 1890
    },
    {
      "epoch": 30.26,
      "grad_norm": 4.2325968742370605,
      "learning_rate": 7.050264923880304e-06,
      "loss": 0.0084,
      "step": 1891
    },
    {
      "epoch": 30.27,
      "grad_norm": 0.4254013001918793,
      "learning_rate": 7.047340700294454e-06,
      "loss": 0.0028,
      "step": 1892
    },
    {
      "epoch": 30.29,
      "grad_norm": 1.6264925003051758,
      "learning_rate": 7.044415635124176e-06,
      "loss": 0.0321,
      "step": 1893
    },
    {
      "epoch": 30.3,
      "grad_norm": 0.016395458951592445,
      "learning_rate": 7.041489729571853e-06,
      "loss": 0.0002,
      "step": 1894
    },
    {
      "epoch": 30.32,
      "grad_norm": 0.7850589156150818,
      "learning_rate": 7.038562984840216e-06,
      "loss": 0.0095,
      "step": 1895
    },
    {
      "epoch": 30.34,
      "grad_norm": 0.015162954106926918,
      "learning_rate": 7.03563540213234e-06,
      "loss": 0.0003,
      "step": 1896
    },
    {
      "epoch": 30.35,
      "grad_norm": 1.2141941785812378,
      "learning_rate": 7.032706982651645e-06,
      "loss": 0.0169,
      "step": 1897
    },
    {
      "epoch": 30.37,
      "grad_norm": 0.8090475797653198,
      "learning_rate": 7.029777727601888e-06,
      "loss": 0.0121,
      "step": 1898
    },
    {
      "epoch": 30.38,
      "grad_norm": 0.022979941219091415,
      "learning_rate": 7.026847638187181e-06,
      "loss": 0.0004,
      "step": 1899
    },
    {
      "epoch": 30.4,
      "grad_norm": 0.13805457949638367,
      "learning_rate": 7.023916715611969e-06,
      "loss": 0.0013,
      "step": 1900
    },
    {
      "epoch": 30.42,
      "grad_norm": 0.015708334743976593,
      "learning_rate": 7.020984961081047e-06,
      "loss": 0.0003,
      "step": 1901
    },
    {
      "epoch": 30.43,
      "grad_norm": 0.021307600662112236,
      "learning_rate": 7.018052375799545e-06,
      "loss": 0.0005,
      "step": 1902
    },
    {
      "epoch": 30.45,
      "grad_norm": 0.9025864005088806,
      "learning_rate": 7.015118960972942e-06,
      "loss": 0.0159,
      "step": 1903
    },
    {
      "epoch": 30.46,
      "grad_norm": 0.03760921210050583,
      "learning_rate": 7.012184717807051e-06,
      "loss": 0.0007,
      "step": 1904
    },
    {
      "epoch": 30.48,
      "grad_norm": 1.8776485919952393,
      "learning_rate": 7.009249647508028e-06,
      "loss": 0.0467,
      "step": 1905
    },
    {
      "epoch": 30.5,
      "grad_norm": 0.7886898517608643,
      "learning_rate": 7.006313751282372e-06,
      "loss": 0.0133,
      "step": 1906
    },
    {
      "epoch": 30.51,
      "grad_norm": 0.30256712436676025,
      "learning_rate": 7.003377030336917e-06,
      "loss": 0.0008,
      "step": 1907
    },
    {
      "epoch": 30.53,
      "grad_norm": 0.05162697657942772,
      "learning_rate": 7.000439485878841e-06,
      "loss": 0.0005,
      "step": 1908
    },
    {
      "epoch": 30.54,
      "grad_norm": 0.014273743145167828,
      "learning_rate": 6.997501119115654e-06,
      "loss": 0.0003,
      "step": 1909
    },
    {
      "epoch": 30.56,
      "grad_norm": 1.348946452140808,
      "learning_rate": 6.994561931255209e-06,
      "loss": 0.0211,
      "step": 1910
    },
    {
      "epoch": 30.58,
      "grad_norm": 0.9131167531013489,
      "learning_rate": 6.991621923505696e-06,
      "loss": 0.0132,
      "step": 1911
    },
    {
      "epoch": 30.59,
      "grad_norm": 0.021096251904964447,
      "learning_rate": 6.98868109707564e-06,
      "loss": 0.0004,
      "step": 1912
    },
    {
      "epoch": 30.61,
      "grad_norm": 1.0194693803787231,
      "learning_rate": 6.985739453173903e-06,
      "loss": 0.0136,
      "step": 1913
    },
    {
      "epoch": 30.62,
      "grad_norm": 0.07638615369796753,
      "learning_rate": 6.982796993009687e-06,
      "loss": 0.0008,
      "step": 1914
    },
    {
      "epoch": 30.64,
      "grad_norm": 0.02730630710721016,
      "learning_rate": 6.9798537177925226e-06,
      "loss": 0.0004,
      "step": 1915
    },
    {
      "epoch": 30.66,
      "grad_norm": 0.015864435583353043,
      "learning_rate": 6.97690962873228e-06,
      "loss": 0.0003,
      "step": 1916
    },
    {
      "epoch": 30.67,
      "grad_norm": 0.04277702420949936,
      "learning_rate": 6.973964727039164e-06,
      "loss": 0.0002,
      "step": 1917
    },
    {
      "epoch": 30.69,
      "grad_norm": 0.5862672328948975,
      "learning_rate": 6.971019013923712e-06,
      "loss": 0.0016,
      "step": 1918
    },
    {
      "epoch": 30.7,
      "grad_norm": 0.024012748152017593,
      "learning_rate": 6.968072490596796e-06,
      "loss": 0.0004,
      "step": 1919
    },
    {
      "epoch": 30.72,
      "grad_norm": 0.7593761086463928,
      "learning_rate": 6.965125158269619e-06,
      "loss": 0.0058,
      "step": 1920
    },
    {
      "epoch": 30.74,
      "grad_norm": 0.7015745639801025,
      "learning_rate": 6.962177018153718e-06,
      "loss": 0.0096,
      "step": 1921
    },
    {
      "epoch": 30.75,
      "grad_norm": 0.01996244490146637,
      "learning_rate": 6.959228071460964e-06,
      "loss": 0.0005,
      "step": 1922
    },
    {
      "epoch": 30.77,
      "grad_norm": 0.1153097152709961,
      "learning_rate": 6.956278319403556e-06,
      "loss": 0.0008,
      "step": 1923
    },
    {
      "epoch": 30.78,
      "grad_norm": 0.16142553091049194,
      "learning_rate": 6.953327763194026e-06,
      "loss": 0.0011,
      "step": 1924
    },
    {
      "epoch": 30.8,
      "grad_norm": 0.02164648286998272,
      "learning_rate": 6.950376404045235e-06,
      "loss": 0.0005,
      "step": 1925
    },
    {
      "epoch": 30.82,
      "grad_norm": 0.030958304181694984,
      "learning_rate": 6.947424243170378e-06,
      "loss": 0.0002,
      "step": 1926
    },
    {
      "epoch": 30.83,
      "grad_norm": 0.04673250392079353,
      "learning_rate": 6.944471281782975e-06,
      "loss": 0.0003,
      "step": 1927
    },
    {
      "epoch": 30.85,
      "grad_norm": 1.1979292631149292,
      "learning_rate": 6.941517521096877e-06,
      "loss": 0.0105,
      "step": 1928
    },
    {
      "epoch": 30.86,
      "grad_norm": 0.15488938987255096,
      "learning_rate": 6.938562962326263e-06,
      "loss": 0.001,
      "step": 1929
    },
    {
      "epoch": 30.88,
      "grad_norm": 0.027510754764080048,
      "learning_rate": 6.935607606685642e-06,
      "loss": 0.0006,
      "step": 1930
    },
    {
      "epoch": 30.9,
      "grad_norm": 0.19831375777721405,
      "learning_rate": 6.932651455389847e-06,
      "loss": 0.0004,
      "step": 1931
    },
    {
      "epoch": 30.91,
      "grad_norm": 1.0881001949310303,
      "learning_rate": 6.929694509654043e-06,
      "loss": 0.015,
      "step": 1932
    },
    {
      "epoch": 30.93,
      "grad_norm": 0.03636162355542183,
      "learning_rate": 6.926736770693715e-06,
      "loss": 0.0003,
      "step": 1933
    },
    {
      "epoch": 30.94,
      "grad_norm": 1.2685314416885376,
      "learning_rate": 6.923778239724681e-06,
      "loss": 0.0121,
      "step": 1934
    },
    {
      "epoch": 30.96,
      "grad_norm": 1.1019772291183472,
      "learning_rate": 6.9208189179630805e-06,
      "loss": 0.0126,
      "step": 1935
    },
    {
      "epoch": 30.98,
      "grad_norm": 0.0316256545484066,
      "learning_rate": 6.917858806625377e-06,
      "loss": 0.0004,
      "step": 1936
    },
    {
      "epoch": 30.99,
      "grad_norm": 0.9681387543678284,
      "learning_rate": 6.914897906928363e-06,
      "loss": 0.0194,
      "step": 1937
    },
    {
      "epoch": 31.01,
      "grad_norm": 0.03351503610610962,
      "learning_rate": 6.9119362200891514e-06,
      "loss": 0.0004,
      "step": 1938
    },
    {
      "epoch": 31.02,
      "grad_norm": 0.384480357170105,
      "learning_rate": 6.908973747325179e-06,
      "loss": 0.0038,
      "step": 1939
    },
    {
      "epoch": 31.04,
      "grad_norm": 0.014183245599269867,
      "learning_rate": 6.906010489854209e-06,
      "loss": 0.0003,
      "step": 1940
    },
    {
      "epoch": 31.06,
      "grad_norm": 0.015025952830910683,
      "learning_rate": 6.903046448894322e-06,
      "loss": 0.0004,
      "step": 1941
    },
    {
      "epoch": 31.07,
      "grad_norm": 1.1119210720062256,
      "learning_rate": 6.900081625663925e-06,
      "loss": 0.0099,
      "step": 1942
    },
    {
      "epoch": 31.09,
      "grad_norm": 0.01970665343105793,
      "learning_rate": 6.8971160213817425e-06,
      "loss": 0.0003,
      "step": 1943
    },
    {
      "epoch": 31.1,
      "grad_norm": 0.9843841195106506,
      "learning_rate": 6.894149637266825e-06,
      "loss": 0.0158,
      "step": 1944
    },
    {
      "epoch": 31.12,
      "grad_norm": 0.6691235303878784,
      "learning_rate": 6.891182474538539e-06,
      "loss": 0.0083,
      "step": 1945
    },
    {
      "epoch": 31.14,
      "grad_norm": 0.016698503866791725,
      "learning_rate": 6.888214534416575e-06,
      "loss": 0.0003,
      "step": 1946
    },
    {
      "epoch": 31.15,
      "grad_norm": 0.0807943046092987,
      "learning_rate": 6.88524581812094e-06,
      "loss": 0.0007,
      "step": 1947
    },
    {
      "epoch": 31.17,
      "grad_norm": 0.649178683757782,
      "learning_rate": 6.88227632687196e-06,
      "loss": 0.0042,
      "step": 1948
    },
    {
      "epoch": 31.18,
      "grad_norm": 3.0834484100341797,
      "learning_rate": 6.879306061890284e-06,
      "loss": 0.0216,
      "step": 1949
    },
    {
      "epoch": 31.2,
      "grad_norm": 0.015954863280057907,
      "learning_rate": 6.876335024396872e-06,
      "loss": 0.0003,
      "step": 1950
    },
    {
      "epoch": 31.22,
      "grad_norm": 0.015936940908432007,
      "learning_rate": 6.873363215613007e-06,
      "loss": 0.0003,
      "step": 1951
    },
    {
      "epoch": 31.23,
      "grad_norm": 0.18638549745082855,
      "learning_rate": 6.870390636760286e-06,
      "loss": 0.0011,
      "step": 1952
    },
    {
      "epoch": 31.25,
      "grad_norm": 0.018380580469965935,
      "learning_rate": 6.867417289060627e-06,
      "loss": 0.0003,
      "step": 1953
    },
    {
      "epoch": 31.26,
      "grad_norm": 0.04437669366598129,
      "learning_rate": 6.8644431737362585e-06,
      "loss": 0.0003,
      "step": 1954
    },
    {
      "epoch": 31.28,
      "grad_norm": 0.9153821468353271,
      "learning_rate": 6.8614682920097265e-06,
      "loss": 0.0081,
      "step": 1955
    },
    {
      "epoch": 31.3,
      "grad_norm": 0.338175892829895,
      "learning_rate": 6.858492645103894e-06,
      "loss": 0.002,
      "step": 1956
    },
    {
      "epoch": 31.31,
      "grad_norm": 0.4239787459373474,
      "learning_rate": 6.855516234241936e-06,
      "loss": 0.0027,
      "step": 1957
    },
    {
      "epoch": 31.33,
      "grad_norm": 0.8466546535491943,
      "learning_rate": 6.852539060647345e-06,
      "loss": 0.0115,
      "step": 1958
    },
    {
      "epoch": 31.34,
      "grad_norm": 1.0270308256149292,
      "learning_rate": 6.849561125543921e-06,
      "loss": 0.0087,
      "step": 1959
    },
    {
      "epoch": 31.36,
      "grad_norm": 1.1210383176803589,
      "learning_rate": 6.846582430155783e-06,
      "loss": 0.0127,
      "step": 1960
    },
    {
      "epoch": 31.38,
      "grad_norm": 0.013527448289096355,
      "learning_rate": 6.843602975707358e-06,
      "loss": 0.0003,
      "step": 1961
    },
    {
      "epoch": 31.39,
      "grad_norm": 0.028930140659213066,
      "learning_rate": 6.840622763423391e-06,
      "loss": 0.0003,
      "step": 1962
    },
    {
      "epoch": 31.41,
      "grad_norm": 0.3154318630695343,
      "learning_rate": 6.837641794528931e-06,
      "loss": 0.0047,
      "step": 1963
    },
    {
      "epoch": 31.42,
      "grad_norm": 0.06474494934082031,
      "learning_rate": 6.834660070249343e-06,
      "loss": 0.0003,
      "step": 1964
    },
    {
      "epoch": 31.44,
      "grad_norm": 0.7350910305976868,
      "learning_rate": 6.831677591810302e-06,
      "loss": 0.0092,
      "step": 1965
    },
    {
      "epoch": 31.46,
      "grad_norm": 0.7087653279304504,
      "learning_rate": 6.82869436043779e-06,
      "loss": 0.0045,
      "step": 1966
    },
    {
      "epoch": 31.47,
      "grad_norm": 0.02420705556869507,
      "learning_rate": 6.825710377358105e-06,
      "loss": 0.0003,
      "step": 1967
    },
    {
      "epoch": 31.49,
      "grad_norm": 1.7347725629806519,
      "learning_rate": 6.822725643797844e-06,
      "loss": 0.0216,
      "step": 1968
    },
    {
      "epoch": 31.5,
      "grad_norm": 0.0215755607932806,
      "learning_rate": 6.819740160983923e-06,
      "loss": 0.0005,
      "step": 1969
    },
    {
      "epoch": 31.52,
      "grad_norm": 0.7772851586341858,
      "learning_rate": 6.816753930143558e-06,
      "loss": 0.0132,
      "step": 1970
    },
    {
      "epoch": 31.54,
      "grad_norm": 0.015316802076995373,
      "learning_rate": 6.813766952504278e-06,
      "loss": 0.0004,
      "step": 1971
    },
    {
      "epoch": 31.55,
      "grad_norm": 0.012661062180995941,
      "learning_rate": 6.810779229293917e-06,
      "loss": 0.0002,
      "step": 1972
    },
    {
      "epoch": 31.57,
      "grad_norm": 0.04851734638214111,
      "learning_rate": 6.807790761740613e-06,
      "loss": 0.0003,
      "step": 1973
    },
    {
      "epoch": 31.58,
      "grad_norm": 1.4884425401687622,
      "learning_rate": 6.8048015510728125e-06,
      "loss": 0.0219,
      "step": 1974
    },
    {
      "epoch": 31.6,
      "grad_norm": 0.012515648268163204,
      "learning_rate": 6.801811598519268e-06,
      "loss": 0.0003,
      "step": 1975
    },
    {
      "epoch": 31.62,
      "grad_norm": 0.7592740058898926,
      "learning_rate": 6.798820905309036e-06,
      "loss": 0.0143,
      "step": 1976
    },
    {
      "epoch": 31.63,
      "grad_norm": 0.012462005950510502,
      "learning_rate": 6.795829472671476e-06,
      "loss": 0.0002,
      "step": 1977
    },
    {
      "epoch": 31.65,
      "grad_norm": 0.013205482624471188,
      "learning_rate": 6.7928373018362546e-06,
      "loss": 0.0002,
      "step": 1978
    },
    {
      "epoch": 31.66,
      "grad_norm": 0.09961198270320892,
      "learning_rate": 6.789844394033342e-06,
      "loss": 0.0009,
      "step": 1979
    },
    {
      "epoch": 31.68,
      "grad_norm": 0.014874223619699478,
      "learning_rate": 6.786850750493006e-06,
      "loss": 0.0003,
      "step": 1980
    },
    {
      "epoch": 31.7,
      "grad_norm": 0.5677860379219055,
      "learning_rate": 6.783856372445821e-06,
      "loss": 0.0031,
      "step": 1981
    },
    {
      "epoch": 31.71,
      "grad_norm": 0.14538586139678955,
      "learning_rate": 6.780861261122663e-06,
      "loss": 0.0014,
      "step": 1982
    },
    {
      "epoch": 31.73,
      "grad_norm": 0.8859603404998779,
      "learning_rate": 6.77786541775471e-06,
      "loss": 0.0133,
      "step": 1983
    },
    {
      "epoch": 31.74,
      "grad_norm": 0.011688698083162308,
      "learning_rate": 6.774868843573441e-06,
      "loss": 0.0002,
      "step": 1984
    },
    {
      "epoch": 31.76,
      "grad_norm": 0.1268884539604187,
      "learning_rate": 6.771871539810633e-06,
      "loss": 0.0011,
      "step": 1985
    },
    {
      "epoch": 31.78,
      "grad_norm": 0.012592940591275692,
      "learning_rate": 6.768873507698363e-06,
      "loss": 0.0003,
      "step": 1986
    },
    {
      "epoch": 31.79,
      "grad_norm": 0.015983251854777336,
      "learning_rate": 6.76587474846901e-06,
      "loss": 0.0003,
      "step": 1987
    },
    {
      "epoch": 31.81,
      "grad_norm": 1.3397520780563354,
      "learning_rate": 6.7628752633552505e-06,
      "loss": 0.0302,
      "step": 1988
    },
    {
      "epoch": 31.82,
      "grad_norm": 1.1281906366348267,
      "learning_rate": 6.759875053590063e-06,
      "loss": 0.0226,
      "step": 1989
    },
    {
      "epoch": 31.84,
      "grad_norm": 0.7546095252037048,
      "learning_rate": 6.7568741204067145e-06,
      "loss": 0.0077,
      "step": 1990
    },
    {
      "epoch": 31.86,
      "grad_norm": 0.19176915287971497,
      "learning_rate": 6.753872465038777e-06,
      "loss": 0.0037,
      "step": 1991
    },
    {
      "epoch": 31.87,
      "grad_norm": 0.012211321853101254,
      "learning_rate": 6.750870088720122e-06,
      "loss": 0.0003,
      "step": 1992
    },
    {
      "epoch": 31.89,
      "grad_norm": 0.0123204430565238,
      "learning_rate": 6.747866992684907e-06,
      "loss": 0.0003,
      "step": 1993
    },
    {
      "epoch": 31.9,
      "grad_norm": 0.014787768013775349,
      "learning_rate": 6.744863178167595e-06,
      "loss": 0.0002,
      "step": 1994
    },
    {
      "epoch": 31.92,
      "grad_norm": 0.30378061532974243,
      "learning_rate": 6.741858646402941e-06,
      "loss": 0.0021,
      "step": 1995
    },
    {
      "epoch": 31.94,
      "grad_norm": 0.011419898830354214,
      "learning_rate": 6.738853398625993e-06,
      "loss": 0.0003,
      "step": 1996
    },
    {
      "epoch": 31.95,
      "grad_norm": 0.8341832160949707,
      "learning_rate": 6.735847436072094e-06,
      "loss": 0.0066,
      "step": 1997
    },
    {
      "epoch": 31.97,
      "grad_norm": 0.011038301512598991,
      "learning_rate": 6.732840759976882e-06,
      "loss": 0.0002,
      "step": 1998
    },
    {
      "epoch": 31.98,
      "grad_norm": 0.8244768381118774,
      "learning_rate": 6.7298333715762886e-06,
      "loss": 0.0134,
      "step": 1999
    },
    {
      "epoch": 32.0,
      "grad_norm": 0.9352164268493652,
      "learning_rate": 6.726825272106539e-06,
      "loss": 0.0133,
      "step": 2000
    },
    {
      "epoch": 32.02,
      "grad_norm": 0.8650842308998108,
      "learning_rate": 6.723816462804148e-06,
      "loss": 0.0065,
      "step": 2001
    },
    {
      "epoch": 32.03,
      "grad_norm": 0.010432681068778038,
      "learning_rate": 6.720806944905922e-06,
      "loss": 0.0002,
      "step": 2002
    },
    {
      "epoch": 32.05,
      "grad_norm": 0.5559468269348145,
      "learning_rate": 6.717796719648963e-06,
      "loss": 0.0046,
      "step": 2003
    },
    {
      "epoch": 32.06,
      "grad_norm": 0.012522405944764614,
      "learning_rate": 6.714785788270658e-06,
      "loss": 0.0003,
      "step": 2004
    },
    {
      "epoch": 32.08,
      "grad_norm": 0.13736294209957123,
      "learning_rate": 6.71177415200869e-06,
      "loss": 0.001,
      "step": 2005
    },
    {
      "epoch": 32.1,
      "grad_norm": 0.011485411785542965,
      "learning_rate": 6.7087618121010265e-06,
      "loss": 0.0003,
      "step": 2006
    },
    {
      "epoch": 32.11,
      "grad_norm": 0.028258470818400383,
      "learning_rate": 6.705748769785928e-06,
      "loss": 0.0006,
      "step": 2007
    },
    {
      "epoch": 32.13,
      "grad_norm": 0.013071054592728615,
      "learning_rate": 6.702735026301942e-06,
      "loss": 0.0003,
      "step": 2008
    },
    {
      "epoch": 32.14,
      "grad_norm": 0.7838575839996338,
      "learning_rate": 6.699720582887904e-06,
      "loss": 0.0085,
      "step": 2009
    },
    {
      "epoch": 32.16,
      "grad_norm": 0.45810383558273315,
      "learning_rate": 6.696705440782939e-06,
      "loss": 0.0036,
      "step": 2010
    },
    {
      "epoch": 32.18,
      "grad_norm": 0.13588453829288483,
      "learning_rate": 6.693689601226458e-06,
      "loss": 0.0011,
      "step": 2011
    },
    {
      "epoch": 32.19,
      "grad_norm": 0.9299982190132141,
      "learning_rate": 6.690673065458158e-06,
      "loss": 0.016,
      "step": 2012
    },
    {
      "epoch": 32.21,
      "grad_norm": 0.012089019641280174,
      "learning_rate": 6.687655834718022e-06,
      "loss": 0.0003,
      "step": 2013
    },
    {
      "epoch": 32.22,
      "grad_norm": 0.06044399365782738,
      "learning_rate": 6.684637910246321e-06,
      "loss": 0.0005,
      "step": 2014
    },
    {
      "epoch": 32.24,
      "grad_norm": 0.6381606459617615,
      "learning_rate": 6.68161929328361e-06,
      "loss": 0.0038,
      "step": 2015
    },
    {
      "epoch": 32.26,
      "grad_norm": 0.44992151856422424,
      "learning_rate": 6.678599985070728e-06,
      "loss": 0.0023,
      "step": 2016
    },
    {
      "epoch": 32.27,
      "grad_norm": 0.007806580979377031,
      "learning_rate": 6.675579986848799e-06,
      "loss": 0.0001,
      "step": 2017
    },
    {
      "epoch": 32.29,
      "grad_norm": 0.010466719046235085,
      "learning_rate": 6.672559299859228e-06,
      "loss": 0.0002,
      "step": 2018
    },
    {
      "epoch": 32.3,
      "grad_norm": 0.018587224185466766,
      "learning_rate": 6.669537925343708e-06,
      "loss": 0.0005,
      "step": 2019
    },
    {
      "epoch": 32.32,
      "grad_norm": 0.011815948411822319,
      "learning_rate": 6.66651586454421e-06,
      "loss": 0.0003,
      "step": 2020
    },
    {
      "epoch": 32.34,
      "grad_norm": 0.15013818442821503,
      "learning_rate": 6.66349311870299e-06,
      "loss": 0.0014,
      "step": 2021
    },
    {
      "epoch": 32.35,
      "grad_norm": 0.008998841978609562,
      "learning_rate": 6.660469689062585e-06,
      "loss": 0.0002,
      "step": 2022
    },
    {
      "epoch": 32.37,
      "grad_norm": 0.33434322476387024,
      "learning_rate": 6.657445576865813e-06,
      "loss": 0.0029,
      "step": 2023
    },
    {
      "epoch": 32.38,
      "grad_norm": 1.0213488340377808,
      "learning_rate": 6.65442078335577e-06,
      "loss": 0.0173,
      "step": 2024
    },
    {
      "epoch": 32.4,
      "grad_norm": 0.03903897479176521,
      "learning_rate": 6.651395309775837e-06,
      "loss": 0.0005,
      "step": 2025
    },
    {
      "epoch": 32.42,
      "grad_norm": 0.508923351764679,
      "learning_rate": 6.648369157369669e-06,
      "loss": 0.0073,
      "step": 2026
    },
    {
      "epoch": 32.43,
      "grad_norm": 0.010142449289560318,
      "learning_rate": 6.645342327381208e-06,
      "loss": 0.0002,
      "step": 2027
    },
    {
      "epoch": 32.45,
      "grad_norm": 0.16738680005073547,
      "learning_rate": 6.642314821054663e-06,
      "loss": 0.0011,
      "step": 2028
    },
    {
      "epoch": 32.46,
      "grad_norm": 0.02339618280529976,
      "learning_rate": 6.639286639634532e-06,
      "loss": 0.0004,
      "step": 2029
    },
    {
      "epoch": 32.48,
      "grad_norm": 1.1244120597839355,
      "learning_rate": 6.636257784365585e-06,
      "loss": 0.0156,
      "step": 2030
    },
    {
      "epoch": 32.5,
      "grad_norm": 0.02055431716144085,
      "learning_rate": 6.633228256492869e-06,
      "loss": 0.0003,
      "step": 2031
    },
    {
      "epoch": 32.51,
      "grad_norm": 1.1069562435150146,
      "learning_rate": 6.63019805726171e-06,
      "loss": 0.0214,
      "step": 2032
    },
    {
      "epoch": 32.53,
      "grad_norm": 1.3400717973709106,
      "learning_rate": 6.627167187917707e-06,
      "loss": 0.0195,
      "step": 2033
    },
    {
      "epoch": 32.54,
      "grad_norm": 0.012127753347158432,
      "learning_rate": 6.624135649706738e-06,
      "loss": 0.0003,
      "step": 2034
    },
    {
      "epoch": 32.56,
      "grad_norm": 0.010533632710576057,
      "learning_rate": 6.62110344387495e-06,
      "loss": 0.0003,
      "step": 2035
    },
    {
      "epoch": 32.58,
      "grad_norm": 0.011047969572246075,
      "learning_rate": 6.618070571668771e-06,
      "loss": 0.0002,
      "step": 2036
    },
    {
      "epoch": 32.59,
      "grad_norm": 0.13817912340164185,
      "learning_rate": 6.615037034334901e-06,
      "loss": 0.0014,
      "step": 2037
    },
    {
      "epoch": 32.61,
      "grad_norm": 0.8246301412582397,
      "learning_rate": 6.6120028331203125e-06,
      "loss": 0.008,
      "step": 2038
    },
    {
      "epoch": 32.62,
      "grad_norm": 0.009782460518181324,
      "learning_rate": 6.608967969272249e-06,
      "loss": 0.0002,
      "step": 2039
    },
    {
      "epoch": 32.64,
      "grad_norm": 0.09939080476760864,
      "learning_rate": 6.605932444038229e-06,
      "loss": 0.0006,
      "step": 2040
    },
    {
      "epoch": 32.66,
      "grad_norm": 0.0132285812869668,
      "learning_rate": 6.602896258666043e-06,
      "loss": 0.0003,
      "step": 2041
    },
    {
      "epoch": 32.67,
      "grad_norm": 0.01150427758693695,
      "learning_rate": 6.599859414403751e-06,
      "loss": 0.0003,
      "step": 2042
    },
    {
      "epoch": 32.69,
      "grad_norm": 0.44728323817253113,
      "learning_rate": 6.596821912499685e-06,
      "loss": 0.0032,
      "step": 2043
    },
    {
      "epoch": 32.7,
      "grad_norm": 0.01129161473363638,
      "learning_rate": 6.59378375420245e-06,
      "loss": 0.0003,
      "step": 2044
    },
    {
      "epoch": 32.72,
      "grad_norm": 1.2291202545166016,
      "learning_rate": 6.5907449407609145e-06,
      "loss": 0.0252,
      "step": 2045
    },
    {
      "epoch": 32.74,
      "grad_norm": 0.01248977892100811,
      "learning_rate": 6.587705473424223e-06,
      "loss": 0.0002,
      "step": 2046
    },
    {
      "epoch": 32.75,
      "grad_norm": 0.8771255612373352,
      "learning_rate": 6.5846653534417826e-06,
      "loss": 0.0111,
      "step": 2047
    },
    {
      "epoch": 32.77,
      "grad_norm": 0.009071625769138336,
      "learning_rate": 6.5816245820632745e-06,
      "loss": 0.0002,
      "step": 2048
    },
    {
      "epoch": 32.78,
      "grad_norm": 1.0480449199676514,
      "learning_rate": 6.578583160538644e-06,
      "loss": 0.0109,
      "step": 2049
    },
    {
      "epoch": 32.8,
      "grad_norm": 0.06916768848896027,
      "learning_rate": 6.575541090118105e-06,
      "loss": 0.0004,
      "step": 2050
    },
    {
      "epoch": 32.82,
      "grad_norm": 0.31616339087486267,
      "learning_rate": 6.572498372052136e-06,
      "loss": 0.002,
      "step": 2051
    },
    {
      "epoch": 32.83,
      "grad_norm": 0.019931845366954803,
      "learning_rate": 6.569455007591485e-06,
      "loss": 0.0002,
      "step": 2052
    },
    {
      "epoch": 32.85,
      "grad_norm": 1.3109617233276367,
      "learning_rate": 6.566410997987164e-06,
      "loss": 0.0266,
      "step": 2053
    },
    {
      "epoch": 32.86,
      "grad_norm": 0.5297471880912781,
      "learning_rate": 6.56336634449045e-06,
      "loss": 0.0039,
      "step": 2054
    },
    {
      "epoch": 32.88,
      "grad_norm": 0.7712801694869995,
      "learning_rate": 6.5603210483528864e-06,
      "loss": 0.0114,
      "step": 2055
    },
    {
      "epoch": 32.9,
      "grad_norm": 0.017163297161459923,
      "learning_rate": 6.557275110826277e-06,
      "loss": 0.0002,
      "step": 2056
    },
    {
      "epoch": 32.91,
      "grad_norm": 0.009407788515090942,
      "learning_rate": 6.554228533162693e-06,
      "loss": 0.0002,
      "step": 2057
    },
    {
      "epoch": 32.93,
      "grad_norm": 1.4942461252212524,
      "learning_rate": 6.551181316614468e-06,
      "loss": 0.0202,
      "step": 2058
    },
    {
      "epoch": 32.94,
      "grad_norm": 1.5688700675964355,
      "learning_rate": 6.548133462434196e-06,
      "loss": 0.0319,
      "step": 2059
    },
    {
      "epoch": 32.96,
      "grad_norm": 0.5904891490936279,
      "learning_rate": 6.545084971874738e-06,
      "loss": 0.0058,
      "step": 2060
    },
    {
      "epoch": 32.98,
      "grad_norm": 0.8917117714881897,
      "learning_rate": 6.542035846189209e-06,
      "loss": 0.0151,
      "step": 2061
    },
    {
      "epoch": 32.99,
      "grad_norm": 0.582062840461731,
      "learning_rate": 6.538986086630991e-06,
      "loss": 0.0071,
      "step": 2062
    },
    {
      "epoch": 33.01,
      "grad_norm": 0.012367021292448044,
      "learning_rate": 6.535935694453728e-06,
      "loss": 0.0003,
      "step": 2063
    },
    {
      "epoch": 33.02,
      "grad_norm": 0.7676097750663757,
      "learning_rate": 6.532884670911317e-06,
      "loss": 0.0061,
      "step": 2064
    },
    {
      "epoch": 33.04,
      "grad_norm": 0.03238530457019806,
      "learning_rate": 6.529833017257919e-06,
      "loss": 0.0007,
      "step": 2065
    },
    {
      "epoch": 33.06,
      "grad_norm": 0.19297374784946442,
      "learning_rate": 6.526780734747956e-06,
      "loss": 0.0013,
      "step": 2066
    },
    {
      "epoch": 33.07,
      "grad_norm": 0.2545773684978485,
      "learning_rate": 6.523727824636103e-06,
      "loss": 0.0017,
      "step": 2067
    },
    {
      "epoch": 33.09,
      "grad_norm": 0.8657088279724121,
      "learning_rate": 6.5206742881772975e-06,
      "loss": 0.0108,
      "step": 2068
    },
    {
      "epoch": 33.1,
      "grad_norm": 0.007921191863715649,
      "learning_rate": 6.517620126626734e-06,
      "loss": 0.0001,
      "step": 2069
    },
    {
      "epoch": 33.12,
      "grad_norm": 0.8887791633605957,
      "learning_rate": 6.514565341239861e-06,
      "loss": 0.0059,
      "step": 2070
    },
    {
      "epoch": 33.14,
      "grad_norm": 0.7256646156311035,
      "learning_rate": 6.511509933272389e-06,
      "loss": 0.0061,
      "step": 2071
    },
    {
      "epoch": 33.15,
      "grad_norm": 0.03717277571558952,
      "learning_rate": 6.508453903980275e-06,
      "loss": 0.0007,
      "step": 2072
    },
    {
      "epoch": 33.17,
      "grad_norm": 0.010856878012418747,
      "learning_rate": 6.505397254619741e-06,
      "loss": 0.0002,
      "step": 2073
    },
    {
      "epoch": 33.18,
      "grad_norm": 0.1712900847196579,
      "learning_rate": 6.50233998644726e-06,
      "loss": 0.0024,
      "step": 2074
    },
    {
      "epoch": 33.2,
      "grad_norm": 0.18469810485839844,
      "learning_rate": 6.499282100719558e-06,
      "loss": 0.0011,
      "step": 2075
    },
    {
      "epoch": 33.22,
      "grad_norm": 0.3859370946884155,
      "learning_rate": 6.496223598693619e-06,
      "loss": 0.0036,
      "step": 2076
    },
    {
      "epoch": 33.23,
      "grad_norm": 0.02247067168354988,
      "learning_rate": 6.493164481626675e-06,
      "loss": 0.0003,
      "step": 2077
    },
    {
      "epoch": 33.25,
      "grad_norm": 0.022476330399513245,
      "learning_rate": 6.490104750776214e-06,
      "loss": 0.0004,
      "step": 2078
    },
    {
      "epoch": 33.26,
      "grad_norm": 0.7732596397399902,
      "learning_rate": 6.487044407399976e-06,
      "loss": 0.0107,
      "step": 2079
    },
    {
      "epoch": 33.28,
      "grad_norm": 0.7032936811447144,
      "learning_rate": 6.483983452755953e-06,
      "loss": 0.0027,
      "step": 2080
    },
    {
      "epoch": 33.3,
      "grad_norm": 0.14087317883968353,
      "learning_rate": 6.480921888102389e-06,
      "loss": 0.0008,
      "step": 2081
    },
    {
      "epoch": 33.31,
      "grad_norm": 0.009333908557891846,
      "learning_rate": 6.477859714697774e-06,
      "loss": 0.0002,
      "step": 2082
    },
    {
      "epoch": 33.33,
      "grad_norm": 1.4983034133911133,
      "learning_rate": 6.474796933800855e-06,
      "loss": 0.0232,
      "step": 2083
    },
    {
      "epoch": 33.34,
      "grad_norm": 0.010913713835179806,
      "learning_rate": 6.471733546670624e-06,
      "loss": 0.0002,
      "step": 2084
    },
    {
      "epoch": 33.36,
      "grad_norm": 0.5927993059158325,
      "learning_rate": 6.468669554566324e-06,
      "loss": 0.0058,
      "step": 2085
    },
    {
      "epoch": 33.38,
      "grad_norm": 0.747157096862793,
      "learning_rate": 6.465604958747448e-06,
      "loss": 0.0089,
      "step": 2086
    },
    {
      "epoch": 33.39,
      "grad_norm": 1.0040203332901,
      "learning_rate": 6.462539760473733e-06,
      "loss": 0.0078,
      "step": 2087
    },
    {
      "epoch": 33.41,
      "grad_norm": 0.013004673644900322,
      "learning_rate": 6.459473961005168e-06,
      "loss": 0.0004,
      "step": 2088
    },
    {
      "epoch": 33.42,
      "grad_norm": 0.005985549185425043,
      "learning_rate": 6.456407561601986e-06,
      "loss": 0.0001,
      "step": 2089
    },
    {
      "epoch": 33.44,
      "grad_norm": 0.009622110985219479,
      "learning_rate": 6.4533405635246696e-06,
      "loss": 0.0002,
      "step": 2090
    },
    {
      "epoch": 33.46,
      "grad_norm": 0.014497683383524418,
      "learning_rate": 6.450272968033944e-06,
      "loss": 0.0004,
      "step": 2091
    },
    {
      "epoch": 33.47,
      "grad_norm": 0.014720563776791096,
      "learning_rate": 6.447204776390783e-06,
      "loss": 0.0003,
      "step": 2092
    },
    {
      "epoch": 33.49,
      "grad_norm": 0.014029953628778458,
      "learning_rate": 6.444135989856405e-06,
      "loss": 0.0004,
      "step": 2093
    },
    {
      "epoch": 33.5,
      "grad_norm": 1.0467298030853271,
      "learning_rate": 6.44106660969227e-06,
      "loss": 0.0091,
      "step": 2094
    },
    {
      "epoch": 33.52,
      "grad_norm": 0.8808051347732544,
      "learning_rate": 6.437996637160086e-06,
      "loss": 0.0129,
      "step": 2095
    },
    {
      "epoch": 33.54,
      "grad_norm": 1.0674673318862915,
      "learning_rate": 6.434926073521804e-06,
      "loss": 0.0134,
      "step": 2096
    },
    {
      "epoch": 33.55,
      "grad_norm": 0.8679616451263428,
      "learning_rate": 6.431854920039613e-06,
      "loss": 0.0154,
      "step": 2097
    },
    {
      "epoch": 33.57,
      "grad_norm": 0.4737643897533417,
      "learning_rate": 6.428783177975953e-06,
      "loss": 0.0059,
      "step": 2098
    },
    {
      "epoch": 33.58,
      "grad_norm": 0.028023183345794678,
      "learning_rate": 6.425710848593496e-06,
      "loss": 0.0004,
      "step": 2099
    },
    {
      "epoch": 33.6,
      "grad_norm": 0.007474836427718401,
      "learning_rate": 6.4226379331551625e-06,
      "loss": 0.0001,
      "step": 2100
    },
    {
      "epoch": 33.62,
      "grad_norm": 0.09503184258937836,
      "learning_rate": 6.419564432924114e-06,
      "loss": 0.0008,
      "step": 2101
    },
    {
      "epoch": 33.63,
      "grad_norm": 0.007952297106385231,
      "learning_rate": 6.4164903491637475e-06,
      "loss": 0.0001,
      "step": 2102
    },
    {
      "epoch": 33.65,
      "grad_norm": 0.01062987744808197,
      "learning_rate": 6.413415683137704e-06,
      "loss": 0.0002,
      "step": 2103
    },
    {
      "epoch": 33.66,
      "grad_norm": 0.00959530845284462,
      "learning_rate": 6.410340436109864e-06,
      "loss": 0.0002,
      "step": 2104
    },
    {
      "epoch": 33.68,
      "grad_norm": 0.010871483944356441,
      "learning_rate": 6.407264609344344e-06,
      "loss": 0.0003,
      "step": 2105
    },
    {
      "epoch": 33.7,
      "grad_norm": 0.23837468028068542,
      "learning_rate": 6.404188204105498e-06,
      "loss": 0.0032,
      "step": 2106
    },
    {
      "epoch": 33.71,
      "grad_norm": 0.7633135914802551,
      "learning_rate": 6.401111221657923e-06,
      "loss": 0.0111,
      "step": 2107
    },
    {
      "epoch": 33.73,
      "grad_norm": 0.05523671582341194,
      "learning_rate": 6.3980336632664505e-06,
      "loss": 0.001,
      "step": 2108
    },
    {
      "epoch": 33.74,
      "grad_norm": 0.009030363522469997,
      "learning_rate": 6.3949555301961474e-06,
      "loss": 0.0002,
      "step": 2109
    },
    {
      "epoch": 33.76,
      "grad_norm": 0.6466816067695618,
      "learning_rate": 6.3918768237123175e-06,
      "loss": 0.0062,
      "step": 2110
    },
    {
      "epoch": 33.78,
      "grad_norm": 0.7239112854003906,
      "learning_rate": 6.388797545080501e-06,
      "loss": 0.0117,
      "step": 2111
    },
    {
      "epoch": 33.79,
      "grad_norm": 0.2863040566444397,
      "learning_rate": 6.385717695566472e-06,
      "loss": 0.0022,
      "step": 2112
    },
    {
      "epoch": 33.81,
      "grad_norm": 0.015259967185556889,
      "learning_rate": 6.382637276436242e-06,
      "loss": 0.0004,
      "step": 2113
    },
    {
      "epoch": 33.82,
      "grad_norm": 0.008254993706941605,
      "learning_rate": 6.379556288956056e-06,
      "loss": 0.0002,
      "step": 2114
    },
    {
      "epoch": 33.84,
      "grad_norm": 0.3453181982040405,
      "learning_rate": 6.376474734392388e-06,
      "loss": 0.0021,
      "step": 2115
    },
    {
      "epoch": 33.86,
      "grad_norm": 0.023327531293034554,
      "learning_rate": 6.373392614011952e-06,
      "loss": 0.0003,
      "step": 2116
    },
    {
      "epoch": 33.87,
      "grad_norm": 0.8910852074623108,
      "learning_rate": 6.370309929081689e-06,
      "loss": 0.0112,
      "step": 2117
    },
    {
      "epoch": 33.89,
      "grad_norm": 1.0409809350967407,
      "learning_rate": 6.367226680868776e-06,
      "loss": 0.0183,
      "step": 2118
    },
    {
      "epoch": 33.9,
      "grad_norm": 0.8765156865119934,
      "learning_rate": 6.364142870640619e-06,
      "loss": 0.0111,
      "step": 2119
    },
    {
      "epoch": 33.92,
      "grad_norm": 0.8375189900398254,
      "learning_rate": 6.361058499664856e-06,
      "loss": 0.0128,
      "step": 2120
    },
    {
      "epoch": 33.94,
      "grad_norm": 0.014327265322208405,
      "learning_rate": 6.357973569209356e-06,
      "loss": 0.0003,
      "step": 2121
    },
    {
      "epoch": 33.95,
      "grad_norm": 0.012672590091824532,
      "learning_rate": 6.354888080542216e-06,
      "loss": 0.0003,
      "step": 2122
    },
    {
      "epoch": 33.97,
      "grad_norm": 0.057673584669828415,
      "learning_rate": 6.3518020349317645e-06,
      "loss": 0.0006,
      "step": 2123
    },
    {
      "epoch": 33.98,
      "grad_norm": 0.008988451212644577,
      "learning_rate": 6.348715433646559e-06,
      "loss": 0.0002,
      "step": 2124
    },
    {
      "epoch": 34.0,
      "grad_norm": 0.030461737886071205,
      "learning_rate": 6.345628277955384e-06,
      "loss": 0.0007,
      "step": 2125
    },
    {
      "epoch": 34.02,
      "grad_norm": 0.8773800730705261,
      "learning_rate": 6.342540569127253e-06,
      "loss": 0.0079,
      "step": 2126
    },
    {
      "epoch": 34.03,
      "grad_norm": 0.24045732617378235,
      "learning_rate": 6.339452308431406e-06,
      "loss": 0.0006,
      "step": 2127
    },
    {
      "epoch": 34.05,
      "grad_norm": 0.6269310712814331,
      "learning_rate": 6.336363497137311e-06,
      "loss": 0.0069,
      "step": 2128
    },
    {
      "epoch": 34.06,
      "grad_norm": 0.009207294322550297,
      "learning_rate": 6.333274136514661e-06,
      "loss": 0.0002,
      "step": 2129
    },
    {
      "epoch": 34.08,
      "grad_norm": 0.007582197897136211,
      "learning_rate": 6.330184227833376e-06,
      "loss": 0.0001,
      "step": 2130
    },
    {
      "epoch": 34.1,
      "grad_norm": 0.00993556808680296,
      "learning_rate": 6.327093772363601e-06,
      "loss": 0.0002,
      "step": 2131
    },
    {
      "epoch": 34.11,
      "grad_norm": 0.08335265517234802,
      "learning_rate": 6.324002771375704e-06,
      "loss": 0.0012,
      "step": 2132
    },
    {
      "epoch": 34.13,
      "grad_norm": 0.02110096998512745,
      "learning_rate": 6.320911226140279e-06,
      "loss": 0.0003,
      "step": 2133
    },
    {
      "epoch": 34.14,
      "grad_norm": 0.00888736080378294,
      "learning_rate": 6.317819137928144e-06,
      "loss": 0.0002,
      "step": 2134
    },
    {
      "epoch": 34.16,
      "grad_norm": 0.20962700247764587,
      "learning_rate": 6.3147265080103405e-06,
      "loss": 0.0015,
      "step": 2135
    },
    {
      "epoch": 34.18,
      "grad_norm": 0.6345184445381165,
      "learning_rate": 6.311633337658132e-06,
      "loss": 0.0044,
      "step": 2136
    },
    {
      "epoch": 34.19,
      "grad_norm": 0.6711463928222656,
      "learning_rate": 6.308539628143001e-06,
      "loss": 0.0089,
      "step": 2137
    },
    {
      "epoch": 34.21,
      "grad_norm": 0.05002935603260994,
      "learning_rate": 6.3054453807366574e-06,
      "loss": 0.0004,
      "step": 2138
    },
    {
      "epoch": 34.22,
      "grad_norm": 0.011137359775602818,
      "learning_rate": 6.30235059671103e-06,
      "loss": 0.0003,
      "step": 2139
    },
    {
      "epoch": 34.24,
      "grad_norm": 0.010297668166458607,
      "learning_rate": 6.299255277338265e-06,
      "loss": 0.0002,
      "step": 2140
    },
    {
      "epoch": 34.26,
      "grad_norm": 0.9262070655822754,
      "learning_rate": 6.296159423890735e-06,
      "loss": 0.011,
      "step": 2141
    },
    {
      "epoch": 34.27,
      "grad_norm": 0.007373315282166004,
      "learning_rate": 6.293063037641023e-06,
      "loss": 0.0002,
      "step": 2142
    },
    {
      "epoch": 34.29,
      "grad_norm": 0.800462543964386,
      "learning_rate": 6.289966119861941e-06,
      "loss": 0.0104,
      "step": 2143
    },
    {
      "epoch": 34.3,
      "grad_norm": 0.7826881408691406,
      "learning_rate": 6.286868671826513e-06,
      "loss": 0.007,
      "step": 2144
    },
    {
      "epoch": 34.32,
      "grad_norm": 0.38241255283355713,
      "learning_rate": 6.283770694807983e-06,
      "loss": 0.0034,
      "step": 2145
    },
    {
      "epoch": 34.34,
      "grad_norm": 0.007600539363920689,
      "learning_rate": 6.280672190079814e-06,
      "loss": 0.0002,
      "step": 2146
    },
    {
      "epoch": 34.35,
      "grad_norm": 0.07427766919136047,
      "learning_rate": 6.277573158915682e-06,
      "loss": 0.0009,
      "step": 2147
    },
    {
      "epoch": 34.37,
      "grad_norm": 0.010572385042905807,
      "learning_rate": 6.274473602589481e-06,
      "loss": 0.0003,
      "step": 2148
    },
    {
      "epoch": 34.38,
      "grad_norm": 0.011532609350979328,
      "learning_rate": 6.271373522375325e-06,
      "loss": 0.0004,
      "step": 2149
    },
    {
      "epoch": 34.4,
      "grad_norm": 0.832009494304657,
      "learning_rate": 6.268272919547537e-06,
      "loss": 0.0131,
      "step": 2150
    },
    {
      "epoch": 34.42,
      "grad_norm": 0.6112145185470581,
      "learning_rate": 6.265171795380659e-06,
      "loss": 0.0051,
      "step": 2151
    },
    {
      "epoch": 34.43,
      "grad_norm": 0.008231132291257381,
      "learning_rate": 6.262070151149447e-06,
      "loss": 0.0002,
      "step": 2152
    },
    {
      "epoch": 34.45,
      "grad_norm": 0.010076943784952164,
      "learning_rate": 6.258967988128867e-06,
      "loss": 0.0003,
      "step": 2153
    },
    {
      "epoch": 34.46,
      "grad_norm": 0.24913731217384338,
      "learning_rate": 6.255865307594102e-06,
      "loss": 0.0014,
      "step": 2154
    },
    {
      "epoch": 34.48,
      "grad_norm": 0.6860871315002441,
      "learning_rate": 6.252762110820548e-06,
      "loss": 0.0085,
      "step": 2155
    },
    {
      "epoch": 34.5,
      "grad_norm": 0.8483584523200989,
      "learning_rate": 6.249658399083811e-06,
      "loss": 0.0041,
      "step": 2156
    },
    {
      "epoch": 34.51,
      "grad_norm": 0.006759305018931627,
      "learning_rate": 6.24655417365971e-06,
      "loss": 0.0001,
      "step": 2157
    },
    {
      "epoch": 34.53,
      "grad_norm": 0.8450071215629578,
      "learning_rate": 6.243449435824276e-06,
      "loss": 0.0082,
      "step": 2158
    },
    {
      "epoch": 34.54,
      "grad_norm": 1.3874309062957764,
      "learning_rate": 6.240344186853746e-06,
      "loss": 0.0343,
      "step": 2159
    },
    {
      "epoch": 34.56,
      "grad_norm": 0.010805447585880756,
      "learning_rate": 6.237238428024573e-06,
      "loss": 0.0003,
      "step": 2160
    },
    {
      "epoch": 34.58,
      "grad_norm": 0.06128714606165886,
      "learning_rate": 6.234132160613418e-06,
      "loss": 0.0002,
      "step": 2161
    },
    {
      "epoch": 34.59,
      "grad_norm": 0.010219445452094078,
      "learning_rate": 6.231025385897147e-06,
      "loss": 0.0003,
      "step": 2162
    },
    {
      "epoch": 34.61,
      "grad_norm": 0.12142814695835114,
      "learning_rate": 6.227918105152841e-06,
      "loss": 0.0019,
      "step": 2163
    },
    {
      "epoch": 34.62,
      "grad_norm": 0.011772586032748222,
      "learning_rate": 6.2248103196577846e-06,
      "loss": 0.0004,
      "step": 2164
    },
    {
      "epoch": 34.64,
      "grad_norm": 1.3497270345687866,
      "learning_rate": 6.2217020306894705e-06,
      "loss": 0.0194,
      "step": 2165
    },
    {
      "epoch": 34.66,
      "grad_norm": 0.505527675151825,
      "learning_rate": 6.2185932395256e-06,
      "loss": 0.0008,
      "step": 2166
    },
    {
      "epoch": 34.67,
      "grad_norm": 0.008820739574730396,
      "learning_rate": 6.2154839474440786e-06,
      "loss": 0.0002,
      "step": 2167
    },
    {
      "epoch": 34.69,
      "grad_norm": 1.15601646900177,
      "learning_rate": 6.21237415572302e-06,
      "loss": 0.0074,
      "step": 2168
    },
    {
      "epoch": 34.7,
      "grad_norm": 0.661776602268219,
      "learning_rate": 6.20926386564074e-06,
      "loss": 0.0043,
      "step": 2169
    },
    {
      "epoch": 34.72,
      "grad_norm": 0.009406717494130135,
      "learning_rate": 6.2061530784757625e-06,
      "loss": 0.0003,
      "step": 2170
    },
    {
      "epoch": 34.74,
      "grad_norm": 0.5512020587921143,
      "learning_rate": 6.203041795506815e-06,
      "loss": 0.0058,
      "step": 2171
    },
    {
      "epoch": 34.75,
      "grad_norm": 0.00858223345130682,
      "learning_rate": 6.19993001801283e-06,
      "loss": 0.0002,
      "step": 2172
    },
    {
      "epoch": 34.77,
      "grad_norm": 0.01550750620663166,
      "learning_rate": 6.196817747272939e-06,
      "loss": 0.0003,
      "step": 2173
    },
    {
      "epoch": 34.78,
      "grad_norm": 0.01171306986361742,
      "learning_rate": 6.1937049845664795e-06,
      "loss": 0.0003,
      "step": 2174
    },
    {
      "epoch": 34.8,
      "grad_norm": 0.010874629020690918,
      "learning_rate": 6.1905917311729915e-06,
      "loss": 0.0003,
      "step": 2175
    },
    {
      "epoch": 34.82,
      "grad_norm": 0.37941160798072815,
      "learning_rate": 6.187477988372216e-06,
      "loss": 0.0027,
      "step": 2176
    },
    {
      "epoch": 34.83,
      "grad_norm": 1.6888645887374878,
      "learning_rate": 6.184363757444093e-06,
      "loss": 0.0287,
      "step": 2177
    },
    {
      "epoch": 34.85,
      "grad_norm": 1.3556236028671265,
      "learning_rate": 6.181249039668766e-06,
      "loss": 0.0226,
      "step": 2178
    },
    {
      "epoch": 34.86,
      "grad_norm": 1.4934983253479004,
      "learning_rate": 6.178133836326581e-06,
      "loss": 0.0179,
      "step": 2179
    },
    {
      "epoch": 34.88,
      "grad_norm": 1.0316156148910522,
      "learning_rate": 6.175018148698077e-06,
      "loss": 0.0147,
      "step": 2180
    },
    {
      "epoch": 34.9,
      "grad_norm": 0.6347327828407288,
      "learning_rate": 6.171901978063995e-06,
      "loss": 0.0074,
      "step": 2181
    },
    {
      "epoch": 34.91,
      "grad_norm": 0.010632351972162724,
      "learning_rate": 6.168785325705278e-06,
      "loss": 0.0003,
      "step": 2182
    },
    {
      "epoch": 34.93,
      "grad_norm": 0.8496425747871399,
      "learning_rate": 6.1656681929030616e-06,
      "loss": 0.0139,
      "step": 2183
    },
    {
      "epoch": 34.94,
      "grad_norm": 0.008784282952547073,
      "learning_rate": 6.162550580938682e-06,
      "loss": 0.0001,
      "step": 2184
    },
    {
      "epoch": 34.96,
      "grad_norm": 0.010937291197478771,
      "learning_rate": 6.1594324910936734e-06,
      "loss": 0.0003,
      "step": 2185
    },
    {
      "epoch": 34.98,
      "grad_norm": 0.8572167754173279,
      "learning_rate": 6.1563139246497615e-06,
      "loss": 0.0119,
      "step": 2186
    },
    {
      "epoch": 34.99,
      "grad_norm": 0.0063039520755410194,
      "learning_rate": 6.1531948828888735e-06,
      "loss": 0.0001,
      "step": 2187
    },
    {
      "epoch": 35.01,
      "grad_norm": 0.44373688101768494,
      "learning_rate": 6.150075367093129e-06,
      "loss": 0.0026,
      "step": 2188
    },
    {
      "epoch": 35.02,
      "grad_norm": 0.8489826321601868,
      "learning_rate": 6.146955378544844e-06,
      "loss": 0.0072,
      "step": 2189
    },
    {
      "epoch": 35.04,
      "grad_norm": 0.01031532697379589,
      "learning_rate": 6.143834918526528e-06,
      "loss": 0.0002,
      "step": 2190
    },
    {
      "epoch": 35.06,
      "grad_norm": 0.008939788676798344,
      "learning_rate": 6.140713988320881e-06,
      "loss": 0.0002,
      "step": 2191
    },
    {
      "epoch": 35.07,
      "grad_norm": 0.011859931983053684,
      "learning_rate": 6.137592589210803e-06,
      "loss": 0.0004,
      "step": 2192
    },
    {
      "epoch": 35.09,
      "grad_norm": 0.3218790590763092,
      "learning_rate": 6.134470722479383e-06,
      "loss": 0.0024,
      "step": 2193
    },
    {
      "epoch": 35.1,
      "grad_norm": 0.6758725047111511,
      "learning_rate": 6.1313483894099006e-06,
      "loss": 0.0044,
      "step": 2194
    },
    {
      "epoch": 35.12,
      "grad_norm": 1.2746739387512207,
      "learning_rate": 6.1282255912858315e-06,
      "loss": 0.0225,
      "step": 2195
    },
    {
      "epoch": 35.14,
      "grad_norm": 0.03812148794531822,
      "learning_rate": 6.125102329390837e-06,
      "loss": 0.0002,
      "step": 2196
    },
    {
      "epoch": 35.15,
      "grad_norm": 0.8642053008079529,
      "learning_rate": 6.121978605008775e-06,
      "loss": 0.0077,
      "step": 2197
    },
    {
      "epoch": 35.17,
      "grad_norm": 0.8248952031135559,
      "learning_rate": 6.1188544194236875e-06,
      "loss": 0.0098,
      "step": 2198
    },
    {
      "epoch": 35.18,
      "grad_norm": 0.06711950153112411,
      "learning_rate": 6.115729773919814e-06,
      "loss": 0.0005,
      "step": 2199
    },
    {
      "epoch": 35.2,
      "grad_norm": 0.6116913557052612,
      "learning_rate": 6.112604669781572e-06,
      "loss": 0.0049,
      "step": 2200
    },
    {
      "epoch": 35.22,
      "grad_norm": 0.7242316007614136,
      "learning_rate": 6.10947910829358e-06,
      "loss": 0.007,
      "step": 2201
    },
    {
      "epoch": 35.23,
      "grad_norm": 0.7555296421051025,
      "learning_rate": 6.106353090740633e-06,
      "loss": 0.0089,
      "step": 2202
    },
    {
      "epoch": 35.25,
      "grad_norm": 1.137062668800354,
      "learning_rate": 6.103226618407723e-06,
      "loss": 0.0097,
      "step": 2203
    },
    {
      "epoch": 35.26,
      "grad_norm": 0.010680406354367733,
      "learning_rate": 6.100099692580021e-06,
      "loss": 0.0003,
      "step": 2204
    },
    {
      "epoch": 35.28,
      "grad_norm": 0.7611815929412842,
      "learning_rate": 6.096972314542889e-06,
      "loss": 0.0111,
      "step": 2205
    },
    {
      "epoch": 35.3,
      "grad_norm": 0.007423089817166328,
      "learning_rate": 6.093844485581876e-06,
      "loss": 0.0002,
      "step": 2206
    },
    {
      "epoch": 35.31,
      "grad_norm": 1.2572451829910278,
      "learning_rate": 6.090716206982714e-06,
      "loss": 0.0199,
      "step": 2207
    },
    {
      "epoch": 35.33,
      "grad_norm": 0.006751930341124535,
      "learning_rate": 6.0875874800313185e-06,
      "loss": 0.0001,
      "step": 2208
    },
    {
      "epoch": 35.34,
      "grad_norm": 0.007886391133069992,
      "learning_rate": 6.084458306013791e-06,
      "loss": 0.0002,
      "step": 2209
    },
    {
      "epoch": 35.36,
      "grad_norm": 0.00899310689419508,
      "learning_rate": 6.0813286862164175e-06,
      "loss": 0.0001,
      "step": 2210
    },
    {
      "epoch": 35.38,
      "grad_norm": 0.007345761172473431,
      "learning_rate": 6.078198621925667e-06,
      "loss": 0.0002,
      "step": 2211
    },
    {
      "epoch": 35.39,
      "grad_norm": 0.7338050603866577,
      "learning_rate": 6.075068114428191e-06,
      "loss": 0.0115,
      "step": 2212
    },
    {
      "epoch": 35.41,
      "grad_norm": 0.19437965750694275,
      "learning_rate": 6.07193716501082e-06,
      "loss": 0.0011,
      "step": 2213
    },
    {
      "epoch": 35.42,
      "grad_norm": 0.03279826045036316,
      "learning_rate": 6.068805774960574e-06,
      "loss": 0.0003,
      "step": 2214
    },
    {
      "epoch": 35.44,
      "grad_norm": 0.017026839777827263,
      "learning_rate": 6.065673945564643e-06,
      "loss": 0.0005,
      "step": 2215
    },
    {
      "epoch": 35.46,
      "grad_norm": 0.8456698060035706,
      "learning_rate": 6.062541678110409e-06,
      "loss": 0.0088,
      "step": 2216
    },
    {
      "epoch": 35.47,
      "grad_norm": 0.00860156212002039,
      "learning_rate": 6.059408973885427e-06,
      "loss": 0.0002,
      "step": 2217
    },
    {
      "epoch": 35.49,
      "grad_norm": 0.010245656594634056,
      "learning_rate": 6.0562758341774315e-06,
      "loss": 0.0003,
      "step": 2218
    },
    {
      "epoch": 35.5,
      "grad_norm": 0.5243954658508301,
      "learning_rate": 6.0531422602743405e-06,
      "loss": 0.0037,
      "step": 2219
    },
    {
      "epoch": 35.52,
      "grad_norm": 0.010281041264533997,
      "learning_rate": 6.050008253464247e-06,
      "loss": 0.0003,
      "step": 2220
    },
    {
      "epoch": 35.54,
      "grad_norm": 0.006549460347741842,
      "learning_rate": 6.046873815035422e-06,
      "loss": 0.0001,
      "step": 2221
    },
    {
      "epoch": 35.55,
      "grad_norm": 1.4330121278762817,
      "learning_rate": 6.043738946276317e-06,
      "loss": 0.0143,
      "step": 2222
    },
    {
      "epoch": 35.57,
      "grad_norm": 1.1068336963653564,
      "learning_rate": 6.040603648475558e-06,
      "loss": 0.0098,
      "step": 2223
    },
    {
      "epoch": 35.58,
      "grad_norm": 0.010372371412813663,
      "learning_rate": 6.037467922921943e-06,
      "loss": 0.0003,
      "step": 2224
    },
    {
      "epoch": 35.6,
      "grad_norm": 0.15449008345603943,
      "learning_rate": 6.034331770904455e-06,
      "loss": 0.0007,
      "step": 2225
    },
    {
      "epoch": 35.62,
      "grad_norm": 0.00979237724095583,
      "learning_rate": 6.031195193712246e-06,
      "loss": 0.0003,
      "step": 2226
    },
    {
      "epoch": 35.63,
      "grad_norm": 0.008968858048319817,
      "learning_rate": 6.0280581926346436e-06,
      "loss": 0.0003,
      "step": 2227
    },
    {
      "epoch": 35.65,
      "grad_norm": 0.8603672385215759,
      "learning_rate": 6.024920768961153e-06,
      "loss": 0.0076,
      "step": 2228
    },
    {
      "epoch": 35.66,
      "grad_norm": 1.1134439706802368,
      "learning_rate": 6.021782923981447e-06,
      "loss": 0.0161,
      "step": 2229
    },
    {
      "epoch": 35.68,
      "grad_norm": 0.18025991320610046,
      "learning_rate": 6.018644658985378e-06,
      "loss": 0.0011,
      "step": 2230
    },
    {
      "epoch": 35.7,
      "grad_norm": 0.4345954358577728,
      "learning_rate": 6.015505975262968e-06,
      "loss": 0.0052,
      "step": 2231
    },
    {
      "epoch": 35.71,
      "grad_norm": 0.6673397421836853,
      "learning_rate": 6.01236687410441e-06,
      "loss": 0.0005,
      "step": 2232
    },
    {
      "epoch": 35.73,
      "grad_norm": 1.2504909038543701,
      "learning_rate": 6.009227356800071e-06,
      "loss": 0.0134,
      "step": 2233
    },
    {
      "epoch": 35.74,
      "grad_norm": 0.0064885662868618965,
      "learning_rate": 6.006087424640486e-06,
      "loss": 0.0002,
      "step": 2234
    },
    {
      "epoch": 35.76,
      "grad_norm": 0.011076518334448338,
      "learning_rate": 6.002947078916365e-06,
      "loss": 0.0003,
      "step": 2235
    },
    {
      "epoch": 35.78,
      "grad_norm": 0.8140588402748108,
      "learning_rate": 5.999806320918584e-06,
      "loss": 0.0141,
      "step": 2236
    },
    {
      "epoch": 35.79,
      "grad_norm": 0.008657911792397499,
      "learning_rate": 5.99666515193819e-06,
      "loss": 0.0002,
      "step": 2237
    },
    {
      "epoch": 35.81,
      "grad_norm": 0.007578186225146055,
      "learning_rate": 5.9935235732664e-06,
      "loss": 0.0001,
      "step": 2238
    },
    {
      "epoch": 35.82,
      "grad_norm": 0.008062574081122875,
      "learning_rate": 5.990381586194599e-06,
      "loss": 0.0001,
      "step": 2239
    },
    {
      "epoch": 35.84,
      "grad_norm": 0.009601653553545475,
      "learning_rate": 5.987239192014336e-06,
      "loss": 0.0003,
      "step": 2240
    },
    {
      "epoch": 35.86,
      "grad_norm": 0.3524598777294159,
      "learning_rate": 5.984096392017333e-06,
      "loss": 0.0056,
      "step": 2241
    },
    {
      "epoch": 35.87,
      "grad_norm": 1.0214208364486694,
      "learning_rate": 5.980953187495476e-06,
      "loss": 0.017,
      "step": 2242
    },
    {
      "epoch": 35.89,
      "grad_norm": 0.6360053420066833,
      "learning_rate": 5.9778095797408174e-06,
      "loss": 0.0101,
      "step": 2243
    },
    {
      "epoch": 35.9,
      "grad_norm": 0.0087485546246171,
      "learning_rate": 5.974665570045577e-06,
      "loss": 0.0002,
      "step": 2244
    },
    {
      "epoch": 35.92,
      "grad_norm": 0.9123141169548035,
      "learning_rate": 5.971521159702136e-06,
      "loss": 0.0123,
      "step": 2245
    },
    {
      "epoch": 35.94,
      "grad_norm": 0.5747480988502502,
      "learning_rate": 5.968376350003044e-06,
      "loss": 0.0055,
      "step": 2246
    },
    {
      "epoch": 35.95,
      "grad_norm": 0.7098726034164429,
      "learning_rate": 5.965231142241013e-06,
      "loss": 0.0068,
      "step": 2247
    },
    {
      "epoch": 35.97,
      "grad_norm": 0.5679289698600769,
      "learning_rate": 5.962085537708918e-06,
      "loss": 0.0076,
      "step": 2248
    },
    {
      "epoch": 35.98,
      "grad_norm": 0.007752980105578899,
      "learning_rate": 5.9589395376998e-06,
      "loss": 0.0002,
      "step": 2249
    },
    {
      "epoch": 36.0,
      "grad_norm": 0.5964195132255554,
      "learning_rate": 5.955793143506863e-06,
      "loss": 0.0088,
      "step": 2250
    },
    {
      "epoch": 36.02,
      "grad_norm": 1.5131827592849731,
      "learning_rate": 5.952646356423466e-06,
      "loss": 0.0152,
      "step": 2251
    },
    {
      "epoch": 36.03,
      "grad_norm": 0.23961694538593292,
      "learning_rate": 5.949499177743137e-06,
      "loss": 0.0022,
      "step": 2252
    },
    {
      "epoch": 36.05,
      "grad_norm": 1.131380319595337,
      "learning_rate": 5.946351608759561e-06,
      "loss": 0.0122,
      "step": 2253
    },
    {
      "epoch": 36.06,
      "grad_norm": 0.5609466433525085,
      "learning_rate": 5.943203650766586e-06,
      "loss": 0.0033,
      "step": 2254
    },
    {
      "epoch": 36.08,
      "grad_norm": 0.009578022174537182,
      "learning_rate": 5.940055305058219e-06,
      "loss": 0.0002,
      "step": 2255
    },
    {
      "epoch": 36.1,
      "grad_norm": 1.2916618585586548,
      "learning_rate": 5.936906572928625e-06,
      "loss": 0.0236,
      "step": 2256
    },
    {
      "epoch": 36.11,
      "grad_norm": 0.27523544430732727,
      "learning_rate": 5.933757455672127e-06,
      "loss": 0.0034,
      "step": 2257
    },
    {
      "epoch": 36.13,
      "grad_norm": 0.6842674612998962,
      "learning_rate": 5.93060795458321e-06,
      "loss": 0.0077,
      "step": 2258
    },
    {
      "epoch": 36.14,
      "grad_norm": 1.0195995569229126,
      "learning_rate": 5.927458070956518e-06,
      "loss": 0.0052,
      "step": 2259
    },
    {
      "epoch": 36.16,
      "grad_norm": 0.009780068881809711,
      "learning_rate": 5.9243078060868445e-06,
      "loss": 0.0002,
      "step": 2260
    },
    {
      "epoch": 36.18,
      "grad_norm": 0.9699280261993408,
      "learning_rate": 5.921157161269146e-06,
      "loss": 0.0071,
      "step": 2261
    },
    {
      "epoch": 36.19,
      "grad_norm": 0.02014562115073204,
      "learning_rate": 5.918006137798533e-06,
      "loss": 0.0005,
      "step": 2262
    },
    {
      "epoch": 36.21,
      "grad_norm": 0.5046611428260803,
      "learning_rate": 5.914854736970274e-06,
      "loss": 0.004,
      "step": 2263
    },
    {
      "epoch": 36.22,
      "grad_norm": 0.00841467920690775,
      "learning_rate": 5.911702960079788e-06,
      "loss": 0.0002,
      "step": 2264
    },
    {
      "epoch": 36.24,
      "grad_norm": 0.8733342289924622,
      "learning_rate": 5.908550808422656e-06,
      "loss": 0.0132,
      "step": 2265
    },
    {
      "epoch": 36.26,
      "grad_norm": 0.2932935953140259,
      "learning_rate": 5.905398283294603e-06,
      "loss": 0.0023,
      "step": 2266
    },
    {
      "epoch": 36.27,
      "grad_norm": 0.01061121467500925,
      "learning_rate": 5.902245385991519e-06,
      "loss": 0.0002,
      "step": 2267
    },
    {
      "epoch": 36.29,
      "grad_norm": 0.013337185606360435,
      "learning_rate": 5.899092117809434e-06,
      "loss": 0.0003,
      "step": 2268
    },
    {
      "epoch": 36.3,
      "grad_norm": 1.2031773328781128,
      "learning_rate": 5.895938480044543e-06,
      "loss": 0.0126,
      "step": 2269
    },
    {
      "epoch": 36.32,
      "grad_norm": 1.2421059608459473,
      "learning_rate": 5.892784473993184e-06,
      "loss": 0.0109,
      "step": 2270
    },
    {
      "epoch": 36.34,
      "grad_norm": 0.008979668840765953,
      "learning_rate": 5.889630100951852e-06,
      "loss": 0.0002,
      "step": 2271
    },
    {
      "epoch": 36.35,
      "grad_norm": 0.04708424583077431,
      "learning_rate": 5.88647536221719e-06,
      "loss": 0.0003,
      "step": 2272
    },
    {
      "epoch": 36.37,
      "grad_norm": 0.043115392327308655,
      "learning_rate": 5.883320259085992e-06,
      "loss": 0.0008,
      "step": 2273
    },
    {
      "epoch": 36.38,
      "grad_norm": 0.010059375315904617,
      "learning_rate": 5.880164792855199e-06,
      "loss": 0.0002,
      "step": 2274
    },
    {
      "epoch": 36.4,
      "grad_norm": 0.006005517672747374,
      "learning_rate": 5.877008964821909e-06,
      "loss": 0.0001,
      "step": 2275
    },
    {
      "epoch": 36.42,
      "grad_norm": 0.8708221316337585,
      "learning_rate": 5.87385277628336e-06,
      "loss": 0.0159,
      "step": 2276
    },
    {
      "epoch": 36.43,
      "grad_norm": 0.5911996960639954,
      "learning_rate": 5.870696228536944e-06,
      "loss": 0.0057,
      "step": 2277
    },
    {
      "epoch": 36.45,
      "grad_norm": 0.7722247838973999,
      "learning_rate": 5.867539322880195e-06,
      "loss": 0.0081,
      "step": 2278
    },
    {
      "epoch": 36.46,
      "grad_norm": 0.006411069538444281,
      "learning_rate": 5.864382060610803e-06,
      "loss": 0.0001,
      "step": 2279
    },
    {
      "epoch": 36.48,
      "grad_norm": 1.3068691492080688,
      "learning_rate": 5.861224443026595e-06,
      "loss": 0.021,
      "step": 2280
    },
    {
      "epoch": 36.5,
      "grad_norm": 0.010080880485475063,
      "learning_rate": 5.85806647142555e-06,
      "loss": 0.0003,
      "step": 2281
    },
    {
      "epoch": 36.51,
      "grad_norm": 0.00996741559356451,
      "learning_rate": 5.8549081471057915e-06,
      "loss": 0.0002,
      "step": 2282
    },
    {
      "epoch": 36.53,
      "grad_norm": 0.0183340385556221,
      "learning_rate": 5.851749471365585e-06,
      "loss": 0.0003,
      "step": 2283
    },
    {
      "epoch": 36.54,
      "grad_norm": 0.015522422268986702,
      "learning_rate": 5.848590445503345e-06,
      "loss": 0.0002,
      "step": 2284
    },
    {
      "epoch": 36.56,
      "grad_norm": 0.46176621317863464,
      "learning_rate": 5.845431070817627e-06,
      "loss": 0.0025,
      "step": 2285
    },
    {
      "epoch": 36.58,
      "grad_norm": 1.1370689868927002,
      "learning_rate": 5.84227134860713e-06,
      "loss": 0.0209,
      "step": 2286
    },
    {
      "epoch": 36.59,
      "grad_norm": 0.6103187203407288,
      "learning_rate": 5.839111280170698e-06,
      "loss": 0.0072,
      "step": 2287
    },
    {
      "epoch": 36.61,
      "grad_norm": 0.03574621304869652,
      "learning_rate": 5.835950866807314e-06,
      "loss": 0.0008,
      "step": 2288
    },
    {
      "epoch": 36.62,
      "grad_norm": 0.45532646775245667,
      "learning_rate": 5.832790109816104e-06,
      "loss": 0.0047,
      "step": 2289
    },
    {
      "epoch": 36.64,
      "grad_norm": 0.14436306059360504,
      "learning_rate": 5.82962901049634e-06,
      "loss": 0.0007,
      "step": 2290
    },
    {
      "epoch": 36.66,
      "grad_norm": 0.13944491744041443,
      "learning_rate": 5.826467570147426e-06,
      "loss": 0.0002,
      "step": 2291
    },
    {
      "epoch": 36.67,
      "grad_norm": 0.017223332077264786,
      "learning_rate": 5.823305790068912e-06,
      "loss": 0.0003,
      "step": 2292
    },
    {
      "epoch": 36.69,
      "grad_norm": 0.028799813240766525,
      "learning_rate": 5.820143671560488e-06,
      "loss": 0.0002,
      "step": 2293
    },
    {
      "epoch": 36.7,
      "grad_norm": 0.5903846025466919,
      "learning_rate": 5.816981215921978e-06,
      "loss": 0.0085,
      "step": 2294
    },
    {
      "epoch": 36.72,
      "grad_norm": 0.4725411534309387,
      "learning_rate": 5.8138184244533516e-06,
      "loss": 0.0046,
      "step": 2295
    },
    {
      "epoch": 36.74,
      "grad_norm": 0.008510318584740162,
      "learning_rate": 5.810655298454711e-06,
      "loss": 0.0002,
      "step": 2296
    },
    {
      "epoch": 36.75,
      "grad_norm": 0.7465438842773438,
      "learning_rate": 5.807491839226298e-06,
      "loss": 0.0088,
      "step": 2297
    },
    {
      "epoch": 36.77,
      "grad_norm": 0.755363404750824,
      "learning_rate": 5.8043280480684925e-06,
      "loss": 0.0119,
      "step": 2298
    },
    {
      "epoch": 36.78,
      "grad_norm": 0.5916280150413513,
      "learning_rate": 5.801163926281809e-06,
      "loss": 0.0051,
      "step": 2299
    },
    {
      "epoch": 36.8,
      "grad_norm": 0.06994328647851944,
      "learning_rate": 5.797999475166897e-06,
      "loss": 0.0007,
      "step": 2300
    },
    {
      "epoch": 36.82,
      "grad_norm": 0.00963984988629818,
      "learning_rate": 5.794834696024544e-06,
      "loss": 0.0002,
      "step": 2301
    },
    {
      "epoch": 36.83,
      "grad_norm": 0.008283499628305435,
      "learning_rate": 5.791669590155671e-06,
      "loss": 0.0002,
      "step": 2302
    },
    {
      "epoch": 36.85,
      "grad_norm": 0.007598127704113722,
      "learning_rate": 5.788504158861333e-06,
      "loss": 0.0001,
      "step": 2303
    },
    {
      "epoch": 36.86,
      "grad_norm": 0.5441107153892517,
      "learning_rate": 5.78533840344272e-06,
      "loss": 0.0064,
      "step": 2304
    },
    {
      "epoch": 36.88,
      "grad_norm": 0.27133435010910034,
      "learning_rate": 5.782172325201155e-06,
      "loss": 0.0009,
      "step": 2305
    },
    {
      "epoch": 36.9,
      "grad_norm": 0.010430149734020233,
      "learning_rate": 5.779005925438092e-06,
      "loss": 0.0003,
      "step": 2306
    },
    {
      "epoch": 36.91,
      "grad_norm": 0.7328068017959595,
      "learning_rate": 5.775839205455118e-06,
      "loss": 0.0015,
      "step": 2307
    },
    {
      "epoch": 36.93,
      "grad_norm": 0.010702744126319885,
      "learning_rate": 5.772672166553952e-06,
      "loss": 0.0003,
      "step": 2308
    },
    {
      "epoch": 36.94,
      "grad_norm": 0.0826251432299614,
      "learning_rate": 5.769504810036447e-06,
      "loss": 0.0008,
      "step": 2309
    },
    {
      "epoch": 36.96,
      "grad_norm": 0.7358044981956482,
      "learning_rate": 5.766337137204579e-06,
      "loss": 0.0078,
      "step": 2310
    },
    {
      "epoch": 36.98,
      "grad_norm": 0.010250081308186054,
      "learning_rate": 5.7631691493604624e-06,
      "loss": 0.0002,
      "step": 2311
    },
    {
      "epoch": 36.99,
      "grad_norm": 0.008281851187348366,
      "learning_rate": 5.760000847806337e-06,
      "loss": 0.0002,
      "step": 2312
    },
    {
      "epoch": 37.01,
      "grad_norm": 0.010937312617897987,
      "learning_rate": 5.756832233844569e-06,
      "loss": 0.0003,
      "step": 2313
    },
    {
      "epoch": 37.02,
      "grad_norm": 0.16211195290088654,
      "learning_rate": 5.7536633087776575e-06,
      "loss": 0.0009,
      "step": 2314
    },
    {
      "epoch": 37.04,
      "grad_norm": 1.0665067434310913,
      "learning_rate": 5.7504940739082305e-06,
      "loss": 0.0153,
      "step": 2315
    },
    {
      "epoch": 37.06,
      "grad_norm": 0.07079541683197021,
      "learning_rate": 5.7473245305390355e-06,
      "loss": 0.0005,
      "step": 2316
    },
    {
      "epoch": 37.07,
      "grad_norm": 0.01030539907515049,
      "learning_rate": 5.744154679972957e-06,
      "loss": 0.0002,
      "step": 2317
    },
    {
      "epoch": 37.09,
      "grad_norm": 0.007802425418049097,
      "learning_rate": 5.740984523512997e-06,
      "loss": 0.0002,
      "step": 2318
    },
    {
      "epoch": 37.1,
      "grad_norm": 0.7663626074790955,
      "learning_rate": 5.737814062462289e-06,
      "loss": 0.0089,
      "step": 2319
    },
    {
      "epoch": 37.12,
      "grad_norm": 0.009414395317435265,
      "learning_rate": 5.734643298124091e-06,
      "loss": 0.0002,
      "step": 2320
    },
    {
      "epoch": 37.14,
      "grad_norm": 1.5093579292297363,
      "learning_rate": 5.731472231801783e-06,
      "loss": 0.0214,
      "step": 2321
    },
    {
      "epoch": 37.15,
      "grad_norm": 0.46736064553260803,
      "learning_rate": 5.728300864798868e-06,
      "loss": 0.0033,
      "step": 2322
    },
    {
      "epoch": 37.17,
      "grad_norm": 0.735587477684021,
      "learning_rate": 5.72512919841898e-06,
      "loss": 0.0052,
      "step": 2323
    },
    {
      "epoch": 37.18,
      "grad_norm": 0.006087402813136578,
      "learning_rate": 5.721957233965868e-06,
      "loss": 0.0001,
      "step": 2324
    },
    {
      "epoch": 37.2,
      "grad_norm": 0.28124964237213135,
      "learning_rate": 5.71878497274341e-06,
      "loss": 0.002,
      "step": 2325
    },
    {
      "epoch": 37.22,
      "grad_norm": 0.5727087259292603,
      "learning_rate": 5.7156124160555985e-06,
      "loss": 0.0069,
      "step": 2326
    },
    {
      "epoch": 37.23,
      "grad_norm": 0.008868129923939705,
      "learning_rate": 5.712439565206555e-06,
      "loss": 0.0002,
      "step": 2327
    },
    {
      "epoch": 37.25,
      "grad_norm": 0.6886732578277588,
      "learning_rate": 5.709266421500516e-06,
      "loss": 0.0086,
      "step": 2328
    },
    {
      "epoch": 37.26,
      "grad_norm": 0.12417136877775192,
      "learning_rate": 5.706092986241842e-06,
      "loss": 0.0008,
      "step": 2329
    },
    {
      "epoch": 37.28,
      "grad_norm": 0.23843896389007568,
      "learning_rate": 5.702919260735015e-06,
      "loss": 0.0018,
      "step": 2330
    },
    {
      "epoch": 37.3,
      "grad_norm": 0.007508687674999237,
      "learning_rate": 5.699745246284632e-06,
      "loss": 0.0002,
      "step": 2331
    },
    {
      "epoch": 37.31,
      "grad_norm": 0.008801986463367939,
      "learning_rate": 5.696570944195407e-06,
      "loss": 0.0002,
      "step": 2332
    },
    {
      "epoch": 37.33,
      "grad_norm": 0.01014175359159708,
      "learning_rate": 5.693396355772181e-06,
      "loss": 0.0003,
      "step": 2333
    },
    {
      "epoch": 37.34,
      "grad_norm": 0.8438974022865295,
      "learning_rate": 5.690221482319903e-06,
      "loss": 0.0121,
      "step": 2334
    },
    {
      "epoch": 37.36,
      "grad_norm": 0.007571087218821049,
      "learning_rate": 5.6870463251436485e-06,
      "loss": 0.0002,
      "step": 2335
    },
    {
      "epoch": 37.38,
      "grad_norm": 1.0648096799850464,
      "learning_rate": 5.683870885548599e-06,
      "loss": 0.0149,
      "step": 2336
    },
    {
      "epoch": 37.39,
      "grad_norm": 0.007401539012789726,
      "learning_rate": 5.680695164840062e-06,
      "loss": 0.0002,
      "step": 2337
    },
    {
      "epoch": 37.41,
      "grad_norm": 0.2230323702096939,
      "learning_rate": 5.677519164323455e-06,
      "loss": 0.0006,
      "step": 2338
    },
    {
      "epoch": 37.42,
      "grad_norm": 0.019469276070594788,
      "learning_rate": 5.674342885304311e-06,
      "loss": 0.0002,
      "step": 2339
    },
    {
      "epoch": 37.44,
      "grad_norm": 0.29316723346710205,
      "learning_rate": 5.671166329088278e-06,
      "loss": 0.0009,
      "step": 2340
    },
    {
      "epoch": 37.46,
      "grad_norm": 1.3540966510772705,
      "learning_rate": 5.66798949698112e-06,
      "loss": 0.0281,
      "step": 2341
    },
    {
      "epoch": 37.47,
      "grad_norm": 0.8322432041168213,
      "learning_rate": 5.6648123902887135e-06,
      "loss": 0.0121,
      "step": 2342
    },
    {
      "epoch": 37.49,
      "grad_norm": 1.077656865119934,
      "learning_rate": 5.6616350103170435e-06,
      "loss": 0.0165,
      "step": 2343
    },
    {
      "epoch": 37.5,
      "grad_norm": 0.2973055839538574,
      "learning_rate": 5.658457358372213e-06,
      "loss": 0.0026,
      "step": 2344
    },
    {
      "epoch": 37.52,
      "grad_norm": 0.7368541955947876,
      "learning_rate": 5.655279435760436e-06,
      "loss": 0.0124,
      "step": 2345
    },
    {
      "epoch": 37.54,
      "grad_norm": 0.009293140843510628,
      "learning_rate": 5.652101243788034e-06,
      "loss": 0.0002,
      "step": 2346
    },
    {
      "epoch": 37.55,
      "grad_norm": 0.10400605946779251,
      "learning_rate": 5.648922783761443e-06,
      "loss": 0.0006,
      "step": 2347
    },
    {
      "epoch": 37.57,
      "grad_norm": 0.008719018660485744,
      "learning_rate": 5.645744056987208e-06,
      "loss": 0.0003,
      "step": 2348
    },
    {
      "epoch": 37.58,
      "grad_norm": 0.058785732835531235,
      "learning_rate": 5.642565064771982e-06,
      "loss": 0.0004,
      "step": 2349
    },
    {
      "epoch": 37.6,
      "grad_norm": 0.009452905505895615,
      "learning_rate": 5.6393858084225305e-06,
      "loss": 0.0003,
      "step": 2350
    },
    {
      "epoch": 37.62,
      "grad_norm": 0.6323121786117554,
      "learning_rate": 5.636206289245725e-06,
      "loss": 0.0043,
      "step": 2351
    },
    {
      "epoch": 37.63,
      "grad_norm": 0.8677934408187866,
      "learning_rate": 5.6330265085485454e-06,
      "loss": 0.009,
      "step": 2352
    },
    {
      "epoch": 37.65,
      "grad_norm": 0.7061460018157959,
      "learning_rate": 5.629846467638079e-06,
      "loss": 0.0066,
      "step": 2353
    },
    {
      "epoch": 37.66,
      "grad_norm": 0.007768597919493914,
      "learning_rate": 5.626666167821522e-06,
      "loss": 0.0002,
      "step": 2354
    },
    {
      "epoch": 37.68,
      "grad_norm": 0.06773538887500763,
      "learning_rate": 5.623485610406174e-06,
      "loss": 0.0003,
      "step": 2355
    },
    {
      "epoch": 37.7,
      "grad_norm": 0.4396829903125763,
      "learning_rate": 5.620304796699443e-06,
      "loss": 0.0077,
      "step": 2356
    },
    {
      "epoch": 37.71,
      "grad_norm": 0.006815753411501646,
      "learning_rate": 5.61712372800884e-06,
      "loss": 0.0001,
      "step": 2357
    },
    {
      "epoch": 37.73,
      "grad_norm": 2.501858949661255,
      "learning_rate": 5.613942405641986e-06,
      "loss": 0.0263,
      "step": 2358
    },
    {
      "epoch": 37.74,
      "grad_norm": 0.007736764382570982,
      "learning_rate": 5.610760830906599e-06,
      "loss": 0.0002,
      "step": 2359
    },
    {
      "epoch": 37.76,
      "grad_norm": 0.009217347018420696,
      "learning_rate": 5.6075790051105025e-06,
      "loss": 0.0002,
      "step": 2360
    },
    {
      "epoch": 37.78,
      "grad_norm": 1.4258829355239868,
      "learning_rate": 5.604396929561629e-06,
      "loss": 0.007,
      "step": 2361
    },
    {
      "epoch": 37.79,
      "grad_norm": 0.4201381802558899,
      "learning_rate": 5.601214605568006e-06,
      "loss": 0.0043,
      "step": 2362
    },
    {
      "epoch": 37.81,
      "grad_norm": 0.5203843116760254,
      "learning_rate": 5.598032034437771e-06,
      "loss": 0.007,
      "step": 2363
    },
    {
      "epoch": 37.82,
      "grad_norm": 0.011209039948880672,
      "learning_rate": 5.594849217479155e-06,
      "loss": 0.0001,
      "step": 2364
    },
    {
      "epoch": 37.84,
      "grad_norm": 0.009734222665429115,
      "learning_rate": 5.5916661560004945e-06,
      "loss": 0.0002,
      "step": 2365
    },
    {
      "epoch": 37.86,
      "grad_norm": 0.3463450074195862,
      "learning_rate": 5.588482851310227e-06,
      "loss": 0.0021,
      "step": 2366
    },
    {
      "epoch": 37.87,
      "grad_norm": 0.010057772509753704,
      "learning_rate": 5.5852993047168894e-06,
      "loss": 0.0002,
      "step": 2367
    },
    {
      "epoch": 37.89,
      "grad_norm": 0.9670130610466003,
      "learning_rate": 5.582115517529114e-06,
      "loss": 0.022,
      "step": 2368
    },
    {
      "epoch": 37.9,
      "grad_norm": 0.8142021298408508,
      "learning_rate": 5.57893149105564e-06,
      "loss": 0.0115,
      "step": 2369
    },
    {
      "epoch": 37.92,
      "grad_norm": 0.45742204785346985,
      "learning_rate": 5.575747226605298e-06,
      "loss": 0.0072,
      "step": 2370
    },
    {
      "epoch": 37.94,
      "grad_norm": 0.010754307731986046,
      "learning_rate": 5.572562725487018e-06,
      "loss": 0.0002,
      "step": 2371
    },
    {
      "epoch": 37.95,
      "grad_norm": 1.1501866579055786,
      "learning_rate": 5.569377989009829e-06,
      "loss": 0.0114,
      "step": 2372
    },
    {
      "epoch": 37.97,
      "grad_norm": 1.0954099893569946,
      "learning_rate": 5.566193018482856e-06,
      "loss": 0.012,
      "step": 2373
    },
    {
      "epoch": 37.98,
      "grad_norm": 0.8087233901023865,
      "learning_rate": 5.5630078152153226e-06,
      "loss": 0.0128,
      "step": 2374
    },
    {
      "epoch": 38.0,
      "grad_norm": 1.1611841917037964,
      "learning_rate": 5.559822380516539e-06,
      "loss": 0.0128,
      "step": 2375
    },
    {
      "epoch": 38.02,
      "grad_norm": 0.7070395350456238,
      "learning_rate": 5.5566367156959246e-06,
      "loss": 0.0074,
      "step": 2376
    },
    {
      "epoch": 38.03,
      "grad_norm": 0.7687029242515564,
      "learning_rate": 5.55345082206298e-06,
      "loss": 0.0114,
      "step": 2377
    },
    {
      "epoch": 38.05,
      "grad_norm": 0.009577359072864056,
      "learning_rate": 5.550264700927309e-06,
      "loss": 0.0002,
      "step": 2378
    },
    {
      "epoch": 38.06,
      "grad_norm": 1.7334295511245728,
      "learning_rate": 5.547078353598605e-06,
      "loss": 0.0275,
      "step": 2379
    },
    {
      "epoch": 38.08,
      "grad_norm": 0.6824097633361816,
      "learning_rate": 5.543891781386655e-06,
      "loss": 0.0089,
      "step": 2380
    },
    {
      "epoch": 38.1,
      "grad_norm": 0.010022731497883797,
      "learning_rate": 5.540704985601338e-06,
      "loss": 0.0003,
      "step": 2381
    },
    {
      "epoch": 38.11,
      "grad_norm": 0.616592288017273,
      "learning_rate": 5.537517967552626e-06,
      "loss": 0.007,
      "step": 2382
    },
    {
      "epoch": 38.13,
      "grad_norm": 0.9579537510871887,
      "learning_rate": 5.53433072855058e-06,
      "loss": 0.0114,
      "step": 2383
    },
    {
      "epoch": 38.14,
      "grad_norm": 0.010842484422028065,
      "learning_rate": 5.531143269905356e-06,
      "loss": 0.0002,
      "step": 2384
    },
    {
      "epoch": 38.16,
      "grad_norm": 0.025966614484786987,
      "learning_rate": 5.527955592927198e-06,
      "loss": 0.0002,
      "step": 2385
    },
    {
      "epoch": 38.18,
      "grad_norm": 0.03795595467090607,
      "learning_rate": 5.524767698926437e-06,
      "loss": 0.0003,
      "step": 2386
    },
    {
      "epoch": 38.19,
      "grad_norm": 0.01382676512002945,
      "learning_rate": 5.521579589213496e-06,
      "loss": 0.0003,
      "step": 2387
    },
    {
      "epoch": 38.21,
      "grad_norm": 0.8985889554023743,
      "learning_rate": 5.518391265098888e-06,
      "loss": 0.0103,
      "step": 2388
    },
    {
      "epoch": 38.22,
      "grad_norm": 0.09479372948408127,
      "learning_rate": 5.515202727893213e-06,
      "loss": 0.0009,
      "step": 2389
    },
    {
      "epoch": 38.24,
      "grad_norm": 0.01416734792292118,
      "learning_rate": 5.512013978907157e-06,
      "loss": 0.0002,
      "step": 2390
    },
    {
      "epoch": 38.26,
      "grad_norm": 0.34301573038101196,
      "learning_rate": 5.508825019451497e-06,
      "loss": 0.0026,
      "step": 2391
    },
    {
      "epoch": 38.27,
      "grad_norm": 0.010419024154543877,
      "learning_rate": 5.5056358508370884e-06,
      "loss": 0.0001,
      "step": 2392
    },
    {
      "epoch": 38.29,
      "grad_norm": 0.6833108067512512,
      "learning_rate": 5.502446474374883e-06,
      "loss": 0.0087,
      "step": 2393
    },
    {
      "epoch": 38.3,
      "grad_norm": 0.5785732269287109,
      "learning_rate": 5.49925689137591e-06,
      "loss": 0.0072,
      "step": 2394
    },
    {
      "epoch": 38.32,
      "grad_norm": 0.007715805899351835,
      "learning_rate": 5.496067103151288e-06,
      "loss": 0.0002,
      "step": 2395
    },
    {
      "epoch": 38.34,
      "grad_norm": 0.6292504668235779,
      "learning_rate": 5.4928771110122185e-06,
      "loss": 0.0091,
      "step": 2396
    },
    {
      "epoch": 38.35,
      "grad_norm": 0.00903149414807558,
      "learning_rate": 5.489686916269985e-06,
      "loss": 0.0002,
      "step": 2397
    },
    {
      "epoch": 38.37,
      "grad_norm": 0.010897706262767315,
      "learning_rate": 5.486496520235959e-06,
      "loss": 0.0002,
      "step": 2398
    },
    {
      "epoch": 38.38,
      "grad_norm": 0.8035064935684204,
      "learning_rate": 5.483305924221588e-06,
      "loss": 0.0049,
      "step": 2399
    },
    {
      "epoch": 38.4,
      "grad_norm": 0.8388124108314514,
      "learning_rate": 5.480115129538409e-06,
      "loss": 0.0118,
      "step": 2400
    },
    {
      "epoch": 38.42,
      "grad_norm": 0.0069028884172439575,
      "learning_rate": 5.4769241374980365e-06,
      "loss": 0.0001,
      "step": 2401
    },
    {
      "epoch": 38.43,
      "grad_norm": 0.07030925154685974,
      "learning_rate": 5.473732949412165e-06,
      "loss": 0.0004,
      "step": 2402
    },
    {
      "epoch": 38.45,
      "grad_norm": 0.938051700592041,
      "learning_rate": 5.470541566592573e-06,
      "loss": 0.014,
      "step": 2403
    },
    {
      "epoch": 38.46,
      "grad_norm": 0.03574535995721817,
      "learning_rate": 5.467349990351116e-06,
      "loss": 0.0005,
      "step": 2404
    },
    {
      "epoch": 38.48,
      "grad_norm": 0.8752819299697876,
      "learning_rate": 5.464158221999731e-06,
      "loss": 0.0124,
      "step": 2405
    },
    {
      "epoch": 38.5,
      "grad_norm": 0.01015132199972868,
      "learning_rate": 5.460966262850434e-06,
      "loss": 0.0003,
      "step": 2406
    },
    {
      "epoch": 38.51,
      "grad_norm": 0.009286761283874512,
      "learning_rate": 5.45777411421532e-06,
      "loss": 0.0002,
      "step": 2407
    },
    {
      "epoch": 38.53,
      "grad_norm": 0.3007400929927826,
      "learning_rate": 5.454581777406559e-06,
      "loss": 0.0018,
      "step": 2408
    },
    {
      "epoch": 38.54,
      "grad_norm": 0.7430912256240845,
      "learning_rate": 5.4513892537364e-06,
      "loss": 0.0116,
      "step": 2409
    },
    {
      "epoch": 38.56,
      "grad_norm": 0.009306009858846664,
      "learning_rate": 5.448196544517168e-06,
      "loss": 0.0002,
      "step": 2410
    },
    {
      "epoch": 38.58,
      "grad_norm": 0.010864551179111004,
      "learning_rate": 5.445003651061267e-06,
      "loss": 0.0003,
      "step": 2411
    },
    {
      "epoch": 38.59,
      "grad_norm": 0.011309126392006874,
      "learning_rate": 5.441810574681175e-06,
      "loss": 0.0002,
      "step": 2412
    },
    {
      "epoch": 38.61,
      "grad_norm": 0.017849955707788467,
      "learning_rate": 5.4386173166894455e-06,
      "loss": 0.0002,
      "step": 2413
    },
    {
      "epoch": 38.62,
      "grad_norm": 0.7250574827194214,
      "learning_rate": 5.435423878398703e-06,
      "loss": 0.0062,
      "step": 2414
    },
    {
      "epoch": 38.64,
      "grad_norm": 0.07416609674692154,
      "learning_rate": 5.4322302611216515e-06,
      "loss": 0.0007,
      "step": 2415
    },
    {
      "epoch": 38.66,
      "grad_norm": 0.524344265460968,
      "learning_rate": 5.429036466171067e-06,
      "loss": 0.0047,
      "step": 2416
    },
    {
      "epoch": 38.67,
      "grad_norm": 0.8589515089988708,
      "learning_rate": 5.425842494859798e-06,
      "loss": 0.0146,
      "step": 2417
    },
    {
      "epoch": 38.69,
      "grad_norm": 0.48247846961021423,
      "learning_rate": 5.422648348500764e-06,
      "loss": 0.0061,
      "step": 2418
    },
    {
      "epoch": 38.7,
      "grad_norm": 1.1684679985046387,
      "learning_rate": 5.4194540284069596e-06,
      "loss": 0.0175,
      "step": 2419
    },
    {
      "epoch": 38.72,
      "grad_norm": 1.2304941415786743,
      "learning_rate": 5.4162595358914475e-06,
      "loss": 0.0125,
      "step": 2420
    },
    {
      "epoch": 38.74,
      "grad_norm": 0.2745431661605835,
      "learning_rate": 5.413064872267364e-06,
      "loss": 0.0021,
      "step": 2421
    },
    {
      "epoch": 38.75,
      "grad_norm": 0.6285266876220703,
      "learning_rate": 5.4098700388479135e-06,
      "loss": 0.0102,
      "step": 2422
    },
    {
      "epoch": 38.77,
      "grad_norm": 0.01921510510146618,
      "learning_rate": 5.406675036946374e-06,
      "loss": 0.0002,
      "step": 2423
    },
    {
      "epoch": 38.78,
      "grad_norm": 1.3113726377487183,
      "learning_rate": 5.403479867876087e-06,
      "loss": 0.0246,
      "step": 2424
    },
    {
      "epoch": 38.8,
      "grad_norm": 1.7303470373153687,
      "learning_rate": 5.4002845329504675e-06,
      "loss": 0.0222,
      "step": 2425
    },
    {
      "epoch": 38.82,
      "grad_norm": 0.1597321480512619,
      "learning_rate": 5.3970890334829976e-06,
      "loss": 0.0017,
      "step": 2426
    },
    {
      "epoch": 38.83,
      "grad_norm": 0.009556349366903305,
      "learning_rate": 5.3938933707872236e-06,
      "loss": 0.0002,
      "step": 2427
    },
    {
      "epoch": 38.85,
      "grad_norm": 0.01829073391854763,
      "learning_rate": 5.390697546176763e-06,
      "loss": 0.0001,
      "step": 2428
    },
    {
      "epoch": 38.86,
      "grad_norm": 0.2204834222793579,
      "learning_rate": 5.387501560965301e-06,
      "loss": 0.0012,
      "step": 2429
    },
    {
      "epoch": 38.88,
      "grad_norm": 0.8643196821212769,
      "learning_rate": 5.384305416466584e-06,
      "loss": 0.0093,
      "step": 2430
    },
    {
      "epoch": 38.9,
      "grad_norm": 0.006392308045178652,
      "learning_rate": 5.381109113994426e-06,
      "loss": 0.0001,
      "step": 2431
    },
    {
      "epoch": 38.91,
      "grad_norm": 0.4189985990524292,
      "learning_rate": 5.377912654862708e-06,
      "loss": 0.003,
      "step": 2432
    },
    {
      "epoch": 38.93,
      "grad_norm": 0.006688305176794529,
      "learning_rate": 5.374716040385371e-06,
      "loss": 0.0001,
      "step": 2433
    },
    {
      "epoch": 38.94,
      "grad_norm": 0.007178297266364098,
      "learning_rate": 5.371519271876426e-06,
      "loss": 0.0001,
      "step": 2434
    },
    {
      "epoch": 38.96,
      "grad_norm": 1.0131326913833618,
      "learning_rate": 5.368322350649942e-06,
      "loss": 0.0199,
      "step": 2435
    },
    {
      "epoch": 38.98,
      "grad_norm": 0.16028548777103424,
      "learning_rate": 5.36512527802005e-06,
      "loss": 0.0007,
      "step": 2436
    },
    {
      "epoch": 38.99,
      "grad_norm": 0.008920036256313324,
      "learning_rate": 5.36192805530095e-06,
      "loss": 0.0002,
      "step": 2437
    },
    {
      "epoch": 39.01,
      "grad_norm": 0.014831178821623325,
      "learning_rate": 5.358730683806897e-06,
      "loss": 0.0002,
      "step": 2438
    },
    {
      "epoch": 39.02,
      "grad_norm": 0.6850144863128662,
      "learning_rate": 5.355533164852211e-06,
      "loss": 0.0084,
      "step": 2439
    },
    {
      "epoch": 39.04,
      "grad_norm": 0.4539133608341217,
      "learning_rate": 5.35233549975127e-06,
      "loss": 0.0037,
      "step": 2440
    },
    {
      "epoch": 39.06,
      "grad_norm": 0.006641480140388012,
      "learning_rate": 5.349137689818514e-06,
      "loss": 0.0001,
      "step": 2441
    },
    {
      "epoch": 39.07,
      "grad_norm": 0.4876936674118042,
      "learning_rate": 5.345939736368442e-06,
      "loss": 0.003,
      "step": 2442
    },
    {
      "epoch": 39.09,
      "grad_norm": 0.02993149496614933,
      "learning_rate": 5.342741640715611e-06,
      "loss": 0.0003,
      "step": 2443
    },
    {
      "epoch": 39.1,
      "grad_norm": 0.3781406283378601,
      "learning_rate": 5.339543404174639e-06,
      "loss": 0.0039,
      "step": 2444
    },
    {
      "epoch": 39.12,
      "grad_norm": 0.010434293188154697,
      "learning_rate": 5.336345028060199e-06,
      "loss": 0.0002,
      "step": 2445
    },
    {
      "epoch": 39.14,
      "grad_norm": 0.06875898689031601,
      "learning_rate": 5.333146513687022e-06,
      "loss": 0.0005,
      "step": 2446
    },
    {
      "epoch": 39.15,
      "grad_norm": 0.007319721393287182,
      "learning_rate": 5.329947862369897e-06,
      "loss": 0.0002,
      "step": 2447
    },
    {
      "epoch": 39.17,
      "grad_norm": 0.7505760788917542,
      "learning_rate": 5.326749075423672e-06,
      "loss": 0.007,
      "step": 2448
    },
    {
      "epoch": 39.18,
      "grad_norm": 0.5206310749053955,
      "learning_rate": 5.3235501541632414e-06,
      "loss": 0.0035,
      "step": 2449
    },
    {
      "epoch": 39.2,
      "grad_norm": 0.00655707623809576,
      "learning_rate": 5.320351099903565e-06,
      "loss": 0.0001,
      "step": 2450
    },
    {
      "epoch": 39.22,
      "grad_norm": 1.076881766319275,
      "learning_rate": 5.317151913959653e-06,
      "loss": 0.0081,
      "step": 2451
    },
    {
      "epoch": 39.23,
      "grad_norm": 1.0564054250717163,
      "learning_rate": 5.3139525976465675e-06,
      "loss": 0.0161,
      "step": 2452
    },
    {
      "epoch": 39.25,
      "grad_norm": 1.0278234481811523,
      "learning_rate": 5.310753152279429e-06,
      "loss": 0.0166,
      "step": 2453
    },
    {
      "epoch": 39.26,
      "grad_norm": 0.6651358008384705,
      "learning_rate": 5.307553579173408e-06,
      "loss": 0.0075,
      "step": 2454
    },
    {
      "epoch": 39.28,
      "grad_norm": 0.17743901908397675,
      "learning_rate": 5.304353879643727e-06,
      "loss": 0.0009,
      "step": 2455
    },
    {
      "epoch": 39.3,
      "grad_norm": 0.01721629686653614,
      "learning_rate": 5.301154055005664e-06,
      "loss": 0.0002,
      "step": 2456
    },
    {
      "epoch": 39.31,
      "grad_norm": 0.009111502207815647,
      "learning_rate": 5.297954106574543e-06,
      "loss": 0.0002,
      "step": 2457
    },
    {
      "epoch": 39.33,
      "grad_norm": 0.5967426300048828,
      "learning_rate": 5.294754035665743e-06,
      "loss": 0.0076,
      "step": 2458
    },
    {
      "epoch": 39.34,
      "grad_norm": 0.009139223955571651,
      "learning_rate": 5.291553843594695e-06,
      "loss": 0.0003,
      "step": 2459
    },
    {
      "epoch": 39.36,
      "grad_norm": 0.009665117599070072,
      "learning_rate": 5.288353531676873e-06,
      "loss": 0.0003,
      "step": 2460
    },
    {
      "epoch": 39.38,
      "grad_norm": 0.3829462230205536,
      "learning_rate": 5.285153101227808e-06,
      "loss": 0.0027,
      "step": 2461
    },
    {
      "epoch": 39.39,
      "grad_norm": 0.005952071864157915,
      "learning_rate": 5.2819525535630725e-06,
      "loss": 0.0001,
      "step": 2462
    },
    {
      "epoch": 39.41,
      "grad_norm": 0.013003252446651459,
      "learning_rate": 5.278751889998292e-06,
      "loss": 0.0003,
      "step": 2463
    },
    {
      "epoch": 39.42,
      "grad_norm": 0.758886992931366,
      "learning_rate": 5.2755511118491405e-06,
      "loss": 0.009,
      "step": 2464
    },
    {
      "epoch": 39.44,
      "grad_norm": 0.01032260525971651,
      "learning_rate": 5.2723502204313346e-06,
      "loss": 0.0003,
      "step": 2465
    },
    {
      "epoch": 39.46,
      "grad_norm": 1.4108202457427979,
      "learning_rate": 5.269149217060642e-06,
      "loss": 0.0196,
      "step": 2466
    },
    {
      "epoch": 39.47,
      "grad_norm": 0.03469788283109665,
      "learning_rate": 5.265948103052871e-06,
      "loss": 0.0006,
      "step": 2467
    },
    {
      "epoch": 39.49,
      "grad_norm": 0.16075171530246735,
      "learning_rate": 5.262746879723882e-06,
      "loss": 0.0025,
      "step": 2468
    },
    {
      "epoch": 39.5,
      "grad_norm": 0.07810904830694199,
      "learning_rate": 5.259545548389575e-06,
      "loss": 0.0005,
      "step": 2469
    },
    {
      "epoch": 39.52,
      "grad_norm": 0.9700582027435303,
      "learning_rate": 5.256344110365896e-06,
      "loss": 0.0134,
      "step": 2470
    },
    {
      "epoch": 39.54,
      "grad_norm": 0.00942268967628479,
      "learning_rate": 5.253142566968838e-06,
      "loss": 0.0002,
      "step": 2471
    },
    {
      "epoch": 39.55,
      "grad_norm": 0.2300838679075241,
      "learning_rate": 5.249940919514434e-06,
      "loss": 0.0003,
      "step": 2472
    },
    {
      "epoch": 39.57,
      "grad_norm": 0.5172207951545715,
      "learning_rate": 5.2467391693187565e-06,
      "loss": 0.0035,
      "step": 2473
    },
    {
      "epoch": 39.58,
      "grad_norm": 0.7264341711997986,
      "learning_rate": 5.243537317697927e-06,
      "loss": 0.0096,
      "step": 2474
    },
    {
      "epoch": 39.6,
      "grad_norm": 0.5244940519332886,
      "learning_rate": 5.240335365968104e-06,
      "loss": 0.0034,
      "step": 2475
    },
    {
      "epoch": 39.62,
      "grad_norm": 0.007152780424803495,
      "learning_rate": 5.237133315445493e-06,
      "loss": 0.0002,
      "step": 2476
    },
    {
      "epoch": 39.63,
      "grad_norm": 0.006756428629159927,
      "learning_rate": 5.233931167446331e-06,
      "loss": 0.0001,
      "step": 2477
    },
    {
      "epoch": 39.65,
      "grad_norm": 0.005412540398538113,
      "learning_rate": 5.230728923286905e-06,
      "loss": 0.0001,
      "step": 2478
    },
    {
      "epoch": 39.66,
      "grad_norm": 0.010276826098561287,
      "learning_rate": 5.227526584283532e-06,
      "loss": 0.0003,
      "step": 2479
    },
    {
      "epoch": 39.68,
      "grad_norm": 0.7171187996864319,
      "learning_rate": 5.224324151752575e-06,
      "loss": 0.0122,
      "step": 2480
    },
    {
      "epoch": 39.7,
      "grad_norm": 0.020277895033359528,
      "learning_rate": 5.2211216270104326e-06,
      "loss": 0.0003,
      "step": 2481
    },
    {
      "epoch": 39.71,
      "grad_norm": 0.008445563726127148,
      "learning_rate": 5.2179190113735425e-06,
      "loss": 0.0002,
      "step": 2482
    },
    {
      "epoch": 39.73,
      "grad_norm": 0.010815802961587906,
      "learning_rate": 5.214716306158378e-06,
      "loss": 0.0003,
      "step": 2483
    },
    {
      "epoch": 39.74,
      "grad_norm": 0.5939118266105652,
      "learning_rate": 5.211513512681451e-06,
      "loss": 0.0122,
      "step": 2484
    },
    {
      "epoch": 39.76,
      "grad_norm": 0.007408527657389641,
      "learning_rate": 5.208310632259308e-06,
      "loss": 0.0002,
      "step": 2485
    },
    {
      "epoch": 39.78,
      "grad_norm": 1.078992247581482,
      "learning_rate": 5.205107666208533e-06,
      "loss": 0.0163,
      "step": 2486
    },
    {
      "epoch": 39.79,
      "grad_norm": 1.2284770011901855,
      "learning_rate": 5.201904615845743e-06,
      "loss": 0.0228,
      "step": 2487
    },
    {
      "epoch": 39.81,
      "grad_norm": 0.06498303264379501,
      "learning_rate": 5.198701482487594e-06,
      "loss": 0.0002,
      "step": 2488
    },
    {
      "epoch": 39.82,
      "grad_norm": 0.00620014313608408,
      "learning_rate": 5.195498267450769e-06,
      "loss": 0.0001,
      "step": 2489
    },
    {
      "epoch": 39.84,
      "grad_norm": 1.5212515592575073,
      "learning_rate": 5.192294972051992e-06,
      "loss": 0.0215,
      "step": 2490
    },
    {
      "epoch": 39.86,
      "grad_norm": 0.519176185131073,
      "learning_rate": 5.189091597608016e-06,
      "loss": 0.0061,
      "step": 2491
    },
    {
      "epoch": 39.87,
      "grad_norm": 0.08112289756536484,
      "learning_rate": 5.185888145435626e-06,
      "loss": 0.0006,
      "step": 2492
    },
    {
      "epoch": 39.89,
      "grad_norm": 0.008038372732698917,
      "learning_rate": 5.182684616851642e-06,
      "loss": 0.0002,
      "step": 2493
    },
    {
      "epoch": 39.9,
      "grad_norm": 0.07869099080562592,
      "learning_rate": 5.179481013172912e-06,
      "loss": 0.0007,
      "step": 2494
    },
    {
      "epoch": 39.92,
      "grad_norm": 0.08655180782079697,
      "learning_rate": 5.1762773357163175e-06,
      "loss": 0.0006,
      "step": 2495
    },
    {
      "epoch": 39.94,
      "grad_norm": 0.007614905014634132,
      "learning_rate": 5.173073585798768e-06,
      "loss": 0.0002,
      "step": 2496
    },
    {
      "epoch": 39.95,
      "grad_norm": 1.3564499616622925,
      "learning_rate": 5.169869764737205e-06,
      "loss": 0.0272,
      "step": 2497
    },
    {
      "epoch": 39.97,
      "grad_norm": 0.8436705470085144,
      "learning_rate": 5.1666658738485985e-06,
      "loss": 0.006,
      "step": 2498
    },
    {
      "epoch": 39.98,
      "grad_norm": 0.8312881588935852,
      "learning_rate": 5.163461914449948e-06,
      "loss": 0.0087,
      "step": 2499
    },
    {
      "epoch": 40.0,
      "grad_norm": 0.02248241938650608,
      "learning_rate": 5.160257887858278e-06,
      "loss": 0.0002,
      "step": 2500
    },
    {
      "epoch": 40.02,
      "grad_norm": 0.00844995304942131,
      "learning_rate": 5.157053795390642e-06,
      "loss": 0.0002,
      "step": 2501
    },
    {
      "epoch": 40.03,
      "grad_norm": 0.7209325432777405,
      "learning_rate": 5.153849638364125e-06,
      "loss": 0.0116,
      "step": 2502
    },
    {
      "epoch": 40.05,
      "grad_norm": 0.007447531446814537,
      "learning_rate": 5.150645418095832e-06,
      "loss": 0.0002,
      "step": 2503
    },
    {
      "epoch": 40.06,
      "grad_norm": 0.007074248511344194,
      "learning_rate": 5.1474411359029e-06,
      "loss": 0.0002,
      "step": 2504
    },
    {
      "epoch": 40.08,
      "grad_norm": 0.0073219058103859425,
      "learning_rate": 5.144236793102485e-06,
      "loss": 0.0002,
      "step": 2505
    },
    {
      "epoch": 40.1,
      "grad_norm": 0.579934298992157,
      "learning_rate": 5.141032391011774e-06,
      "loss": 0.0038,
      "step": 2506
    },
    {
      "epoch": 40.11,
      "grad_norm": 0.20141573250293732,
      "learning_rate": 5.137827930947972e-06,
      "loss": 0.0014,
      "step": 2507
    },
    {
      "epoch": 40.13,
      "grad_norm": 0.0064179180189967155,
      "learning_rate": 5.134623414228315e-06,
      "loss": 0.0001,
      "step": 2508
    },
    {
      "epoch": 40.14,
      "grad_norm": 0.8834043145179749,
      "learning_rate": 5.131418842170056e-06,
      "loss": 0.0178,
      "step": 2509
    },
    {
      "epoch": 40.16,
      "grad_norm": 1.148027777671814,
      "learning_rate": 5.128214216090478e-06,
      "loss": 0.017,
      "step": 2510
    },
    {
      "epoch": 40.18,
      "grad_norm": 0.007729173172265291,
      "learning_rate": 5.125009537306878e-06,
      "loss": 0.0002,
      "step": 2511
    },
    {
      "epoch": 40.19,
      "grad_norm": 0.7950640916824341,
      "learning_rate": 5.121804807136578e-06,
      "loss": 0.0048,
      "step": 2512
    },
    {
      "epoch": 40.21,
      "grad_norm": 0.06129692494869232,
      "learning_rate": 5.118600026896923e-06,
      "loss": 0.0006,
      "step": 2513
    },
    {
      "epoch": 40.22,
      "grad_norm": 0.7550431489944458,
      "learning_rate": 5.115395197905277e-06,
      "loss": 0.0102,
      "step": 2514
    },
    {
      "epoch": 40.24,
      "grad_norm": 0.0066039590165019035,
      "learning_rate": 5.112190321479026e-06,
      "loss": 0.0002,
      "step": 2515
    },
    {
      "epoch": 40.26,
      "grad_norm": 0.008461146615445614,
      "learning_rate": 5.108985398935569e-06,
      "loss": 0.0002,
      "step": 2516
    },
    {
      "epoch": 40.27,
      "grad_norm": 1.06779146194458,
      "learning_rate": 5.105780431592333e-06,
      "loss": 0.0139,
      "step": 2517
    },
    {
      "epoch": 40.29,
      "grad_norm": 0.7154924273490906,
      "learning_rate": 5.1025754207667565e-06,
      "loss": 0.0067,
      "step": 2518
    },
    {
      "epoch": 40.3,
      "grad_norm": 0.06051580235362053,
      "learning_rate": 5.0993703677762985e-06,
      "loss": 0.0003,
      "step": 2519
    },
    {
      "epoch": 40.32,
      "grad_norm": 0.008064554072916508,
      "learning_rate": 5.0961652739384356e-06,
      "loss": 0.0002,
      "step": 2520
    },
    {
      "epoch": 40.34,
      "grad_norm": 0.6093773245811462,
      "learning_rate": 5.092960140570664e-06,
      "loss": 0.0047,
      "step": 2521
    },
    {
      "epoch": 40.35,
      "grad_norm": 0.11542167514562607,
      "learning_rate": 5.0897549689904865e-06,
      "loss": 0.0013,
      "step": 2522
    },
    {
      "epoch": 40.37,
      "grad_norm": 0.2406003624200821,
      "learning_rate": 5.0865497605154335e-06,
      "loss": 0.0017,
      "step": 2523
    },
    {
      "epoch": 40.38,
      "grad_norm": 0.010143562220036983,
      "learning_rate": 5.083344516463043e-06,
      "loss": 0.0003,
      "step": 2524
    },
    {
      "epoch": 40.4,
      "grad_norm": 0.3227926790714264,
      "learning_rate": 5.080139238150869e-06,
      "loss": 0.0024,
      "step": 2525
    },
    {
      "epoch": 40.42,
      "grad_norm": 0.7779590487480164,
      "learning_rate": 5.076933926896484e-06,
      "loss": 0.0073,
      "step": 2526
    },
    {
      "epoch": 40.43,
      "grad_norm": 0.673112690448761,
      "learning_rate": 5.073728584017465e-06,
      "loss": 0.0087,
      "step": 2527
    },
    {
      "epoch": 40.45,
      "grad_norm": 0.007815228775143623,
      "learning_rate": 5.07052321083141e-06,
      "loss": 0.0002,
      "step": 2528
    },
    {
      "epoch": 40.46,
      "grad_norm": 0.8638114929199219,
      "learning_rate": 5.067317808655927e-06,
      "loss": 0.0152,
      "step": 2529
    },
    {
      "epoch": 40.48,
      "grad_norm": 0.558436393737793,
      "learning_rate": 5.064112378808636e-06,
      "loss": 0.0051,
      "step": 2530
    },
    {
      "epoch": 40.5,
      "grad_norm": 0.005954761523753405,
      "learning_rate": 5.06090692260717e-06,
      "loss": 0.0001,
      "step": 2531
    },
    {
      "epoch": 40.51,
      "grad_norm": 0.008412087336182594,
      "learning_rate": 5.057701441369167e-06,
      "loss": 0.0002,
      "step": 2532
    },
    {
      "epoch": 40.53,
      "grad_norm": 0.5215818285942078,
      "learning_rate": 5.054495936412281e-06,
      "loss": 0.0036,
      "step": 2533
    },
    {
      "epoch": 40.54,
      "grad_norm": 0.012907732278108597,
      "learning_rate": 5.051290409054173e-06,
      "loss": 0.0001,
      "step": 2534
    },
    {
      "epoch": 40.56,
      "grad_norm": 0.00842563807964325,
      "learning_rate": 5.048084860612516e-06,
      "loss": 0.0002,
      "step": 2535
    },
    {
      "epoch": 40.58,
      "grad_norm": 0.008281380869448185,
      "learning_rate": 5.04487929240499e-06,
      "loss": 0.0002,
      "step": 2536
    },
    {
      "epoch": 40.59,
      "grad_norm": 0.6207687258720398,
      "learning_rate": 5.041673705749281e-06,
      "loss": 0.0052,
      "step": 2537
    },
    {
      "epoch": 40.61,
      "grad_norm": 0.9548721313476562,
      "learning_rate": 5.038468101963087e-06,
      "loss": 0.0195,
      "step": 2538
    },
    {
      "epoch": 40.62,
      "grad_norm": 0.05437018349766731,
      "learning_rate": 5.03526248236411e-06,
      "loss": 0.0006,
      "step": 2539
    },
    {
      "epoch": 40.64,
      "grad_norm": 1.4248402118682861,
      "learning_rate": 5.032056848270056e-06,
      "loss": 0.0206,
      "step": 2540
    },
    {
      "epoch": 40.66,
      "grad_norm": 1.1832184791564941,
      "learning_rate": 5.028851200998644e-06,
      "loss": 0.0193,
      "step": 2541
    },
    {
      "epoch": 40.67,
      "grad_norm": 0.29267987608909607,
      "learning_rate": 5.025645541867592e-06,
      "loss": 0.0018,
      "step": 2542
    },
    {
      "epoch": 40.69,
      "grad_norm": 0.8996452689170837,
      "learning_rate": 5.022439872194629e-06,
      "loss": 0.0075,
      "step": 2543
    },
    {
      "epoch": 40.7,
      "grad_norm": 0.06580163538455963,
      "learning_rate": 5.01923419329748e-06,
      "loss": 0.0002,
      "step": 2544
    },
    {
      "epoch": 40.72,
      "grad_norm": 0.008628349751234055,
      "learning_rate": 5.016028506493881e-06,
      "loss": 0.0003,
      "step": 2545
    },
    {
      "epoch": 40.74,
      "grad_norm": 0.787102997303009,
      "learning_rate": 5.012822813101568e-06,
      "loss": 0.0147,
      "step": 2546
    },
    {
      "epoch": 40.75,
      "grad_norm": 0.0076470086351037025,
      "learning_rate": 5.0096171144382814e-06,
      "loss": 0.0002,
      "step": 2547
    },
    {
      "epoch": 40.77,
      "grad_norm": 1.3051804304122925,
      "learning_rate": 5.006411411821762e-06,
      "loss": 0.0182,
      "step": 2548
    },
    {
      "epoch": 40.78,
      "grad_norm": 0.007982647977769375,
      "learning_rate": 5.003205706569753e-06,
      "loss": 0.0002,
      "step": 2549
    },
    {
      "epoch": 40.8,
      "grad_norm": 0.011345896869897842,
      "learning_rate": 5e-06,
      "loss": 0.0003,
      "step": 2550
    },
    {
      "epoch": 40.82,
      "grad_norm": 0.005735102575272322,
      "learning_rate": 4.9967942934302475e-06,
      "loss": 0.0001,
      "step": 2551
    },
    {
      "epoch": 40.83,
      "grad_norm": 0.9646934270858765,
      "learning_rate": 4.993588588178239e-06,
      "loss": 0.0067,
      "step": 2552
    },
    {
      "epoch": 40.85,
      "grad_norm": 0.6539013385772705,
      "learning_rate": 4.99038288556172e-06,
      "loss": 0.0095,
      "step": 2553
    },
    {
      "epoch": 40.86,
      "grad_norm": 0.0062205735594034195,
      "learning_rate": 4.987177186898434e-06,
      "loss": 0.0001,
      "step": 2554
    },
    {
      "epoch": 40.88,
      "grad_norm": 0.1196727305650711,
      "learning_rate": 4.9839714935061215e-06,
      "loss": 0.0006,
      "step": 2555
    },
    {
      "epoch": 40.9,
      "grad_norm": 0.5087059140205383,
      "learning_rate": 4.980765806702522e-06,
      "loss": 0.0047,
      "step": 2556
    },
    {
      "epoch": 40.91,
      "grad_norm": 0.007831939496099949,
      "learning_rate": 4.977560127805373e-06,
      "loss": 0.0002,
      "step": 2557
    },
    {
      "epoch": 40.93,
      "grad_norm": 0.006413348019123077,
      "learning_rate": 4.974354458132409e-06,
      "loss": 0.0001,
      "step": 2558
    },
    {
      "epoch": 40.94,
      "grad_norm": 0.14796023070812225,
      "learning_rate": 4.971148799001357e-06,
      "loss": 0.0008,
      "step": 2559
    },
    {
      "epoch": 40.96,
      "grad_norm": 0.9226523637771606,
      "learning_rate": 4.967943151729945e-06,
      "loss": 0.0121,
      "step": 2560
    },
    {
      "epoch": 40.98,
      "grad_norm": 1.2012779712677002,
      "learning_rate": 4.964737517635892e-06,
      "loss": 0.0118,
      "step": 2561
    },
    {
      "epoch": 40.99,
      "grad_norm": 0.005924027878791094,
      "learning_rate": 4.961531898036913e-06,
      "loss": 0.0001,
      "step": 2562
    },
    {
      "epoch": 41.01,
      "grad_norm": 0.0038473885506391525,
      "learning_rate": 4.9583262942507195e-06,
      "loss": 0.0001,
      "step": 2563
    },
    {
      "epoch": 41.02,
      "grad_norm": 0.006898540537804365,
      "learning_rate": 4.955120707595011e-06,
      "loss": 0.0002,
      "step": 2564
    },
    {
      "epoch": 41.04,
      "grad_norm": 1.5012117624282837,
      "learning_rate": 4.951915139387484e-06,
      "loss": 0.0258,
      "step": 2565
    },
    {
      "epoch": 41.06,
      "grad_norm": 0.5197537541389465,
      "learning_rate": 4.9487095909458275e-06,
      "loss": 0.0033,
      "step": 2566
    },
    {
      "epoch": 41.07,
      "grad_norm": 0.006087742745876312,
      "learning_rate": 4.94550406358772e-06,
      "loss": 0.0002,
      "step": 2567
    },
    {
      "epoch": 41.09,
      "grad_norm": 0.20985598862171173,
      "learning_rate": 4.942298558630834e-06,
      "loss": 0.0016,
      "step": 2568
    },
    {
      "epoch": 41.1,
      "grad_norm": 0.008760238997638226,
      "learning_rate": 4.939093077392831e-06,
      "loss": 0.0003,
      "step": 2569
    },
    {
      "epoch": 41.12,
      "grad_norm": 0.030729450285434723,
      "learning_rate": 4.935887621191364e-06,
      "loss": 0.0002,
      "step": 2570
    },
    {
      "epoch": 41.14,
      "grad_norm": 0.6256901621818542,
      "learning_rate": 4.932682191344073e-06,
      "loss": 0.009,
      "step": 2571
    },
    {
      "epoch": 41.15,
      "grad_norm": 0.5199926495552063,
      "learning_rate": 4.9294767891685904e-06,
      "loss": 0.01,
      "step": 2572
    },
    {
      "epoch": 41.17,
      "grad_norm": 0.004526217468082905,
      "learning_rate": 4.926271415982538e-06,
      "loss": 0.0001,
      "step": 2573
    },
    {
      "epoch": 41.18,
      "grad_norm": 0.21725226938724518,
      "learning_rate": 4.92306607310352e-06,
      "loss": 0.0017,
      "step": 2574
    },
    {
      "epoch": 41.2,
      "grad_norm": 0.004836488049477339,
      "learning_rate": 4.919860761849132e-06,
      "loss": 0.0001,
      "step": 2575
    },
    {
      "epoch": 41.22,
      "grad_norm": 0.43294140696525574,
      "learning_rate": 4.91665548353696e-06,
      "loss": 0.0027,
      "step": 2576
    },
    {
      "epoch": 41.23,
      "grad_norm": 0.023763753473758698,
      "learning_rate": 4.913450239484569e-06,
      "loss": 0.0003,
      "step": 2577
    },
    {
      "epoch": 41.25,
      "grad_norm": 0.2147233784198761,
      "learning_rate": 4.910245031009515e-06,
      "loss": 0.0012,
      "step": 2578
    },
    {
      "epoch": 41.26,
      "grad_norm": 0.006221645046025515,
      "learning_rate": 4.907039859429339e-06,
      "loss": 0.0002,
      "step": 2579
    },
    {
      "epoch": 41.28,
      "grad_norm": 0.026514418423175812,
      "learning_rate": 4.903834726061565e-06,
      "loss": 0.0003,
      "step": 2580
    },
    {
      "epoch": 41.3,
      "grad_norm": 0.004674298223108053,
      "learning_rate": 4.900629632223704e-06,
      "loss": 0.0001,
      "step": 2581
    },
    {
      "epoch": 41.31,
      "grad_norm": 0.4845057725906372,
      "learning_rate": 4.897424579233246e-06,
      "loss": 0.0053,
      "step": 2582
    },
    {
      "epoch": 41.33,
      "grad_norm": 0.5264017581939697,
      "learning_rate": 4.89421956840767e-06,
      "loss": 0.007,
      "step": 2583
    },
    {
      "epoch": 41.34,
      "grad_norm": 0.009076010435819626,
      "learning_rate": 4.891014601064432e-06,
      "loss": 0.0002,
      "step": 2584
    },
    {
      "epoch": 41.36,
      "grad_norm": 0.008048190735280514,
      "learning_rate": 4.887809678520976e-06,
      "loss": 0.0002,
      "step": 2585
    },
    {
      "epoch": 41.38,
      "grad_norm": 0.006873550824820995,
      "learning_rate": 4.884604802094724e-06,
      "loss": 0.0001,
      "step": 2586
    },
    {
      "epoch": 41.39,
      "grad_norm": 0.8879989981651306,
      "learning_rate": 4.881399973103078e-06,
      "loss": 0.0119,
      "step": 2587
    },
    {
      "epoch": 41.41,
      "grad_norm": 0.7162732481956482,
      "learning_rate": 4.8781951928634235e-06,
      "loss": 0.0093,
      "step": 2588
    },
    {
      "epoch": 41.42,
      "grad_norm": 0.009049482643604279,
      "learning_rate": 4.874990462693124e-06,
      "loss": 0.0002,
      "step": 2589
    },
    {
      "epoch": 41.44,
      "grad_norm": 0.9999358057975769,
      "learning_rate": 4.871785783909523e-06,
      "loss": 0.0107,
      "step": 2590
    },
    {
      "epoch": 41.46,
      "grad_norm": 0.13425900042057037,
      "learning_rate": 4.8685811578299445e-06,
      "loss": 0.0027,
      "step": 2591
    },
    {
      "epoch": 41.47,
      "grad_norm": 0.005792548879981041,
      "learning_rate": 4.865376585771687e-06,
      "loss": 0.0001,
      "step": 2592
    },
    {
      "epoch": 41.49,
      "grad_norm": 0.02496729977428913,
      "learning_rate": 4.86217206905203e-06,
      "loss": 0.0005,
      "step": 2593
    },
    {
      "epoch": 41.5,
      "grad_norm": 0.0067621502093970776,
      "learning_rate": 4.858967608988229e-06,
      "loss": 0.0002,
      "step": 2594
    },
    {
      "epoch": 41.52,
      "grad_norm": 0.10189977288246155,
      "learning_rate": 4.855763206897516e-06,
      "loss": 0.0006,
      "step": 2595
    },
    {
      "epoch": 41.54,
      "grad_norm": 0.805399477481842,
      "learning_rate": 4.852558864097101e-06,
      "loss": 0.0127,
      "step": 2596
    },
    {
      "epoch": 41.55,
      "grad_norm": 1.2645947933197021,
      "learning_rate": 4.849354581904169e-06,
      "loss": 0.0251,
      "step": 2597
    },
    {
      "epoch": 41.57,
      "grad_norm": 0.025671441107988358,
      "learning_rate": 4.846150361635876e-06,
      "loss": 0.0005,
      "step": 2598
    },
    {
      "epoch": 41.58,
      "grad_norm": 0.00614361185580492,
      "learning_rate": 4.842946204609359e-06,
      "loss": 0.0002,
      "step": 2599
    },
    {
      "epoch": 41.6,
      "grad_norm": 0.005831696558743715,
      "learning_rate": 4.839742112141725e-06,
      "loss": 0.0001,
      "step": 2600
    },
    {
      "epoch": 41.62,
      "grad_norm": 0.008023976348340511,
      "learning_rate": 4.836538085550054e-06,
      "loss": 0.0002,
      "step": 2601
    },
    {
      "epoch": 41.63,
      "grad_norm": 0.3821341097354889,
      "learning_rate": 4.833334126151403e-06,
      "loss": 0.0043,
      "step": 2602
    },
    {
      "epoch": 41.65,
      "grad_norm": 0.7684336304664612,
      "learning_rate": 4.8301302352627964e-06,
      "loss": 0.0114,
      "step": 2603
    },
    {
      "epoch": 41.66,
      "grad_norm": 0.8251558542251587,
      "learning_rate": 4.826926414201234e-06,
      "loss": 0.013,
      "step": 2604
    },
    {
      "epoch": 41.68,
      "grad_norm": 0.008615250699222088,
      "learning_rate": 4.823722664283684e-06,
      "loss": 0.0002,
      "step": 2605
    },
    {
      "epoch": 41.7,
      "grad_norm": 0.004851972218602896,
      "learning_rate": 4.8205189868270894e-06,
      "loss": 0.0001,
      "step": 2606
    },
    {
      "epoch": 41.71,
      "grad_norm": 0.010249989107251167,
      "learning_rate": 4.8173153831483595e-06,
      "loss": 0.0003,
      "step": 2607
    },
    {
      "epoch": 41.73,
      "grad_norm": 0.4787639081478119,
      "learning_rate": 4.814111854564375e-06,
      "loss": 0.0027,
      "step": 2608
    },
    {
      "epoch": 41.74,
      "grad_norm": 0.005592711269855499,
      "learning_rate": 4.8109084023919846e-06,
      "loss": 0.0001,
      "step": 2609
    },
    {
      "epoch": 41.76,
      "grad_norm": 0.005596420262008905,
      "learning_rate": 4.807705027948008e-06,
      "loss": 0.0001,
      "step": 2610
    },
    {
      "epoch": 41.78,
      "grad_norm": 1.108017921447754,
      "learning_rate": 4.804501732549231e-06,
      "loss": 0.0185,
      "step": 2611
    },
    {
      "epoch": 41.79,
      "grad_norm": 0.14925549924373627,
      "learning_rate": 4.801298517512408e-06,
      "loss": 0.0026,
      "step": 2612
    },
    {
      "epoch": 41.81,
      "grad_norm": 0.8377874493598938,
      "learning_rate": 4.7980953841542575e-06,
      "loss": 0.0099,
      "step": 2613
    },
    {
      "epoch": 41.82,
      "grad_norm": 0.6508551239967346,
      "learning_rate": 4.794892333791468e-06,
      "loss": 0.0091,
      "step": 2614
    },
    {
      "epoch": 41.84,
      "grad_norm": 0.007317453157156706,
      "learning_rate": 4.7916893677406925e-06,
      "loss": 0.0002,
      "step": 2615
    },
    {
      "epoch": 41.86,
      "grad_norm": 0.004998756106942892,
      "learning_rate": 4.7884864873185485e-06,
      "loss": 0.0001,
      "step": 2616
    },
    {
      "epoch": 41.87,
      "grad_norm": 0.8704103827476501,
      "learning_rate": 4.785283693841623e-06,
      "loss": 0.0127,
      "step": 2617
    },
    {
      "epoch": 41.89,
      "grad_norm": 0.005554793868213892,
      "learning_rate": 4.782080988626458e-06,
      "loss": 0.0001,
      "step": 2618
    },
    {
      "epoch": 41.9,
      "grad_norm": 0.006706856191158295,
      "learning_rate": 4.778878372989569e-06,
      "loss": 0.0002,
      "step": 2619
    },
    {
      "epoch": 41.92,
      "grad_norm": 0.005922179203480482,
      "learning_rate": 4.775675848247427e-06,
      "loss": 0.0001,
      "step": 2620
    },
    {
      "epoch": 41.94,
      "grad_norm": 0.005259127356112003,
      "learning_rate": 4.772473415716471e-06,
      "loss": 0.0001,
      "step": 2621
    },
    {
      "epoch": 41.95,
      "grad_norm": 0.4978635311126709,
      "learning_rate": 4.7692710767130975e-06,
      "loss": 0.004,
      "step": 2622
    },
    {
      "epoch": 41.97,
      "grad_norm": 0.013844206929206848,
      "learning_rate": 4.76606883255367e-06,
      "loss": 0.0002,
      "step": 2623
    },
    {
      "epoch": 41.98,
      "grad_norm": 0.6577901244163513,
      "learning_rate": 4.762866684554509e-06,
      "loss": 0.0084,
      "step": 2624
    },
    {
      "epoch": 42.0,
      "grad_norm": 0.00568407541140914,
      "learning_rate": 4.759664634031897e-06,
      "loss": 0.0001,
      "step": 2625
    },
    {
      "epoch": 42.02,
      "grad_norm": 1.0437861680984497,
      "learning_rate": 4.756462682302076e-06,
      "loss": 0.009,
      "step": 2626
    },
    {
      "epoch": 42.03,
      "grad_norm": 0.5840205550193787,
      "learning_rate": 4.753260830681247e-06,
      "loss": 0.0094,
      "step": 2627
    },
    {
      "epoch": 42.05,
      "grad_norm": 0.5173162817955017,
      "learning_rate": 4.7500590804855695e-06,
      "loss": 0.0054,
      "step": 2628
    },
    {
      "epoch": 42.06,
      "grad_norm": 0.0043279207311570644,
      "learning_rate": 4.746857433031163e-06,
      "loss": 0.0001,
      "step": 2629
    },
    {
      "epoch": 42.08,
      "grad_norm": 0.006003967020660639,
      "learning_rate": 4.743655889634105e-06,
      "loss": 0.0001,
      "step": 2630
    },
    {
      "epoch": 42.1,
      "grad_norm": 0.058135904371738434,
      "learning_rate": 4.740454451610427e-06,
      "loss": 0.0003,
      "step": 2631
    },
    {
      "epoch": 42.11,
      "grad_norm": 0.008242622949182987,
      "learning_rate": 4.73725312027612e-06,
      "loss": 0.0002,
      "step": 2632
    },
    {
      "epoch": 42.13,
      "grad_norm": 0.8455325961112976,
      "learning_rate": 4.73405189694713e-06,
      "loss": 0.0106,
      "step": 2633
    },
    {
      "epoch": 42.14,
      "grad_norm": 0.21043865382671356,
      "learning_rate": 4.73085078293936e-06,
      "loss": 0.0012,
      "step": 2634
    },
    {
      "epoch": 42.16,
      "grad_norm": 0.004622792825102806,
      "learning_rate": 4.727649779568666e-06,
      "loss": 0.0001,
      "step": 2635
    },
    {
      "epoch": 42.18,
      "grad_norm": 0.005673663690686226,
      "learning_rate": 4.724448888150861e-06,
      "loss": 0.0001,
      "step": 2636
    },
    {
      "epoch": 42.19,
      "grad_norm": 1.1804653406143188,
      "learning_rate": 4.7212481100017085e-06,
      "loss": 0.0075,
      "step": 2637
    },
    {
      "epoch": 42.21,
      "grad_norm": 0.01076913345605135,
      "learning_rate": 4.718047446436929e-06,
      "loss": 0.0003,
      "step": 2638
    },
    {
      "epoch": 42.22,
      "grad_norm": 0.005486777517944574,
      "learning_rate": 4.714846898772194e-06,
      "loss": 0.0001,
      "step": 2639
    },
    {
      "epoch": 42.24,
      "grad_norm": 0.659490168094635,
      "learning_rate": 4.711646468323129e-06,
      "loss": 0.004,
      "step": 2640
    },
    {
      "epoch": 42.26,
      "grad_norm": 0.006562817841768265,
      "learning_rate": 4.708446156405308e-06,
      "loss": 0.0001,
      "step": 2641
    },
    {
      "epoch": 42.27,
      "grad_norm": 0.01154312863945961,
      "learning_rate": 4.7052459643342576e-06,
      "loss": 0.0003,
      "step": 2642
    },
    {
      "epoch": 42.29,
      "grad_norm": 0.004960433579981327,
      "learning_rate": 4.702045893425459e-06,
      "loss": 0.0001,
      "step": 2643
    },
    {
      "epoch": 42.3,
      "grad_norm": 0.7922657132148743,
      "learning_rate": 4.698845944994338e-06,
      "loss": 0.0099,
      "step": 2644
    },
    {
      "epoch": 42.32,
      "grad_norm": 0.5202757120132446,
      "learning_rate": 4.695646120356275e-06,
      "loss": 0.0067,
      "step": 2645
    },
    {
      "epoch": 42.34,
      "grad_norm": 0.013490704819560051,
      "learning_rate": 4.6924464208265945e-06,
      "loss": 0.0002,
      "step": 2646
    },
    {
      "epoch": 42.35,
      "grad_norm": 0.00552219245582819,
      "learning_rate": 4.689246847720572e-06,
      "loss": 0.0001,
      "step": 2647
    },
    {
      "epoch": 42.37,
      "grad_norm": 0.00806375127285719,
      "learning_rate": 4.686047402353433e-06,
      "loss": 0.0002,
      "step": 2648
    },
    {
      "epoch": 42.38,
      "grad_norm": 0.00719034019857645,
      "learning_rate": 4.682848086040348e-06,
      "loss": 0.0002,
      "step": 2649
    },
    {
      "epoch": 42.4,
      "grad_norm": 0.8013741374015808,
      "learning_rate": 4.679648900096436e-06,
      "loss": 0.0113,
      "step": 2650
    },
    {
      "epoch": 42.42,
      "grad_norm": 1.0925098657608032,
      "learning_rate": 4.676449845836759e-06,
      "loss": 0.0175,
      "step": 2651
    },
    {
      "epoch": 42.43,
      "grad_norm": 0.8173314929008484,
      "learning_rate": 4.67325092457633e-06,
      "loss": 0.0175,
      "step": 2652
    },
    {
      "epoch": 42.45,
      "grad_norm": 0.7387760281562805,
      "learning_rate": 4.670052137630103e-06,
      "loss": 0.0086,
      "step": 2653
    },
    {
      "epoch": 42.46,
      "grad_norm": 0.0054681068286299706,
      "learning_rate": 4.666853486312978e-06,
      "loss": 0.0001,
      "step": 2654
    },
    {
      "epoch": 42.48,
      "grad_norm": 0.06687253713607788,
      "learning_rate": 4.663654971939802e-06,
      "loss": 0.0005,
      "step": 2655
    },
    {
      "epoch": 42.5,
      "grad_norm": 0.004714978393167257,
      "learning_rate": 4.660456595825362e-06,
      "loss": 0.0001,
      "step": 2656
    },
    {
      "epoch": 42.51,
      "grad_norm": 0.6494026780128479,
      "learning_rate": 4.657258359284389e-06,
      "loss": 0.0068,
      "step": 2657
    },
    {
      "epoch": 42.53,
      "grad_norm": 0.06885717064142227,
      "learning_rate": 4.654060263631558e-06,
      "loss": 0.0005,
      "step": 2658
    },
    {
      "epoch": 42.54,
      "grad_norm": 1.2949243783950806,
      "learning_rate": 4.650862310181487e-06,
      "loss": 0.0238,
      "step": 2659
    },
    {
      "epoch": 42.56,
      "grad_norm": 0.005623510107398033,
      "learning_rate": 4.64766450024873e-06,
      "loss": 0.0001,
      "step": 2660
    },
    {
      "epoch": 42.58,
      "grad_norm": 0.006191922817379236,
      "learning_rate": 4.64446683514779e-06,
      "loss": 0.0002,
      "step": 2661
    },
    {
      "epoch": 42.59,
      "grad_norm": 0.34227997064590454,
      "learning_rate": 4.641269316193104e-06,
      "loss": 0.0031,
      "step": 2662
    },
    {
      "epoch": 42.61,
      "grad_norm": 0.007733316160738468,
      "learning_rate": 4.638071944699051e-06,
      "loss": 0.0002,
      "step": 2663
    },
    {
      "epoch": 42.62,
      "grad_norm": 0.04527539759874344,
      "learning_rate": 4.634874721979952e-06,
      "loss": 0.0007,
      "step": 2664
    },
    {
      "epoch": 42.64,
      "grad_norm": 0.7444280982017517,
      "learning_rate": 4.6316776493500615e-06,
      "loss": 0.0117,
      "step": 2665
    },
    {
      "epoch": 42.66,
      "grad_norm": 0.7851223349571228,
      "learning_rate": 4.628480728123576e-06,
      "loss": 0.0119,
      "step": 2666
    },
    {
      "epoch": 42.67,
      "grad_norm": 0.018698621541261673,
      "learning_rate": 4.6252839596146295e-06,
      "loss": 0.0004,
      "step": 2667
    },
    {
      "epoch": 42.69,
      "grad_norm": 0.7574641704559326,
      "learning_rate": 4.622087345137295e-06,
      "loss": 0.0105,
      "step": 2668
    },
    {
      "epoch": 42.7,
      "grad_norm": 0.018686769530177116,
      "learning_rate": 4.618890886005576e-06,
      "loss": 0.0002,
      "step": 2669
    },
    {
      "epoch": 42.72,
      "grad_norm": 0.006189827341586351,
      "learning_rate": 4.615694583533418e-06,
      "loss": 0.0001,
      "step": 2670
    },
    {
      "epoch": 42.74,
      "grad_norm": 0.2614726424217224,
      "learning_rate": 4.612498439034701e-06,
      "loss": 0.0016,
      "step": 2671
    },
    {
      "epoch": 42.75,
      "grad_norm": 0.0063092526979744434,
      "learning_rate": 4.609302453823238e-06,
      "loss": 0.0002,
      "step": 2672
    },
    {
      "epoch": 42.77,
      "grad_norm": 0.7199267148971558,
      "learning_rate": 4.606106629212779e-06,
      "loss": 0.0132,
      "step": 2673
    },
    {
      "epoch": 42.78,
      "grad_norm": 0.7038324475288391,
      "learning_rate": 4.602910966517006e-06,
      "loss": 0.0072,
      "step": 2674
    },
    {
      "epoch": 42.8,
      "grad_norm": 0.6304022669792175,
      "learning_rate": 4.599715467049534e-06,
      "loss": 0.0043,
      "step": 2675
    },
    {
      "epoch": 42.82,
      "grad_norm": 0.00498613528907299,
      "learning_rate": 4.596520132123915e-06,
      "loss": 0.0001,
      "step": 2676
    },
    {
      "epoch": 42.83,
      "grad_norm": 0.0060598053969442844,
      "learning_rate": 4.593324963053628e-06,
      "loss": 0.0001,
      "step": 2677
    },
    {
      "epoch": 42.85,
      "grad_norm": 0.00726291025057435,
      "learning_rate": 4.590129961152087e-06,
      "loss": 0.0001,
      "step": 2678
    },
    {
      "epoch": 42.86,
      "grad_norm": 0.007264797110110521,
      "learning_rate": 4.586935127732638e-06,
      "loss": 0.0002,
      "step": 2679
    },
    {
      "epoch": 42.88,
      "grad_norm": 1.0099424123764038,
      "learning_rate": 4.583740464108554e-06,
      "loss": 0.0128,
      "step": 2680
    },
    {
      "epoch": 42.9,
      "grad_norm": 0.007913761772215366,
      "learning_rate": 4.580545971593042e-06,
      "loss": 0.0002,
      "step": 2681
    },
    {
      "epoch": 42.91,
      "grad_norm": 1.1669855117797852,
      "learning_rate": 4.577351651499237e-06,
      "loss": 0.0126,
      "step": 2682
    },
    {
      "epoch": 42.93,
      "grad_norm": 0.006038091145455837,
      "learning_rate": 4.574157505140204e-06,
      "loss": 0.0002,
      "step": 2683
    },
    {
      "epoch": 42.94,
      "grad_norm": 0.006140104960650206,
      "learning_rate": 4.570963533828934e-06,
      "loss": 0.0001,
      "step": 2684
    },
    {
      "epoch": 42.96,
      "grad_norm": 1.1188249588012695,
      "learning_rate": 4.56776973887835e-06,
      "loss": 0.0112,
      "step": 2685
    },
    {
      "epoch": 42.98,
      "grad_norm": 0.005284041631966829,
      "learning_rate": 4.564576121601298e-06,
      "loss": 0.0001,
      "step": 2686
    },
    {
      "epoch": 42.99,
      "grad_norm": 1.040964126586914,
      "learning_rate": 4.561382683310557e-06,
      "loss": 0.0092,
      "step": 2687
    },
    {
      "epoch": 43.01,
      "grad_norm": 1.270931363105774,
      "learning_rate": 4.558189425318826e-06,
      "loss": 0.0189,
      "step": 2688
    },
    {
      "epoch": 43.02,
      "grad_norm": 0.005823131185024977,
      "learning_rate": 4.554996348938734e-06,
      "loss": 0.0001,
      "step": 2689
    },
    {
      "epoch": 43.04,
      "grad_norm": 0.01140638068318367,
      "learning_rate": 4.551803455482833e-06,
      "loss": 0.0002,
      "step": 2690
    },
    {
      "epoch": 43.06,
      "grad_norm": 0.006506317760795355,
      "learning_rate": 4.548610746263602e-06,
      "loss": 0.0002,
      "step": 2691
    },
    {
      "epoch": 43.07,
      "grad_norm": 0.00578578794375062,
      "learning_rate": 4.545418222593442e-06,
      "loss": 0.0001,
      "step": 2692
    },
    {
      "epoch": 43.09,
      "grad_norm": 1.138696312904358,
      "learning_rate": 4.54222588578468e-06,
      "loss": 0.0111,
      "step": 2693
    },
    {
      "epoch": 43.1,
      "grad_norm": 0.7982004284858704,
      "learning_rate": 4.5390337371495665e-06,
      "loss": 0.011,
      "step": 2694
    },
    {
      "epoch": 43.12,
      "grad_norm": 0.292547345161438,
      "learning_rate": 4.53584177800027e-06,
      "loss": 0.0025,
      "step": 2695
    },
    {
      "epoch": 43.14,
      "grad_norm": 0.9894854426383972,
      "learning_rate": 4.532650009648885e-06,
      "loss": 0.0115,
      "step": 2696
    },
    {
      "epoch": 43.15,
      "grad_norm": 0.9392075538635254,
      "learning_rate": 4.529458433407429e-06,
      "loss": 0.0091,
      "step": 2697
    },
    {
      "epoch": 43.17,
      "grad_norm": 0.7015284299850464,
      "learning_rate": 4.526267050587836e-06,
      "loss": 0.0069,
      "step": 2698
    },
    {
      "epoch": 43.18,
      "grad_norm": 0.007342983037233353,
      "learning_rate": 4.523075862501965e-06,
      "loss": 0.0002,
      "step": 2699
    },
    {
      "epoch": 43.2,
      "grad_norm": 0.013152061961591244,
      "learning_rate": 4.5198848704615915e-06,
      "loss": 0.0002,
      "step": 2700
    },
    {
      "epoch": 43.22,
      "grad_norm": 0.007628802675753832,
      "learning_rate": 4.516694075778412e-06,
      "loss": 0.0002,
      "step": 2701
    },
    {
      "epoch": 43.23,
      "grad_norm": 1.2804672718048096,
      "learning_rate": 4.513503479764042e-06,
      "loss": 0.0266,
      "step": 2702
    },
    {
      "epoch": 43.25,
      "grad_norm": 0.17659702897071838,
      "learning_rate": 4.5103130837300145e-06,
      "loss": 0.001,
      "step": 2703
    },
    {
      "epoch": 43.26,
      "grad_norm": 0.004323251079767942,
      "learning_rate": 4.507122888987782e-06,
      "loss": 0.0001,
      "step": 2704
    },
    {
      "epoch": 43.28,
      "grad_norm": 0.2886374592781067,
      "learning_rate": 4.503932896848713e-06,
      "loss": 0.0017,
      "step": 2705
    },
    {
      "epoch": 43.3,
      "grad_norm": 0.5094526410102844,
      "learning_rate": 4.5007431086240905e-06,
      "loss": 0.0041,
      "step": 2706
    },
    {
      "epoch": 43.31,
      "grad_norm": 0.035308729857206345,
      "learning_rate": 4.497553525625118e-06,
      "loss": 0.0006,
      "step": 2707
    },
    {
      "epoch": 43.33,
      "grad_norm": 0.6621132493019104,
      "learning_rate": 4.4943641491629115e-06,
      "loss": 0.0038,
      "step": 2708
    },
    {
      "epoch": 43.34,
      "grad_norm": 0.9433101415634155,
      "learning_rate": 4.491174980548506e-06,
      "loss": 0.0165,
      "step": 2709
    },
    {
      "epoch": 43.36,
      "grad_norm": 0.48977354168891907,
      "learning_rate": 4.487986021092844e-06,
      "loss": 0.004,
      "step": 2710
    },
    {
      "epoch": 43.38,
      "grad_norm": 0.8999226689338684,
      "learning_rate": 4.48479727210679e-06,
      "loss": 0.0093,
      "step": 2711
    },
    {
      "epoch": 43.39,
      "grad_norm": 0.029898477718234062,
      "learning_rate": 4.481608734901114e-06,
      "loss": 0.0002,
      "step": 2712
    },
    {
      "epoch": 43.41,
      "grad_norm": 0.6298733949661255,
      "learning_rate": 4.478420410786507e-06,
      "loss": 0.0113,
      "step": 2713
    },
    {
      "epoch": 43.42,
      "grad_norm": 0.00417992053553462,
      "learning_rate": 4.475232301073567e-06,
      "loss": 0.0001,
      "step": 2714
    },
    {
      "epoch": 43.44,
      "grad_norm": 0.3897210359573364,
      "learning_rate": 4.472044407072805e-06,
      "loss": 0.0056,
      "step": 2715
    },
    {
      "epoch": 43.46,
      "grad_norm": 0.006349478382617235,
      "learning_rate": 4.468856730094646e-06,
      "loss": 0.0002,
      "step": 2716
    },
    {
      "epoch": 43.47,
      "grad_norm": 0.007523945067077875,
      "learning_rate": 4.4656692714494224e-06,
      "loss": 0.0002,
      "step": 2717
    },
    {
      "epoch": 43.49,
      "grad_norm": 0.6870447993278503,
      "learning_rate": 4.462482032447377e-06,
      "loss": 0.0063,
      "step": 2718
    },
    {
      "epoch": 43.5,
      "grad_norm": 0.731329619884491,
      "learning_rate": 4.459295014398664e-06,
      "loss": 0.0094,
      "step": 2719
    },
    {
      "epoch": 43.52,
      "grad_norm": 0.0042901672422885895,
      "learning_rate": 4.456108218613346e-06,
      "loss": 0.0001,
      "step": 2720
    },
    {
      "epoch": 43.54,
      "grad_norm": 0.0058331117033958435,
      "learning_rate": 4.4529216464013955e-06,
      "loss": 0.0001,
      "step": 2721
    },
    {
      "epoch": 43.55,
      "grad_norm": 0.0055302781984210014,
      "learning_rate": 4.4497352990726926e-06,
      "loss": 0.0001,
      "step": 2722
    },
    {
      "epoch": 43.57,
      "grad_norm": 0.7284227609634399,
      "learning_rate": 4.446549177937021e-06,
      "loss": 0.0073,
      "step": 2723
    },
    {
      "epoch": 43.58,
      "grad_norm": 0.01067306287586689,
      "learning_rate": 4.443363284304077e-06,
      "loss": 0.0002,
      "step": 2724
    },
    {
      "epoch": 43.6,
      "grad_norm": 0.006473981309682131,
      "learning_rate": 4.4401776194834615e-06,
      "loss": 0.0002,
      "step": 2725
    },
    {
      "epoch": 43.62,
      "grad_norm": 0.004616161808371544,
      "learning_rate": 4.43699218478468e-06,
      "loss": 0.0001,
      "step": 2726
    },
    {
      "epoch": 43.63,
      "grad_norm": 0.0060694459825754166,
      "learning_rate": 4.4338069815171446e-06,
      "loss": 0.0001,
      "step": 2727
    },
    {
      "epoch": 43.65,
      "grad_norm": 0.04653023183345795,
      "learning_rate": 4.430622010990172e-06,
      "loss": 0.0003,
      "step": 2728
    },
    {
      "epoch": 43.66,
      "grad_norm": 0.029883215203881264,
      "learning_rate": 4.427437274512983e-06,
      "loss": 0.0003,
      "step": 2729
    },
    {
      "epoch": 43.68,
      "grad_norm": 0.976472020149231,
      "learning_rate": 4.424252773394704e-06,
      "loss": 0.0144,
      "step": 2730
    },
    {
      "epoch": 43.7,
      "grad_norm": 0.0071978941559791565,
      "learning_rate": 4.421068508944361e-06,
      "loss": 0.0002,
      "step": 2731
    },
    {
      "epoch": 43.71,
      "grad_norm": 0.007385434582829475,
      "learning_rate": 4.417884482470887e-06,
      "loss": 0.0002,
      "step": 2732
    },
    {
      "epoch": 43.73,
      "grad_norm": 0.007505093701183796,
      "learning_rate": 4.414700695283113e-06,
      "loss": 0.0002,
      "step": 2733
    },
    {
      "epoch": 43.74,
      "grad_norm": 0.5061332583427429,
      "learning_rate": 4.411517148689774e-06,
      "loss": 0.0062,
      "step": 2734
    },
    {
      "epoch": 43.76,
      "grad_norm": 0.005697204265743494,
      "learning_rate": 4.408333843999506e-06,
      "loss": 0.0001,
      "step": 2735
    },
    {
      "epoch": 43.78,
      "grad_norm": 0.7059211134910583,
      "learning_rate": 4.405150782520846e-06,
      "loss": 0.0111,
      "step": 2736
    },
    {
      "epoch": 43.79,
      "grad_norm": 0.005586786195635796,
      "learning_rate": 4.401967965562232e-06,
      "loss": 0.0001,
      "step": 2737
    },
    {
      "epoch": 43.81,
      "grad_norm": 0.9922623634338379,
      "learning_rate": 4.398785394431995e-06,
      "loss": 0.0097,
      "step": 2738
    },
    {
      "epoch": 43.82,
      "grad_norm": 0.004498285707086325,
      "learning_rate": 4.395603070438373e-06,
      "loss": 0.0001,
      "step": 2739
    },
    {
      "epoch": 43.84,
      "grad_norm": 0.8054384589195251,
      "learning_rate": 4.392420994889498e-06,
      "loss": 0.01,
      "step": 2740
    },
    {
      "epoch": 43.86,
      "grad_norm": 0.6841188669204712,
      "learning_rate": 4.3892391690934035e-06,
      "loss": 0.0055,
      "step": 2741
    },
    {
      "epoch": 43.87,
      "grad_norm": 0.007425469346344471,
      "learning_rate": 4.386057594358017e-06,
      "loss": 0.0002,
      "step": 2742
    },
    {
      "epoch": 43.89,
      "grad_norm": 0.005521147977560759,
      "learning_rate": 4.38287627199116e-06,
      "loss": 0.0001,
      "step": 2743
    },
    {
      "epoch": 43.9,
      "grad_norm": 0.006984639912843704,
      "learning_rate": 4.379695203300558e-06,
      "loss": 0.0002,
      "step": 2744
    },
    {
      "epoch": 43.92,
      "grad_norm": 0.2279575914144516,
      "learning_rate": 4.376514389593827e-06,
      "loss": 0.0012,
      "step": 2745
    },
    {
      "epoch": 43.94,
      "grad_norm": 1.4905015230178833,
      "learning_rate": 4.373333832178478e-06,
      "loss": 0.0258,
      "step": 2746
    },
    {
      "epoch": 43.95,
      "grad_norm": 0.009451069869101048,
      "learning_rate": 4.370153532361921e-06,
      "loss": 0.0002,
      "step": 2747
    },
    {
      "epoch": 43.97,
      "grad_norm": 0.5239890217781067,
      "learning_rate": 4.366973491451456e-06,
      "loss": 0.0044,
      "step": 2748
    },
    {
      "epoch": 43.98,
      "grad_norm": 0.6470205783843994,
      "learning_rate": 4.363793710754276e-06,
      "loss": 0.0097,
      "step": 2749
    },
    {
      "epoch": 44.0,
      "grad_norm": 0.12420312315225601,
      "learning_rate": 4.3606141915774695e-06,
      "loss": 0.001,
      "step": 2750
    },
    {
      "epoch": 44.02,
      "grad_norm": 0.8570719361305237,
      "learning_rate": 4.357434935228017e-06,
      "loss": 0.0123,
      "step": 2751
    },
    {
      "epoch": 44.03,
      "grad_norm": 0.006232906132936478,
      "learning_rate": 4.354255943012793e-06,
      "loss": 0.0002,
      "step": 2752
    },
    {
      "epoch": 44.05,
      "grad_norm": 0.06768883764743805,
      "learning_rate": 4.351077216238558e-06,
      "loss": 0.0005,
      "step": 2753
    },
    {
      "epoch": 44.06,
      "grad_norm": 0.38531866669654846,
      "learning_rate": 4.347898756211966e-06,
      "loss": 0.0026,
      "step": 2754
    },
    {
      "epoch": 44.08,
      "grad_norm": 0.004188536666333675,
      "learning_rate": 4.344720564239567e-06,
      "loss": 0.0001,
      "step": 2755
    },
    {
      "epoch": 44.1,
      "grad_norm": 0.5377297401428223,
      "learning_rate": 4.3415426416277885e-06,
      "loss": 0.0032,
      "step": 2756
    },
    {
      "epoch": 44.11,
      "grad_norm": 0.4357602894306183,
      "learning_rate": 4.338364989682959e-06,
      "loss": 0.0028,
      "step": 2757
    },
    {
      "epoch": 44.13,
      "grad_norm": 0.6288336515426636,
      "learning_rate": 4.335187609711289e-06,
      "loss": 0.0087,
      "step": 2758
    },
    {
      "epoch": 44.14,
      "grad_norm": 0.0036250893026590347,
      "learning_rate": 4.332010503018881e-06,
      "loss": 0.0001,
      "step": 2759
    },
    {
      "epoch": 44.16,
      "grad_norm": 0.0380672886967659,
      "learning_rate": 4.3288336709117246e-06,
      "loss": 0.0003,
      "step": 2760
    },
    {
      "epoch": 44.18,
      "grad_norm": 0.07402621954679489,
      "learning_rate": 4.325657114695692e-06,
      "loss": 0.0005,
      "step": 2761
    },
    {
      "epoch": 44.19,
      "grad_norm": 0.7898853421211243,
      "learning_rate": 4.322480835676548e-06,
      "loss": 0.0118,
      "step": 2762
    },
    {
      "epoch": 44.21,
      "grad_norm": 0.974367082118988,
      "learning_rate": 4.3193048351599395e-06,
      "loss": 0.0076,
      "step": 2763
    },
    {
      "epoch": 44.22,
      "grad_norm": 0.005133687984198332,
      "learning_rate": 4.316129114451402e-06,
      "loss": 0.0001,
      "step": 2764
    },
    {
      "epoch": 44.24,
      "grad_norm": 0.03253236040472984,
      "learning_rate": 4.312953674856355e-06,
      "loss": 0.0002,
      "step": 2765
    },
    {
      "epoch": 44.26,
      "grad_norm": 0.0061606960371136665,
      "learning_rate": 4.3097785176800985e-06,
      "loss": 0.0002,
      "step": 2766
    },
    {
      "epoch": 44.27,
      "grad_norm": 1.160157322883606,
      "learning_rate": 4.306603644227821e-06,
      "loss": 0.0143,
      "step": 2767
    },
    {
      "epoch": 44.29,
      "grad_norm": 0.01681998372077942,
      "learning_rate": 4.303429055804594e-06,
      "loss": 0.0003,
      "step": 2768
    },
    {
      "epoch": 44.3,
      "grad_norm": 0.0056153894402086735,
      "learning_rate": 4.300254753715371e-06,
      "loss": 0.0001,
      "step": 2769
    },
    {
      "epoch": 44.32,
      "grad_norm": 0.005699565634131432,
      "learning_rate": 4.297080739264987e-06,
      "loss": 0.0001,
      "step": 2770
    },
    {
      "epoch": 44.34,
      "grad_norm": 0.7522907853126526,
      "learning_rate": 4.2939070137581584e-06,
      "loss": 0.007,
      "step": 2771
    },
    {
      "epoch": 44.35,
      "grad_norm": 1.2300986051559448,
      "learning_rate": 4.290733578499486e-06,
      "loss": 0.0256,
      "step": 2772
    },
    {
      "epoch": 44.37,
      "grad_norm": 0.9340582489967346,
      "learning_rate": 4.287560434793448e-06,
      "loss": 0.0124,
      "step": 2773
    },
    {
      "epoch": 44.38,
      "grad_norm": 1.0155714750289917,
      "learning_rate": 4.284387583944403e-06,
      "loss": 0.0117,
      "step": 2774
    },
    {
      "epoch": 44.4,
      "grad_norm": 0.6949024200439453,
      "learning_rate": 4.281215027256592e-06,
      "loss": 0.0059,
      "step": 2775
    },
    {
      "epoch": 44.42,
      "grad_norm": 0.004715528804808855,
      "learning_rate": 4.278042766034134e-06,
      "loss": 0.0001,
      "step": 2776
    },
    {
      "epoch": 44.43,
      "grad_norm": 0.005487601738423109,
      "learning_rate": 4.274870801581021e-06,
      "loss": 0.0002,
      "step": 2777
    },
    {
      "epoch": 44.45,
      "grad_norm": 0.005971095524728298,
      "learning_rate": 4.271699135201133e-06,
      "loss": 0.0002,
      "step": 2778
    },
    {
      "epoch": 44.46,
      "grad_norm": 0.006784998811781406,
      "learning_rate": 4.26852776819822e-06,
      "loss": 0.0001,
      "step": 2779
    },
    {
      "epoch": 44.48,
      "grad_norm": 1.380172848701477,
      "learning_rate": 4.265356701875911e-06,
      "loss": 0.0247,
      "step": 2780
    },
    {
      "epoch": 44.5,
      "grad_norm": 0.005995127372443676,
      "learning_rate": 4.262185937537713e-06,
      "loss": 0.0001,
      "step": 2781
    },
    {
      "epoch": 44.51,
      "grad_norm": 0.7685173153877258,
      "learning_rate": 4.259015476487005e-06,
      "loss": 0.006,
      "step": 2782
    },
    {
      "epoch": 44.53,
      "grad_norm": 0.5664007663726807,
      "learning_rate": 4.255845320027046e-06,
      "loss": 0.004,
      "step": 2783
    },
    {
      "epoch": 44.54,
      "grad_norm": 0.7511151432991028,
      "learning_rate": 4.252675469460965e-06,
      "loss": 0.0146,
      "step": 2784
    },
    {
      "epoch": 44.56,
      "grad_norm": 0.04353911057114601,
      "learning_rate": 4.249505926091771e-06,
      "loss": 0.0004,
      "step": 2785
    },
    {
      "epoch": 44.58,
      "grad_norm": 0.006318871863186359,
      "learning_rate": 4.246336691222343e-06,
      "loss": 0.0002,
      "step": 2786
    },
    {
      "epoch": 44.59,
      "grad_norm": 0.05071995407342911,
      "learning_rate": 4.243167766155433e-06,
      "loss": 0.0005,
      "step": 2787
    },
    {
      "epoch": 44.61,
      "grad_norm": 1.252353549003601,
      "learning_rate": 4.239999152193664e-06,
      "loss": 0.0093,
      "step": 2788
    },
    {
      "epoch": 44.62,
      "grad_norm": 0.006598916370421648,
      "learning_rate": 4.2368308506395375e-06,
      "loss": 0.0002,
      "step": 2789
    },
    {
      "epoch": 44.64,
      "grad_norm": 0.6270228624343872,
      "learning_rate": 4.23366286279542e-06,
      "loss": 0.0072,
      "step": 2790
    },
    {
      "epoch": 44.66,
      "grad_norm": 0.8449736833572388,
      "learning_rate": 4.230495189963554e-06,
      "loss": 0.0126,
      "step": 2791
    },
    {
      "epoch": 44.67,
      "grad_norm": 0.006227667909115553,
      "learning_rate": 4.227327833446047e-06,
      "loss": 0.0001,
      "step": 2792
    },
    {
      "epoch": 44.69,
      "grad_norm": 0.004781284369528294,
      "learning_rate": 4.224160794544883e-06,
      "loss": 0.0001,
      "step": 2793
    },
    {
      "epoch": 44.7,
      "grad_norm": 0.006178704556077719,
      "learning_rate": 4.220994074561909e-06,
      "loss": 0.0002,
      "step": 2794
    },
    {
      "epoch": 44.72,
      "grad_norm": 0.005943710915744305,
      "learning_rate": 4.217827674798845e-06,
      "loss": 0.0002,
      "step": 2795
    },
    {
      "epoch": 44.74,
      "grad_norm": 0.4287553131580353,
      "learning_rate": 4.2146615965572804e-06,
      "loss": 0.0024,
      "step": 2796
    },
    {
      "epoch": 44.75,
      "grad_norm": 0.9128930568695068,
      "learning_rate": 4.211495841138668e-06,
      "loss": 0.0079,
      "step": 2797
    },
    {
      "epoch": 44.77,
      "grad_norm": 0.006758238188922405,
      "learning_rate": 4.20833040984433e-06,
      "loss": 0.0002,
      "step": 2798
    },
    {
      "epoch": 44.78,
      "grad_norm": 0.836144745349884,
      "learning_rate": 4.205165303975457e-06,
      "loss": 0.0094,
      "step": 2799
    },
    {
      "epoch": 44.8,
      "grad_norm": 0.007025350816547871,
      "learning_rate": 4.2020005248331056e-06,
      "loss": 0.0002,
      "step": 2800
    },
    {
      "epoch": 44.82,
      "grad_norm": 0.0047486089169979095,
      "learning_rate": 4.1988360737181935e-06,
      "loss": 0.0001,
      "step": 2801
    },
    {
      "epoch": 44.83,
      "grad_norm": 0.6334127187728882,
      "learning_rate": 4.195671951931509e-06,
      "loss": 0.0072,
      "step": 2802
    },
    {
      "epoch": 44.85,
      "grad_norm": 0.008016662672162056,
      "learning_rate": 4.192508160773703e-06,
      "loss": 0.0002,
      "step": 2803
    },
    {
      "epoch": 44.86,
      "grad_norm": 0.0057703545317053795,
      "learning_rate": 4.189344701545291e-06,
      "loss": 0.0002,
      "step": 2804
    },
    {
      "epoch": 44.88,
      "grad_norm": 0.5125591158866882,
      "learning_rate": 4.186181575546651e-06,
      "loss": 0.0062,
      "step": 2805
    },
    {
      "epoch": 44.9,
      "grad_norm": 0.0053742327727377415,
      "learning_rate": 4.1830187840780236e-06,
      "loss": 0.0001,
      "step": 2806
    },
    {
      "epoch": 44.91,
      "grad_norm": 0.25468185544013977,
      "learning_rate": 4.179856328439515e-06,
      "loss": 0.0018,
      "step": 2807
    },
    {
      "epoch": 44.93,
      "grad_norm": 1.0706576108932495,
      "learning_rate": 4.176694209931089e-06,
      "loss": 0.0156,
      "step": 2808
    },
    {
      "epoch": 44.94,
      "grad_norm": 0.004595936741679907,
      "learning_rate": 4.1735324298525765e-06,
      "loss": 0.0001,
      "step": 2809
    },
    {
      "epoch": 44.96,
      "grad_norm": 0.004687449894845486,
      "learning_rate": 4.170370989503662e-06,
      "loss": 0.0001,
      "step": 2810
    },
    {
      "epoch": 44.98,
      "grad_norm": 0.005018691532313824,
      "learning_rate": 4.1672098901838965e-06,
      "loss": 0.0001,
      "step": 2811
    },
    {
      "epoch": 44.99,
      "grad_norm": 0.5151967406272888,
      "learning_rate": 4.164049133192688e-06,
      "loss": 0.0041,
      "step": 2812
    },
    {
      "epoch": 45.01,
      "grad_norm": 0.010061760433018208,
      "learning_rate": 4.160888719829303e-06,
      "loss": 0.0003,
      "step": 2813
    },
    {
      "epoch": 45.02,
      "grad_norm": 1.1064311265945435,
      "learning_rate": 4.1577286513928715e-06,
      "loss": 0.0092,
      "step": 2814
    },
    {
      "epoch": 45.04,
      "grad_norm": 0.591602087020874,
      "learning_rate": 4.154568929182374e-06,
      "loss": 0.0073,
      "step": 2815
    },
    {
      "epoch": 45.06,
      "grad_norm": 0.00481907045468688,
      "learning_rate": 4.1514095544966556e-06,
      "loss": 0.0001,
      "step": 2816
    },
    {
      "epoch": 45.07,
      "grad_norm": 0.2982198894023895,
      "learning_rate": 4.148250528634416e-06,
      "loss": 0.0019,
      "step": 2817
    },
    {
      "epoch": 45.09,
      "grad_norm": 0.005285999737679958,
      "learning_rate": 4.145091852894209e-06,
      "loss": 0.0001,
      "step": 2818
    },
    {
      "epoch": 45.1,
      "grad_norm": 1.1280056238174438,
      "learning_rate": 4.141933528574451e-06,
      "loss": 0.0174,
      "step": 2819
    },
    {
      "epoch": 45.12,
      "grad_norm": 0.015260635875165462,
      "learning_rate": 4.138775556973406e-06,
      "loss": 0.0002,
      "step": 2820
    },
    {
      "epoch": 45.14,
      "grad_norm": 0.6528685688972473,
      "learning_rate": 4.135617939389198e-06,
      "loss": 0.004,
      "step": 2821
    },
    {
      "epoch": 45.15,
      "grad_norm": 0.7582687139511108,
      "learning_rate": 4.132460677119805e-06,
      "loss": 0.0103,
      "step": 2822
    },
    {
      "epoch": 45.17,
      "grad_norm": 0.7301880121231079,
      "learning_rate": 4.1293037714630575e-06,
      "loss": 0.0046,
      "step": 2823
    },
    {
      "epoch": 45.18,
      "grad_norm": 0.006965654902160168,
      "learning_rate": 4.126147223716642e-06,
      "loss": 0.0002,
      "step": 2824
    },
    {
      "epoch": 45.2,
      "grad_norm": 0.7206504940986633,
      "learning_rate": 4.122991035178093e-06,
      "loss": 0.0055,
      "step": 2825
    },
    {
      "epoch": 45.22,
      "grad_norm": 0.07937198132276535,
      "learning_rate": 4.119835207144801e-06,
      "loss": 0.0006,
      "step": 2826
    },
    {
      "epoch": 45.23,
      "grad_norm": 0.005493187345564365,
      "learning_rate": 4.11667974091401e-06,
      "loss": 0.0001,
      "step": 2827
    },
    {
      "epoch": 45.25,
      "grad_norm": 0.005043788347393274,
      "learning_rate": 4.113524637782812e-06,
      "loss": 0.0001,
      "step": 2828
    },
    {
      "epoch": 45.26,
      "grad_norm": 0.5462927222251892,
      "learning_rate": 4.11036989904815e-06,
      "loss": 0.0037,
      "step": 2829
    },
    {
      "epoch": 45.28,
      "grad_norm": 0.004591457080096006,
      "learning_rate": 4.107215526006818e-06,
      "loss": 0.0001,
      "step": 2830
    },
    {
      "epoch": 45.3,
      "grad_norm": 0.006206532008945942,
      "learning_rate": 4.104061519955459e-06,
      "loss": 0.0002,
      "step": 2831
    },
    {
      "epoch": 45.31,
      "grad_norm": 0.5619220733642578,
      "learning_rate": 4.100907882190567e-06,
      "loss": 0.0063,
      "step": 2832
    },
    {
      "epoch": 45.33,
      "grad_norm": 0.029265567660331726,
      "learning_rate": 4.097754614008484e-06,
      "loss": 0.0003,
      "step": 2833
    },
    {
      "epoch": 45.34,
      "grad_norm": 0.004796503111720085,
      "learning_rate": 4.094601716705398e-06,
      "loss": 0.0001,
      "step": 2834
    },
    {
      "epoch": 45.36,
      "grad_norm": 0.8125227093696594,
      "learning_rate": 4.091449191577346e-06,
      "loss": 0.0132,
      "step": 2835
    },
    {
      "epoch": 45.38,
      "grad_norm": 1.0913430452346802,
      "learning_rate": 4.088297039920212e-06,
      "loss": 0.0188,
      "step": 2836
    },
    {
      "epoch": 45.39,
      "grad_norm": 0.005369358230382204,
      "learning_rate": 4.085145263029727e-06,
      "loss": 0.0001,
      "step": 2837
    },
    {
      "epoch": 45.41,
      "grad_norm": 1.1064218282699585,
      "learning_rate": 4.081993862201467e-06,
      "loss": 0.0153,
      "step": 2838
    },
    {
      "epoch": 45.42,
      "grad_norm": 0.004658095072954893,
      "learning_rate": 4.078842838730854e-06,
      "loss": 0.0001,
      "step": 2839
    },
    {
      "epoch": 45.44,
      "grad_norm": 0.0054214405827224255,
      "learning_rate": 4.075692193913156e-06,
      "loss": 0.0001,
      "step": 2840
    },
    {
      "epoch": 45.46,
      "grad_norm": 0.6879462003707886,
      "learning_rate": 4.072541929043483e-06,
      "loss": 0.0039,
      "step": 2841
    },
    {
      "epoch": 45.47,
      "grad_norm": 0.006379224825650454,
      "learning_rate": 4.069392045416789e-06,
      "loss": 0.0002,
      "step": 2842
    },
    {
      "epoch": 45.49,
      "grad_norm": 0.4903659522533417,
      "learning_rate": 4.066242544327873e-06,
      "loss": 0.0031,
      "step": 2843
    },
    {
      "epoch": 45.5,
      "grad_norm": 0.03833512216806412,
      "learning_rate": 4.063093427071376e-06,
      "loss": 0.0002,
      "step": 2844
    },
    {
      "epoch": 45.52,
      "grad_norm": 0.004473007749766111,
      "learning_rate": 4.059944694941783e-06,
      "loss": 0.0001,
      "step": 2845
    },
    {
      "epoch": 45.54,
      "grad_norm": 0.007746732793748379,
      "learning_rate": 4.056796349233415e-06,
      "loss": 0.0002,
      "step": 2846
    },
    {
      "epoch": 45.55,
      "grad_norm": 0.028565149754285812,
      "learning_rate": 4.053648391240441e-06,
      "loss": 0.0004,
      "step": 2847
    },
    {
      "epoch": 45.57,
      "grad_norm": 0.006385808810591698,
      "learning_rate": 4.0505008222568655e-06,
      "loss": 0.0002,
      "step": 2848
    },
    {
      "epoch": 45.58,
      "grad_norm": 0.4511135518550873,
      "learning_rate": 4.0473536435765364e-06,
      "loss": 0.0033,
      "step": 2849
    },
    {
      "epoch": 45.6,
      "grad_norm": 0.31129586696624756,
      "learning_rate": 4.04420685649314e-06,
      "loss": 0.0025,
      "step": 2850
    },
    {
      "epoch": 45.62,
      "grad_norm": 0.00578926270827651,
      "learning_rate": 4.0410604623002004e-06,
      "loss": 0.0002,
      "step": 2851
    },
    {
      "epoch": 45.63,
      "grad_norm": 0.6212890148162842,
      "learning_rate": 4.037914462291085e-06,
      "loss": 0.0053,
      "step": 2852
    },
    {
      "epoch": 45.65,
      "grad_norm": 0.8548423051834106,
      "learning_rate": 4.0347688577589905e-06,
      "loss": 0.0134,
      "step": 2853
    },
    {
      "epoch": 45.66,
      "grad_norm": 0.37584149837493896,
      "learning_rate": 4.031623649996959e-06,
      "loss": 0.0054,
      "step": 2854
    },
    {
      "epoch": 45.68,
      "grad_norm": 0.919564425945282,
      "learning_rate": 4.028478840297867e-06,
      "loss": 0.0147,
      "step": 2855
    },
    {
      "epoch": 45.7,
      "grad_norm": 0.07955232262611389,
      "learning_rate": 4.025334429954425e-06,
      "loss": 0.0005,
      "step": 2856
    },
    {
      "epoch": 45.71,
      "grad_norm": 0.00522760022431612,
      "learning_rate": 4.022190420259184e-06,
      "loss": 0.0001,
      "step": 2857
    },
    {
      "epoch": 45.73,
      "grad_norm": 1.3136166334152222,
      "learning_rate": 4.019046812504526e-06,
      "loss": 0.0116,
      "step": 2858
    },
    {
      "epoch": 45.74,
      "grad_norm": 0.4486149251461029,
      "learning_rate": 4.015903607982668e-06,
      "loss": 0.007,
      "step": 2859
    },
    {
      "epoch": 45.76,
      "grad_norm": 0.39272552728652954,
      "learning_rate": 4.012760807985665e-06,
      "loss": 0.0025,
      "step": 2860
    },
    {
      "epoch": 45.78,
      "grad_norm": 1.355817437171936,
      "learning_rate": 4.0096184138054024e-06,
      "loss": 0.0193,
      "step": 2861
    },
    {
      "epoch": 45.79,
      "grad_norm": 0.474525511264801,
      "learning_rate": 4.006476426733601e-06,
      "loss": 0.0035,
      "step": 2862
    },
    {
      "epoch": 45.81,
      "grad_norm": 0.1482921689748764,
      "learning_rate": 4.003334848061811e-06,
      "loss": 0.0009,
      "step": 2863
    },
    {
      "epoch": 45.82,
      "grad_norm": 0.007615880109369755,
      "learning_rate": 4.0001936790814175e-06,
      "loss": 0.0002,
      "step": 2864
    },
    {
      "epoch": 45.84,
      "grad_norm": 1.0424317121505737,
      "learning_rate": 3.997052921083637e-06,
      "loss": 0.0128,
      "step": 2865
    },
    {
      "epoch": 45.86,
      "grad_norm": 1.5459561347961426,
      "learning_rate": 3.993912575359515e-06,
      "loss": 0.022,
      "step": 2866
    },
    {
      "epoch": 45.87,
      "grad_norm": 0.3437289893627167,
      "learning_rate": 3.990772643199931e-06,
      "loss": 0.0026,
      "step": 2867
    },
    {
      "epoch": 45.89,
      "grad_norm": 0.7968734502792358,
      "learning_rate": 3.987633125895593e-06,
      "loss": 0.0077,
      "step": 2868
    },
    {
      "epoch": 45.9,
      "grad_norm": 0.006024484522640705,
      "learning_rate": 3.984494024737034e-06,
      "loss": 0.0002,
      "step": 2869
    },
    {
      "epoch": 45.92,
      "grad_norm": 0.006100389640778303,
      "learning_rate": 3.9813553410146225e-06,
      "loss": 0.0002,
      "step": 2870
    },
    {
      "epoch": 45.94,
      "grad_norm": 0.006468042731285095,
      "learning_rate": 3.978217076018553e-06,
      "loss": 0.0002,
      "step": 2871
    },
    {
      "epoch": 45.95,
      "grad_norm": 0.005388636142015457,
      "learning_rate": 3.975079231038848e-06,
      "loss": 0.0001,
      "step": 2872
    },
    {
      "epoch": 45.97,
      "grad_norm": 0.684383749961853,
      "learning_rate": 3.971941807365357e-06,
      "loss": 0.0089,
      "step": 2873
    },
    {
      "epoch": 45.98,
      "grad_norm": 0.00418419623747468,
      "learning_rate": 3.968804806287756e-06,
      "loss": 0.0001,
      "step": 2874
    },
    {
      "epoch": 46.0,
      "grad_norm": 0.006555334664881229,
      "learning_rate": 3.965668229095546e-06,
      "loss": 0.0002,
      "step": 2875
    },
    {
      "epoch": 46.02,
      "grad_norm": 0.8879930973052979,
      "learning_rate": 3.962532077078058e-06,
      "loss": 0.0119,
      "step": 2876
    },
    {
      "epoch": 46.03,
      "grad_norm": 0.06135054677724838,
      "learning_rate": 3.9593963515244436e-06,
      "loss": 0.0005,
      "step": 2877
    },
    {
      "epoch": 46.05,
      "grad_norm": 0.004608053248375654,
      "learning_rate": 3.9562610537236836e-06,
      "loss": 0.0001,
      "step": 2878
    },
    {
      "epoch": 46.06,
      "grad_norm": 0.607576847076416,
      "learning_rate": 3.953126184964578e-06,
      "loss": 0.0067,
      "step": 2879
    },
    {
      "epoch": 46.08,
      "grad_norm": 0.0037418045103549957,
      "learning_rate": 3.949991746535753e-06,
      "loss": 0.0001,
      "step": 2880
    },
    {
      "epoch": 46.1,
      "grad_norm": 0.13442660868167877,
      "learning_rate": 3.9468577397256594e-06,
      "loss": 0.0015,
      "step": 2881
    },
    {
      "epoch": 46.11,
      "grad_norm": 0.49880892038345337,
      "learning_rate": 3.943724165822569e-06,
      "loss": 0.0028,
      "step": 2882
    },
    {
      "epoch": 46.13,
      "grad_norm": 0.004950396716594696,
      "learning_rate": 3.940591026114575e-06,
      "loss": 0.0001,
      "step": 2883
    },
    {
      "epoch": 46.14,
      "grad_norm": 0.005270879250019789,
      "learning_rate": 3.937458321889592e-06,
      "loss": 0.0002,
      "step": 2884
    },
    {
      "epoch": 46.16,
      "grad_norm": 0.07402949780225754,
      "learning_rate": 3.934326054435358e-06,
      "loss": 0.0008,
      "step": 2885
    },
    {
      "epoch": 46.18,
      "grad_norm": 0.9706636667251587,
      "learning_rate": 3.931194225039427e-06,
      "loss": 0.014,
      "step": 2886
    },
    {
      "epoch": 46.19,
      "grad_norm": 0.004270502831786871,
      "learning_rate": 3.928062834989179e-06,
      "loss": 0.0001,
      "step": 2887
    },
    {
      "epoch": 46.21,
      "grad_norm": 0.005033309571444988,
      "learning_rate": 3.924931885571811e-06,
      "loss": 0.0001,
      "step": 2888
    },
    {
      "epoch": 46.22,
      "grad_norm": 0.005564488936215639,
      "learning_rate": 3.921801378074334e-06,
      "loss": 0.0001,
      "step": 2889
    },
    {
      "epoch": 46.24,
      "grad_norm": 0.004547114949673414,
      "learning_rate": 3.918671313783583e-06,
      "loss": 0.0001,
      "step": 2890
    },
    {
      "epoch": 46.26,
      "grad_norm": 1.0149177312850952,
      "learning_rate": 3.915541693986212e-06,
      "loss": 0.0171,
      "step": 2891
    },
    {
      "epoch": 46.27,
      "grad_norm": 0.8406611680984497,
      "learning_rate": 3.912412519968685e-06,
      "loss": 0.0112,
      "step": 2892
    },
    {
      "epoch": 46.29,
      "grad_norm": 0.12969650328159332,
      "learning_rate": 3.909283793017289e-06,
      "loss": 0.0008,
      "step": 2893
    },
    {
      "epoch": 46.3,
      "grad_norm": 0.3516545295715332,
      "learning_rate": 3.906155514418125e-06,
      "loss": 0.0025,
      "step": 2894
    },
    {
      "epoch": 46.32,
      "grad_norm": 0.005586577113717794,
      "learning_rate": 3.903027685457112e-06,
      "loss": 0.0001,
      "step": 2895
    },
    {
      "epoch": 46.34,
      "grad_norm": 0.017923859879374504,
      "learning_rate": 3.899900307419982e-06,
      "loss": 0.0003,
      "step": 2896
    },
    {
      "epoch": 46.35,
      "grad_norm": 0.006112818606197834,
      "learning_rate": 3.896773381592281e-06,
      "loss": 0.0002,
      "step": 2897
    },
    {
      "epoch": 46.37,
      "grad_norm": 0.43038180470466614,
      "learning_rate": 3.893646909259368e-06,
      "loss": 0.0027,
      "step": 2898
    },
    {
      "epoch": 46.38,
      "grad_norm": 0.6446430683135986,
      "learning_rate": 3.890520891706422e-06,
      "loss": 0.0085,
      "step": 2899
    },
    {
      "epoch": 46.4,
      "grad_norm": 0.7134196162223816,
      "learning_rate": 3.887395330218429e-06,
      "loss": 0.0079,
      "step": 2900
    },
    {
      "epoch": 46.42,
      "grad_norm": 1.030121088027954,
      "learning_rate": 3.884270226080189e-06,
      "loss": 0.0076,
      "step": 2901
    },
    {
      "epoch": 46.43,
      "grad_norm": 0.7161945700645447,
      "learning_rate": 3.881145580576313e-06,
      "loss": 0.0108,
      "step": 2902
    },
    {
      "epoch": 46.45,
      "grad_norm": 0.003905448829755187,
      "learning_rate": 3.878021394991227e-06,
      "loss": 0.0001,
      "step": 2903
    },
    {
      "epoch": 46.46,
      "grad_norm": 0.02133665606379509,
      "learning_rate": 3.874897670609164e-06,
      "loss": 0.0002,
      "step": 2904
    },
    {
      "epoch": 46.48,
      "grad_norm": 0.7854897379875183,
      "learning_rate": 3.87177440871417e-06,
      "loss": 0.0065,
      "step": 2905
    },
    {
      "epoch": 46.5,
      "grad_norm": 0.01361137256026268,
      "learning_rate": 3.8686516105901e-06,
      "loss": 0.0003,
      "step": 2906
    },
    {
      "epoch": 46.51,
      "grad_norm": 1.1434977054595947,
      "learning_rate": 3.8655292775206185e-06,
      "loss": 0.0162,
      "step": 2907
    },
    {
      "epoch": 46.53,
      "grad_norm": 0.005556201096624136,
      "learning_rate": 3.862407410789198e-06,
      "loss": 0.0002,
      "step": 2908
    },
    {
      "epoch": 46.54,
      "grad_norm": 0.006075737997889519,
      "learning_rate": 3.8592860116791195e-06,
      "loss": 0.0002,
      "step": 2909
    },
    {
      "epoch": 46.56,
      "grad_norm": 0.005585230886936188,
      "learning_rate": 3.856165081473474e-06,
      "loss": 0.0001,
      "step": 2910
    },
    {
      "epoch": 46.58,
      "grad_norm": 0.23443755507469177,
      "learning_rate": 3.8530446214551576e-06,
      "loss": 0.0017,
      "step": 2911
    },
    {
      "epoch": 46.59,
      "grad_norm": 0.43257373571395874,
      "learning_rate": 3.849924632906872e-06,
      "loss": 0.0033,
      "step": 2912
    },
    {
      "epoch": 46.61,
      "grad_norm": 0.005751538090407848,
      "learning_rate": 3.846805117111127e-06,
      "loss": 0.0002,
      "step": 2913
    },
    {
      "epoch": 46.62,
      "grad_norm": 0.5666627883911133,
      "learning_rate": 3.843686075350239e-06,
      "loss": 0.0045,
      "step": 2914
    },
    {
      "epoch": 46.64,
      "grad_norm": 0.6537567377090454,
      "learning_rate": 3.840567508906328e-06,
      "loss": 0.0088,
      "step": 2915
    },
    {
      "epoch": 46.66,
      "grad_norm": 1.422370433807373,
      "learning_rate": 3.83744941906132e-06,
      "loss": 0.0322,
      "step": 2916
    },
    {
      "epoch": 46.67,
      "grad_norm": 0.014587599784135818,
      "learning_rate": 3.83433180709694e-06,
      "loss": 0.0002,
      "step": 2917
    },
    {
      "epoch": 46.69,
      "grad_norm": 0.0063551547937095165,
      "learning_rate": 3.831214674294724e-06,
      "loss": 0.0002,
      "step": 2918
    },
    {
      "epoch": 46.7,
      "grad_norm": 0.6062019467353821,
      "learning_rate": 3.828098021936006e-06,
      "loss": 0.0035,
      "step": 2919
    },
    {
      "epoch": 46.72,
      "grad_norm": 0.08246858417987823,
      "learning_rate": 3.824981851301924e-06,
      "loss": 0.0005,
      "step": 2920
    },
    {
      "epoch": 46.74,
      "grad_norm": 0.10176967084407806,
      "learning_rate": 3.821866163673421e-06,
      "loss": 0.0015,
      "step": 2921
    },
    {
      "epoch": 46.75,
      "grad_norm": 0.006643412634730339,
      "learning_rate": 3.8187509603312345e-06,
      "loss": 0.0001,
      "step": 2922
    },
    {
      "epoch": 46.77,
      "grad_norm": 0.005490623880177736,
      "learning_rate": 3.815636242555908e-06,
      "loss": 0.0002,
      "step": 2923
    },
    {
      "epoch": 46.78,
      "grad_norm": 0.005423536989837885,
      "learning_rate": 3.8125220116277855e-06,
      "loss": 0.0001,
      "step": 2924
    },
    {
      "epoch": 46.8,
      "grad_norm": 0.005382680334150791,
      "learning_rate": 3.809408268827009e-06,
      "loss": 0.0001,
      "step": 2925
    },
    {
      "epoch": 46.82,
      "grad_norm": 0.005081133916974068,
      "learning_rate": 3.8062950154335205e-06,
      "loss": 0.0001,
      "step": 2926
    },
    {
      "epoch": 46.83,
      "grad_norm": 0.005959030240774155,
      "learning_rate": 3.8031822527270624e-06,
      "loss": 0.0002,
      "step": 2927
    },
    {
      "epoch": 46.85,
      "grad_norm": 0.005591878667473793,
      "learning_rate": 3.8000699819871704e-06,
      "loss": 0.0002,
      "step": 2928
    },
    {
      "epoch": 46.86,
      "grad_norm": 0.6950308680534363,
      "learning_rate": 3.796958204493184e-06,
      "loss": 0.0064,
      "step": 2929
    },
    {
      "epoch": 46.88,
      "grad_norm": 0.0041364869102835655,
      "learning_rate": 3.7938469215242374e-06,
      "loss": 0.0001,
      "step": 2930
    },
    {
      "epoch": 46.9,
      "grad_norm": 0.7993980050086975,
      "learning_rate": 3.79073613435926e-06,
      "loss": 0.0054,
      "step": 2931
    },
    {
      "epoch": 46.91,
      "grad_norm": 1.4801268577575684,
      "learning_rate": 3.787625844276982e-06,
      "loss": 0.0207,
      "step": 2932
    },
    {
      "epoch": 46.93,
      "grad_norm": 0.0061382511630654335,
      "learning_rate": 3.7845160525559223e-06,
      "loss": 0.0002,
      "step": 2933
    },
    {
      "epoch": 46.94,
      "grad_norm": 0.004887847695499659,
      "learning_rate": 3.781406760474401e-06,
      "loss": 0.0001,
      "step": 2934
    },
    {
      "epoch": 46.96,
      "grad_norm": 0.7037631869316101,
      "learning_rate": 3.778297969310529e-06,
      "loss": 0.0123,
      "step": 2935
    },
    {
      "epoch": 46.98,
      "grad_norm": 0.7538358569145203,
      "learning_rate": 3.775189680342217e-06,
      "loss": 0.0069,
      "step": 2936
    },
    {
      "epoch": 46.99,
      "grad_norm": 0.0051068407483398914,
      "learning_rate": 3.7720818948471605e-06,
      "loss": 0.0001,
      "step": 2937
    },
    {
      "epoch": 47.01,
      "grad_norm": 1.1038939952850342,
      "learning_rate": 3.7689746141028542e-06,
      "loss": 0.0163,
      "step": 2938
    },
    {
      "epoch": 47.02,
      "grad_norm": 0.004465656820684671,
      "learning_rate": 3.7658678393865856e-06,
      "loss": 0.0001,
      "step": 2939
    },
    {
      "epoch": 47.04,
      "grad_norm": 0.6889680624008179,
      "learning_rate": 3.7627615719754294e-06,
      "loss": 0.0043,
      "step": 2940
    },
    {
      "epoch": 47.06,
      "grad_norm": 0.005651540122926235,
      "learning_rate": 3.7596558131462566e-06,
      "loss": 0.0001,
      "step": 2941
    },
    {
      "epoch": 47.07,
      "grad_norm": 0.7357791662216187,
      "learning_rate": 3.756550564175727e-06,
      "loss": 0.0057,
      "step": 2942
    },
    {
      "epoch": 47.09,
      "grad_norm": 0.004484742879867554,
      "learning_rate": 3.7534458263402916e-06,
      "loss": 0.0001,
      "step": 2943
    },
    {
      "epoch": 47.1,
      "grad_norm": 0.6302669048309326,
      "learning_rate": 3.7503416009161915e-06,
      "loss": 0.008,
      "step": 2944
    },
    {
      "epoch": 47.12,
      "grad_norm": 0.5861204266548157,
      "learning_rate": 3.7472378891794537e-06,
      "loss": 0.0053,
      "step": 2945
    },
    {
      "epoch": 47.14,
      "grad_norm": 1.2689088582992554,
      "learning_rate": 3.7441346924059e-06,
      "loss": 0.0161,
      "step": 2946
    },
    {
      "epoch": 47.15,
      "grad_norm": 0.15362538397312164,
      "learning_rate": 3.7410320118711353e-06,
      "loss": 0.0008,
      "step": 2947
    },
    {
      "epoch": 47.17,
      "grad_norm": 0.00469590350985527,
      "learning_rate": 3.737929848850555e-06,
      "loss": 0.0001,
      "step": 2948
    },
    {
      "epoch": 47.18,
      "grad_norm": 0.004892344120889902,
      "learning_rate": 3.7348282046193425e-06,
      "loss": 0.0001,
      "step": 2949
    },
    {
      "epoch": 47.2,
      "grad_norm": 0.4320456385612488,
      "learning_rate": 3.731727080452464e-06,
      "loss": 0.0025,
      "step": 2950
    },
    {
      "epoch": 47.22,
      "grad_norm": 0.004348777234554291,
      "learning_rate": 3.7286264776246766e-06,
      "loss": 0.0001,
      "step": 2951
    },
    {
      "epoch": 47.23,
      "grad_norm": 0.5811488032341003,
      "learning_rate": 3.72552639741052e-06,
      "loss": 0.0051,
      "step": 2952
    },
    {
      "epoch": 47.25,
      "grad_norm": 1.3265202045440674,
      "learning_rate": 3.72242684108432e-06,
      "loss": 0.018,
      "step": 2953
    },
    {
      "epoch": 47.26,
      "grad_norm": 0.005316073074936867,
      "learning_rate": 3.719327809920188e-06,
      "loss": 0.0001,
      "step": 2954
    },
    {
      "epoch": 47.28,
      "grad_norm": 0.06900739669799805,
      "learning_rate": 3.7162293051920185e-06,
      "loss": 0.0005,
      "step": 2955
    },
    {
      "epoch": 47.3,
      "grad_norm": 1.044310450553894,
      "learning_rate": 3.7131313281734895e-06,
      "loss": 0.0158,
      "step": 2956
    },
    {
      "epoch": 47.31,
      "grad_norm": 0.004137409385293722,
      "learning_rate": 3.710033880138061e-06,
      "loss": 0.0001,
      "step": 2957
    },
    {
      "epoch": 47.33,
      "grad_norm": 0.5359659790992737,
      "learning_rate": 3.7069369623589786e-06,
      "loss": 0.0074,
      "step": 2958
    },
    {
      "epoch": 47.34,
      "grad_norm": 0.005697028711438179,
      "learning_rate": 3.703840576109268e-06,
      "loss": 0.0002,
      "step": 2959
    },
    {
      "epoch": 47.36,
      "grad_norm": 1.01981782913208,
      "learning_rate": 3.7007447226617367e-06,
      "loss": 0.018,
      "step": 2960
    },
    {
      "epoch": 47.38,
      "grad_norm": 0.29696205258369446,
      "learning_rate": 3.697649403288972e-06,
      "loss": 0.0021,
      "step": 2961
    },
    {
      "epoch": 47.39,
      "grad_norm": 0.8856340646743774,
      "learning_rate": 3.694554619263343e-06,
      "loss": 0.013,
      "step": 2962
    },
    {
      "epoch": 47.41,
      "grad_norm": 0.5328153371810913,
      "learning_rate": 3.6914603718569996e-06,
      "loss": 0.0042,
      "step": 2963
    },
    {
      "epoch": 47.42,
      "grad_norm": 0.005481000058352947,
      "learning_rate": 3.6883666623418702e-06,
      "loss": 0.0002,
      "step": 2964
    },
    {
      "epoch": 47.44,
      "grad_norm": 0.004555067978799343,
      "learning_rate": 3.685273491989661e-06,
      "loss": 0.0001,
      "step": 2965
    },
    {
      "epoch": 47.46,
      "grad_norm": 1.0097094774246216,
      "learning_rate": 3.6821808620718567e-06,
      "loss": 0.0091,
      "step": 2966
    },
    {
      "epoch": 47.47,
      "grad_norm": 0.7821159362792969,
      "learning_rate": 3.6790887738597227e-06,
      "loss": 0.008,
      "step": 2967
    },
    {
      "epoch": 47.49,
      "grad_norm": 0.6812970042228699,
      "learning_rate": 3.6759972286242977e-06,
      "loss": 0.0101,
      "step": 2968
    },
    {
      "epoch": 47.5,
      "grad_norm": 0.004062228370457888,
      "learning_rate": 3.6729062276364003e-06,
      "loss": 0.0001,
      "step": 2969
    },
    {
      "epoch": 47.52,
      "grad_norm": 0.021425094455480576,
      "learning_rate": 3.669815772166625e-06,
      "loss": 0.0003,
      "step": 2970
    },
    {
      "epoch": 47.54,
      "grad_norm": 0.31668487191200256,
      "learning_rate": 3.6667258634853396e-06,
      "loss": 0.0057,
      "step": 2971
    },
    {
      "epoch": 47.55,
      "grad_norm": 0.005291067995131016,
      "learning_rate": 3.663636502862689e-06,
      "loss": 0.0002,
      "step": 2972
    },
    {
      "epoch": 47.57,
      "grad_norm": 0.007162862457334995,
      "learning_rate": 3.6605476915685936e-06,
      "loss": 0.0003,
      "step": 2973
    },
    {
      "epoch": 47.58,
      "grad_norm": 0.06659499555826187,
      "learning_rate": 3.657459430872746e-06,
      "loss": 0.0007,
      "step": 2974
    },
    {
      "epoch": 47.6,
      "grad_norm": 0.6917598247528076,
      "learning_rate": 3.654371722044616e-06,
      "loss": 0.0047,
      "step": 2975
    },
    {
      "epoch": 47.62,
      "grad_norm": 1.0984545946121216,
      "learning_rate": 3.6512845663534412e-06,
      "loss": 0.0122,
      "step": 2976
    },
    {
      "epoch": 47.63,
      "grad_norm": 0.004730957560241222,
      "learning_rate": 3.6481979650682355e-06,
      "loss": 0.0001,
      "step": 2977
    },
    {
      "epoch": 47.65,
      "grad_norm": 0.004925804678350687,
      "learning_rate": 3.6451119194577842e-06,
      "loss": 0.0001,
      "step": 2978
    },
    {
      "epoch": 47.66,
      "grad_norm": 0.006440412253141403,
      "learning_rate": 3.6420264307906442e-06,
      "loss": 0.0002,
      "step": 2979
    },
    {
      "epoch": 47.68,
      "grad_norm": 0.004573703743517399,
      "learning_rate": 3.638941500335145e-06,
      "loss": 0.0001,
      "step": 2980
    },
    {
      "epoch": 47.7,
      "grad_norm": 0.5386179685592651,
      "learning_rate": 3.6358571293593813e-06,
      "loss": 0.0047,
      "step": 2981
    },
    {
      "epoch": 47.71,
      "grad_norm": 0.05065620690584183,
      "learning_rate": 3.6327733191312254e-06,
      "loss": 0.0007,
      "step": 2982
    },
    {
      "epoch": 47.73,
      "grad_norm": 0.005429903045296669,
      "learning_rate": 3.6296900709183132e-06,
      "loss": 0.0002,
      "step": 2983
    },
    {
      "epoch": 47.74,
      "grad_norm": 0.9628492593765259,
      "learning_rate": 3.62660738598805e-06,
      "loss": 0.0153,
      "step": 2984
    },
    {
      "epoch": 47.76,
      "grad_norm": 0.9422197341918945,
      "learning_rate": 3.6235252656076138e-06,
      "loss": 0.0154,
      "step": 2985
    },
    {
      "epoch": 47.78,
      "grad_norm": 0.0062926760874688625,
      "learning_rate": 3.620443711043946e-06,
      "loss": 0.0001,
      "step": 2986
    },
    {
      "epoch": 47.79,
      "grad_norm": 0.04478800296783447,
      "learning_rate": 3.6173627235637587e-06,
      "loss": 0.0007,
      "step": 2987
    },
    {
      "epoch": 47.81,
      "grad_norm": 0.005320632364600897,
      "learning_rate": 3.6142823044335306e-06,
      "loss": 0.0001,
      "step": 2988
    },
    {
      "epoch": 47.82,
      "grad_norm": 0.005863325670361519,
      "learning_rate": 3.6112024549195026e-06,
      "loss": 0.0002,
      "step": 2989
    },
    {
      "epoch": 47.84,
      "grad_norm": 0.7703959345817566,
      "learning_rate": 3.608123176287685e-06,
      "loss": 0.006,
      "step": 2990
    },
    {
      "epoch": 47.86,
      "grad_norm": 0.6164332628250122,
      "learning_rate": 3.6050444698038547e-06,
      "loss": 0.0078,
      "step": 2991
    },
    {
      "epoch": 47.87,
      "grad_norm": 0.004101646598428488,
      "learning_rate": 3.6019663367335507e-06,
      "loss": 0.0001,
      "step": 2992
    },
    {
      "epoch": 47.89,
      "grad_norm": 0.004876249004155397,
      "learning_rate": 3.5988887783420778e-06,
      "loss": 0.0001,
      "step": 2993
    },
    {
      "epoch": 47.9,
      "grad_norm": 0.011242348700761795,
      "learning_rate": 3.5958117958945033e-06,
      "loss": 0.0002,
      "step": 2994
    },
    {
      "epoch": 47.92,
      "grad_norm": 0.842821478843689,
      "learning_rate": 3.5927353906556583e-06,
      "loss": 0.0145,
      "step": 2995
    },
    {
      "epoch": 47.94,
      "grad_norm": 0.046018920838832855,
      "learning_rate": 3.5896595638901373e-06,
      "loss": 0.0003,
      "step": 2996
    },
    {
      "epoch": 47.95,
      "grad_norm": 0.004677602089941502,
      "learning_rate": 3.586584316862296e-06,
      "loss": 0.0001,
      "step": 2997
    },
    {
      "epoch": 47.97,
      "grad_norm": 0.3812125623226166,
      "learning_rate": 3.583509650836254e-06,
      "loss": 0.0027,
      "step": 2998
    },
    {
      "epoch": 47.98,
      "grad_norm": 0.005905634257942438,
      "learning_rate": 3.580435567075888e-06,
      "loss": 0.0002,
      "step": 2999
    },
    {
      "epoch": 48.0,
      "grad_norm": 0.6078150272369385,
      "learning_rate": 3.5773620668448384e-06,
      "loss": 0.0077,
      "step": 3000
    },
    {
      "epoch": 48.02,
      "grad_norm": 0.0038716087583452463,
      "learning_rate": 3.574289151406506e-06,
      "loss": 0.0001,
      "step": 3001
    },
    {
      "epoch": 48.03,
      "grad_norm": 0.820595920085907,
      "learning_rate": 3.5712168220240494e-06,
      "loss": 0.0068,
      "step": 3002
    },
    {
      "epoch": 48.05,
      "grad_norm": 0.008578803390264511,
      "learning_rate": 3.568145079960388e-06,
      "loss": 0.0001,
      "step": 3003
    },
    {
      "epoch": 48.06,
      "grad_norm": 0.6444911360740662,
      "learning_rate": 3.5650739264781976e-06,
      "loss": 0.0083,
      "step": 3004
    },
    {
      "epoch": 48.08,
      "grad_norm": 0.005524089559912682,
      "learning_rate": 3.562003362839914e-06,
      "loss": 0.0002,
      "step": 3005
    },
    {
      "epoch": 48.1,
      "grad_norm": 0.004213094711303711,
      "learning_rate": 3.5589333903077306e-06,
      "loss": 0.0001,
      "step": 3006
    },
    {
      "epoch": 48.11,
      "grad_norm": 0.6901656985282898,
      "learning_rate": 3.555864010143596e-06,
      "loss": 0.0065,
      "step": 3007
    },
    {
      "epoch": 48.13,
      "grad_norm": 0.5560442805290222,
      "learning_rate": 3.552795223609219e-06,
      "loss": 0.0072,
      "step": 3008
    },
    {
      "epoch": 48.14,
      "grad_norm": 0.06324932724237442,
      "learning_rate": 3.5497270319660576e-06,
      "loss": 0.0004,
      "step": 3009
    },
    {
      "epoch": 48.16,
      "grad_norm": 0.24402064085006714,
      "learning_rate": 3.5466594364753325e-06,
      "loss": 0.0015,
      "step": 3010
    },
    {
      "epoch": 48.18,
      "grad_norm": 0.004825367592275143,
      "learning_rate": 3.5435924383980154e-06,
      "loss": 0.0001,
      "step": 3011
    },
    {
      "epoch": 48.19,
      "grad_norm": 0.004423442296683788,
      "learning_rate": 3.540526038994834e-06,
      "loss": 0.0001,
      "step": 3012
    },
    {
      "epoch": 48.21,
      "grad_norm": 0.005957636516541243,
      "learning_rate": 3.537460239526269e-06,
      "loss": 0.0002,
      "step": 3013
    },
    {
      "epoch": 48.22,
      "grad_norm": 0.4344293475151062,
      "learning_rate": 3.534395041252554e-06,
      "loss": 0.003,
      "step": 3014
    },
    {
      "epoch": 48.24,
      "grad_norm": 0.6529598236083984,
      "learning_rate": 3.531330445433677e-06,
      "loss": 0.0051,
      "step": 3015
    },
    {
      "epoch": 48.26,
      "grad_norm": 0.6231726408004761,
      "learning_rate": 3.5282664533293763e-06,
      "loss": 0.0041,
      "step": 3016
    },
    {
      "epoch": 48.27,
      "grad_norm": 0.004368731286376715,
      "learning_rate": 3.5252030661991455e-06,
      "loss": 0.0001,
      "step": 3017
    },
    {
      "epoch": 48.29,
      "grad_norm": 0.8555240035057068,
      "learning_rate": 3.5221402853022256e-06,
      "loss": 0.0112,
      "step": 3018
    },
    {
      "epoch": 48.3,
      "grad_norm": 0.006396363954991102,
      "learning_rate": 3.5190781118976125e-06,
      "loss": 0.0002,
      "step": 3019
    },
    {
      "epoch": 48.32,
      "grad_norm": 0.004717404488474131,
      "learning_rate": 3.516016547244047e-06,
      "loss": 0.0001,
      "step": 3020
    },
    {
      "epoch": 48.34,
      "grad_norm": 0.004899441264569759,
      "learning_rate": 3.5129555926000237e-06,
      "loss": 0.0001,
      "step": 3021
    },
    {
      "epoch": 48.35,
      "grad_norm": 0.34543097019195557,
      "learning_rate": 3.5098952492237858e-06,
      "loss": 0.0028,
      "step": 3022
    },
    {
      "epoch": 48.37,
      "grad_norm": 1.1250673532485962,
      "learning_rate": 3.506835518373325e-06,
      "loss": 0.0122,
      "step": 3023
    },
    {
      "epoch": 48.38,
      "grad_norm": 0.004063715226948261,
      "learning_rate": 3.5037764013063825e-06,
      "loss": 0.0001,
      "step": 3024
    },
    {
      "epoch": 48.4,
      "grad_norm": 0.5774524807929993,
      "learning_rate": 3.500717899280442e-06,
      "loss": 0.0055,
      "step": 3025
    },
    {
      "epoch": 48.42,
      "grad_norm": 0.06412211060523987,
      "learning_rate": 3.4976600135527403e-06,
      "loss": 0.0007,
      "step": 3026
    },
    {
      "epoch": 48.43,
      "grad_norm": 1.167411208152771,
      "learning_rate": 3.494602745380261e-06,
      "loss": 0.0092,
      "step": 3027
    },
    {
      "epoch": 48.45,
      "grad_norm": 1.149051547050476,
      "learning_rate": 3.4915460960197277e-06,
      "loss": 0.0091,
      "step": 3028
    },
    {
      "epoch": 48.46,
      "grad_norm": 0.006281139794737101,
      "learning_rate": 3.4884900667276143e-06,
      "loss": 0.0002,
      "step": 3029
    },
    {
      "epoch": 48.48,
      "grad_norm": 0.6143824458122253,
      "learning_rate": 3.48543465876014e-06,
      "loss": 0.0048,
      "step": 3030
    },
    {
      "epoch": 48.5,
      "grad_norm": 0.4588354229927063,
      "learning_rate": 3.4823798733732684e-06,
      "loss": 0.0036,
      "step": 3031
    },
    {
      "epoch": 48.51,
      "grad_norm": 0.45545694231987,
      "learning_rate": 3.479325711822704e-06,
      "loss": 0.0047,
      "step": 3032
    },
    {
      "epoch": 48.53,
      "grad_norm": 0.7697136998176575,
      "learning_rate": 3.4762721753638994e-06,
      "loss": 0.0068,
      "step": 3033
    },
    {
      "epoch": 48.54,
      "grad_norm": 0.006074320990592241,
      "learning_rate": 3.473219265252047e-06,
      "loss": 0.0002,
      "step": 3034
    },
    {
      "epoch": 48.56,
      "grad_norm": 0.004805040080100298,
      "learning_rate": 3.4701669827420827e-06,
      "loss": 0.0001,
      "step": 3035
    },
    {
      "epoch": 48.58,
      "grad_norm": 0.941459596157074,
      "learning_rate": 3.4671153290886863e-06,
      "loss": 0.0138,
      "step": 3036
    },
    {
      "epoch": 48.59,
      "grad_norm": 0.802513599395752,
      "learning_rate": 3.464064305546274e-06,
      "loss": 0.0054,
      "step": 3037
    },
    {
      "epoch": 48.61,
      "grad_norm": 0.004126295913010836,
      "learning_rate": 3.461013913369009e-06,
      "loss": 0.0001,
      "step": 3038
    },
    {
      "epoch": 48.62,
      "grad_norm": 0.0051967790350317955,
      "learning_rate": 3.4579641538107923e-06,
      "loss": 0.0002,
      "step": 3039
    },
    {
      "epoch": 48.64,
      "grad_norm": 0.2031330168247223,
      "learning_rate": 3.4549150281252635e-06,
      "loss": 0.0015,
      "step": 3040
    },
    {
      "epoch": 48.66,
      "grad_norm": 0.5113160014152527,
      "learning_rate": 3.4518665375658044e-06,
      "loss": 0.0036,
      "step": 3041
    },
    {
      "epoch": 48.67,
      "grad_norm": 0.6947211027145386,
      "learning_rate": 3.4488186833855334e-06,
      "loss": 0.0109,
      "step": 3042
    },
    {
      "epoch": 48.69,
      "grad_norm": 0.0056444513611495495,
      "learning_rate": 3.4457714668373075e-06,
      "loss": 0.0002,
      "step": 3043
    },
    {
      "epoch": 48.7,
      "grad_norm": 0.0053609805181622505,
      "learning_rate": 3.442724889173724e-06,
      "loss": 0.0001,
      "step": 3044
    },
    {
      "epoch": 48.72,
      "grad_norm": 0.00952465832233429,
      "learning_rate": 3.4396789516471152e-06,
      "loss": 0.0002,
      "step": 3045
    },
    {
      "epoch": 48.74,
      "grad_norm": 0.006515088956803083,
      "learning_rate": 3.43663365550955e-06,
      "loss": 0.0002,
      "step": 3046
    },
    {
      "epoch": 48.75,
      "grad_norm": 0.004371607210487127,
      "learning_rate": 3.4335890020128382e-06,
      "loss": 0.0001,
      "step": 3047
    },
    {
      "epoch": 48.77,
      "grad_norm": 0.004305979702621698,
      "learning_rate": 3.4305449924085165e-06,
      "loss": 0.0001,
      "step": 3048
    },
    {
      "epoch": 48.78,
      "grad_norm": 0.019055696204304695,
      "learning_rate": 3.4275016279478657e-06,
      "loss": 0.0003,
      "step": 3049
    },
    {
      "epoch": 48.8,
      "grad_norm": 1.0128430128097534,
      "learning_rate": 3.424458909881897e-06,
      "loss": 0.0169,
      "step": 3050
    },
    {
      "epoch": 48.82,
      "grad_norm": 0.002865132410079241,
      "learning_rate": 3.4214168394613566e-06,
      "loss": 0.0001,
      "step": 3051
    },
    {
      "epoch": 48.83,
      "grad_norm": 1.140634298324585,
      "learning_rate": 3.4183754179367268e-06,
      "loss": 0.0208,
      "step": 3052
    },
    {
      "epoch": 48.85,
      "grad_norm": 1.0457491874694824,
      "learning_rate": 3.4153346465582183e-06,
      "loss": 0.0186,
      "step": 3053
    },
    {
      "epoch": 48.86,
      "grad_norm": 0.004830331075936556,
      "learning_rate": 3.412294526575779e-06,
      "loss": 0.0001,
      "step": 3054
    },
    {
      "epoch": 48.88,
      "grad_norm": 0.7183544039726257,
      "learning_rate": 3.409255059239086e-06,
      "loss": 0.0113,
      "step": 3055
    },
    {
      "epoch": 48.9,
      "grad_norm": 0.07831457257270813,
      "learning_rate": 3.406216245797551e-06,
      "loss": 0.0004,
      "step": 3056
    },
    {
      "epoch": 48.91,
      "grad_norm": 0.004482991527765989,
      "learning_rate": 3.4031780875003162e-06,
      "loss": 0.0001,
      "step": 3057
    },
    {
      "epoch": 48.93,
      "grad_norm": 0.8716245889663696,
      "learning_rate": 3.400140585596251e-06,
      "loss": 0.0138,
      "step": 3058
    },
    {
      "epoch": 48.94,
      "grad_norm": 0.790056586265564,
      "learning_rate": 3.3971037413339592e-06,
      "loss": 0.0118,
      "step": 3059
    },
    {
      "epoch": 48.96,
      "grad_norm": 0.004689113236963749,
      "learning_rate": 3.3940675559617724e-06,
      "loss": 0.0001,
      "step": 3060
    },
    {
      "epoch": 48.98,
      "grad_norm": 0.8650524616241455,
      "learning_rate": 3.3910320307277522e-06,
      "loss": 0.0062,
      "step": 3061
    },
    {
      "epoch": 48.99,
      "grad_norm": 0.004731088876724243,
      "learning_rate": 3.3879971668796896e-06,
      "loss": 0.0001,
      "step": 3062
    },
    {
      "epoch": 49.01,
      "grad_norm": 0.4241997003555298,
      "learning_rate": 3.3849629656650996e-06,
      "loss": 0.0043,
      "step": 3063
    },
    {
      "epoch": 49.02,
      "grad_norm": 0.02964124083518982,
      "learning_rate": 3.3819294283312286e-06,
      "loss": 0.0002,
      "step": 3064
    },
    {
      "epoch": 49.04,
      "grad_norm": 0.6584404706954956,
      "learning_rate": 3.37889655612505e-06,
      "loss": 0.0075,
      "step": 3065
    },
    {
      "epoch": 49.06,
      "grad_norm": 0.6407914161682129,
      "learning_rate": 3.375864350293263e-06,
      "loss": 0.0063,
      "step": 3066
    },
    {
      "epoch": 49.07,
      "grad_norm": 0.35574087500572205,
      "learning_rate": 3.372832812082294e-06,
      "loss": 0.0023,
      "step": 3067
    },
    {
      "epoch": 49.09,
      "grad_norm": 0.005183210130780935,
      "learning_rate": 3.3698019427382912e-06,
      "loss": 0.0001,
      "step": 3068
    },
    {
      "epoch": 49.1,
      "grad_norm": 0.005224660970270634,
      "learning_rate": 3.366771743507131e-06,
      "loss": 0.0001,
      "step": 3069
    },
    {
      "epoch": 49.12,
      "grad_norm": 0.00638118851929903,
      "learning_rate": 3.363742215634416e-06,
      "loss": 0.0002,
      "step": 3070
    },
    {
      "epoch": 49.14,
      "grad_norm": 1.21071457862854,
      "learning_rate": 3.3607133603654685e-06,
      "loss": 0.0081,
      "step": 3071
    },
    {
      "epoch": 49.15,
      "grad_norm": 0.05038849264383316,
      "learning_rate": 3.357685178945339e-06,
      "loss": 0.0003,
      "step": 3072
    },
    {
      "epoch": 49.17,
      "grad_norm": 0.00513919722288847,
      "learning_rate": 3.3546576726187953e-06,
      "loss": 0.0001,
      "step": 3073
    },
    {
      "epoch": 49.18,
      "grad_norm": 0.0039325011894106865,
      "learning_rate": 3.3516308426303324e-06,
      "loss": 0.0001,
      "step": 3074
    },
    {
      "epoch": 49.2,
      "grad_norm": 0.716248095035553,
      "learning_rate": 3.3486046902241663e-06,
      "loss": 0.0092,
      "step": 3075
    },
    {
      "epoch": 49.22,
      "grad_norm": 0.5768285989761353,
      "learning_rate": 3.3455792166442323e-06,
      "loss": 0.0037,
      "step": 3076
    },
    {
      "epoch": 49.23,
      "grad_norm": 0.009190999902784824,
      "learning_rate": 3.34255442313419e-06,
      "loss": 0.0002,
      "step": 3077
    },
    {
      "epoch": 49.25,
      "grad_norm": 0.47166433930397034,
      "learning_rate": 3.3395303109374165e-06,
      "loss": 0.0037,
      "step": 3078
    },
    {
      "epoch": 49.26,
      "grad_norm": 0.003056818852201104,
      "learning_rate": 3.3365068812970113e-06,
      "loss": 0.0001,
      "step": 3079
    },
    {
      "epoch": 49.28,
      "grad_norm": 0.3283284902572632,
      "learning_rate": 3.3334841354557923e-06,
      "loss": 0.0026,
      "step": 3080
    },
    {
      "epoch": 49.3,
      "grad_norm": 0.3601260185241699,
      "learning_rate": 3.330462074656295e-06,
      "loss": 0.0025,
      "step": 3081
    },
    {
      "epoch": 49.31,
      "grad_norm": 0.0051989746280014515,
      "learning_rate": 3.327440700140774e-06,
      "loss": 0.0002,
      "step": 3082
    },
    {
      "epoch": 49.33,
      "grad_norm": 0.4936774671077728,
      "learning_rate": 3.324420013151204e-06,
      "loss": 0.0032,
      "step": 3083
    },
    {
      "epoch": 49.34,
      "grad_norm": 0.6081396341323853,
      "learning_rate": 3.3214000149292734e-06,
      "loss": 0.0082,
      "step": 3084
    },
    {
      "epoch": 49.36,
      "grad_norm": 0.37211641669273376,
      "learning_rate": 3.318380706716392e-06,
      "loss": 0.0022,
      "step": 3085
    },
    {
      "epoch": 49.38,
      "grad_norm": 0.38056161999702454,
      "learning_rate": 3.315362089753681e-06,
      "loss": 0.0023,
      "step": 3086
    },
    {
      "epoch": 49.39,
      "grad_norm": 0.15775074064731598,
      "learning_rate": 3.31234416528198e-06,
      "loss": 0.0009,
      "step": 3087
    },
    {
      "epoch": 49.41,
      "grad_norm": 1.0781186819076538,
      "learning_rate": 3.3093269345418443e-06,
      "loss": 0.0169,
      "step": 3088
    },
    {
      "epoch": 49.42,
      "grad_norm": 0.0054409196600317955,
      "learning_rate": 3.3063103987735433e-06,
      "loss": 0.0001,
      "step": 3089
    },
    {
      "epoch": 49.44,
      "grad_norm": 0.03227543085813522,
      "learning_rate": 3.303294559217063e-06,
      "loss": 0.0003,
      "step": 3090
    },
    {
      "epoch": 49.46,
      "grad_norm": 0.005309269763529301,
      "learning_rate": 3.3002794171120978e-06,
      "loss": 0.0001,
      "step": 3091
    },
    {
      "epoch": 49.47,
      "grad_norm": 0.004223429597914219,
      "learning_rate": 3.29726497369806e-06,
      "loss": 0.0001,
      "step": 3092
    },
    {
      "epoch": 49.49,
      "grad_norm": 1.0460973978042603,
      "learning_rate": 3.2942512302140735e-06,
      "loss": 0.0105,
      "step": 3093
    },
    {
      "epoch": 49.5,
      "grad_norm": 1.0034692287445068,
      "learning_rate": 3.2912381878989748e-06,
      "loss": 0.0081,
      "step": 3094
    },
    {
      "epoch": 49.52,
      "grad_norm": 0.0049752360209822655,
      "learning_rate": 3.288225847991312e-06,
      "loss": 0.0001,
      "step": 3095
    },
    {
      "epoch": 49.54,
      "grad_norm": 0.35171347856521606,
      "learning_rate": 3.2852142117293435e-06,
      "loss": 0.0038,
      "step": 3096
    },
    {
      "epoch": 49.55,
      "grad_norm": 0.07119246572256088,
      "learning_rate": 3.2822032803510385e-06,
      "loss": 0.0004,
      "step": 3097
    },
    {
      "epoch": 49.57,
      "grad_norm": 0.004275286104530096,
      "learning_rate": 3.279193055094079e-06,
      "loss": 0.0001,
      "step": 3098
    },
    {
      "epoch": 49.58,
      "grad_norm": 0.4182763993740082,
      "learning_rate": 3.276183537195854e-06,
      "loss": 0.0024,
      "step": 3099
    },
    {
      "epoch": 49.6,
      "grad_norm": 0.005173278506845236,
      "learning_rate": 3.273174727893463e-06,
      "loss": 0.0001,
      "step": 3100
    },
    {
      "epoch": 49.62,
      "grad_norm": 0.0050388528034091,
      "learning_rate": 3.2701666284237123e-06,
      "loss": 0.0001,
      "step": 3101
    },
    {
      "epoch": 49.63,
      "grad_norm": 0.003705826587975025,
      "learning_rate": 3.26715924002312e-06,
      "loss": 0.0001,
      "step": 3102
    },
    {
      "epoch": 49.65,
      "grad_norm": 0.7651016712188721,
      "learning_rate": 3.264152563927908e-06,
      "loss": 0.0057,
      "step": 3103
    },
    {
      "epoch": 49.66,
      "grad_norm": 0.6844230890274048,
      "learning_rate": 3.261146601374009e-06,
      "loss": 0.0124,
      "step": 3104
    },
    {
      "epoch": 49.68,
      "grad_norm": 0.6987189650535583,
      "learning_rate": 3.2581413535970597e-06,
      "loss": 0.0092,
      "step": 3105
    },
    {
      "epoch": 49.7,
      "grad_norm": 0.7502516508102417,
      "learning_rate": 3.255136821832405e-06,
      "loss": 0.0053,
      "step": 3106
    },
    {
      "epoch": 49.71,
      "grad_norm": 0.05537284165620804,
      "learning_rate": 3.252133007315093e-06,
      "loss": 0.0009,
      "step": 3107
    },
    {
      "epoch": 49.73,
      "grad_norm": 0.06713084876537323,
      "learning_rate": 3.2491299112798793e-06,
      "loss": 0.0005,
      "step": 3108
    },
    {
      "epoch": 49.74,
      "grad_norm": 0.7272692918777466,
      "learning_rate": 3.246127534961222e-06,
      "loss": 0.0076,
      "step": 3109
    },
    {
      "epoch": 49.76,
      "grad_norm": 0.004269561264663935,
      "learning_rate": 3.2431258795932863e-06,
      "loss": 0.0001,
      "step": 3110
    },
    {
      "epoch": 49.78,
      "grad_norm": 0.11550348997116089,
      "learning_rate": 3.240124946409939e-06,
      "loss": 0.0017,
      "step": 3111
    },
    {
      "epoch": 49.79,
      "grad_norm": 0.5651724338531494,
      "learning_rate": 3.237124736644749e-06,
      "loss": 0.0035,
      "step": 3112
    },
    {
      "epoch": 49.81,
      "grad_norm": 0.004647868685424328,
      "learning_rate": 3.234125251530991e-06,
      "loss": 0.0001,
      "step": 3113
    },
    {
      "epoch": 49.82,
      "grad_norm": 0.23051755130290985,
      "learning_rate": 3.2311264923016384e-06,
      "loss": 0.0016,
      "step": 3114
    },
    {
      "epoch": 49.84,
      "grad_norm": 0.6767038106918335,
      "learning_rate": 3.228128460189368e-06,
      "loss": 0.0099,
      "step": 3115
    },
    {
      "epoch": 49.86,
      "grad_norm": 0.6719081997871399,
      "learning_rate": 3.22513115642656e-06,
      "loss": 0.0098,
      "step": 3116
    },
    {
      "epoch": 49.87,
      "grad_norm": 0.6031797528266907,
      "learning_rate": 3.22213458224529e-06,
      "loss": 0.0065,
      "step": 3117
    },
    {
      "epoch": 49.89,
      "grad_norm": 0.004901631269603968,
      "learning_rate": 3.2191387388773393e-06,
      "loss": 0.0001,
      "step": 3118
    },
    {
      "epoch": 49.9,
      "grad_norm": 0.003664308227598667,
      "learning_rate": 3.216143627554182e-06,
      "loss": 0.0001,
      "step": 3119
    },
    {
      "epoch": 49.92,
      "grad_norm": 0.5106207728385925,
      "learning_rate": 3.213149249506997e-06,
      "loss": 0.004,
      "step": 3120
    },
    {
      "epoch": 49.94,
      "grad_norm": 1.2500438690185547,
      "learning_rate": 3.2101556059666607e-06,
      "loss": 0.0216,
      "step": 3121
    },
    {
      "epoch": 49.95,
      "grad_norm": 0.0050313579849898815,
      "learning_rate": 3.207162698163746e-06,
      "loss": 0.0001,
      "step": 3122
    },
    {
      "epoch": 49.97,
      "grad_norm": 0.0064927963539958,
      "learning_rate": 3.204170527328526e-06,
      "loss": 0.0002,
      "step": 3123
    },
    {
      "epoch": 49.98,
      "grad_norm": 0.00447508879005909,
      "learning_rate": 3.2011790946909673e-06,
      "loss": 0.0001,
      "step": 3124
    },
    {
      "epoch": 50.0,
      "grad_norm": 0.05606585741043091,
      "learning_rate": 3.198188401480734e-06,
      "loss": 0.0003,
      "step": 3125
    },
    {
      "epoch": 50.02,
      "grad_norm": 0.8522617220878601,
      "learning_rate": 3.195198448927189e-06,
      "loss": 0.0147,
      "step": 3126
    },
    {
      "epoch": 50.03,
      "grad_norm": 0.07252654433250427,
      "learning_rate": 3.1922092382593894e-06,
      "loss": 0.0007,
      "step": 3127
    },
    {
      "epoch": 50.05,
      "grad_norm": 0.6671404838562012,
      "learning_rate": 3.189220770706086e-06,
      "loss": 0.0093,
      "step": 3128
    },
    {
      "epoch": 50.06,
      "grad_norm": 0.7042477130889893,
      "learning_rate": 3.1862330474957237e-06,
      "loss": 0.004,
      "step": 3129
    },
    {
      "epoch": 50.08,
      "grad_norm": 0.005074490327388048,
      "learning_rate": 3.183246069856443e-06,
      "loss": 0.0001,
      "step": 3130
    },
    {
      "epoch": 50.1,
      "grad_norm": 0.006266664247959852,
      "learning_rate": 3.1802598390160788e-06,
      "loss": 0.0002,
      "step": 3131
    },
    {
      "epoch": 50.11,
      "grad_norm": 0.5256800651550293,
      "learning_rate": 3.177274356202157e-06,
      "loss": 0.004,
      "step": 3132
    },
    {
      "epoch": 50.13,
      "grad_norm": 0.00464945612475276,
      "learning_rate": 3.1742896226418973e-06,
      "loss": 0.0001,
      "step": 3133
    },
    {
      "epoch": 50.14,
      "grad_norm": 0.753649115562439,
      "learning_rate": 3.171305639562211e-06,
      "loss": 0.0127,
      "step": 3134
    },
    {
      "epoch": 50.16,
      "grad_norm": 1.129903793334961,
      "learning_rate": 3.1683224081897e-06,
      "loss": 0.0164,
      "step": 3135
    },
    {
      "epoch": 50.18,
      "grad_norm": 0.17860133945941925,
      "learning_rate": 3.1653399297506583e-06,
      "loss": 0.001,
      "step": 3136
    },
    {
      "epoch": 50.19,
      "grad_norm": 0.05613851919770241,
      "learning_rate": 3.1623582054710705e-06,
      "loss": 0.0002,
      "step": 3137
    },
    {
      "epoch": 50.21,
      "grad_norm": 0.5107777118682861,
      "learning_rate": 3.1593772365766107e-06,
      "loss": 0.0036,
      "step": 3138
    },
    {
      "epoch": 50.22,
      "grad_norm": 0.20212875306606293,
      "learning_rate": 3.1563970242926433e-06,
      "loss": 0.0014,
      "step": 3139
    },
    {
      "epoch": 50.24,
      "grad_norm": 0.9451802968978882,
      "learning_rate": 3.1534175698442194e-06,
      "loss": 0.0139,
      "step": 3140
    },
    {
      "epoch": 50.26,
      "grad_norm": 0.0040048579685389996,
      "learning_rate": 3.1504388744560804e-06,
      "loss": 0.0001,
      "step": 3141
    },
    {
      "epoch": 50.27,
      "grad_norm": 0.003406287869438529,
      "learning_rate": 3.147460939352657e-06,
      "loss": 0.0001,
      "step": 3142
    },
    {
      "epoch": 50.29,
      "grad_norm": 0.5654851794242859,
      "learning_rate": 3.144483765758064e-06,
      "loss": 0.0064,
      "step": 3143
    },
    {
      "epoch": 50.3,
      "grad_norm": 0.0035726253408938646,
      "learning_rate": 3.141507354896107e-06,
      "loss": 0.0001,
      "step": 3144
    },
    {
      "epoch": 50.32,
      "grad_norm": 0.003429719479754567,
      "learning_rate": 3.1385317079902743e-06,
      "loss": 0.0001,
      "step": 3145
    },
    {
      "epoch": 50.34,
      "grad_norm": 0.004531911574304104,
      "learning_rate": 3.135556826263743e-06,
      "loss": 0.0001,
      "step": 3146
    },
    {
      "epoch": 50.35,
      "grad_norm": 0.004239839967340231,
      "learning_rate": 3.132582710939374e-06,
      "loss": 0.0001,
      "step": 3147
    },
    {
      "epoch": 50.37,
      "grad_norm": 0.0039175101555883884,
      "learning_rate": 3.129609363239714e-06,
      "loss": 0.0001,
      "step": 3148
    },
    {
      "epoch": 50.38,
      "grad_norm": 0.006397453602403402,
      "learning_rate": 3.126636784386995e-06,
      "loss": 0.0002,
      "step": 3149
    },
    {
      "epoch": 50.4,
      "grad_norm": 0.9712042808532715,
      "learning_rate": 3.12366497560313e-06,
      "loss": 0.0065,
      "step": 3150
    },
    {
      "epoch": 50.42,
      "grad_norm": 0.07540645450353622,
      "learning_rate": 3.1206939381097177e-06,
      "loss": 0.0005,
      "step": 3151
    },
    {
      "epoch": 50.43,
      "grad_norm": 0.4334144592285156,
      "learning_rate": 3.11772367312804e-06,
      "loss": 0.0032,
      "step": 3152
    },
    {
      "epoch": 50.45,
      "grad_norm": 0.6713696718215942,
      "learning_rate": 3.1147541818790605e-06,
      "loss": 0.0095,
      "step": 3153
    },
    {
      "epoch": 50.46,
      "grad_norm": 0.9324890375137329,
      "learning_rate": 3.111785465583426e-06,
      "loss": 0.0098,
      "step": 3154
    },
    {
      "epoch": 50.48,
      "grad_norm": 0.004902772605419159,
      "learning_rate": 3.1088175254614616e-06,
      "loss": 0.0001,
      "step": 3155
    },
    {
      "epoch": 50.5,
      "grad_norm": 0.0042469799518585205,
      "learning_rate": 3.105850362733176e-06,
      "loss": 0.0001,
      "step": 3156
    },
    {
      "epoch": 50.51,
      "grad_norm": 0.004776917397975922,
      "learning_rate": 3.102883978618258e-06,
      "loss": 0.0001,
      "step": 3157
    },
    {
      "epoch": 50.53,
      "grad_norm": 0.005074622109532356,
      "learning_rate": 3.099918374336076e-06,
      "loss": 0.0001,
      "step": 3158
    },
    {
      "epoch": 50.54,
      "grad_norm": 0.004217247478663921,
      "learning_rate": 3.096953551105679e-06,
      "loss": 0.0001,
      "step": 3159
    },
    {
      "epoch": 50.56,
      "grad_norm": 0.7700916528701782,
      "learning_rate": 3.093989510145792e-06,
      "loss": 0.011,
      "step": 3160
    },
    {
      "epoch": 50.58,
      "grad_norm": 0.061853718012571335,
      "learning_rate": 3.0910262526748206e-06,
      "loss": 0.0005,
      "step": 3161
    },
    {
      "epoch": 50.59,
      "grad_norm": 0.030544785782694817,
      "learning_rate": 3.0880637799108494e-06,
      "loss": 0.0003,
      "step": 3162
    },
    {
      "epoch": 50.61,
      "grad_norm": 0.005626334343105555,
      "learning_rate": 3.0851020930716393e-06,
      "loss": 0.0002,
      "step": 3163
    },
    {
      "epoch": 50.62,
      "grad_norm": 0.3332301378250122,
      "learning_rate": 3.082141193374625e-06,
      "loss": 0.002,
      "step": 3164
    },
    {
      "epoch": 50.64,
      "grad_norm": 0.8148558735847473,
      "learning_rate": 3.079181082036922e-06,
      "loss": 0.0123,
      "step": 3165
    },
    {
      "epoch": 50.66,
      "grad_norm": 0.8170970678329468,
      "learning_rate": 3.076221760275321e-06,
      "loss": 0.0136,
      "step": 3166
    },
    {
      "epoch": 50.67,
      "grad_norm": 0.8422272205352783,
      "learning_rate": 3.0732632293062873e-06,
      "loss": 0.011,
      "step": 3167
    },
    {
      "epoch": 50.69,
      "grad_norm": 0.004345164634287357,
      "learning_rate": 3.0703054903459607e-06,
      "loss": 0.0001,
      "step": 3168
    },
    {
      "epoch": 50.7,
      "grad_norm": 0.9064509272575378,
      "learning_rate": 3.067348544610155e-06,
      "loss": 0.0091,
      "step": 3169
    },
    {
      "epoch": 50.72,
      "grad_norm": 0.004367112647742033,
      "learning_rate": 3.0643923933143603e-06,
      "loss": 0.0001,
      "step": 3170
    },
    {
      "epoch": 50.74,
      "grad_norm": 0.06257564574480057,
      "learning_rate": 3.061437037673739e-06,
      "loss": 0.0004,
      "step": 3171
    },
    {
      "epoch": 50.75,
      "grad_norm": 0.0044473581947386265,
      "learning_rate": 3.0584824789031266e-06,
      "loss": 0.0001,
      "step": 3172
    },
    {
      "epoch": 50.77,
      "grad_norm": 1.32931649684906,
      "learning_rate": 3.055528718217028e-06,
      "loss": 0.0128,
      "step": 3173
    },
    {
      "epoch": 50.78,
      "grad_norm": 0.004928726702928543,
      "learning_rate": 3.0525757568296242e-06,
      "loss": 0.0001,
      "step": 3174
    },
    {
      "epoch": 50.8,
      "grad_norm": 1.0832021236419678,
      "learning_rate": 3.049623595954766e-06,
      "loss": 0.0176,
      "step": 3175
    },
    {
      "epoch": 50.82,
      "grad_norm": 1.173150897026062,
      "learning_rate": 3.046672236805976e-06,
      "loss": 0.0091,
      "step": 3176
    },
    {
      "epoch": 50.83,
      "grad_norm": 0.8353785276412964,
      "learning_rate": 3.0437216805964463e-06,
      "loss": 0.007,
      "step": 3177
    },
    {
      "epoch": 50.85,
      "grad_norm": 0.003484574379399419,
      "learning_rate": 3.040771928539038e-06,
      "loss": 0.0001,
      "step": 3178
    },
    {
      "epoch": 50.86,
      "grad_norm": 0.004334918688982725,
      "learning_rate": 3.0378229818462835e-06,
      "loss": 0.0001,
      "step": 3179
    },
    {
      "epoch": 50.88,
      "grad_norm": 0.24943071603775024,
      "learning_rate": 3.0348748417303826e-06,
      "loss": 0.0014,
      "step": 3180
    },
    {
      "epoch": 50.9,
      "grad_norm": 0.7988921403884888,
      "learning_rate": 3.0319275094032053e-06,
      "loss": 0.0053,
      "step": 3181
    },
    {
      "epoch": 50.91,
      "grad_norm": 0.005743570160120726,
      "learning_rate": 3.0289809860762893e-06,
      "loss": 0.0002,
      "step": 3182
    },
    {
      "epoch": 50.93,
      "grad_norm": 0.807512104511261,
      "learning_rate": 3.026035272960837e-06,
      "loss": 0.0081,
      "step": 3183
    },
    {
      "epoch": 50.94,
      "grad_norm": 0.5137665271759033,
      "learning_rate": 3.0230903712677207e-06,
      "loss": 0.0027,
      "step": 3184
    },
    {
      "epoch": 50.96,
      "grad_norm": 0.0052925958298146725,
      "learning_rate": 3.020146282207479e-06,
      "loss": 0.0001,
      "step": 3185
    },
    {
      "epoch": 50.98,
      "grad_norm": 0.8701528310775757,
      "learning_rate": 3.0172030069903147e-06,
      "loss": 0.0149,
      "step": 3186
    },
    {
      "epoch": 50.99,
      "grad_norm": 0.33618438243865967,
      "learning_rate": 3.0142605468260976e-06,
      "loss": 0.0027,
      "step": 3187
    },
    {
      "epoch": 51.01,
      "grad_norm": 0.6931339502334595,
      "learning_rate": 3.0113189029243616e-06,
      "loss": 0.0042,
      "step": 3188
    },
    {
      "epoch": 51.02,
      "grad_norm": 0.03794395178556442,
      "learning_rate": 3.008378076494306e-06,
      "loss": 0.0002,
      "step": 3189
    },
    {
      "epoch": 51.04,
      "grad_norm": 0.005461446475237608,
      "learning_rate": 3.005438068744792e-06,
      "loss": 0.0002,
      "step": 3190
    },
    {
      "epoch": 51.06,
      "grad_norm": 0.0049779354594647884,
      "learning_rate": 3.0024988808843475e-06,
      "loss": 0.0001,
      "step": 3191
    },
    {
      "epoch": 51.07,
      "grad_norm": 0.44155946373939514,
      "learning_rate": 2.9995605141211615e-06,
      "loss": 0.0044,
      "step": 3192
    },
    {
      "epoch": 51.09,
      "grad_norm": 0.005192824173718691,
      "learning_rate": 2.9966229696630837e-06,
      "loss": 0.0002,
      "step": 3193
    },
    {
      "epoch": 51.1,
      "grad_norm": 0.5464431047439575,
      "learning_rate": 2.9936862487176295e-06,
      "loss": 0.0042,
      "step": 3194
    },
    {
      "epoch": 51.12,
      "grad_norm": 0.004447559360414743,
      "learning_rate": 2.9907503524919734e-06,
      "loss": 0.0001,
      "step": 3195
    },
    {
      "epoch": 51.14,
      "grad_norm": 0.7710789442062378,
      "learning_rate": 2.987815282192951e-06,
      "loss": 0.0114,
      "step": 3196
    },
    {
      "epoch": 51.15,
      "grad_norm": 0.21273858845233917,
      "learning_rate": 2.984881039027059e-06,
      "loss": 0.0013,
      "step": 3197
    },
    {
      "epoch": 51.17,
      "grad_norm": 0.005236174911260605,
      "learning_rate": 2.981947624200455e-06,
      "loss": 0.0001,
      "step": 3198
    },
    {
      "epoch": 51.18,
      "grad_norm": 0.004961356986314058,
      "learning_rate": 2.9790150389189544e-06,
      "loss": 0.0001,
      "step": 3199
    },
    {
      "epoch": 51.2,
      "grad_norm": 0.7265400290489197,
      "learning_rate": 2.976083284388031e-06,
      "loss": 0.0094,
      "step": 3200
    },
    {
      "epoch": 51.22,
      "grad_norm": 0.34463223814964294,
      "learning_rate": 2.97315236181282e-06,
      "loss": 0.0022,
      "step": 3201
    },
    {
      "epoch": 51.23,
      "grad_norm": 0.004055086523294449,
      "learning_rate": 2.9702222723981113e-06,
      "loss": 0.0001,
      "step": 3202
    },
    {
      "epoch": 51.25,
      "grad_norm": 0.6401256322860718,
      "learning_rate": 2.967293017348357e-06,
      "loss": 0.0049,
      "step": 3203
    },
    {
      "epoch": 51.26,
      "grad_norm": 0.25707054138183594,
      "learning_rate": 2.964364597867659e-06,
      "loss": 0.0037,
      "step": 3204
    },
    {
      "epoch": 51.28,
      "grad_norm": 0.6849274635314941,
      "learning_rate": 2.9614370151597837e-06,
      "loss": 0.0057,
      "step": 3205
    },
    {
      "epoch": 51.3,
      "grad_norm": 0.8456332683563232,
      "learning_rate": 2.9585102704281475e-06,
      "loss": 0.0086,
      "step": 3206
    },
    {
      "epoch": 51.31,
      "grad_norm": 0.004270451609045267,
      "learning_rate": 2.9555843648758255e-06,
      "loss": 0.0001,
      "step": 3207
    },
    {
      "epoch": 51.33,
      "grad_norm": 0.004801135044544935,
      "learning_rate": 2.9526592997055488e-06,
      "loss": 0.0001,
      "step": 3208
    },
    {
      "epoch": 51.34,
      "grad_norm": 0.8027840256690979,
      "learning_rate": 2.949735076119697e-06,
      "loss": 0.0053,
      "step": 3209
    },
    {
      "epoch": 51.36,
      "grad_norm": 0.004288461524993181,
      "learning_rate": 2.9468116953203107e-06,
      "loss": 0.0001,
      "step": 3210
    },
    {
      "epoch": 51.38,
      "grad_norm": 0.003977520857006311,
      "learning_rate": 2.94388915850908e-06,
      "loss": 0.0001,
      "step": 3211
    },
    {
      "epoch": 51.39,
      "grad_norm": 0.004207978490740061,
      "learning_rate": 2.940967466887351e-06,
      "loss": 0.0001,
      "step": 3212
    },
    {
      "epoch": 51.41,
      "grad_norm": 0.5766898989677429,
      "learning_rate": 2.9380466216561186e-06,
      "loss": 0.0046,
      "step": 3213
    },
    {
      "epoch": 51.42,
      "grad_norm": 0.4604875147342682,
      "learning_rate": 2.9351266240160344e-06,
      "loss": 0.0045,
      "step": 3214
    },
    {
      "epoch": 51.44,
      "grad_norm": 0.003408245276659727,
      "learning_rate": 2.932207475167398e-06,
      "loss": 0.0001,
      "step": 3215
    },
    {
      "epoch": 51.46,
      "grad_norm": 0.004216694738715887,
      "learning_rate": 2.929289176310161e-06,
      "loss": 0.0001,
      "step": 3216
    },
    {
      "epoch": 51.47,
      "grad_norm": 0.9147136807441711,
      "learning_rate": 2.9263717286439254e-06,
      "loss": 0.0141,
      "step": 3217
    },
    {
      "epoch": 51.49,
      "grad_norm": 0.004161378834396601,
      "learning_rate": 2.923455133367945e-06,
      "loss": 0.0001,
      "step": 3218
    },
    {
      "epoch": 51.5,
      "grad_norm": 0.004324260633438826,
      "learning_rate": 2.920539391681121e-06,
      "loss": 0.0001,
      "step": 3219
    },
    {
      "epoch": 51.52,
      "grad_norm": 0.003867145162075758,
      "learning_rate": 2.9176245047820064e-06,
      "loss": 0.0001,
      "step": 3220
    },
    {
      "epoch": 51.54,
      "grad_norm": 0.00486036017537117,
      "learning_rate": 2.9147104738687975e-06,
      "loss": 0.0001,
      "step": 3221
    },
    {
      "epoch": 51.55,
      "grad_norm": 0.5649200677871704,
      "learning_rate": 2.911797300139345e-06,
      "loss": 0.0082,
      "step": 3222
    },
    {
      "epoch": 51.57,
      "grad_norm": 0.08805157244205475,
      "learning_rate": 2.908884984791145e-06,
      "loss": 0.0005,
      "step": 3223
    },
    {
      "epoch": 51.58,
      "grad_norm": 0.005390196107327938,
      "learning_rate": 2.9059735290213387e-06,
      "loss": 0.0001,
      "step": 3224
    },
    {
      "epoch": 51.6,
      "grad_norm": 1.1035465002059937,
      "learning_rate": 2.9030629340267165e-06,
      "loss": 0.0157,
      "step": 3225
    },
    {
      "epoch": 51.62,
      "grad_norm": 0.06905601918697357,
      "learning_rate": 2.9001532010037136e-06,
      "loss": 0.0006,
      "step": 3226
    },
    {
      "epoch": 51.63,
      "grad_norm": 0.7287222146987915,
      "learning_rate": 2.8972443311484116e-06,
      "loss": 0.0065,
      "step": 3227
    },
    {
      "epoch": 51.65,
      "grad_norm": 1.0679489374160767,
      "learning_rate": 2.8943363256565394e-06,
      "loss": 0.0126,
      "step": 3228
    },
    {
      "epoch": 51.66,
      "grad_norm": 0.00475417310371995,
      "learning_rate": 2.891429185723464e-06,
      "loss": 0.0001,
      "step": 3229
    },
    {
      "epoch": 51.68,
      "grad_norm": 0.005519320257008076,
      "learning_rate": 2.8885229125442022e-06,
      "loss": 0.0002,
      "step": 3230
    },
    {
      "epoch": 51.7,
      "grad_norm": 0.0041059223003685474,
      "learning_rate": 2.885617507313414e-06,
      "loss": 0.0001,
      "step": 3231
    },
    {
      "epoch": 51.71,
      "grad_norm": 0.0034733046777546406,
      "learning_rate": 2.882712971225401e-06,
      "loss": 0.0001,
      "step": 3232
    },
    {
      "epoch": 51.73,
      "grad_norm": 0.11362771689891815,
      "learning_rate": 2.879809305474108e-06,
      "loss": 0.0007,
      "step": 3233
    },
    {
      "epoch": 51.74,
      "grad_norm": 0.0052544004283845425,
      "learning_rate": 2.8769065112531243e-06,
      "loss": 0.0002,
      "step": 3234
    },
    {
      "epoch": 51.76,
      "grad_norm": 0.04594328626990318,
      "learning_rate": 2.8740045897556766e-06,
      "loss": 0.0003,
      "step": 3235
    },
    {
      "epoch": 51.78,
      "grad_norm": 0.894984245300293,
      "learning_rate": 2.871103542174637e-06,
      "loss": 0.0136,
      "step": 3236
    },
    {
      "epoch": 51.79,
      "grad_norm": 0.7469921112060547,
      "learning_rate": 2.8682033697025145e-06,
      "loss": 0.0071,
      "step": 3237
    },
    {
      "epoch": 51.81,
      "grad_norm": 0.004040562082082033,
      "learning_rate": 2.865304073531463e-06,
      "loss": 0.0001,
      "step": 3238
    },
    {
      "epoch": 51.82,
      "grad_norm": 0.005234600976109505,
      "learning_rate": 2.8624056548532738e-06,
      "loss": 0.0001,
      "step": 3239
    },
    {
      "epoch": 51.84,
      "grad_norm": 0.00443037785589695,
      "learning_rate": 2.859508114859374e-06,
      "loss": 0.0001,
      "step": 3240
    },
    {
      "epoch": 51.86,
      "grad_norm": 0.027481816709041595,
      "learning_rate": 2.8566114547408342e-06,
      "loss": 0.0002,
      "step": 3241
    },
    {
      "epoch": 51.87,
      "grad_norm": 0.46892452239990234,
      "learning_rate": 2.8537156756883634e-06,
      "loss": 0.0032,
      "step": 3242
    },
    {
      "epoch": 51.89,
      "grad_norm": 0.004156880080699921,
      "learning_rate": 2.850820778892305e-06,
      "loss": 0.0001,
      "step": 3243
    },
    {
      "epoch": 51.9,
      "grad_norm": 0.5975485444068909,
      "learning_rate": 2.8479267655426435e-06,
      "loss": 0.0064,
      "step": 3244
    },
    {
      "epoch": 51.92,
      "grad_norm": 1.1368964910507202,
      "learning_rate": 2.845033636828998e-06,
      "loss": 0.0218,
      "step": 3245
    },
    {
      "epoch": 51.94,
      "grad_norm": 1.5335344076156616,
      "learning_rate": 2.842141393940624e-06,
      "loss": 0.0236,
      "step": 3246
    },
    {
      "epoch": 51.95,
      "grad_norm": 1.241507649421692,
      "learning_rate": 2.8392500380664128e-06,
      "loss": 0.0185,
      "step": 3247
    },
    {
      "epoch": 51.97,
      "grad_norm": 0.003706233808770776,
      "learning_rate": 2.8363595703948933e-06,
      "loss": 0.0001,
      "step": 3248
    },
    {
      "epoch": 51.98,
      "grad_norm": 1.1002233028411865,
      "learning_rate": 2.833469992114228e-06,
      "loss": 0.0079,
      "step": 3249
    },
    {
      "epoch": 52.0,
      "grad_norm": 0.4210453927516937,
      "learning_rate": 2.83058130441221e-06,
      "loss": 0.0056,
      "step": 3250
    },
    {
      "epoch": 52.02,
      "grad_norm": 0.0035350900143384933,
      "learning_rate": 2.827693508476271e-06,
      "loss": 0.0001,
      "step": 3251
    },
    {
      "epoch": 52.03,
      "grad_norm": 0.003911945037543774,
      "learning_rate": 2.824806605493477e-06,
      "loss": 0.0001,
      "step": 3252
    },
    {
      "epoch": 52.05,
      "grad_norm": 0.6002715826034546,
      "learning_rate": 2.8219205966505224e-06,
      "loss": 0.0069,
      "step": 3253
    },
    {
      "epoch": 52.06,
      "grad_norm": 0.6292989253997803,
      "learning_rate": 2.81903548313374e-06,
      "loss": 0.0049,
      "step": 3254
    },
    {
      "epoch": 52.08,
      "grad_norm": 0.0034216451458632946,
      "learning_rate": 2.8161512661290847e-06,
      "loss": 0.0001,
      "step": 3255
    },
    {
      "epoch": 52.1,
      "grad_norm": 0.003992303740233183,
      "learning_rate": 2.8132679468221537e-06,
      "loss": 0.0001,
      "step": 3256
    },
    {
      "epoch": 52.11,
      "grad_norm": 0.07096786051988602,
      "learning_rate": 2.8103855263981695e-06,
      "loss": 0.0005,
      "step": 3257
    },
    {
      "epoch": 52.13,
      "grad_norm": 0.004174195695668459,
      "learning_rate": 2.8075040060419867e-06,
      "loss": 0.0001,
      "step": 3258
    },
    {
      "epoch": 52.14,
      "grad_norm": 0.00558779900893569,
      "learning_rate": 2.804623386938089e-06,
      "loss": 0.0002,
      "step": 3259
    },
    {
      "epoch": 52.16,
      "grad_norm": 0.17392829060554504,
      "learning_rate": 2.80174367027059e-06,
      "loss": 0.0024,
      "step": 3260
    },
    {
      "epoch": 52.18,
      "grad_norm": 0.0035523483529686928,
      "learning_rate": 2.798864857223235e-06,
      "loss": 0.0001,
      "step": 3261
    },
    {
      "epoch": 52.19,
      "grad_norm": 0.6605638861656189,
      "learning_rate": 2.7959869489793912e-06,
      "loss": 0.009,
      "step": 3262
    },
    {
      "epoch": 52.21,
      "grad_norm": 0.004273510072380304,
      "learning_rate": 2.7931099467220594e-06,
      "loss": 0.0001,
      "step": 3263
    },
    {
      "epoch": 52.22,
      "grad_norm": 0.7388250231742859,
      "learning_rate": 2.790233851633868e-06,
      "loss": 0.0073,
      "step": 3264
    },
    {
      "epoch": 52.24,
      "grad_norm": 0.005659543909132481,
      "learning_rate": 2.7873586648970686e-06,
      "loss": 0.0002,
      "step": 3265
    },
    {
      "epoch": 52.26,
      "grad_norm": 0.004472135566174984,
      "learning_rate": 2.7844843876935446e-06,
      "loss": 0.0001,
      "step": 3266
    },
    {
      "epoch": 52.27,
      "grad_norm": 0.33005067706108093,
      "learning_rate": 2.781611021204801e-06,
      "loss": 0.0018,
      "step": 3267
    },
    {
      "epoch": 52.29,
      "grad_norm": 0.023244434967637062,
      "learning_rate": 2.7787385666119704e-06,
      "loss": 0.0002,
      "step": 3268
    },
    {
      "epoch": 52.3,
      "grad_norm": 0.0035344837233424187,
      "learning_rate": 2.775867025095811e-06,
      "loss": 0.0001,
      "step": 3269
    },
    {
      "epoch": 52.32,
      "grad_norm": 0.7330057621002197,
      "learning_rate": 2.772996397836704e-06,
      "loss": 0.0073,
      "step": 3270
    },
    {
      "epoch": 52.34,
      "grad_norm": 0.8611868619918823,
      "learning_rate": 2.7701266860146575e-06,
      "loss": 0.0076,
      "step": 3271
    },
    {
      "epoch": 52.35,
      "grad_norm": 0.005768454633653164,
      "learning_rate": 2.7672578908093024e-06,
      "loss": 0.0002,
      "step": 3272
    },
    {
      "epoch": 52.37,
      "grad_norm": 0.19467322528362274,
      "learning_rate": 2.764390013399888e-06,
      "loss": 0.0012,
      "step": 3273
    },
    {
      "epoch": 52.38,
      "grad_norm": 0.003677803324535489,
      "learning_rate": 2.7615230549652933e-06,
      "loss": 0.0001,
      "step": 3274
    },
    {
      "epoch": 52.4,
      "grad_norm": 1.1759729385375977,
      "learning_rate": 2.7586570166840154e-06,
      "loss": 0.0261,
      "step": 3275
    },
    {
      "epoch": 52.42,
      "grad_norm": 0.006415958050638437,
      "learning_rate": 2.755791899734176e-06,
      "loss": 0.0002,
      "step": 3276
    },
    {
      "epoch": 52.43,
      "grad_norm": 0.8591054081916809,
      "learning_rate": 2.7529277052935142e-06,
      "loss": 0.0114,
      "step": 3277
    },
    {
      "epoch": 52.45,
      "grad_norm": 0.8223052620887756,
      "learning_rate": 2.7500644345393945e-06,
      "loss": 0.0123,
      "step": 3278
    },
    {
      "epoch": 52.46,
      "grad_norm": 0.004961179569363594,
      "learning_rate": 2.7472020886487977e-06,
      "loss": 0.0002,
      "step": 3279
    },
    {
      "epoch": 52.48,
      "grad_norm": 0.5381295084953308,
      "learning_rate": 2.7443406687983267e-06,
      "loss": 0.0042,
      "step": 3280
    },
    {
      "epoch": 52.5,
      "grad_norm": 0.005328504368662834,
      "learning_rate": 2.741480176164203e-06,
      "loss": 0.0001,
      "step": 3281
    },
    {
      "epoch": 52.51,
      "grad_norm": 0.0052853901870548725,
      "learning_rate": 2.738620611922269e-06,
      "loss": 0.0002,
      "step": 3282
    },
    {
      "epoch": 52.53,
      "grad_norm": 0.0049712806940078735,
      "learning_rate": 2.7357619772479805e-06,
      "loss": 0.0001,
      "step": 3283
    },
    {
      "epoch": 52.54,
      "grad_norm": 0.003137251827865839,
      "learning_rate": 2.7329042733164145e-06,
      "loss": 0.0001,
      "step": 3284
    },
    {
      "epoch": 52.56,
      "grad_norm": 0.006092164199799299,
      "learning_rate": 2.7300475013022666e-06,
      "loss": 0.0002,
      "step": 3285
    },
    {
      "epoch": 52.58,
      "grad_norm": 0.005024273414164782,
      "learning_rate": 2.727191662379847e-06,
      "loss": 0.0001,
      "step": 3286
    },
    {
      "epoch": 52.59,
      "grad_norm": 1.1689280271530151,
      "learning_rate": 2.724336757723084e-06,
      "loss": 0.0103,
      "step": 3287
    },
    {
      "epoch": 52.61,
      "grad_norm": 0.5300410985946655,
      "learning_rate": 2.7214827885055194e-06,
      "loss": 0.0045,
      "step": 3288
    },
    {
      "epoch": 52.62,
      "grad_norm": 0.5288600921630859,
      "learning_rate": 2.718629755900315e-06,
      "loss": 0.0029,
      "step": 3289
    },
    {
      "epoch": 52.64,
      "grad_norm": 0.6990388035774231,
      "learning_rate": 2.7157776610802416e-06,
      "loss": 0.0046,
      "step": 3290
    },
    {
      "epoch": 52.66,
      "grad_norm": 0.7363569140434265,
      "learning_rate": 2.7129265052176902e-06,
      "loss": 0.0059,
      "step": 3291
    },
    {
      "epoch": 52.67,
      "grad_norm": 0.0051764328964054585,
      "learning_rate": 2.7100762894846633e-06,
      "loss": 0.0001,
      "step": 3292
    },
    {
      "epoch": 52.69,
      "grad_norm": 0.005054599139839411,
      "learning_rate": 2.707227015052774e-06,
      "loss": 0.0001,
      "step": 3293
    },
    {
      "epoch": 52.7,
      "grad_norm": 0.7514073252677917,
      "learning_rate": 2.704378683093254e-06,
      "loss": 0.0099,
      "step": 3294
    },
    {
      "epoch": 52.72,
      "grad_norm": 0.00556224538013339,
      "learning_rate": 2.7015312947769436e-06,
      "loss": 0.0002,
      "step": 3295
    },
    {
      "epoch": 52.74,
      "grad_norm": 0.21336469054222107,
      "learning_rate": 2.698684851274297e-06,
      "loss": 0.0012,
      "step": 3296
    },
    {
      "epoch": 52.75,
      "grad_norm": 0.004642525687813759,
      "learning_rate": 2.6958393537553793e-06,
      "loss": 0.0001,
      "step": 3297
    },
    {
      "epoch": 52.77,
      "grad_norm": 0.781996488571167,
      "learning_rate": 2.6929948033898683e-06,
      "loss": 0.0046,
      "step": 3298
    },
    {
      "epoch": 52.78,
      "grad_norm": 0.005888346116989851,
      "learning_rate": 2.6901512013470523e-06,
      "loss": 0.0002,
      "step": 3299
    },
    {
      "epoch": 52.8,
      "grad_norm": 0.8784402012825012,
      "learning_rate": 2.687308548795825e-06,
      "loss": 0.0122,
      "step": 3300
    },
    {
      "epoch": 52.82,
      "grad_norm": 0.04038110375404358,
      "learning_rate": 2.684466846904695e-06,
      "loss": 0.0004,
      "step": 3301
    },
    {
      "epoch": 52.83,
      "grad_norm": 1.0995328426361084,
      "learning_rate": 2.6816260968417785e-06,
      "loss": 0.0175,
      "step": 3302
    },
    {
      "epoch": 52.85,
      "grad_norm": 0.01718386448919773,
      "learning_rate": 2.6787862997748014e-06,
      "loss": 0.0004,
      "step": 3303
    },
    {
      "epoch": 52.86,
      "grad_norm": 0.004842210095375776,
      "learning_rate": 2.675947456871096e-06,
      "loss": 0.0001,
      "step": 3304
    },
    {
      "epoch": 52.88,
      "grad_norm": 0.7787160873413086,
      "learning_rate": 2.6731095692976073e-06,
      "loss": 0.0147,
      "step": 3305
    },
    {
      "epoch": 52.9,
      "grad_norm": 0.773801326751709,
      "learning_rate": 2.6702726382208775e-06,
      "loss": 0.0049,
      "step": 3306
    },
    {
      "epoch": 52.91,
      "grad_norm": 0.6487459540367126,
      "learning_rate": 2.667436664807065e-06,
      "loss": 0.0057,
      "step": 3307
    },
    {
      "epoch": 52.93,
      "grad_norm": 0.018748026341199875,
      "learning_rate": 2.6646016502219304e-06,
      "loss": 0.0002,
      "step": 3308
    },
    {
      "epoch": 52.94,
      "grad_norm": 0.004957956727594137,
      "learning_rate": 2.661767595630842e-06,
      "loss": 0.0001,
      "step": 3309
    },
    {
      "epoch": 52.96,
      "grad_norm": 0.9588764309883118,
      "learning_rate": 2.6589345021987725e-06,
      "loss": 0.011,
      "step": 3310
    },
    {
      "epoch": 52.98,
      "grad_norm": 1.2227354049682617,
      "learning_rate": 2.6561023710902985e-06,
      "loss": 0.0128,
      "step": 3311
    },
    {
      "epoch": 52.99,
      "grad_norm": 0.0057945894077420235,
      "learning_rate": 2.6532712034696034e-06,
      "loss": 0.0002,
      "step": 3312
    },
    {
      "epoch": 53.01,
      "grad_norm": 0.6747198700904846,
      "learning_rate": 2.6504410005004734e-06,
      "loss": 0.0085,
      "step": 3313
    },
    {
      "epoch": 53.02,
      "grad_norm": 0.004935269244015217,
      "learning_rate": 2.6476117633462966e-06,
      "loss": 0.0001,
      "step": 3314
    },
    {
      "epoch": 53.04,
      "grad_norm": 0.005890815984457731,
      "learning_rate": 2.6447834931700688e-06,
      "loss": 0.0002,
      "step": 3315
    },
    {
      "epoch": 53.06,
      "grad_norm": 0.004377690609544516,
      "learning_rate": 2.6419561911343812e-06,
      "loss": 0.0001,
      "step": 3316
    },
    {
      "epoch": 53.07,
      "grad_norm": 0.005186902824789286,
      "learning_rate": 2.6391298584014313e-06,
      "loss": 0.0001,
      "step": 3317
    },
    {
      "epoch": 53.09,
      "grad_norm": 1.0452603101730347,
      "learning_rate": 2.6363044961330196e-06,
      "loss": 0.0123,
      "step": 3318
    },
    {
      "epoch": 53.1,
      "grad_norm": 0.8882623910903931,
      "learning_rate": 2.6334801054905447e-06,
      "loss": 0.0118,
      "step": 3319
    },
    {
      "epoch": 53.12,
      "grad_norm": 0.007714170962572098,
      "learning_rate": 2.6306566876350072e-06,
      "loss": 0.0002,
      "step": 3320
    },
    {
      "epoch": 53.14,
      "grad_norm": 0.20055657625198364,
      "learning_rate": 2.627834243727008e-06,
      "loss": 0.0013,
      "step": 3321
    },
    {
      "epoch": 53.15,
      "grad_norm": 0.018481837585568428,
      "learning_rate": 2.6250127749267458e-06,
      "loss": 0.0004,
      "step": 3322
    },
    {
      "epoch": 53.17,
      "grad_norm": 0.029693681746721268,
      "learning_rate": 2.6221922823940205e-06,
      "loss": 0.0002,
      "step": 3323
    },
    {
      "epoch": 53.18,
      "grad_norm": 0.005168253090232611,
      "learning_rate": 2.6193727672882308e-06,
      "loss": 0.0001,
      "step": 3324
    },
    {
      "epoch": 53.2,
      "grad_norm": 0.0038550428580492735,
      "learning_rate": 2.6165542307683744e-06,
      "loss": 0.0001,
      "step": 3325
    },
    {
      "epoch": 53.22,
      "grad_norm": 0.2273104190826416,
      "learning_rate": 2.6137366739930415e-06,
      "loss": 0.0014,
      "step": 3326
    },
    {
      "epoch": 53.23,
      "grad_norm": 0.4762633442878723,
      "learning_rate": 2.610920098120424e-06,
      "loss": 0.0042,
      "step": 3327
    },
    {
      "epoch": 53.25,
      "grad_norm": 0.6540608406066895,
      "learning_rate": 2.608104504308311e-06,
      "loss": 0.0043,
      "step": 3328
    },
    {
      "epoch": 53.26,
      "grad_norm": 0.003485987428575754,
      "learning_rate": 2.605289893714087e-06,
      "loss": 0.0001,
      "step": 3329
    },
    {
      "epoch": 53.28,
      "grad_norm": 0.8299210667610168,
      "learning_rate": 2.6024762674947313e-06,
      "loss": 0.0105,
      "step": 3330
    },
    {
      "epoch": 53.3,
      "grad_norm": 0.7932385206222534,
      "learning_rate": 2.5996636268068186e-06,
      "loss": 0.0121,
      "step": 3331
    },
    {
      "epoch": 53.31,
      "grad_norm": 0.005532723385840654,
      "learning_rate": 2.596851972806522e-06,
      "loss": 0.0002,
      "step": 3332
    },
    {
      "epoch": 53.33,
      "grad_norm": 0.712884247303009,
      "learning_rate": 2.5940413066496033e-06,
      "loss": 0.0088,
      "step": 3333
    },
    {
      "epoch": 53.34,
      "grad_norm": 0.005351847968995571,
      "learning_rate": 2.5912316294914232e-06,
      "loss": 0.0002,
      "step": 3334
    },
    {
      "epoch": 53.36,
      "grad_norm": 0.003631461411714554,
      "learning_rate": 2.588422942486932e-06,
      "loss": 0.0001,
      "step": 3335
    },
    {
      "epoch": 53.38,
      "grad_norm": 0.0049086506478488445,
      "learning_rate": 2.5856152467906793e-06,
      "loss": 0.0001,
      "step": 3336
    },
    {
      "epoch": 53.39,
      "grad_norm": 0.754925549030304,
      "learning_rate": 2.582808543556796e-06,
      "loss": 0.0054,
      "step": 3337
    },
    {
      "epoch": 53.41,
      "grad_norm": 0.003283861791715026,
      "learning_rate": 2.5800028339390164e-06,
      "loss": 0.0001,
      "step": 3338
    },
    {
      "epoch": 53.42,
      "grad_norm": 0.6574115753173828,
      "learning_rate": 2.57719811909066e-06,
      "loss": 0.0068,
      "step": 3339
    },
    {
      "epoch": 53.44,
      "grad_norm": 1.1187876462936401,
      "learning_rate": 2.5743944001646394e-06,
      "loss": 0.026,
      "step": 3340
    },
    {
      "epoch": 53.46,
      "grad_norm": 0.7409780025482178,
      "learning_rate": 2.5715916783134583e-06,
      "loss": 0.0056,
      "step": 3341
    },
    {
      "epoch": 53.47,
      "grad_norm": 1.089383840560913,
      "learning_rate": 2.5687899546892087e-06,
      "loss": 0.0096,
      "step": 3342
    },
    {
      "epoch": 53.49,
      "grad_norm": 0.3248181939125061,
      "learning_rate": 2.565989230443574e-06,
      "loss": 0.0018,
      "step": 3343
    },
    {
      "epoch": 53.5,
      "grad_norm": 0.4845569133758545,
      "learning_rate": 2.563189506727828e-06,
      "loss": 0.005,
      "step": 3344
    },
    {
      "epoch": 53.52,
      "grad_norm": 0.004152172245085239,
      "learning_rate": 2.5603907846928277e-06,
      "loss": 0.0001,
      "step": 3345
    },
    {
      "epoch": 53.54,
      "grad_norm": 0.0052421074360609055,
      "learning_rate": 2.5575930654890235e-06,
      "loss": 0.0002,
      "step": 3346
    },
    {
      "epoch": 53.55,
      "grad_norm": 0.003657542634755373,
      "learning_rate": 2.5547963502664518e-06,
      "loss": 0.0001,
      "step": 3347
    },
    {
      "epoch": 53.57,
      "grad_norm": 0.06630219519138336,
      "learning_rate": 2.55200064017474e-06,
      "loss": 0.0003,
      "step": 3348
    },
    {
      "epoch": 53.58,
      "grad_norm": 0.8411647081375122,
      "learning_rate": 2.549205936363094e-06,
      "loss": 0.0064,
      "step": 3349
    },
    {
      "epoch": 53.6,
      "grad_norm": 0.5289415717124939,
      "learning_rate": 2.5464122399803126e-06,
      "loss": 0.0065,
      "step": 3350
    },
    {
      "epoch": 53.62,
      "grad_norm": 0.005203116685152054,
      "learning_rate": 2.5436195521747788e-06,
      "loss": 0.0002,
      "step": 3351
    },
    {
      "epoch": 53.63,
      "grad_norm": 0.043199457228183746,
      "learning_rate": 2.540827874094462e-06,
      "loss": 0.0002,
      "step": 3352
    },
    {
      "epoch": 53.65,
      "grad_norm": 0.05754075571894646,
      "learning_rate": 2.5380372068869152e-06,
      "loss": 0.0004,
      "step": 3353
    },
    {
      "epoch": 53.66,
      "grad_norm": 0.47254782915115356,
      "learning_rate": 2.5352475516992758e-06,
      "loss": 0.0034,
      "step": 3354
    },
    {
      "epoch": 53.68,
      "grad_norm": 0.004621056839823723,
      "learning_rate": 2.532458909678266e-06,
      "loss": 0.0001,
      "step": 3355
    },
    {
      "epoch": 53.7,
      "grad_norm": 0.45237067341804504,
      "learning_rate": 2.529671281970192e-06,
      "loss": 0.0026,
      "step": 3356
    },
    {
      "epoch": 53.71,
      "grad_norm": 0.04203035309910774,
      "learning_rate": 2.5268846697209407e-06,
      "loss": 0.0004,
      "step": 3357
    },
    {
      "epoch": 53.73,
      "grad_norm": 1.1190279722213745,
      "learning_rate": 2.5240990740759843e-06,
      "loss": 0.0215,
      "step": 3358
    },
    {
      "epoch": 53.74,
      "grad_norm": 0.6301279664039612,
      "learning_rate": 2.5213144961803778e-06,
      "loss": 0.009,
      "step": 3359
    },
    {
      "epoch": 53.76,
      "grad_norm": 0.004092485643923283,
      "learning_rate": 2.5185309371787515e-06,
      "loss": 0.0001,
      "step": 3360
    },
    {
      "epoch": 53.78,
      "grad_norm": 0.0638030394911766,
      "learning_rate": 2.5157483982153237e-06,
      "loss": 0.0005,
      "step": 3361
    },
    {
      "epoch": 53.79,
      "grad_norm": 0.515744686126709,
      "learning_rate": 2.512966880433891e-06,
      "loss": 0.0061,
      "step": 3362
    },
    {
      "epoch": 53.81,
      "grad_norm": 0.004689019173383713,
      "learning_rate": 2.5101863849778304e-06,
      "loss": 0.0001,
      "step": 3363
    },
    {
      "epoch": 53.82,
      "grad_norm": 0.7103774547576904,
      "learning_rate": 2.507406912990098e-06,
      "loss": 0.0076,
      "step": 3364
    },
    {
      "epoch": 53.84,
      "grad_norm": 0.7540913224220276,
      "learning_rate": 2.50462846561323e-06,
      "loss": 0.0135,
      "step": 3365
    },
    {
      "epoch": 53.86,
      "grad_norm": 0.0045778462663292885,
      "learning_rate": 2.501851043989343e-06,
      "loss": 0.0001,
      "step": 3366
    },
    {
      "epoch": 53.87,
      "grad_norm": 0.5754638314247131,
      "learning_rate": 2.499074649260127e-06,
      "loss": 0.0086,
      "step": 3367
    },
    {
      "epoch": 53.89,
      "grad_norm": 1.073836326599121,
      "learning_rate": 2.4962992825668546e-06,
      "loss": 0.0086,
      "step": 3368
    },
    {
      "epoch": 53.9,
      "grad_norm": 0.004301951266825199,
      "learning_rate": 2.493524945050376e-06,
      "loss": 0.0001,
      "step": 3369
    },
    {
      "epoch": 53.92,
      "grad_norm": 0.9102852940559387,
      "learning_rate": 2.4907516378511137e-06,
      "loss": 0.0101,
      "step": 3370
    },
    {
      "epoch": 53.94,
      "grad_norm": 0.0040984549559652805,
      "learning_rate": 2.48797936210907e-06,
      "loss": 0.0001,
      "step": 3371
    },
    {
      "epoch": 53.95,
      "grad_norm": 0.696053683757782,
      "learning_rate": 2.4852081189638227e-06,
      "loss": 0.0043,
      "step": 3372
    },
    {
      "epoch": 53.97,
      "grad_norm": 0.004983814433217049,
      "learning_rate": 2.4824379095545257e-06,
      "loss": 0.0001,
      "step": 3373
    },
    {
      "epoch": 53.98,
      "grad_norm": 0.7275201082229614,
      "learning_rate": 2.4796687350199077e-06,
      "loss": 0.0057,
      "step": 3374
    },
    {
      "epoch": 54.0,
      "grad_norm": 0.003590475767850876,
      "learning_rate": 2.4769005964982718e-06,
      "loss": 0.0001,
      "step": 3375
    },
    {
      "epoch": 54.02,
      "grad_norm": 0.49515190720558167,
      "learning_rate": 2.4741334951274948e-06,
      "loss": 0.0027,
      "step": 3376
    },
    {
      "epoch": 54.03,
      "grad_norm": 0.864362359046936,
      "learning_rate": 2.4713674320450276e-06,
      "loss": 0.0087,
      "step": 3377
    },
    {
      "epoch": 54.05,
      "grad_norm": 1.055056095123291,
      "learning_rate": 2.468602408387894e-06,
      "loss": 0.0144,
      "step": 3378
    },
    {
      "epoch": 54.06,
      "grad_norm": 0.0034542439971119165,
      "learning_rate": 2.465838425292693e-06,
      "loss": 0.0001,
      "step": 3379
    },
    {
      "epoch": 54.08,
      "grad_norm": 0.7950597405433655,
      "learning_rate": 2.46307548389559e-06,
      "loss": 0.0107,
      "step": 3380
    },
    {
      "epoch": 54.1,
      "grad_norm": 0.0038838961627334356,
      "learning_rate": 2.4603135853323275e-06,
      "loss": 0.0001,
      "step": 3381
    },
    {
      "epoch": 54.11,
      "grad_norm": 0.06423671543598175,
      "learning_rate": 2.4575527307382174e-06,
      "loss": 0.0003,
      "step": 3382
    },
    {
      "epoch": 54.13,
      "grad_norm": 0.34179797768592834,
      "learning_rate": 2.4547929212481436e-06,
      "loss": 0.0024,
      "step": 3383
    },
    {
      "epoch": 54.14,
      "grad_norm": 0.6847120523452759,
      "learning_rate": 2.452034157996559e-06,
      "loss": 0.0056,
      "step": 3384
    },
    {
      "epoch": 54.16,
      "grad_norm": 0.371444433927536,
      "learning_rate": 2.4492764421174863e-06,
      "loss": 0.0024,
      "step": 3385
    },
    {
      "epoch": 54.18,
      "grad_norm": 0.003897068090736866,
      "learning_rate": 2.4465197747445197e-06,
      "loss": 0.0001,
      "step": 3386
    },
    {
      "epoch": 54.19,
      "grad_norm": 0.0035027582198381424,
      "learning_rate": 2.44376415701082e-06,
      "loss": 0.0001,
      "step": 3387
    },
    {
      "epoch": 54.21,
      "grad_norm": 0.004479003604501486,
      "learning_rate": 2.441009590049118e-06,
      "loss": 0.0001,
      "step": 3388
    },
    {
      "epoch": 54.22,
      "grad_norm": 0.007530575152486563,
      "learning_rate": 2.438256074991712e-06,
      "loss": 0.0002,
      "step": 3389
    },
    {
      "epoch": 54.24,
      "grad_norm": 0.7280623316764832,
      "learning_rate": 2.43550361297047e-06,
      "loss": 0.0096,
      "step": 3390
    },
    {
      "epoch": 54.26,
      "grad_norm": 0.003728158539161086,
      "learning_rate": 2.432752205116821e-06,
      "loss": 0.0001,
      "step": 3391
    },
    {
      "epoch": 54.27,
      "grad_norm": 0.005546885076910257,
      "learning_rate": 2.430001852561769e-06,
      "loss": 0.0002,
      "step": 3392
    },
    {
      "epoch": 54.29,
      "grad_norm": 0.0033502059523016214,
      "learning_rate": 2.4272525564358767e-06,
      "loss": 0.0001,
      "step": 3393
    },
    {
      "epoch": 54.3,
      "grad_norm": 1.1573394536972046,
      "learning_rate": 2.4245043178692766e-06,
      "loss": 0.0185,
      "step": 3394
    },
    {
      "epoch": 54.32,
      "grad_norm": 0.7245527505874634,
      "learning_rate": 2.4217571379916673e-06,
      "loss": 0.0071,
      "step": 3395
    },
    {
      "epoch": 54.34,
      "grad_norm": 0.18728838860988617,
      "learning_rate": 2.419011017932309e-06,
      "loss": 0.001,
      "step": 3396
    },
    {
      "epoch": 54.35,
      "grad_norm": 0.004717971198260784,
      "learning_rate": 2.4162659588200287e-06,
      "loss": 0.0002,
      "step": 3397
    },
    {
      "epoch": 54.37,
      "grad_norm": 0.00466098403558135,
      "learning_rate": 2.4135219617832173e-06,
      "loss": 0.0001,
      "step": 3398
    },
    {
      "epoch": 54.38,
      "grad_norm": 0.004057026468217373,
      "learning_rate": 2.4107790279498273e-06,
      "loss": 0.0001,
      "step": 3399
    },
    {
      "epoch": 54.4,
      "grad_norm": 0.6077424883842468,
      "learning_rate": 2.408037158447375e-06,
      "loss": 0.007,
      "step": 3400
    },
    {
      "epoch": 54.42,
      "grad_norm": 0.0047861123457551,
      "learning_rate": 2.4052963544029405e-06,
      "loss": 0.0001,
      "step": 3401
    },
    {
      "epoch": 54.43,
      "grad_norm": 0.005983517039567232,
      "learning_rate": 2.402556616943166e-06,
      "loss": 0.0002,
      "step": 3402
    },
    {
      "epoch": 54.45,
      "grad_norm": 0.810875654220581,
      "learning_rate": 2.39981794719425e-06,
      "loss": 0.0122,
      "step": 3403
    },
    {
      "epoch": 54.46,
      "grad_norm": 0.7833091020584106,
      "learning_rate": 2.3970803462819586e-06,
      "loss": 0.0046,
      "step": 3404
    },
    {
      "epoch": 54.48,
      "grad_norm": 0.02161520905792713,
      "learning_rate": 2.394343815331616e-06,
      "loss": 0.0002,
      "step": 3405
    },
    {
      "epoch": 54.5,
      "grad_norm": 1.556854009628296,
      "learning_rate": 2.3916083554681064e-06,
      "loss": 0.0294,
      "step": 3406
    },
    {
      "epoch": 54.51,
      "grad_norm": 0.004964303225278854,
      "learning_rate": 2.3888739678158746e-06,
      "loss": 0.0001,
      "step": 3407
    },
    {
      "epoch": 54.53,
      "grad_norm": 0.005300778429955244,
      "learning_rate": 2.386140653498924e-06,
      "loss": 0.0001,
      "step": 3408
    },
    {
      "epoch": 54.54,
      "grad_norm": 0.004449567291885614,
      "learning_rate": 2.3834084136408163e-06,
      "loss": 0.0001,
      "step": 3409
    },
    {
      "epoch": 54.56,
      "grad_norm": 0.004021500702947378,
      "learning_rate": 2.3806772493646725e-06,
      "loss": 0.0001,
      "step": 3410
    },
    {
      "epoch": 54.58,
      "grad_norm": 0.8107782602310181,
      "learning_rate": 2.377947161793171e-06,
      "loss": 0.0115,
      "step": 3411
    },
    {
      "epoch": 54.59,
      "grad_norm": 0.3297312259674072,
      "learning_rate": 2.3752181520485497e-06,
      "loss": 0.0018,
      "step": 3412
    },
    {
      "epoch": 54.61,
      "grad_norm": 0.1799500733613968,
      "learning_rate": 2.3724902212525972e-06,
      "loss": 0.001,
      "step": 3413
    },
    {
      "epoch": 54.62,
      "grad_norm": 0.04630643129348755,
      "learning_rate": 2.3697633705266652e-06,
      "loss": 0.0003,
      "step": 3414
    },
    {
      "epoch": 54.64,
      "grad_norm": 0.05226832255721092,
      "learning_rate": 2.3670376009916596e-06,
      "loss": 0.0004,
      "step": 3415
    },
    {
      "epoch": 54.66,
      "grad_norm": 0.02263357676565647,
      "learning_rate": 2.36431291376804e-06,
      "loss": 0.0002,
      "step": 3416
    },
    {
      "epoch": 54.67,
      "grad_norm": 0.8257670402526855,
      "learning_rate": 2.3615893099758227e-06,
      "loss": 0.0076,
      "step": 3417
    },
    {
      "epoch": 54.69,
      "grad_norm": 0.003999995067715645,
      "learning_rate": 2.3588667907345787e-06,
      "loss": 0.0001,
      "step": 3418
    },
    {
      "epoch": 54.7,
      "grad_norm": 0.004287975374609232,
      "learning_rate": 2.3561453571634323e-06,
      "loss": 0.0001,
      "step": 3419
    },
    {
      "epoch": 54.72,
      "grad_norm": 0.5697367787361145,
      "learning_rate": 2.353425010381063e-06,
      "loss": 0.0038,
      "step": 3420
    },
    {
      "epoch": 54.74,
      "grad_norm": 0.7807684540748596,
      "learning_rate": 2.350705751505702e-06,
      "loss": 0.0113,
      "step": 3421
    },
    {
      "epoch": 54.75,
      "grad_norm": 0.0038954713381826878,
      "learning_rate": 2.3479875816551338e-06,
      "loss": 0.0001,
      "step": 3422
    },
    {
      "epoch": 54.77,
      "grad_norm": 1.0225937366485596,
      "learning_rate": 2.3452705019466977e-06,
      "loss": 0.0094,
      "step": 3423
    },
    {
      "epoch": 54.78,
      "grad_norm": 0.005163569003343582,
      "learning_rate": 2.342554513497278e-06,
      "loss": 0.0002,
      "step": 3424
    },
    {
      "epoch": 54.8,
      "grad_norm": 0.6525600552558899,
      "learning_rate": 2.339839617423318e-06,
      "loss": 0.0066,
      "step": 3425
    },
    {
      "epoch": 54.82,
      "grad_norm": 0.004400779493153095,
      "learning_rate": 2.3371258148408075e-06,
      "loss": 0.0001,
      "step": 3426
    },
    {
      "epoch": 54.83,
      "grad_norm": 0.0036569801159203053,
      "learning_rate": 2.3344131068652894e-06,
      "loss": 0.0001,
      "step": 3427
    },
    {
      "epoch": 54.85,
      "grad_norm": 0.6982718110084534,
      "learning_rate": 2.331701494611855e-06,
      "loss": 0.0109,
      "step": 3428
    },
    {
      "epoch": 54.86,
      "grad_norm": 0.004642462357878685,
      "learning_rate": 2.3289909791951454e-06,
      "loss": 0.0001,
      "step": 3429
    },
    {
      "epoch": 54.88,
      "grad_norm": 0.6297886371612549,
      "learning_rate": 2.3262815617293517e-06,
      "loss": 0.0071,
      "step": 3430
    },
    {
      "epoch": 54.9,
      "grad_norm": 0.6890516877174377,
      "learning_rate": 2.3235732433282127e-06,
      "loss": 0.0118,
      "step": 3431
    },
    {
      "epoch": 54.91,
      "grad_norm": 0.8953027129173279,
      "learning_rate": 2.320866025105016e-06,
      "loss": 0.0076,
      "step": 3432
    },
    {
      "epoch": 54.93,
      "grad_norm": 0.0050294119864702225,
      "learning_rate": 2.3181599081725986e-06,
      "loss": 0.0002,
      "step": 3433
    },
    {
      "epoch": 54.94,
      "grad_norm": 1.0757697820663452,
      "learning_rate": 2.3154548936433387e-06,
      "loss": 0.0124,
      "step": 3434
    },
    {
      "epoch": 54.96,
      "grad_norm": 0.0045224823988974094,
      "learning_rate": 2.31275098262917e-06,
      "loss": 0.0001,
      "step": 3435
    },
    {
      "epoch": 54.98,
      "grad_norm": 0.004795428831130266,
      "learning_rate": 2.3100481762415642e-06,
      "loss": 0.0001,
      "step": 3436
    },
    {
      "epoch": 54.99,
      "grad_norm": 0.004791565239429474,
      "learning_rate": 2.307346475591545e-06,
      "loss": 0.0001,
      "step": 3437
    },
    {
      "epoch": 55.01,
      "grad_norm": 1.2955011129379272,
      "learning_rate": 2.304645881789679e-06,
      "loss": 0.0241,
      "step": 3438
    },
    {
      "epoch": 55.02,
      "grad_norm": 0.003978666849434376,
      "learning_rate": 2.3019463959460784e-06,
      "loss": 0.0001,
      "step": 3439
    },
    {
      "epoch": 55.04,
      "grad_norm": 0.004354532808065414,
      "learning_rate": 2.2992480191704003e-06,
      "loss": 0.0001,
      "step": 3440
    },
    {
      "epoch": 55.06,
      "grad_norm": 0.5483556389808655,
      "learning_rate": 2.2965507525718457e-06,
      "loss": 0.0034,
      "step": 3441
    },
    {
      "epoch": 55.07,
      "grad_norm": 0.6737673282623291,
      "learning_rate": 2.2938545972591575e-06,
      "loss": 0.0063,
      "step": 3442
    },
    {
      "epoch": 55.09,
      "grad_norm": 0.436413049697876,
      "learning_rate": 2.291159554340625e-06,
      "loss": 0.0025,
      "step": 3443
    },
    {
      "epoch": 55.1,
      "grad_norm": 0.005511073861271143,
      "learning_rate": 2.288465624924078e-06,
      "loss": 0.0002,
      "step": 3444
    },
    {
      "epoch": 55.12,
      "grad_norm": 0.004715577699244022,
      "learning_rate": 2.28577281011689e-06,
      "loss": 0.0001,
      "step": 3445
    },
    {
      "epoch": 55.14,
      "grad_norm": 0.9586828947067261,
      "learning_rate": 2.283081111025973e-06,
      "loss": 0.0134,
      "step": 3446
    },
    {
      "epoch": 55.15,
      "grad_norm": 0.5929465889930725,
      "learning_rate": 2.280390528757785e-06,
      "loss": 0.0052,
      "step": 3447
    },
    {
      "epoch": 55.17,
      "grad_norm": 0.8407828211784363,
      "learning_rate": 2.277701064418321e-06,
      "loss": 0.0126,
      "step": 3448
    },
    {
      "epoch": 55.18,
      "grad_norm": 0.6545264720916748,
      "learning_rate": 2.2750127191131195e-06,
      "loss": 0.0056,
      "step": 3449
    },
    {
      "epoch": 55.2,
      "grad_norm": 0.004333558026701212,
      "learning_rate": 2.272325493947257e-06,
      "loss": 0.0001,
      "step": 3450
    },
    {
      "epoch": 55.22,
      "grad_norm": 0.47734832763671875,
      "learning_rate": 2.2696393900253515e-06,
      "loss": 0.0029,
      "step": 3451
    },
    {
      "epoch": 55.23,
      "grad_norm": 0.0037210953887552023,
      "learning_rate": 2.2669544084515578e-06,
      "loss": 0.0001,
      "step": 3452
    },
    {
      "epoch": 55.25,
      "grad_norm": 0.004578134510666132,
      "learning_rate": 2.2642705503295705e-06,
      "loss": 0.0001,
      "step": 3453
    },
    {
      "epoch": 55.26,
      "grad_norm": 0.004766917787492275,
      "learning_rate": 2.2615878167626222e-06,
      "loss": 0.0001,
      "step": 3454
    },
    {
      "epoch": 55.28,
      "grad_norm": 0.049730636179447174,
      "learning_rate": 2.2589062088534837e-06,
      "loss": 0.0003,
      "step": 3455
    },
    {
      "epoch": 55.3,
      "grad_norm": 0.0045636375434696674,
      "learning_rate": 2.2562257277044645e-06,
      "loss": 0.0001,
      "step": 3456
    },
    {
      "epoch": 55.31,
      "grad_norm": 0.0033793163020163774,
      "learning_rate": 2.253546374417405e-06,
      "loss": 0.0001,
      "step": 3457
    },
    {
      "epoch": 55.33,
      "grad_norm": 0.6654516458511353,
      "learning_rate": 2.2508681500936893e-06,
      "loss": 0.0093,
      "step": 3458
    },
    {
      "epoch": 55.34,
      "grad_norm": 0.005277967546135187,
      "learning_rate": 2.2481910558342324e-06,
      "loss": 0.0002,
      "step": 3459
    },
    {
      "epoch": 55.36,
      "grad_norm": 0.9052342772483826,
      "learning_rate": 2.245515092739488e-06,
      "loss": 0.0177,
      "step": 3460
    },
    {
      "epoch": 55.38,
      "grad_norm": 0.7494279146194458,
      "learning_rate": 2.2428402619094425e-06,
      "loss": 0.0046,
      "step": 3461
    },
    {
      "epoch": 55.39,
      "grad_norm": 0.1333409547805786,
      "learning_rate": 2.2401665644436187e-06,
      "loss": 0.0018,
      "step": 3462
    },
    {
      "epoch": 55.41,
      "grad_norm": 0.6596691012382507,
      "learning_rate": 2.2374940014410724e-06,
      "loss": 0.0105,
      "step": 3463
    },
    {
      "epoch": 55.42,
      "grad_norm": 0.1568683236837387,
      "learning_rate": 2.2348225740003927e-06,
      "loss": 0.0008,
      "step": 3464
    },
    {
      "epoch": 55.44,
      "grad_norm": 0.28202784061431885,
      "learning_rate": 2.2321522832197036e-06,
      "loss": 0.002,
      "step": 3465
    },
    {
      "epoch": 55.46,
      "grad_norm": 0.8113627433776855,
      "learning_rate": 2.229483130196661e-06,
      "loss": 0.0053,
      "step": 3466
    },
    {
      "epoch": 55.47,
      "grad_norm": 0.0613020658493042,
      "learning_rate": 2.2268151160284508e-06,
      "loss": 0.0003,
      "step": 3467
    },
    {
      "epoch": 55.49,
      "grad_norm": 0.327464759349823,
      "learning_rate": 2.224148241811794e-06,
      "loss": 0.0018,
      "step": 3468
    },
    {
      "epoch": 55.5,
      "grad_norm": 0.003366413526237011,
      "learning_rate": 2.2214825086429415e-06,
      "loss": 0.0001,
      "step": 3469
    },
    {
      "epoch": 55.52,
      "grad_norm": 0.5670442581176758,
      "learning_rate": 2.2188179176176767e-06,
      "loss": 0.0046,
      "step": 3470
    },
    {
      "epoch": 55.54,
      "grad_norm": 0.003841820638626814,
      "learning_rate": 2.2161544698313107e-06,
      "loss": 0.0001,
      "step": 3471
    },
    {
      "epoch": 55.55,
      "grad_norm": 0.004194473382085562,
      "learning_rate": 2.2134921663786875e-06,
      "loss": 0.0001,
      "step": 3472
    },
    {
      "epoch": 55.57,
      "grad_norm": 0.005761559586971998,
      "learning_rate": 2.210831008354179e-06,
      "loss": 0.0001,
      "step": 3473
    },
    {
      "epoch": 55.58,
      "grad_norm": 0.7309333086013794,
      "learning_rate": 2.2081709968516867e-06,
      "loss": 0.0082,
      "step": 3474
    },
    {
      "epoch": 55.6,
      "grad_norm": 0.004532385617494583,
      "learning_rate": 2.2055121329646416e-06,
      "loss": 0.0001,
      "step": 3475
    },
    {
      "epoch": 55.62,
      "grad_norm": 0.004590742290019989,
      "learning_rate": 2.2028544177860028e-06,
      "loss": 0.0001,
      "step": 3476
    },
    {
      "epoch": 55.63,
      "grad_norm": 0.5970122218132019,
      "learning_rate": 2.200197852408254e-06,
      "loss": 0.0059,
      "step": 3477
    },
    {
      "epoch": 55.65,
      "grad_norm": 0.7202723026275635,
      "learning_rate": 2.19754243792341e-06,
      "loss": 0.0049,
      "step": 3478
    },
    {
      "epoch": 55.66,
      "grad_norm": 1.5003151893615723,
      "learning_rate": 2.1948881754230116e-06,
      "loss": 0.025,
      "step": 3479
    },
    {
      "epoch": 55.68,
      "grad_norm": 0.8464963436126709,
      "learning_rate": 2.1922350659981262e-06,
      "loss": 0.0058,
      "step": 3480
    },
    {
      "epoch": 55.7,
      "grad_norm": 0.004881936591118574,
      "learning_rate": 2.1895831107393485e-06,
      "loss": 0.0001,
      "step": 3481
    },
    {
      "epoch": 55.71,
      "grad_norm": 0.0035482326056808233,
      "learning_rate": 2.1869323107367936e-06,
      "loss": 0.0001,
      "step": 3482
    },
    {
      "epoch": 55.73,
      "grad_norm": 0.0038800816982984543,
      "learning_rate": 2.1842826670801063e-06,
      "loss": 0.0001,
      "step": 3483
    },
    {
      "epoch": 55.74,
      "grad_norm": 0.0050537860952317715,
      "learning_rate": 2.1816341808584564e-06,
      "loss": 0.0001,
      "step": 3484
    },
    {
      "epoch": 55.76,
      "grad_norm": 0.004909766372293234,
      "learning_rate": 2.178986853160535e-06,
      "loss": 0.0001,
      "step": 3485
    },
    {
      "epoch": 55.78,
      "grad_norm": 0.2216210514307022,
      "learning_rate": 2.1763406850745596e-06,
      "loss": 0.0016,
      "step": 3486
    },
    {
      "epoch": 55.79,
      "grad_norm": 0.004050006624311209,
      "learning_rate": 2.1736956776882697e-06,
      "loss": 0.0001,
      "step": 3487
    },
    {
      "epoch": 55.81,
      "grad_norm": 0.4265989363193512,
      "learning_rate": 2.171051832088928e-06,
      "loss": 0.0024,
      "step": 3488
    },
    {
      "epoch": 55.82,
      "grad_norm": 0.0035277409479022026,
      "learning_rate": 2.1684091493633207e-06,
      "loss": 0.0001,
      "step": 3489
    },
    {
      "epoch": 55.84,
      "grad_norm": 0.7035301327705383,
      "learning_rate": 2.165767630597752e-06,
      "loss": 0.0048,
      "step": 3490
    },
    {
      "epoch": 55.86,
      "grad_norm": 0.9119212031364441,
      "learning_rate": 2.163127276878052e-06,
      "loss": 0.013,
      "step": 3491
    },
    {
      "epoch": 55.87,
      "grad_norm": 0.004582577385008335,
      "learning_rate": 2.1604880892895707e-06,
      "loss": 0.0001,
      "step": 3492
    },
    {
      "epoch": 55.89,
      "grad_norm": 0.6916447281837463,
      "learning_rate": 2.1578500689171777e-06,
      "loss": 0.0042,
      "step": 3493
    },
    {
      "epoch": 55.9,
      "grad_norm": 0.564361572265625,
      "learning_rate": 2.1552132168452646e-06,
      "loss": 0.0037,
      "step": 3494
    },
    {
      "epoch": 55.92,
      "grad_norm": 0.6275722980499268,
      "learning_rate": 2.1525775341577404e-06,
      "loss": 0.0078,
      "step": 3495
    },
    {
      "epoch": 55.94,
      "grad_norm": 0.7401648163795471,
      "learning_rate": 2.1499430219380357e-06,
      "loss": 0.0049,
      "step": 3496
    },
    {
      "epoch": 55.95,
      "grad_norm": 0.004833094775676727,
      "learning_rate": 2.1473096812690986e-06,
      "loss": 0.0001,
      "step": 3497
    },
    {
      "epoch": 55.97,
      "grad_norm": 0.5857310891151428,
      "learning_rate": 2.1446775132333957e-06,
      "loss": 0.0036,
      "step": 3498
    },
    {
      "epoch": 55.98,
      "grad_norm": 0.9852180480957031,
      "learning_rate": 2.1420465189129147e-06,
      "loss": 0.0115,
      "step": 3499
    },
    {
      "epoch": 56.0,
      "grad_norm": 0.003373724175617099,
      "learning_rate": 2.139416699389153e-06,
      "loss": 0.0001,
      "step": 3500
    },
    {
      "epoch": 56.02,
      "grad_norm": 0.004720491357147694,
      "learning_rate": 2.1367880557431325e-06,
      "loss": 0.0001,
      "step": 3501
    },
    {
      "epoch": 56.03,
      "grad_norm": 0.02594904974102974,
      "learning_rate": 2.1341605890553895e-06,
      "loss": 0.0005,
      "step": 3502
    },
    {
      "epoch": 56.05,
      "grad_norm": 0.005323620978742838,
      "learning_rate": 2.1315343004059763e-06,
      "loss": 0.0002,
      "step": 3503
    },
    {
      "epoch": 56.06,
      "grad_norm": 0.8946878910064697,
      "learning_rate": 2.128909190874461e-06,
      "loss": 0.0129,
      "step": 3504
    },
    {
      "epoch": 56.08,
      "grad_norm": 0.0035540866665542126,
      "learning_rate": 2.126285261539926e-06,
      "loss": 0.0001,
      "step": 3505
    },
    {
      "epoch": 56.1,
      "grad_norm": 0.7151337265968323,
      "learning_rate": 2.1236625134809707e-06,
      "loss": 0.0102,
      "step": 3506
    },
    {
      "epoch": 56.11,
      "grad_norm": 0.6924718022346497,
      "learning_rate": 2.121040947775707e-06,
      "loss": 0.0073,
      "step": 3507
    },
    {
      "epoch": 56.13,
      "grad_norm": 0.6542638540267944,
      "learning_rate": 2.118420565501762e-06,
      "loss": 0.0056,
      "step": 3508
    },
    {
      "epoch": 56.14,
      "grad_norm": 0.0579165555536747,
      "learning_rate": 2.115801367736276e-06,
      "loss": 0.0003,
      "step": 3509
    },
    {
      "epoch": 56.16,
      "grad_norm": 0.003963660914450884,
      "learning_rate": 2.1131833555559037e-06,
      "loss": 0.0001,
      "step": 3510
    },
    {
      "epoch": 56.18,
      "grad_norm": 0.9305896759033203,
      "learning_rate": 2.110566530036808e-06,
      "loss": 0.0066,
      "step": 3511
    },
    {
      "epoch": 56.19,
      "grad_norm": 0.0037969083059579134,
      "learning_rate": 2.107950892254668e-06,
      "loss": 0.0001,
      "step": 3512
    },
    {
      "epoch": 56.21,
      "grad_norm": 0.5692081451416016,
      "learning_rate": 2.1053364432846735e-06,
      "loss": 0.0048,
      "step": 3513
    },
    {
      "epoch": 56.22,
      "grad_norm": 0.020377272740006447,
      "learning_rate": 2.102723184201526e-06,
      "loss": 0.0003,
      "step": 3514
    },
    {
      "epoch": 56.24,
      "grad_norm": 0.297256201505661,
      "learning_rate": 2.1001111160794387e-06,
      "loss": 0.0023,
      "step": 3515
    },
    {
      "epoch": 56.26,
      "grad_norm": 0.8390339016914368,
      "learning_rate": 2.097500239992132e-06,
      "loss": 0.0082,
      "step": 3516
    },
    {
      "epoch": 56.27,
      "grad_norm": 0.7414215207099915,
      "learning_rate": 2.094890557012841e-06,
      "loss": 0.0042,
      "step": 3517
    },
    {
      "epoch": 56.29,
      "grad_norm": 1.1066391468048096,
      "learning_rate": 2.0922820682143057e-06,
      "loss": 0.0185,
      "step": 3518
    },
    {
      "epoch": 56.3,
      "grad_norm": 0.8536766767501831,
      "learning_rate": 2.0896747746687785e-06,
      "loss": 0.0118,
      "step": 3519
    },
    {
      "epoch": 56.32,
      "grad_norm": 1.0604355335235596,
      "learning_rate": 2.08706867744802e-06,
      "loss": 0.0074,
      "step": 3520
    },
    {
      "epoch": 56.34,
      "grad_norm": 0.04980824887752533,
      "learning_rate": 2.0844637776232954e-06,
      "loss": 0.0003,
      "step": 3521
    },
    {
      "epoch": 56.35,
      "grad_norm": 1.1761184930801392,
      "learning_rate": 2.081860076265383e-06,
      "loss": 0.0163,
      "step": 3522
    },
    {
      "epoch": 56.37,
      "grad_norm": 0.0035327633377164602,
      "learning_rate": 2.0792575744445654e-06,
      "loss": 0.0001,
      "step": 3523
    },
    {
      "epoch": 56.38,
      "grad_norm": 0.8763847351074219,
      "learning_rate": 2.0766562732306323e-06,
      "loss": 0.0119,
      "step": 3524
    },
    {
      "epoch": 56.4,
      "grad_norm": 0.004578402265906334,
      "learning_rate": 2.074056173692881e-06,
      "loss": 0.0001,
      "step": 3525
    },
    {
      "epoch": 56.42,
      "grad_norm": 0.7935625910758972,
      "learning_rate": 2.071457276900116e-06,
      "loss": 0.0057,
      "step": 3526
    },
    {
      "epoch": 56.43,
      "grad_norm": 0.6809918880462646,
      "learning_rate": 2.0688595839206425e-06,
      "loss": 0.0077,
      "step": 3527
    },
    {
      "epoch": 56.45,
      "grad_norm": 0.34075039625167847,
      "learning_rate": 2.0662630958222747e-06,
      "loss": 0.002,
      "step": 3528
    },
    {
      "epoch": 56.46,
      "grad_norm": 0.006320277228951454,
      "learning_rate": 2.0636678136723314e-06,
      "loss": 0.0002,
      "step": 3529
    },
    {
      "epoch": 56.48,
      "grad_norm": 0.29998254776000977,
      "learning_rate": 2.061073738537635e-06,
      "loss": 0.0018,
      "step": 3530
    },
    {
      "epoch": 56.5,
      "grad_norm": 0.8243132829666138,
      "learning_rate": 2.0584808714845118e-06,
      "loss": 0.0136,
      "step": 3531
    },
    {
      "epoch": 56.51,
      "grad_norm": 0.003696249797940254,
      "learning_rate": 2.0558892135787927e-06,
      "loss": 0.0001,
      "step": 3532
    },
    {
      "epoch": 56.53,
      "grad_norm": 0.06767396628856659,
      "learning_rate": 2.053298765885808e-06,
      "loss": 0.0005,
      "step": 3533
    },
    {
      "epoch": 56.54,
      "grad_norm": 0.00357925146818161,
      "learning_rate": 2.0507095294703932e-06,
      "loss": 0.0001,
      "step": 3534
    },
    {
      "epoch": 56.56,
      "grad_norm": 0.6330764889717102,
      "learning_rate": 2.0481215053968874e-06,
      "loss": 0.009,
      "step": 3535
    },
    {
      "epoch": 56.58,
      "grad_norm": 0.0034064725041389465,
      "learning_rate": 2.0455346947291277e-06,
      "loss": 0.0001,
      "step": 3536
    },
    {
      "epoch": 56.59,
      "grad_norm": 0.003448312636464834,
      "learning_rate": 2.0429490985304556e-06,
      "loss": 0.0001,
      "step": 3537
    },
    {
      "epoch": 56.61,
      "grad_norm": 0.819413423538208,
      "learning_rate": 2.040364717863711e-06,
      "loss": 0.0099,
      "step": 3538
    },
    {
      "epoch": 56.62,
      "grad_norm": 0.8932132124900818,
      "learning_rate": 2.037781553791236e-06,
      "loss": 0.0121,
      "step": 3539
    },
    {
      "epoch": 56.64,
      "grad_norm": 0.935501754283905,
      "learning_rate": 2.0351996073748713e-06,
      "loss": 0.006,
      "step": 3540
    },
    {
      "epoch": 56.66,
      "grad_norm": 0.005084410775452852,
      "learning_rate": 2.0326188796759583e-06,
      "loss": 0.0002,
      "step": 3541
    },
    {
      "epoch": 56.67,
      "grad_norm": 0.004514250438660383,
      "learning_rate": 2.0300393717553355e-06,
      "loss": 0.0001,
      "step": 3542
    },
    {
      "epoch": 56.69,
      "grad_norm": 0.0051665292121469975,
      "learning_rate": 2.027461084673344e-06,
      "loss": 0.0002,
      "step": 3543
    },
    {
      "epoch": 56.7,
      "grad_norm": 0.004444750025868416,
      "learning_rate": 2.0248840194898155e-06,
      "loss": 0.0001,
      "step": 3544
    },
    {
      "epoch": 56.72,
      "grad_norm": 0.004870413802564144,
      "learning_rate": 2.0223081772640867e-06,
      "loss": 0.0001,
      "step": 3545
    },
    {
      "epoch": 56.74,
      "grad_norm": 0.004702840466052294,
      "learning_rate": 2.019733559054989e-06,
      "loss": 0.0001,
      "step": 3546
    },
    {
      "epoch": 56.75,
      "grad_norm": 0.004370605107396841,
      "learning_rate": 2.0171601659208516e-06,
      "loss": 0.0001,
      "step": 3547
    },
    {
      "epoch": 56.77,
      "grad_norm": 0.25795507431030273,
      "learning_rate": 2.014587998919498e-06,
      "loss": 0.0013,
      "step": 3548
    },
    {
      "epoch": 56.78,
      "grad_norm": 0.004063498694449663,
      "learning_rate": 2.0120170591082484e-06,
      "loss": 0.0001,
      "step": 3549
    },
    {
      "epoch": 56.8,
      "grad_norm": 0.004823934752494097,
      "learning_rate": 2.00944734754392e-06,
      "loss": 0.0002,
      "step": 3550
    },
    {
      "epoch": 56.82,
      "grad_norm": 0.026265759021043777,
      "learning_rate": 2.006878865282824e-06,
      "loss": 0.0003,
      "step": 3551
    },
    {
      "epoch": 56.83,
      "grad_norm": 1.0559548139572144,
      "learning_rate": 2.0043116133807673e-06,
      "loss": 0.0083,
      "step": 3552
    },
    {
      "epoch": 56.85,
      "grad_norm": 0.40609267354011536,
      "learning_rate": 2.00174559289305e-06,
      "loss": 0.0032,
      "step": 3553
    },
    {
      "epoch": 56.86,
      "grad_norm": 0.8905984163284302,
      "learning_rate": 1.999180804874464e-06,
      "loss": 0.0094,
      "step": 3554
    },
    {
      "epoch": 56.88,
      "grad_norm": 0.005420620087534189,
      "learning_rate": 1.9966172503792986e-06,
      "loss": 0.0002,
      "step": 3555
    },
    {
      "epoch": 56.9,
      "grad_norm": 0.42154160141944885,
      "learning_rate": 1.9940549304613334e-06,
      "loss": 0.0043,
      "step": 3556
    },
    {
      "epoch": 56.91,
      "grad_norm": 0.003438649233430624,
      "learning_rate": 1.991493846173842e-06,
      "loss": 0.0001,
      "step": 3557
    },
    {
      "epoch": 56.93,
      "grad_norm": 0.6484870910644531,
      "learning_rate": 1.9889339985695894e-06,
      "loss": 0.0053,
      "step": 3558
    },
    {
      "epoch": 56.94,
      "grad_norm": 0.0036030407063663006,
      "learning_rate": 1.986375388700832e-06,
      "loss": 0.0001,
      "step": 3559
    },
    {
      "epoch": 56.96,
      "grad_norm": 0.0034779298584908247,
      "learning_rate": 1.983818017619318e-06,
      "loss": 0.0001,
      "step": 3560
    },
    {
      "epoch": 56.98,
      "grad_norm": 0.2654409408569336,
      "learning_rate": 1.981261886376285e-06,
      "loss": 0.0015,
      "step": 3561
    },
    {
      "epoch": 56.99,
      "grad_norm": 0.00538274273276329,
      "learning_rate": 1.9787069960224635e-06,
      "loss": 0.0002,
      "step": 3562
    },
    {
      "epoch": 57.01,
      "grad_norm": 0.004117123316973448,
      "learning_rate": 1.9761533476080736e-06,
      "loss": 0.0001,
      "step": 3563
    },
    {
      "epoch": 57.02,
      "grad_norm": 0.0037429870571941137,
      "learning_rate": 1.9736009421828196e-06,
      "loss": 0.0001,
      "step": 3564
    },
    {
      "epoch": 57.04,
      "grad_norm": 0.5745965242385864,
      "learning_rate": 1.971049780795901e-06,
      "loss": 0.007,
      "step": 3565
    },
    {
      "epoch": 57.06,
      "grad_norm": 0.0040741367265582085,
      "learning_rate": 1.9684998644960045e-06,
      "loss": 0.0001,
      "step": 3566
    },
    {
      "epoch": 57.07,
      "grad_norm": 0.004080502782016993,
      "learning_rate": 1.9659511943313043e-06,
      "loss": 0.0001,
      "step": 3567
    },
    {
      "epoch": 57.09,
      "grad_norm": 0.004754436202347279,
      "learning_rate": 1.963403771349461e-06,
      "loss": 0.0001,
      "step": 3568
    },
    {
      "epoch": 57.1,
      "grad_norm": 0.0035430355928838253,
      "learning_rate": 1.960857596597626e-06,
      "loss": 0.0001,
      "step": 3569
    },
    {
      "epoch": 57.12,
      "grad_norm": 0.431374728679657,
      "learning_rate": 1.9583126711224342e-06,
      "loss": 0.0024,
      "step": 3570
    },
    {
      "epoch": 57.14,
      "grad_norm": 0.7623483538627625,
      "learning_rate": 1.9557689959700105e-06,
      "loss": 0.0109,
      "step": 3571
    },
    {
      "epoch": 57.15,
      "grad_norm": 0.18962417542934418,
      "learning_rate": 1.95322657218596e-06,
      "loss": 0.0012,
      "step": 3572
    },
    {
      "epoch": 57.17,
      "grad_norm": 0.004094387404620647,
      "learning_rate": 1.950685400815379e-06,
      "loss": 0.0001,
      "step": 3573
    },
    {
      "epoch": 57.18,
      "grad_norm": 0.0032137001398950815,
      "learning_rate": 1.9481454829028474e-06,
      "loss": 0.0001,
      "step": 3574
    },
    {
      "epoch": 57.2,
      "grad_norm": 0.715778112411499,
      "learning_rate": 1.945606819492429e-06,
      "loss": 0.0074,
      "step": 3575
    },
    {
      "epoch": 57.22,
      "grad_norm": 0.8120583891868591,
      "learning_rate": 1.9430694116276745e-06,
      "loss": 0.0052,
      "step": 3576
    },
    {
      "epoch": 57.23,
      "grad_norm": 0.004197884351015091,
      "learning_rate": 1.940533260351612e-06,
      "loss": 0.0001,
      "step": 3577
    },
    {
      "epoch": 57.25,
      "grad_norm": 0.0035177054814994335,
      "learning_rate": 1.937998366706761e-06,
      "loss": 0.0001,
      "step": 3578
    },
    {
      "epoch": 57.26,
      "grad_norm": 0.004523660987615585,
      "learning_rate": 1.9354647317351187e-06,
      "loss": 0.0001,
      "step": 3579
    },
    {
      "epoch": 57.28,
      "grad_norm": 0.003282822435721755,
      "learning_rate": 1.932932356478168e-06,
      "loss": 0.0001,
      "step": 3580
    },
    {
      "epoch": 57.3,
      "grad_norm": 0.3523296117782593,
      "learning_rate": 1.930401241976872e-06,
      "loss": 0.0031,
      "step": 3581
    },
    {
      "epoch": 57.31,
      "grad_norm": 0.8346202969551086,
      "learning_rate": 1.927871389271677e-06,
      "loss": 0.0109,
      "step": 3582
    },
    {
      "epoch": 57.33,
      "grad_norm": 0.4309738874435425,
      "learning_rate": 1.925342799402509e-06,
      "loss": 0.0037,
      "step": 3583
    },
    {
      "epoch": 57.34,
      "grad_norm": 0.9544217586517334,
      "learning_rate": 1.9228154734087766e-06,
      "loss": 0.0157,
      "step": 3584
    },
    {
      "epoch": 57.36,
      "grad_norm": 0.3369881510734558,
      "learning_rate": 1.9202894123293677e-06,
      "loss": 0.002,
      "step": 3585
    },
    {
      "epoch": 57.38,
      "grad_norm": 0.7279613614082336,
      "learning_rate": 1.9177646172026513e-06,
      "loss": 0.0041,
      "step": 3586
    },
    {
      "epoch": 57.39,
      "grad_norm": 0.0045485347509384155,
      "learning_rate": 1.915241089066474e-06,
      "loss": 0.0001,
      "step": 3587
    },
    {
      "epoch": 57.41,
      "grad_norm": 0.00361592136323452,
      "learning_rate": 1.912718828958163e-06,
      "loss": 0.0001,
      "step": 3588
    },
    {
      "epoch": 57.42,
      "grad_norm": 0.04958978295326233,
      "learning_rate": 1.9101978379145246e-06,
      "loss": 0.0003,
      "step": 3589
    },
    {
      "epoch": 57.44,
      "grad_norm": 0.9903340935707092,
      "learning_rate": 1.9076781169718426e-06,
      "loss": 0.0152,
      "step": 3590
    },
    {
      "epoch": 57.46,
      "grad_norm": 0.600546658039093,
      "learning_rate": 1.90515966716588e-06,
      "loss": 0.0041,
      "step": 3591
    },
    {
      "epoch": 57.47,
      "grad_norm": 0.0034252642653882504,
      "learning_rate": 1.9026424895318762e-06,
      "loss": 0.0001,
      "step": 3592
    },
    {
      "epoch": 57.49,
      "grad_norm": 0.5128521919250488,
      "learning_rate": 1.900126585104547e-06,
      "loss": 0.0044,
      "step": 3593
    },
    {
      "epoch": 57.5,
      "grad_norm": 0.004185599274933338,
      "learning_rate": 1.8976119549180865e-06,
      "loss": 0.0001,
      "step": 3594
    },
    {
      "epoch": 57.52,
      "grad_norm": 1.2081613540649414,
      "learning_rate": 1.895098600006164e-06,
      "loss": 0.0253,
      "step": 3595
    },
    {
      "epoch": 57.54,
      "grad_norm": 0.5984421968460083,
      "learning_rate": 1.892586521401924e-06,
      "loss": 0.0036,
      "step": 3596
    },
    {
      "epoch": 57.55,
      "grad_norm": 0.2291594296693802,
      "learning_rate": 1.8900757201379899e-06,
      "loss": 0.0013,
      "step": 3597
    },
    {
      "epoch": 57.57,
      "grad_norm": 0.0034992671571671963,
      "learning_rate": 1.8875661972464532e-06,
      "loss": 0.0001,
      "step": 3598
    },
    {
      "epoch": 57.58,
      "grad_norm": 0.00381073378957808,
      "learning_rate": 1.8850579537588865e-06,
      "loss": 0.0001,
      "step": 3599
    },
    {
      "epoch": 57.6,
      "grad_norm": 0.6676615476608276,
      "learning_rate": 1.8825509907063328e-06,
      "loss": 0.0083,
      "step": 3600
    },
    {
      "epoch": 57.62,
      "grad_norm": 0.7376383543014526,
      "learning_rate": 1.8800453091193104e-06,
      "loss": 0.006,
      "step": 3601
    },
    {
      "epoch": 57.63,
      "grad_norm": 0.2714240252971649,
      "learning_rate": 1.8775409100278108e-06,
      "loss": 0.0019,
      "step": 3602
    },
    {
      "epoch": 57.65,
      "grad_norm": 0.07201661914587021,
      "learning_rate": 1.8750377944612975e-06,
      "loss": 0.0003,
      "step": 3603
    },
    {
      "epoch": 57.66,
      "grad_norm": 0.6678610444068909,
      "learning_rate": 1.8725359634487068e-06,
      "loss": 0.0093,
      "step": 3604
    },
    {
      "epoch": 57.68,
      "grad_norm": 0.019620446488261223,
      "learning_rate": 1.8700354180184465e-06,
      "loss": 0.0002,
      "step": 3605
    },
    {
      "epoch": 57.7,
      "grad_norm": 0.7881647944450378,
      "learning_rate": 1.867536159198397e-06,
      "loss": 0.0047,
      "step": 3606
    },
    {
      "epoch": 57.71,
      "grad_norm": 0.005440900102257729,
      "learning_rate": 1.8650381880159108e-06,
      "loss": 0.0002,
      "step": 3607
    },
    {
      "epoch": 57.73,
      "grad_norm": 0.0035552612971514463,
      "learning_rate": 1.8625415054978058e-06,
      "loss": 0.0001,
      "step": 3608
    },
    {
      "epoch": 57.74,
      "grad_norm": 0.004815137013792992,
      "learning_rate": 1.8600461126703755e-06,
      "loss": 0.0002,
      "step": 3609
    },
    {
      "epoch": 57.76,
      "grad_norm": 0.004055338446050882,
      "learning_rate": 1.857552010559382e-06,
      "loss": 0.0001,
      "step": 3610
    },
    {
      "epoch": 57.78,
      "grad_norm": 0.6138587594032288,
      "learning_rate": 1.8550592001900565e-06,
      "loss": 0.0092,
      "step": 3611
    },
    {
      "epoch": 57.79,
      "grad_norm": 1.2139891386032104,
      "learning_rate": 1.8525676825870986e-06,
      "loss": 0.0085,
      "step": 3612
    },
    {
      "epoch": 57.81,
      "grad_norm": 0.004292681347578764,
      "learning_rate": 1.8500774587746777e-06,
      "loss": 0.0001,
      "step": 3613
    },
    {
      "epoch": 57.82,
      "grad_norm": 0.39485687017440796,
      "learning_rate": 1.8475885297764307e-06,
      "loss": 0.0023,
      "step": 3614
    },
    {
      "epoch": 57.84,
      "grad_norm": 0.8172933459281921,
      "learning_rate": 1.8451008966154622e-06,
      "loss": 0.0053,
      "step": 3615
    },
    {
      "epoch": 57.86,
      "grad_norm": 0.4764975607395172,
      "learning_rate": 1.8426145603143441e-06,
      "loss": 0.0032,
      "step": 3616
    },
    {
      "epoch": 57.87,
      "grad_norm": 0.4869478642940521,
      "learning_rate": 1.8401295218951165e-06,
      "loss": 0.0064,
      "step": 3617
    },
    {
      "epoch": 57.89,
      "grad_norm": 0.22243186831474304,
      "learning_rate": 1.8376457823792826e-06,
      "loss": 0.0014,
      "step": 3618
    },
    {
      "epoch": 57.9,
      "grad_norm": 0.003434988670051098,
      "learning_rate": 1.8351633427878163e-06,
      "loss": 0.0001,
      "step": 3619
    },
    {
      "epoch": 57.92,
      "grad_norm": 0.9790017008781433,
      "learning_rate": 1.8326822041411524e-06,
      "loss": 0.0133,
      "step": 3620
    },
    {
      "epoch": 57.94,
      "grad_norm": 0.0038498167414218187,
      "learning_rate": 1.8302023674591934e-06,
      "loss": 0.0001,
      "step": 3621
    },
    {
      "epoch": 57.95,
      "grad_norm": 0.07638315856456757,
      "learning_rate": 1.827723833761308e-06,
      "loss": 0.0006,
      "step": 3622
    },
    {
      "epoch": 57.97,
      "grad_norm": 0.8423414826393127,
      "learning_rate": 1.8252466040663275e-06,
      "loss": 0.0064,
      "step": 3623
    },
    {
      "epoch": 57.98,
      "grad_norm": 0.6256903409957886,
      "learning_rate": 1.8227706793925464e-06,
      "loss": 0.0056,
      "step": 3624
    },
    {
      "epoch": 58.0,
      "grad_norm": 0.8604100346565247,
      "learning_rate": 1.8202960607577246e-06,
      "loss": 0.0092,
      "step": 3625
    },
    {
      "epoch": 58.02,
      "grad_norm": 0.024470200762152672,
      "learning_rate": 1.817822749179085e-06,
      "loss": 0.0002,
      "step": 3626
    },
    {
      "epoch": 58.03,
      "grad_norm": 0.003138140309602022,
      "learning_rate": 1.8153507456733115e-06,
      "loss": 0.0001,
      "step": 3627
    },
    {
      "epoch": 58.05,
      "grad_norm": 0.7558830976486206,
      "learning_rate": 1.8128800512565514e-06,
      "loss": 0.0098,
      "step": 3628
    },
    {
      "epoch": 58.06,
      "grad_norm": 0.6326963901519775,
      "learning_rate": 1.8104106669444148e-06,
      "loss": 0.0041,
      "step": 3629
    },
    {
      "epoch": 58.08,
      "grad_norm": 0.7990353107452393,
      "learning_rate": 1.8079425937519729e-06,
      "loss": 0.0156,
      "step": 3630
    },
    {
      "epoch": 58.1,
      "grad_norm": 0.42658597230911255,
      "learning_rate": 1.8054758326937548e-06,
      "loss": 0.0025,
      "step": 3631
    },
    {
      "epoch": 58.11,
      "grad_norm": 1.0684804916381836,
      "learning_rate": 1.8030103847837538e-06,
      "loss": 0.0101,
      "step": 3632
    },
    {
      "epoch": 58.13,
      "grad_norm": 0.7635871171951294,
      "learning_rate": 1.8005462510354233e-06,
      "loss": 0.0132,
      "step": 3633
    },
    {
      "epoch": 58.14,
      "grad_norm": 1.3838096857070923,
      "learning_rate": 1.798083432461674e-06,
      "loss": 0.0177,
      "step": 3634
    },
    {
      "epoch": 58.16,
      "grad_norm": 0.004947398789227009,
      "learning_rate": 1.7956219300748796e-06,
      "loss": 0.0002,
      "step": 3635
    },
    {
      "epoch": 58.18,
      "grad_norm": 0.004188099410384893,
      "learning_rate": 1.79316174488687e-06,
      "loss": 0.0001,
      "step": 3636
    },
    {
      "epoch": 58.19,
      "grad_norm": 0.0038292824756354094,
      "learning_rate": 1.7907028779089335e-06,
      "loss": 0.0001,
      "step": 3637
    },
    {
      "epoch": 58.21,
      "grad_norm": 0.004199284594506025,
      "learning_rate": 1.7882453301518182e-06,
      "loss": 0.0001,
      "step": 3638
    },
    {
      "epoch": 58.22,
      "grad_norm": 0.003079627640545368,
      "learning_rate": 1.7857891026257295e-06,
      "loss": 0.0001,
      "step": 3639
    },
    {
      "epoch": 58.24,
      "grad_norm": 0.004089705646038055,
      "learning_rate": 1.7833341963403312e-06,
      "loss": 0.0001,
      "step": 3640
    },
    {
      "epoch": 58.26,
      "grad_norm": 0.004394833464175463,
      "learning_rate": 1.7808806123047385e-06,
      "loss": 0.0001,
      "step": 3641
    },
    {
      "epoch": 58.27,
      "grad_norm": 0.005097492597997189,
      "learning_rate": 1.7784283515275292e-06,
      "loss": 0.0002,
      "step": 3642
    },
    {
      "epoch": 58.29,
      "grad_norm": 0.7798806428909302,
      "learning_rate": 1.7759774150167352e-06,
      "loss": 0.0057,
      "step": 3643
    },
    {
      "epoch": 58.3,
      "grad_norm": 0.09996993839740753,
      "learning_rate": 1.7735278037798442e-06,
      "loss": 0.0013,
      "step": 3644
    },
    {
      "epoch": 58.32,
      "grad_norm": 0.0038095624186098576,
      "learning_rate": 1.771079518823799e-06,
      "loss": 0.0001,
      "step": 3645
    },
    {
      "epoch": 58.34,
      "grad_norm": 0.0035735538695007563,
      "learning_rate": 1.768632561154996e-06,
      "loss": 0.0001,
      "step": 3646
    },
    {
      "epoch": 58.35,
      "grad_norm": 0.6391451954841614,
      "learning_rate": 1.766186931779288e-06,
      "loss": 0.005,
      "step": 3647
    },
    {
      "epoch": 58.37,
      "grad_norm": 0.003950885031372309,
      "learning_rate": 1.7637426317019801e-06,
      "loss": 0.0001,
      "step": 3648
    },
    {
      "epoch": 58.38,
      "grad_norm": 0.9700881242752075,
      "learning_rate": 1.7612996619278322e-06,
      "loss": 0.013,
      "step": 3649
    },
    {
      "epoch": 58.4,
      "grad_norm": 0.003988615237176418,
      "learning_rate": 1.7588580234610592e-06,
      "loss": 0.0001,
      "step": 3650
    },
    {
      "epoch": 58.42,
      "grad_norm": 0.23136526346206665,
      "learning_rate": 1.7564177173053215e-06,
      "loss": 0.0014,
      "step": 3651
    },
    {
      "epoch": 58.43,
      "grad_norm": 0.003973982762545347,
      "learning_rate": 1.7539787444637402e-06,
      "loss": 0.0001,
      "step": 3652
    },
    {
      "epoch": 58.45,
      "grad_norm": 1.0114823579788208,
      "learning_rate": 1.7515411059388837e-06,
      "loss": 0.0068,
      "step": 3653
    },
    {
      "epoch": 58.46,
      "grad_norm": 0.004424118436872959,
      "learning_rate": 1.7491048027327739e-06,
      "loss": 0.0001,
      "step": 3654
    },
    {
      "epoch": 58.48,
      "grad_norm": 0.8270829916000366,
      "learning_rate": 1.7466698358468825e-06,
      "loss": 0.0095,
      "step": 3655
    },
    {
      "epoch": 58.5,
      "grad_norm": 0.17160668969154358,
      "learning_rate": 1.7442362062821323e-06,
      "loss": 0.0026,
      "step": 3656
    },
    {
      "epoch": 58.51,
      "grad_norm": 0.6044793725013733,
      "learning_rate": 1.7418039150388971e-06,
      "loss": 0.0042,
      "step": 3657
    },
    {
      "epoch": 58.53,
      "grad_norm": 0.0043237777426838875,
      "learning_rate": 1.7393729631169993e-06,
      "loss": 0.0001,
      "step": 3658
    },
    {
      "epoch": 58.54,
      "grad_norm": 0.8540436029434204,
      "learning_rate": 1.7369433515157118e-06,
      "loss": 0.0057,
      "step": 3659
    },
    {
      "epoch": 58.56,
      "grad_norm": 0.004357167519629002,
      "learning_rate": 1.7345150812337564e-06,
      "loss": 0.0001,
      "step": 3660
    },
    {
      "epoch": 58.58,
      "grad_norm": 1.0896086692810059,
      "learning_rate": 1.732088153269304e-06,
      "loss": 0.0218,
      "step": 3661
    },
    {
      "epoch": 58.59,
      "grad_norm": 0.0037572637666016817,
      "learning_rate": 1.7296625686199708e-06,
      "loss": 0.0001,
      "step": 3662
    },
    {
      "epoch": 58.61,
      "grad_norm": 0.39754340052604675,
      "learning_rate": 1.7272383282828254e-06,
      "loss": 0.0024,
      "step": 3663
    },
    {
      "epoch": 58.62,
      "grad_norm": 1.1279339790344238,
      "learning_rate": 1.7248154332543788e-06,
      "loss": 0.0207,
      "step": 3664
    },
    {
      "epoch": 58.64,
      "grad_norm": 0.004646367393434048,
      "learning_rate": 1.7223938845305932e-06,
      "loss": 0.0001,
      "step": 3665
    },
    {
      "epoch": 58.66,
      "grad_norm": 0.00368099519982934,
      "learning_rate": 1.7199736831068758e-06,
      "loss": 0.0001,
      "step": 3666
    },
    {
      "epoch": 58.67,
      "grad_norm": 0.003150631906464696,
      "learning_rate": 1.7175548299780791e-06,
      "loss": 0.0001,
      "step": 3667
    },
    {
      "epoch": 58.69,
      "grad_norm": 0.005062705837190151,
      "learning_rate": 1.715137326138504e-06,
      "loss": 0.0002,
      "step": 3668
    },
    {
      "epoch": 58.7,
      "grad_norm": 0.026081716641783714,
      "learning_rate": 1.7127211725818933e-06,
      "loss": 0.0002,
      "step": 3669
    },
    {
      "epoch": 58.72,
      "grad_norm": 0.004127523861825466,
      "learning_rate": 1.7103063703014372e-06,
      "loss": 0.0001,
      "step": 3670
    },
    {
      "epoch": 58.74,
      "grad_norm": 0.0037794343661516905,
      "learning_rate": 1.70789292028977e-06,
      "loss": 0.0001,
      "step": 3671
    },
    {
      "epoch": 58.75,
      "grad_norm": 1.3436720371246338,
      "learning_rate": 1.7054808235389696e-06,
      "loss": 0.0155,
      "step": 3672
    },
    {
      "epoch": 58.77,
      "grad_norm": 0.841858983039856,
      "learning_rate": 1.7030700810405592e-06,
      "loss": 0.0056,
      "step": 3673
    },
    {
      "epoch": 58.78,
      "grad_norm": 0.7246634364128113,
      "learning_rate": 1.7006606937855008e-06,
      "loss": 0.0067,
      "step": 3674
    },
    {
      "epoch": 58.8,
      "grad_norm": 0.004539859481155872,
      "learning_rate": 1.6982526627642043e-06,
      "loss": 0.0001,
      "step": 3675
    },
    {
      "epoch": 58.82,
      "grad_norm": 0.2828567326068878,
      "learning_rate": 1.6958459889665202e-06,
      "loss": 0.0025,
      "step": 3676
    },
    {
      "epoch": 58.83,
      "grad_norm": 0.4989824593067169,
      "learning_rate": 1.6934406733817417e-06,
      "loss": 0.0031,
      "step": 3677
    },
    {
      "epoch": 58.85,
      "grad_norm": 0.6182042956352234,
      "learning_rate": 1.6910367169986018e-06,
      "loss": 0.0052,
      "step": 3678
    },
    {
      "epoch": 58.86,
      "grad_norm": 0.3418673872947693,
      "learning_rate": 1.6886341208052775e-06,
      "loss": 0.002,
      "step": 3679
    },
    {
      "epoch": 58.88,
      "grad_norm": 0.8389013409614563,
      "learning_rate": 1.6862328857893856e-06,
      "loss": 0.0114,
      "step": 3680
    },
    {
      "epoch": 58.9,
      "grad_norm": 0.004594172351062298,
      "learning_rate": 1.6838330129379816e-06,
      "loss": 0.0001,
      "step": 3681
    },
    {
      "epoch": 58.91,
      "grad_norm": 0.023847397416830063,
      "learning_rate": 1.6814345032375633e-06,
      "loss": 0.0001,
      "step": 3682
    },
    {
      "epoch": 58.93,
      "grad_norm": 0.8071305155754089,
      "learning_rate": 1.67903735767407e-06,
      "loss": 0.0101,
      "step": 3683
    },
    {
      "epoch": 58.94,
      "grad_norm": 0.005202156025916338,
      "learning_rate": 1.6766415772328732e-06,
      "loss": 0.0002,
      "step": 3684
    },
    {
      "epoch": 58.96,
      "grad_norm": 0.19994664192199707,
      "learning_rate": 1.6742471628987894e-06,
      "loss": 0.0013,
      "step": 3685
    },
    {
      "epoch": 58.98,
      "grad_norm": 0.0045616053976118565,
      "learning_rate": 1.6718541156560725e-06,
      "loss": 0.0001,
      "step": 3686
    },
    {
      "epoch": 58.99,
      "grad_norm": 0.21304452419281006,
      "learning_rate": 1.6694624364884138e-06,
      "loss": 0.0011,
      "step": 3687
    },
    {
      "epoch": 59.01,
      "grad_norm": 0.3757571280002594,
      "learning_rate": 1.667072126378942e-06,
      "loss": 0.0021,
      "step": 3688
    },
    {
      "epoch": 59.02,
      "grad_norm": 0.004024434369057417,
      "learning_rate": 1.664683186310223e-06,
      "loss": 0.0001,
      "step": 3689
    },
    {
      "epoch": 59.04,
      "grad_norm": 0.7493088245391846,
      "learning_rate": 1.6622956172642601e-06,
      "loss": 0.0063,
      "step": 3690
    },
    {
      "epoch": 59.06,
      "grad_norm": 0.004188759718090296,
      "learning_rate": 1.6599094202224936e-06,
      "loss": 0.0001,
      "step": 3691
    },
    {
      "epoch": 59.07,
      "grad_norm": 0.0035609055776149035,
      "learning_rate": 1.6575245961657977e-06,
      "loss": 0.0001,
      "step": 3692
    },
    {
      "epoch": 59.09,
      "grad_norm": 0.024971436709165573,
      "learning_rate": 1.6551411460744836e-06,
      "loss": 0.0002,
      "step": 3693
    },
    {
      "epoch": 59.1,
      "grad_norm": 0.0047279237769544125,
      "learning_rate": 1.6527590709283003e-06,
      "loss": 0.0001,
      "step": 3694
    },
    {
      "epoch": 59.12,
      "grad_norm": 0.18456383049488068,
      "learning_rate": 1.6503783717064247e-06,
      "loss": 0.0009,
      "step": 3695
    },
    {
      "epoch": 59.14,
      "grad_norm": 0.003521297825500369,
      "learning_rate": 1.6479990493874741e-06,
      "loss": 0.0001,
      "step": 3696
    },
    {
      "epoch": 59.15,
      "grad_norm": 0.003018888644874096,
      "learning_rate": 1.6456211049494986e-06,
      "loss": 0.0001,
      "step": 3697
    },
    {
      "epoch": 59.17,
      "grad_norm": 0.0043236128985881805,
      "learning_rate": 1.6432445393699803e-06,
      "loss": 0.0001,
      "step": 3698
    },
    {
      "epoch": 59.18,
      "grad_norm": 0.004500883631408215,
      "learning_rate": 1.6408693536258364e-06,
      "loss": 0.0001,
      "step": 3699
    },
    {
      "epoch": 59.2,
      "grad_norm": 0.23401719331741333,
      "learning_rate": 1.6384955486934157e-06,
      "loss": 0.0014,
      "step": 3700
    },
    {
      "epoch": 59.22,
      "grad_norm": 0.004294875077903271,
      "learning_rate": 1.6361231255485e-06,
      "loss": 0.0001,
      "step": 3701
    },
    {
      "epoch": 59.23,
      "grad_norm": 0.0036429413594305515,
      "learning_rate": 1.6337520851663025e-06,
      "loss": 0.0001,
      "step": 3702
    },
    {
      "epoch": 59.25,
      "grad_norm": 0.005032103508710861,
      "learning_rate": 1.6313824285214686e-06,
      "loss": 0.0002,
      "step": 3703
    },
    {
      "epoch": 59.26,
      "grad_norm": 0.7627158761024475,
      "learning_rate": 1.6290141565880758e-06,
      "loss": 0.0048,
      "step": 3704
    },
    {
      "epoch": 59.28,
      "grad_norm": 0.8104154467582703,
      "learning_rate": 1.6266472703396286e-06,
      "loss": 0.0057,
      "step": 3705
    },
    {
      "epoch": 59.3,
      "grad_norm": 0.004151921719312668,
      "learning_rate": 1.624281770749066e-06,
      "loss": 0.0001,
      "step": 3706
    },
    {
      "epoch": 59.31,
      "grad_norm": 0.7580747008323669,
      "learning_rate": 1.6219176587887569e-06,
      "loss": 0.0153,
      "step": 3707
    },
    {
      "epoch": 59.33,
      "grad_norm": 0.9497491121292114,
      "learning_rate": 1.6195549354304952e-06,
      "loss": 0.0107,
      "step": 3708
    },
    {
      "epoch": 59.34,
      "grad_norm": 0.004051724448800087,
      "learning_rate": 1.6171936016455091e-06,
      "loss": 0.0001,
      "step": 3709
    },
    {
      "epoch": 59.36,
      "grad_norm": 0.3407289385795593,
      "learning_rate": 1.6148336584044539e-06,
      "loss": 0.0021,
      "step": 3710
    },
    {
      "epoch": 59.38,
      "grad_norm": 0.8907229900360107,
      "learning_rate": 1.6124751066774124e-06,
      "loss": 0.0138,
      "step": 3711
    },
    {
      "epoch": 59.39,
      "grad_norm": 0.5663819313049316,
      "learning_rate": 1.610117947433897e-06,
      "loss": 0.0033,
      "step": 3712
    },
    {
      "epoch": 59.41,
      "grad_norm": 0.0038207739125937223,
      "learning_rate": 1.6077621816428457e-06,
      "loss": 0.0001,
      "step": 3713
    },
    {
      "epoch": 59.42,
      "grad_norm": 0.003033876186236739,
      "learning_rate": 1.6054078102726257e-06,
      "loss": 0.0001,
      "step": 3714
    },
    {
      "epoch": 59.44,
      "grad_norm": 0.003343927673995495,
      "learning_rate": 1.6030548342910302e-06,
      "loss": 0.0001,
      "step": 3715
    },
    {
      "epoch": 59.46,
      "grad_norm": 0.6865584254264832,
      "learning_rate": 1.6007032546652784e-06,
      "loss": 0.0045,
      "step": 3716
    },
    {
      "epoch": 59.47,
      "grad_norm": 0.5296164155006409,
      "learning_rate": 1.5983530723620173e-06,
      "loss": 0.0032,
      "step": 3717
    },
    {
      "epoch": 59.49,
      "grad_norm": 0.004130223300307989,
      "learning_rate": 1.5960042883473154e-06,
      "loss": 0.0001,
      "step": 3718
    },
    {
      "epoch": 59.5,
      "grad_norm": 0.003665174823254347,
      "learning_rate": 1.59365690358667e-06,
      "loss": 0.0001,
      "step": 3719
    },
    {
      "epoch": 59.52,
      "grad_norm": 0.08656302839517593,
      "learning_rate": 1.5913109190450033e-06,
      "loss": 0.0005,
      "step": 3720
    },
    {
      "epoch": 59.54,
      "grad_norm": 0.054805587977170944,
      "learning_rate": 1.5889663356866597e-06,
      "loss": 0.0003,
      "step": 3721
    },
    {
      "epoch": 59.55,
      "grad_norm": 0.3655526638031006,
      "learning_rate": 1.58662315447541e-06,
      "loss": 0.0023,
      "step": 3722
    },
    {
      "epoch": 59.57,
      "grad_norm": 0.0032557689119130373,
      "learning_rate": 1.5842813763744468e-06,
      "loss": 0.0001,
      "step": 3723
    },
    {
      "epoch": 59.58,
      "grad_norm": 1.2124794721603394,
      "learning_rate": 1.581941002346387e-06,
      "loss": 0.0091,
      "step": 3724
    },
    {
      "epoch": 59.6,
      "grad_norm": 0.0037050708197057247,
      "learning_rate": 1.5796020333532696e-06,
      "loss": 0.0001,
      "step": 3725
    },
    {
      "epoch": 59.62,
      "grad_norm": 0.0037755481898784637,
      "learning_rate": 1.5772644703565564e-06,
      "loss": 0.0001,
      "step": 3726
    },
    {
      "epoch": 59.63,
      "grad_norm": 0.004269784316420555,
      "learning_rate": 1.5749283143171335e-06,
      "loss": 0.0001,
      "step": 3727
    },
    {
      "epoch": 59.65,
      "grad_norm": 0.004106807988137007,
      "learning_rate": 1.5725935661953024e-06,
      "loss": 0.0001,
      "step": 3728
    },
    {
      "epoch": 59.66,
      "grad_norm": 0.05479693040251732,
      "learning_rate": 1.5702602269507917e-06,
      "loss": 0.0004,
      "step": 3729
    },
    {
      "epoch": 59.68,
      "grad_norm": 0.6476985216140747,
      "learning_rate": 1.567928297542749e-06,
      "loss": 0.0101,
      "step": 3730
    },
    {
      "epoch": 59.7,
      "grad_norm": 0.7711346745491028,
      "learning_rate": 1.5655977789297428e-06,
      "loss": 0.0102,
      "step": 3731
    },
    {
      "epoch": 59.71,
      "grad_norm": 0.6198732852935791,
      "learning_rate": 1.5632686720697604e-06,
      "loss": 0.0065,
      "step": 3732
    },
    {
      "epoch": 59.73,
      "grad_norm": 0.890152096748352,
      "learning_rate": 1.5609409779202105e-06,
      "loss": 0.0061,
      "step": 3733
    },
    {
      "epoch": 59.74,
      "grad_norm": 0.8286775350570679,
      "learning_rate": 1.5586146974379201e-06,
      "loss": 0.0104,
      "step": 3734
    },
    {
      "epoch": 59.76,
      "grad_norm": 1.2085727453231812,
      "learning_rate": 1.5562898315791354e-06,
      "loss": 0.0167,
      "step": 3735
    },
    {
      "epoch": 59.78,
      "grad_norm": 1.0764092206954956,
      "learning_rate": 1.5539663812995204e-06,
      "loss": 0.0186,
      "step": 3736
    },
    {
      "epoch": 59.79,
      "grad_norm": 0.0050072805024683475,
      "learning_rate": 1.5516443475541592e-06,
      "loss": 0.0002,
      "step": 3737
    },
    {
      "epoch": 59.81,
      "grad_norm": 0.003941462375223637,
      "learning_rate": 1.5493237312975495e-06,
      "loss": 0.0001,
      "step": 3738
    },
    {
      "epoch": 59.82,
      "grad_norm": 0.06742147356271744,
      "learning_rate": 1.547004533483611e-06,
      "loss": 0.0004,
      "step": 3739
    },
    {
      "epoch": 59.84,
      "grad_norm": 0.004464250989258289,
      "learning_rate": 1.544686755065677e-06,
      "loss": 0.0001,
      "step": 3740
    },
    {
      "epoch": 59.86,
      "grad_norm": 0.5569339394569397,
      "learning_rate": 1.5423703969964998e-06,
      "loss": 0.0046,
      "step": 3741
    },
    {
      "epoch": 59.87,
      "grad_norm": 0.0026863550301641226,
      "learning_rate": 1.5400554602282465e-06,
      "loss": 0.0001,
      "step": 3742
    },
    {
      "epoch": 59.89,
      "grad_norm": 0.005438696593046188,
      "learning_rate": 1.5377419457124993e-06,
      "loss": 0.0002,
      "step": 3743
    },
    {
      "epoch": 59.9,
      "grad_norm": 0.004239299334585667,
      "learning_rate": 1.5354298544002576e-06,
      "loss": 0.0001,
      "step": 3744
    },
    {
      "epoch": 59.92,
      "grad_norm": 0.002364270854741335,
      "learning_rate": 1.5331191872419349e-06,
      "loss": 0.0,
      "step": 3745
    },
    {
      "epoch": 59.94,
      "grad_norm": 0.8158041834831238,
      "learning_rate": 1.5308099451873582e-06,
      "loss": 0.0094,
      "step": 3746
    },
    {
      "epoch": 59.95,
      "grad_norm": 0.041369467973709106,
      "learning_rate": 1.5285021291857705e-06,
      "loss": 0.0004,
      "step": 3747
    },
    {
      "epoch": 59.97,
      "grad_norm": 1.039449691772461,
      "learning_rate": 1.526195740185829e-06,
      "loss": 0.0142,
      "step": 3748
    },
    {
      "epoch": 59.98,
      "grad_norm": 0.849889874458313,
      "learning_rate": 1.5238907791356005e-06,
      "loss": 0.0172,
      "step": 3749
    },
    {
      "epoch": 60.0,
      "grad_norm": 0.3672577142715454,
      "learning_rate": 1.5215872469825682e-06,
      "loss": 0.0032,
      "step": 3750
    },
    {
      "epoch": 60.02,
      "grad_norm": 0.004454199690371752,
      "learning_rate": 1.5192851446736278e-06,
      "loss": 0.0001,
      "step": 3751
    },
    {
      "epoch": 60.03,
      "grad_norm": 0.967329740524292,
      "learning_rate": 1.516984473155086e-06,
      "loss": 0.0128,
      "step": 3752
    },
    {
      "epoch": 60.05,
      "grad_norm": 0.003405150957405567,
      "learning_rate": 1.5146852333726642e-06,
      "loss": 0.0001,
      "step": 3753
    },
    {
      "epoch": 60.06,
      "grad_norm": 0.004488485865294933,
      "learning_rate": 1.5123874262714893e-06,
      "loss": 0.0001,
      "step": 3754
    },
    {
      "epoch": 60.08,
      "grad_norm": 0.31367090344429016,
      "learning_rate": 1.510091052796105e-06,
      "loss": 0.0019,
      "step": 3755
    },
    {
      "epoch": 60.1,
      "grad_norm": 0.005027805455029011,
      "learning_rate": 1.5077961138904628e-06,
      "loss": 0.0002,
      "step": 3756
    },
    {
      "epoch": 60.11,
      "grad_norm": 0.0034334936644881964,
      "learning_rate": 1.505502610497927e-06,
      "loss": 0.0001,
      "step": 3757
    },
    {
      "epoch": 60.13,
      "grad_norm": 0.003876042552292347,
      "learning_rate": 1.5032105435612693e-06,
      "loss": 0.0001,
      "step": 3758
    },
    {
      "epoch": 60.14,
      "grad_norm": 0.7750892639160156,
      "learning_rate": 1.500919914022671e-06,
      "loss": 0.0053,
      "step": 3759
    },
    {
      "epoch": 60.16,
      "grad_norm": 0.7856922745704651,
      "learning_rate": 1.4986307228237268e-06,
      "loss": 0.0118,
      "step": 3760
    },
    {
      "epoch": 60.18,
      "grad_norm": 0.00435915170237422,
      "learning_rate": 1.4963429709054323e-06,
      "loss": 0.0001,
      "step": 3761
    },
    {
      "epoch": 60.19,
      "grad_norm": 0.04685661941766739,
      "learning_rate": 1.4940566592081973e-06,
      "loss": 0.0002,
      "step": 3762
    },
    {
      "epoch": 60.21,
      "grad_norm": 0.003997078165411949,
      "learning_rate": 1.4917717886718392e-06,
      "loss": 0.0001,
      "step": 3763
    },
    {
      "epoch": 60.22,
      "grad_norm": 0.002731364918872714,
      "learning_rate": 1.4894883602355808e-06,
      "loss": 0.0001,
      "step": 3764
    },
    {
      "epoch": 60.24,
      "grad_norm": 0.5391544103622437,
      "learning_rate": 1.4872063748380544e-06,
      "loss": 0.0054,
      "step": 3765
    },
    {
      "epoch": 60.26,
      "grad_norm": 0.003845082363113761,
      "learning_rate": 1.4849258334172973e-06,
      "loss": 0.0001,
      "step": 3766
    },
    {
      "epoch": 60.27,
      "grad_norm": 0.003508636960759759,
      "learning_rate": 1.4826467369107545e-06,
      "loss": 0.0001,
      "step": 3767
    },
    {
      "epoch": 60.29,
      "grad_norm": 0.004211392719298601,
      "learning_rate": 1.4803690862552755e-06,
      "loss": 0.0001,
      "step": 3768
    },
    {
      "epoch": 60.3,
      "grad_norm": 0.9944117069244385,
      "learning_rate": 1.478092882387117e-06,
      "loss": 0.0075,
      "step": 3769
    },
    {
      "epoch": 60.32,
      "grad_norm": 0.062396857887506485,
      "learning_rate": 1.4758181262419425e-06,
      "loss": 0.0005,
      "step": 3770
    },
    {
      "epoch": 60.34,
      "grad_norm": 0.7705948948860168,
      "learning_rate": 1.4735448187548147e-06,
      "loss": 0.011,
      "step": 3771
    },
    {
      "epoch": 60.35,
      "grad_norm": 0.9225200414657593,
      "learning_rate": 1.4712729608602062e-06,
      "loss": 0.006,
      "step": 3772
    },
    {
      "epoch": 60.37,
      "grad_norm": 0.6160871386528015,
      "learning_rate": 1.4690025534919916e-06,
      "loss": 0.0035,
      "step": 3773
    },
    {
      "epoch": 60.38,
      "grad_norm": 0.8390366435050964,
      "learning_rate": 1.4667335975834495e-06,
      "loss": 0.005,
      "step": 3774
    },
    {
      "epoch": 60.4,
      "grad_norm": 0.003652979386970401,
      "learning_rate": 1.4644660940672628e-06,
      "loss": 0.0001,
      "step": 3775
    },
    {
      "epoch": 60.42,
      "grad_norm": 0.004222482442855835,
      "learning_rate": 1.4622000438755157e-06,
      "loss": 0.0001,
      "step": 3776
    },
    {
      "epoch": 60.43,
      "grad_norm": 0.5433337688446045,
      "learning_rate": 1.4599354479396966e-06,
      "loss": 0.003,
      "step": 3777
    },
    {
      "epoch": 60.45,
      "grad_norm": 0.3979884684085846,
      "learning_rate": 1.4576723071906945e-06,
      "loss": 0.0024,
      "step": 3778
    },
    {
      "epoch": 60.46,
      "grad_norm": 0.0034374541137367487,
      "learning_rate": 1.4554106225588017e-06,
      "loss": 0.0001,
      "step": 3779
    },
    {
      "epoch": 60.48,
      "grad_norm": 1.2321586608886719,
      "learning_rate": 1.4531503949737107e-06,
      "loss": 0.022,
      "step": 3780
    },
    {
      "epoch": 60.5,
      "grad_norm": 0.8892538547515869,
      "learning_rate": 1.4508916253645183e-06,
      "loss": 0.0091,
      "step": 3781
    },
    {
      "epoch": 60.51,
      "grad_norm": 1.051352620124817,
      "learning_rate": 1.4486343146597154e-06,
      "loss": 0.0067,
      "step": 3782
    },
    {
      "epoch": 60.53,
      "grad_norm": 0.6986448168754578,
      "learning_rate": 1.4463784637871991e-06,
      "loss": 0.0098,
      "step": 3783
    },
    {
      "epoch": 60.54,
      "grad_norm": 0.0034463934134691954,
      "learning_rate": 1.444124073674264e-06,
      "loss": 0.0001,
      "step": 3784
    },
    {
      "epoch": 60.56,
      "grad_norm": 0.5284764766693115,
      "learning_rate": 1.4418711452476048e-06,
      "loss": 0.0038,
      "step": 3785
    },
    {
      "epoch": 60.58,
      "grad_norm": 0.003908954095095396,
      "learning_rate": 1.439619679433316e-06,
      "loss": 0.0001,
      "step": 3786
    },
    {
      "epoch": 60.59,
      "grad_norm": 0.004383035469800234,
      "learning_rate": 1.4373696771568896e-06,
      "loss": 0.0001,
      "step": 3787
    },
    {
      "epoch": 60.61,
      "grad_norm": 0.8880817890167236,
      "learning_rate": 1.4351211393432162e-06,
      "loss": 0.007,
      "step": 3788
    },
    {
      "epoch": 60.62,
      "grad_norm": 0.8319147229194641,
      "learning_rate": 1.4328740669165858e-06,
      "loss": 0.0115,
      "step": 3789
    },
    {
      "epoch": 60.64,
      "grad_norm": 0.7235404849052429,
      "learning_rate": 1.4306284608006837e-06,
      "loss": 0.0122,
      "step": 3790
    },
    {
      "epoch": 60.66,
      "grad_norm": 0.9176530838012695,
      "learning_rate": 1.4283843219185966e-06,
      "loss": 0.0135,
      "step": 3791
    },
    {
      "epoch": 60.67,
      "grad_norm": 0.777933657169342,
      "learning_rate": 1.4261416511928012e-06,
      "loss": 0.0068,
      "step": 3792
    },
    {
      "epoch": 60.69,
      "grad_norm": 0.8315265774726868,
      "learning_rate": 1.4239004495451763e-06,
      "loss": 0.007,
      "step": 3793
    },
    {
      "epoch": 60.7,
      "grad_norm": 0.7239397168159485,
      "learning_rate": 1.421660717896996e-06,
      "loss": 0.0094,
      "step": 3794
    },
    {
      "epoch": 60.72,
      "grad_norm": 0.38675370812416077,
      "learning_rate": 1.4194224571689286e-06,
      "loss": 0.0023,
      "step": 3795
    },
    {
      "epoch": 60.74,
      "grad_norm": 0.13277111947536469,
      "learning_rate": 1.4171856682810386e-06,
      "loss": 0.0007,
      "step": 3796
    },
    {
      "epoch": 60.75,
      "grad_norm": 0.3419014513492584,
      "learning_rate": 1.4149503521527862e-06,
      "loss": 0.002,
      "step": 3797
    },
    {
      "epoch": 60.77,
      "grad_norm": 0.26443028450012207,
      "learning_rate": 1.412716509703026e-06,
      "loss": 0.0016,
      "step": 3798
    },
    {
      "epoch": 60.78,
      "grad_norm": 0.003064765129238367,
      "learning_rate": 1.4104841418500038e-06,
      "loss": 0.0001,
      "step": 3799
    },
    {
      "epoch": 60.8,
      "grad_norm": 0.7409687042236328,
      "learning_rate": 1.4082532495113627e-06,
      "loss": 0.0097,
      "step": 3800
    },
    {
      "epoch": 60.82,
      "grad_norm": 0.003523506922647357,
      "learning_rate": 1.4060238336041388e-06,
      "loss": 0.0001,
      "step": 3801
    },
    {
      "epoch": 60.83,
      "grad_norm": 0.4289538562297821,
      "learning_rate": 1.4037958950447604e-06,
      "loss": 0.0042,
      "step": 3802
    },
    {
      "epoch": 60.85,
      "grad_norm": 0.003299442119896412,
      "learning_rate": 1.401569434749051e-06,
      "loss": 0.0001,
      "step": 3803
    },
    {
      "epoch": 60.86,
      "grad_norm": 0.09008128941059113,
      "learning_rate": 1.3993444536322204e-06,
      "loss": 0.0007,
      "step": 3804
    },
    {
      "epoch": 60.88,
      "grad_norm": 0.9453000426292419,
      "learning_rate": 1.3971209526088764e-06,
      "loss": 0.0098,
      "step": 3805
    },
    {
      "epoch": 60.9,
      "grad_norm": 0.06364299356937408,
      "learning_rate": 1.3948989325930162e-06,
      "loss": 0.0005,
      "step": 3806
    },
    {
      "epoch": 60.91,
      "grad_norm": 0.6617623567581177,
      "learning_rate": 1.3926783944980283e-06,
      "loss": 0.0058,
      "step": 3807
    },
    {
      "epoch": 60.93,
      "grad_norm": 0.23620961606502533,
      "learning_rate": 1.3904593392366916e-06,
      "loss": 0.0019,
      "step": 3808
    },
    {
      "epoch": 60.94,
      "grad_norm": 0.0033219007309526205,
      "learning_rate": 1.3882417677211769e-06,
      "loss": 0.0001,
      "step": 3809
    },
    {
      "epoch": 60.96,
      "grad_norm": 0.003165815258398652,
      "learning_rate": 1.3860256808630429e-06,
      "loss": 0.0001,
      "step": 3810
    },
    {
      "epoch": 60.98,
      "grad_norm": 0.004311834461987019,
      "learning_rate": 1.3838110795732396e-06,
      "loss": 0.0001,
      "step": 3811
    },
    {
      "epoch": 60.99,
      "grad_norm": 0.004563233815133572,
      "learning_rate": 1.3815979647621063e-06,
      "loss": 0.0001,
      "step": 3812
    },
    {
      "epoch": 61.01,
      "grad_norm": 0.003134356811642647,
      "learning_rate": 1.3793863373393711e-06,
      "loss": 0.0001,
      "step": 3813
    },
    {
      "epoch": 61.02,
      "grad_norm": 1.0209388732910156,
      "learning_rate": 1.3771761982141513e-06,
      "loss": 0.0217,
      "step": 3814
    },
    {
      "epoch": 61.04,
      "grad_norm": 0.6745994091033936,
      "learning_rate": 1.3749675482949487e-06,
      "loss": 0.0042,
      "step": 3815
    },
    {
      "epoch": 61.06,
      "grad_norm": 0.004066163673996925,
      "learning_rate": 1.3727603884896578e-06,
      "loss": 0.0001,
      "step": 3816
    },
    {
      "epoch": 61.07,
      "grad_norm": 0.004779897630214691,
      "learning_rate": 1.3705547197055586e-06,
      "loss": 0.0001,
      "step": 3817
    },
    {
      "epoch": 61.09,
      "grad_norm": 0.7783480286598206,
      "learning_rate": 1.3683505428493177e-06,
      "loss": 0.0104,
      "step": 3818
    },
    {
      "epoch": 61.1,
      "grad_norm": 0.9377388954162598,
      "learning_rate": 1.3661478588269889e-06,
      "loss": 0.0153,
      "step": 3819
    },
    {
      "epoch": 61.12,
      "grad_norm": 0.5651983618736267,
      "learning_rate": 1.3639466685440133e-06,
      "loss": 0.0047,
      "step": 3820
    },
    {
      "epoch": 61.14,
      "grad_norm": 0.004314622841775417,
      "learning_rate": 1.3617469729052162e-06,
      "loss": 0.0001,
      "step": 3821
    },
    {
      "epoch": 61.15,
      "grad_norm": 0.8442909717559814,
      "learning_rate": 1.3595487728148099e-06,
      "loss": 0.0073,
      "step": 3822
    },
    {
      "epoch": 61.17,
      "grad_norm": 0.699087917804718,
      "learning_rate": 1.3573520691763914e-06,
      "loss": 0.0084,
      "step": 3823
    },
    {
      "epoch": 61.18,
      "grad_norm": 1.2042850255966187,
      "learning_rate": 1.3551568628929434e-06,
      "loss": 0.0146,
      "step": 3824
    },
    {
      "epoch": 61.2,
      "grad_norm": 0.0037528665270656347,
      "learning_rate": 1.3529631548668298e-06,
      "loss": 0.0001,
      "step": 3825
    },
    {
      "epoch": 61.22,
      "grad_norm": 0.5914552211761475,
      "learning_rate": 1.3507709459998032e-06,
      "loss": 0.0079,
      "step": 3826
    },
    {
      "epoch": 61.23,
      "grad_norm": 0.6651610732078552,
      "learning_rate": 1.3485802371929968e-06,
      "loss": 0.0074,
      "step": 3827
    },
    {
      "epoch": 61.25,
      "grad_norm": 0.6712389588356018,
      "learning_rate": 1.346391029346929e-06,
      "loss": 0.0059,
      "step": 3828
    },
    {
      "epoch": 61.26,
      "grad_norm": 0.9951600432395935,
      "learning_rate": 1.3442033233614998e-06,
      "loss": 0.0079,
      "step": 3829
    },
    {
      "epoch": 61.28,
      "grad_norm": 0.003927626181393862,
      "learning_rate": 1.3420171201359933e-06,
      "loss": 0.0001,
      "step": 3830
    },
    {
      "epoch": 61.3,
      "grad_norm": 0.7493341565132141,
      "learning_rate": 1.3398324205690744e-06,
      "loss": 0.0084,
      "step": 3831
    },
    {
      "epoch": 61.31,
      "grad_norm": 0.6254400014877319,
      "learning_rate": 1.3376492255587909e-06,
      "loss": 0.005,
      "step": 3832
    },
    {
      "epoch": 61.33,
      "grad_norm": 0.0025805726181715727,
      "learning_rate": 1.3354675360025709e-06,
      "loss": 0.0001,
      "step": 3833
    },
    {
      "epoch": 61.34,
      "grad_norm": 0.0036625040229409933,
      "learning_rate": 1.333287352797228e-06,
      "loss": 0.0001,
      "step": 3834
    },
    {
      "epoch": 61.36,
      "grad_norm": 0.0029562273994088173,
      "learning_rate": 1.331108676838948e-06,
      "loss": 0.0001,
      "step": 3835
    },
    {
      "epoch": 61.38,
      "grad_norm": 0.74802565574646,
      "learning_rate": 1.3289315090233056e-06,
      "loss": 0.0113,
      "step": 3836
    },
    {
      "epoch": 61.39,
      "grad_norm": 0.6894825100898743,
      "learning_rate": 1.326755850245251e-06,
      "loss": 0.0097,
      "step": 3837
    },
    {
      "epoch": 61.41,
      "grad_norm": 0.21980246901512146,
      "learning_rate": 1.3245817013991164e-06,
      "loss": 0.0013,
      "step": 3838
    },
    {
      "epoch": 61.42,
      "grad_norm": 0.04877534508705139,
      "learning_rate": 1.3224090633786112e-06,
      "loss": 0.0003,
      "step": 3839
    },
    {
      "epoch": 61.44,
      "grad_norm": 0.0030756627675145864,
      "learning_rate": 1.3202379370768254e-06,
      "loss": 0.0001,
      "step": 3840
    },
    {
      "epoch": 61.46,
      "grad_norm": 0.004302987363189459,
      "learning_rate": 1.3180683233862268e-06,
      "loss": 0.0001,
      "step": 3841
    },
    {
      "epoch": 61.47,
      "grad_norm": 0.0034001406747847795,
      "learning_rate": 1.3159002231986617e-06,
      "loss": 0.0001,
      "step": 3842
    },
    {
      "epoch": 61.49,
      "grad_norm": 0.0034540670458227396,
      "learning_rate": 1.3137336374053544e-06,
      "loss": 0.0001,
      "step": 3843
    },
    {
      "epoch": 61.5,
      "grad_norm": 0.003412638558074832,
      "learning_rate": 1.3115685668969075e-06,
      "loss": 0.0001,
      "step": 3844
    },
    {
      "epoch": 61.52,
      "grad_norm": 0.6374660730361938,
      "learning_rate": 1.3094050125632973e-06,
      "loss": 0.007,
      "step": 3845
    },
    {
      "epoch": 61.54,
      "grad_norm": 0.003682795912027359,
      "learning_rate": 1.3072429752938803e-06,
      "loss": 0.0001,
      "step": 3846
    },
    {
      "epoch": 61.55,
      "grad_norm": 0.3509564697742462,
      "learning_rate": 1.30508245597739e-06,
      "loss": 0.0027,
      "step": 3847
    },
    {
      "epoch": 61.57,
      "grad_norm": 0.7362402677536011,
      "learning_rate": 1.3029234555019315e-06,
      "loss": 0.0067,
      "step": 3848
    },
    {
      "epoch": 61.58,
      "grad_norm": 0.004053045064210892,
      "learning_rate": 1.30076597475499e-06,
      "loss": 0.0001,
      "step": 3849
    },
    {
      "epoch": 61.6,
      "grad_norm": 0.05765305459499359,
      "learning_rate": 1.298610014623423e-06,
      "loss": 0.0003,
      "step": 3850
    },
    {
      "epoch": 61.62,
      "grad_norm": 0.2599582374095917,
      "learning_rate": 1.296455575993466e-06,
      "loss": 0.0016,
      "step": 3851
    },
    {
      "epoch": 61.63,
      "grad_norm": 0.003897481132298708,
      "learning_rate": 1.2943026597507268e-06,
      "loss": 0.0001,
      "step": 3852
    },
    {
      "epoch": 61.65,
      "grad_norm": 0.004712477792054415,
      "learning_rate": 1.2921512667801871e-06,
      "loss": 0.0001,
      "step": 3853
    },
    {
      "epoch": 61.66,
      "grad_norm": 0.39958226680755615,
      "learning_rate": 1.2900013979662046e-06,
      "loss": 0.0037,
      "step": 3854
    },
    {
      "epoch": 61.68,
      "grad_norm": 0.003184772562235594,
      "learning_rate": 1.2878530541925077e-06,
      "loss": 0.0001,
      "step": 3855
    },
    {
      "epoch": 61.7,
      "grad_norm": 0.004545646719634533,
      "learning_rate": 1.2857062363422007e-06,
      "loss": 0.0002,
      "step": 3856
    },
    {
      "epoch": 61.71,
      "grad_norm": 0.004284017253667116,
      "learning_rate": 1.2835609452977604e-06,
      "loss": 0.0001,
      "step": 3857
    },
    {
      "epoch": 61.73,
      "grad_norm": 0.0032831719145178795,
      "learning_rate": 1.2814171819410315e-06,
      "loss": 0.0001,
      "step": 3858
    },
    {
      "epoch": 61.74,
      "grad_norm": 0.0036749907303601503,
      "learning_rate": 1.2792749471532361e-06,
      "loss": 0.0001,
      "step": 3859
    },
    {
      "epoch": 61.76,
      "grad_norm": 0.0034808325581252575,
      "learning_rate": 1.2771342418149658e-06,
      "loss": 0.0001,
      "step": 3860
    },
    {
      "epoch": 61.78,
      "grad_norm": 0.005565463099628687,
      "learning_rate": 1.274995066806184e-06,
      "loss": 0.0001,
      "step": 3861
    },
    {
      "epoch": 61.79,
      "grad_norm": 0.844373345375061,
      "learning_rate": 1.2728574230062242e-06,
      "loss": 0.0089,
      "step": 3862
    },
    {
      "epoch": 61.81,
      "grad_norm": 0.07891308516263962,
      "learning_rate": 1.2707213112937911e-06,
      "loss": 0.0006,
      "step": 3863
    },
    {
      "epoch": 61.82,
      "grad_norm": 0.6085353493690491,
      "learning_rate": 1.2685867325469603e-06,
      "loss": 0.0038,
      "step": 3864
    },
    {
      "epoch": 61.84,
      "grad_norm": 0.03950236365199089,
      "learning_rate": 1.2664536876431755e-06,
      "loss": 0.0002,
      "step": 3865
    },
    {
      "epoch": 61.86,
      "grad_norm": 0.8973478674888611,
      "learning_rate": 1.2643221774592517e-06,
      "loss": 0.0061,
      "step": 3866
    },
    {
      "epoch": 61.87,
      "grad_norm": 0.004507365170866251,
      "learning_rate": 1.2621922028713719e-06,
      "loss": 0.0001,
      "step": 3867
    },
    {
      "epoch": 61.89,
      "grad_norm": 0.9255931377410889,
      "learning_rate": 1.2600637647550889e-06,
      "loss": 0.0148,
      "step": 3868
    },
    {
      "epoch": 61.9,
      "grad_norm": 0.003631731728091836,
      "learning_rate": 1.2579368639853217e-06,
      "loss": 0.0001,
      "step": 3869
    },
    {
      "epoch": 61.92,
      "grad_norm": 0.5642327666282654,
      "learning_rate": 1.2558115014363592e-06,
      "loss": 0.0047,
      "step": 3870
    },
    {
      "epoch": 61.94,
      "grad_norm": 0.15640419721603394,
      "learning_rate": 1.2536876779818586e-06,
      "loss": 0.0008,
      "step": 3871
    },
    {
      "epoch": 61.95,
      "grad_norm": 0.003942739684134722,
      "learning_rate": 1.2515653944948424e-06,
      "loss": 0.0001,
      "step": 3872
    },
    {
      "epoch": 61.97,
      "grad_norm": 0.00423071626573801,
      "learning_rate": 1.2494446518477022e-06,
      "loss": 0.0001,
      "step": 3873
    },
    {
      "epoch": 61.98,
      "grad_norm": 1.0129190683364868,
      "learning_rate": 1.2473254509121947e-06,
      "loss": 0.0068,
      "step": 3874
    },
    {
      "epoch": 62.0,
      "grad_norm": 0.004162938799709082,
      "learning_rate": 1.2452077925594435e-06,
      "loss": 0.0001,
      "step": 3875
    },
    {
      "epoch": 62.02,
      "grad_norm": 0.5976277589797974,
      "learning_rate": 1.2430916776599378e-06,
      "loss": 0.0065,
      "step": 3876
    },
    {
      "epoch": 62.03,
      "grad_norm": 0.004463178105652332,
      "learning_rate": 1.2409771070835324e-06,
      "loss": 0.0001,
      "step": 3877
    },
    {
      "epoch": 62.05,
      "grad_norm": 0.003902438096702099,
      "learning_rate": 1.238864081699449e-06,
      "loss": 0.0001,
      "step": 3878
    },
    {
      "epoch": 62.06,
      "grad_norm": 0.5452641844749451,
      "learning_rate": 1.23675260237627e-06,
      "loss": 0.0044,
      "step": 3879
    },
    {
      "epoch": 62.08,
      "grad_norm": 0.004913367796689272,
      "learning_rate": 1.234642669981946e-06,
      "loss": 0.0002,
      "step": 3880
    },
    {
      "epoch": 62.1,
      "grad_norm": 0.47213226556777954,
      "learning_rate": 1.2325342853837896e-06,
      "loss": 0.0084,
      "step": 3881
    },
    {
      "epoch": 62.11,
      "grad_norm": 0.004262948874384165,
      "learning_rate": 1.2304274494484796e-06,
      "loss": 0.0001,
      "step": 3882
    },
    {
      "epoch": 62.13,
      "grad_norm": 0.003280326258391142,
      "learning_rate": 1.2283221630420556e-06,
      "loss": 0.0001,
      "step": 3883
    },
    {
      "epoch": 62.14,
      "grad_norm": 0.05129401013255119,
      "learning_rate": 1.2262184270299215e-06,
      "loss": 0.0002,
      "step": 3884
    },
    {
      "epoch": 62.16,
      "grad_norm": 0.005044514313340187,
      "learning_rate": 1.2241162422768444e-06,
      "loss": 0.0002,
      "step": 3885
    },
    {
      "epoch": 62.18,
      "grad_norm": 0.003577684285119176,
      "learning_rate": 1.222015609646952e-06,
      "loss": 0.0001,
      "step": 3886
    },
    {
      "epoch": 62.19,
      "grad_norm": 0.12049218267202377,
      "learning_rate": 1.2199165300037358e-06,
      "loss": 0.0007,
      "step": 3887
    },
    {
      "epoch": 62.21,
      "grad_norm": 0.0034261569380760193,
      "learning_rate": 1.217819004210049e-06,
      "loss": 0.0001,
      "step": 3888
    },
    {
      "epoch": 62.22,
      "grad_norm": 0.9906449317932129,
      "learning_rate": 1.2157230331281028e-06,
      "loss": 0.0155,
      "step": 3889
    },
    {
      "epoch": 62.24,
      "grad_norm": 0.5904327034950256,
      "learning_rate": 1.2136286176194744e-06,
      "loss": 0.0077,
      "step": 3890
    },
    {
      "epoch": 62.26,
      "grad_norm": 0.4880281388759613,
      "learning_rate": 1.2115357585450965e-06,
      "loss": 0.0029,
      "step": 3891
    },
    {
      "epoch": 62.27,
      "grad_norm": 0.445649117231369,
      "learning_rate": 1.2094444567652652e-06,
      "loss": 0.0025,
      "step": 3892
    },
    {
      "epoch": 62.29,
      "grad_norm": 0.5160130262374878,
      "learning_rate": 1.2073547131396357e-06,
      "loss": 0.0068,
      "step": 3893
    },
    {
      "epoch": 62.3,
      "grad_norm": 0.15476250648498535,
      "learning_rate": 1.205266528527223e-06,
      "loss": 0.002,
      "step": 3894
    },
    {
      "epoch": 62.32,
      "grad_norm": 0.4213283658027649,
      "learning_rate": 1.203179903786401e-06,
      "loss": 0.0026,
      "step": 3895
    },
    {
      "epoch": 62.34,
      "grad_norm": 0.030092457309365273,
      "learning_rate": 1.2010948397749022e-06,
      "loss": 0.0002,
      "step": 3896
    },
    {
      "epoch": 62.35,
      "grad_norm": 0.003890745574608445,
      "learning_rate": 1.1990113373498174e-06,
      "loss": 0.0001,
      "step": 3897
    },
    {
      "epoch": 62.37,
      "grad_norm": 0.5426028370857239,
      "learning_rate": 1.1969293973675961e-06,
      "loss": 0.0032,
      "step": 3898
    },
    {
      "epoch": 62.38,
      "grad_norm": 0.1529855728149414,
      "learning_rate": 1.1948490206840447e-06,
      "loss": 0.0007,
      "step": 3899
    },
    {
      "epoch": 62.4,
      "grad_norm": 0.9889546036720276,
      "learning_rate": 1.1927702081543279e-06,
      "loss": 0.0207,
      "step": 3900
    },
    {
      "epoch": 62.42,
      "grad_norm": 0.0037351897917687893,
      "learning_rate": 1.1906929606329682e-06,
      "loss": 0.0001,
      "step": 3901
    },
    {
      "epoch": 62.43,
      "grad_norm": 0.0036948127672076225,
      "learning_rate": 1.188617278973841e-06,
      "loss": 0.0001,
      "step": 3902
    },
    {
      "epoch": 62.45,
      "grad_norm": 0.3427668511867523,
      "learning_rate": 1.1865431640301816e-06,
      "loss": 0.0023,
      "step": 3903
    },
    {
      "epoch": 62.46,
      "grad_norm": 0.0038035153411328793,
      "learning_rate": 1.1844706166545811e-06,
      "loss": 0.0001,
      "step": 3904
    },
    {
      "epoch": 62.48,
      "grad_norm": 0.005064764525741339,
      "learning_rate": 1.1823996376989849e-06,
      "loss": 0.0001,
      "step": 3905
    },
    {
      "epoch": 62.5,
      "grad_norm": 0.005078479647636414,
      "learning_rate": 1.1803302280146938e-06,
      "loss": 0.0002,
      "step": 3906
    },
    {
      "epoch": 62.51,
      "grad_norm": 0.3018219769001007,
      "learning_rate": 1.1782623884523647e-06,
      "loss": 0.0018,
      "step": 3907
    },
    {
      "epoch": 62.53,
      "grad_norm": 0.0022261322010308504,
      "learning_rate": 1.1761961198620081e-06,
      "loss": 0.0,
      "step": 3908
    },
    {
      "epoch": 62.54,
      "grad_norm": 0.0039526475593447685,
      "learning_rate": 1.174131423092989e-06,
      "loss": 0.0001,
      "step": 3909
    },
    {
      "epoch": 62.56,
      "grad_norm": 0.23767241835594177,
      "learning_rate": 1.1720682989940264e-06,
      "loss": 0.0017,
      "step": 3910
    },
    {
      "epoch": 62.58,
      "grad_norm": 0.004181737545877695,
      "learning_rate": 1.1700067484131932e-06,
      "loss": 0.0001,
      "step": 3911
    },
    {
      "epoch": 62.59,
      "grad_norm": 0.8613832592964172,
      "learning_rate": 1.1679467721979132e-06,
      "loss": 0.0056,
      "step": 3912
    },
    {
      "epoch": 62.61,
      "grad_norm": 0.9019849896430969,
      "learning_rate": 1.1658883711949658e-06,
      "loss": 0.009,
      "step": 3913
    },
    {
      "epoch": 62.62,
      "grad_norm": 0.004297049716114998,
      "learning_rate": 1.1638315462504817e-06,
      "loss": 0.0001,
      "step": 3914
    },
    {
      "epoch": 62.64,
      "grad_norm": 0.054641254246234894,
      "learning_rate": 1.1617762982099446e-06,
      "loss": 0.0004,
      "step": 3915
    },
    {
      "epoch": 62.66,
      "grad_norm": 0.0502154715359211,
      "learning_rate": 1.159722627918189e-06,
      "loss": 0.0003,
      "step": 3916
    },
    {
      "epoch": 62.67,
      "grad_norm": 0.0043874941766262054,
      "learning_rate": 1.1576705362194008e-06,
      "loss": 0.0001,
      "step": 3917
    },
    {
      "epoch": 62.69,
      "grad_norm": 0.8712506294250488,
      "learning_rate": 1.1556200239571175e-06,
      "loss": 0.0089,
      "step": 3918
    },
    {
      "epoch": 62.7,
      "grad_norm": 0.00436814408749342,
      "learning_rate": 1.1535710919742278e-06,
      "loss": 0.0001,
      "step": 3919
    },
    {
      "epoch": 62.72,
      "grad_norm": 0.04799388349056244,
      "learning_rate": 1.1515237411129698e-06,
      "loss": 0.0005,
      "step": 3920
    },
    {
      "epoch": 62.74,
      "grad_norm": 0.0032559935934841633,
      "learning_rate": 1.1494779722149334e-06,
      "loss": 0.0001,
      "step": 3921
    },
    {
      "epoch": 62.75,
      "grad_norm": 0.0030372359324246645,
      "learning_rate": 1.1474337861210543e-06,
      "loss": 0.0001,
      "step": 3922
    },
    {
      "epoch": 62.77,
      "grad_norm": 0.7659863829612732,
      "learning_rate": 1.145391183671622e-06,
      "loss": 0.0053,
      "step": 3923
    },
    {
      "epoch": 62.78,
      "grad_norm": 0.5265865921974182,
      "learning_rate": 1.1433501657062723e-06,
      "loss": 0.0063,
      "step": 3924
    },
    {
      "epoch": 62.8,
      "grad_norm": 0.4975990056991577,
      "learning_rate": 1.141310733063991e-06,
      "loss": 0.0031,
      "step": 3925
    },
    {
      "epoch": 62.82,
      "grad_norm": 0.0034369342029094696,
      "learning_rate": 1.1392728865831127e-06,
      "loss": 0.0001,
      "step": 3926
    },
    {
      "epoch": 62.83,
      "grad_norm": 0.002850508550181985,
      "learning_rate": 1.1372366271013175e-06,
      "loss": 0.0001,
      "step": 3927
    },
    {
      "epoch": 62.85,
      "grad_norm": 1.090532898902893,
      "learning_rate": 1.135201955455636e-06,
      "loss": 0.0173,
      "step": 3928
    },
    {
      "epoch": 62.86,
      "grad_norm": 0.7263997793197632,
      "learning_rate": 1.133168872482444e-06,
      "loss": 0.0053,
      "step": 3929
    },
    {
      "epoch": 62.88,
      "grad_norm": 0.03371249511837959,
      "learning_rate": 1.1311373790174656e-06,
      "loss": 0.0005,
      "step": 3930
    },
    {
      "epoch": 62.9,
      "grad_norm": 0.9091538786888123,
      "learning_rate": 1.1291074758957715e-06,
      "loss": 0.0123,
      "step": 3931
    },
    {
      "epoch": 62.91,
      "grad_norm": 0.5788856744766235,
      "learning_rate": 1.1270791639517786e-06,
      "loss": 0.0056,
      "step": 3932
    },
    {
      "epoch": 62.93,
      "grad_norm": 1.0834152698516846,
      "learning_rate": 1.1250524440192472e-06,
      "loss": 0.0148,
      "step": 3933
    },
    {
      "epoch": 62.94,
      "grad_norm": 0.0028920131735503674,
      "learning_rate": 1.1230273169312878e-06,
      "loss": 0.0001,
      "step": 3934
    },
    {
      "epoch": 62.96,
      "grad_norm": 0.7086097598075867,
      "learning_rate": 1.1210037835203508e-06,
      "loss": 0.0097,
      "step": 3935
    },
    {
      "epoch": 62.98,
      "grad_norm": 0.004319139290601015,
      "learning_rate": 1.118981844618236e-06,
      "loss": 0.0001,
      "step": 3936
    },
    {
      "epoch": 62.99,
      "grad_norm": 0.7687281370162964,
      "learning_rate": 1.1169615010560863e-06,
      "loss": 0.0087,
      "step": 3937
    },
    {
      "epoch": 63.01,
      "grad_norm": 0.6269226670265198,
      "learning_rate": 1.1149427536643882e-06,
      "loss": 0.0052,
      "step": 3938
    },
    {
      "epoch": 63.02,
      "grad_norm": 0.0033369786106050014,
      "learning_rate": 1.1129256032729724e-06,
      "loss": 0.0001,
      "step": 3939
    },
    {
      "epoch": 63.04,
      "grad_norm": 0.907423734664917,
      "learning_rate": 1.1109100507110133e-06,
      "loss": 0.0121,
      "step": 3940
    },
    {
      "epoch": 63.06,
      "grad_norm": 1.118916392326355,
      "learning_rate": 1.108896096807029e-06,
      "loss": 0.0138,
      "step": 3941
    },
    {
      "epoch": 63.07,
      "grad_norm": 0.40792033076286316,
      "learning_rate": 1.106883742388879e-06,
      "loss": 0.0022,
      "step": 3942
    },
    {
      "epoch": 63.09,
      "grad_norm": 0.0031005293130874634,
      "learning_rate": 1.1048729882837671e-06,
      "loss": 0.0001,
      "step": 3943
    },
    {
      "epoch": 63.1,
      "grad_norm": 0.050873447209596634,
      "learning_rate": 1.1028638353182392e-06,
      "loss": 0.0003,
      "step": 3944
    },
    {
      "epoch": 63.12,
      "grad_norm": 0.16863468289375305,
      "learning_rate": 1.1008562843181796e-06,
      "loss": 0.001,
      "step": 3945
    },
    {
      "epoch": 63.14,
      "grad_norm": 0.07294051349163055,
      "learning_rate": 1.0988503361088177e-06,
      "loss": 0.0006,
      "step": 3946
    },
    {
      "epoch": 63.15,
      "grad_norm": 0.728004515171051,
      "learning_rate": 1.0968459915147234e-06,
      "loss": 0.0124,
      "step": 3947
    },
    {
      "epoch": 63.17,
      "grad_norm": 0.05159793421626091,
      "learning_rate": 1.0948432513598073e-06,
      "loss": 0.0003,
      "step": 3948
    },
    {
      "epoch": 63.18,
      "grad_norm": 0.0031117741018533707,
      "learning_rate": 1.0928421164673191e-06,
      "loss": 0.0001,
      "step": 3949
    },
    {
      "epoch": 63.2,
      "grad_norm": 0.9887914657592773,
      "learning_rate": 1.0908425876598512e-06,
      "loss": 0.0142,
      "step": 3950
    },
    {
      "epoch": 63.22,
      "grad_norm": 0.0035305956844240427,
      "learning_rate": 1.0888446657593337e-06,
      "loss": 0.0001,
      "step": 3951
    },
    {
      "epoch": 63.23,
      "grad_norm": 0.0030742257367819548,
      "learning_rate": 1.086848351587037e-06,
      "loss": 0.0001,
      "step": 3952
    },
    {
      "epoch": 63.25,
      "grad_norm": 0.003146521747112274,
      "learning_rate": 1.08485364596357e-06,
      "loss": 0.0001,
      "step": 3953
    },
    {
      "epoch": 63.26,
      "grad_norm": 0.22926193475723267,
      "learning_rate": 1.0828605497088823e-06,
      "loss": 0.0018,
      "step": 3954
    },
    {
      "epoch": 63.28,
      "grad_norm": 0.002280719578266144,
      "learning_rate": 1.0808690636422587e-06,
      "loss": 0.0,
      "step": 3955
    },
    {
      "epoch": 63.3,
      "grad_norm": 0.002558825770393014,
      "learning_rate": 1.0788791885823236e-06,
      "loss": 0.0001,
      "step": 3956
    },
    {
      "epoch": 63.31,
      "grad_norm": 0.615929126739502,
      "learning_rate": 1.076890925347041e-06,
      "loss": 0.0035,
      "step": 3957
    },
    {
      "epoch": 63.33,
      "grad_norm": 0.7821260690689087,
      "learning_rate": 1.0749042747537097e-06,
      "loss": 0.0048,
      "step": 3958
    },
    {
      "epoch": 63.34,
      "grad_norm": 0.004486814606934786,
      "learning_rate": 1.0729192376189678e-06,
      "loss": 0.0001,
      "step": 3959
    },
    {
      "epoch": 63.36,
      "grad_norm": 0.0039117899723351,
      "learning_rate": 1.0709358147587883e-06,
      "loss": 0.0001,
      "step": 3960
    },
    {
      "epoch": 63.38,
      "grad_norm": 0.3192325830459595,
      "learning_rate": 1.0689540069884814e-06,
      "loss": 0.0027,
      "step": 3961
    },
    {
      "epoch": 63.39,
      "grad_norm": 0.0027573707047849894,
      "learning_rate": 1.0669738151226939e-06,
      "loss": 0.0001,
      "step": 3962
    },
    {
      "epoch": 63.41,
      "grad_norm": 0.0038676403928548098,
      "learning_rate": 1.0649952399754077e-06,
      "loss": 0.0001,
      "step": 3963
    },
    {
      "epoch": 63.42,
      "grad_norm": 0.8768360018730164,
      "learning_rate": 1.06301828235994e-06,
      "loss": 0.0055,
      "step": 3964
    },
    {
      "epoch": 63.44,
      "grad_norm": 0.8921276330947876,
      "learning_rate": 1.0610429430889451e-06,
      "loss": 0.0119,
      "step": 3965
    },
    {
      "epoch": 63.46,
      "grad_norm": 0.004212399944663048,
      "learning_rate": 1.0590692229744075e-06,
      "loss": 0.0001,
      "step": 3966
    },
    {
      "epoch": 63.47,
      "grad_norm": 0.5853251218795776,
      "learning_rate": 1.05709712282765e-06,
      "loss": 0.0035,
      "step": 3967
    },
    {
      "epoch": 63.49,
      "grad_norm": 0.0044325245544314384,
      "learning_rate": 1.0551266434593293e-06,
      "loss": 0.0001,
      "step": 3968
    },
    {
      "epoch": 63.5,
      "grad_norm": 0.003064177231863141,
      "learning_rate": 1.0531577856794346e-06,
      "loss": 0.0001,
      "step": 3969
    },
    {
      "epoch": 63.52,
      "grad_norm": 0.004508074373006821,
      "learning_rate": 1.0511905502972885e-06,
      "loss": 0.0001,
      "step": 3970
    },
    {
      "epoch": 63.54,
      "grad_norm": 0.04357820749282837,
      "learning_rate": 1.049224938121548e-06,
      "loss": 0.0003,
      "step": 3971
    },
    {
      "epoch": 63.55,
      "grad_norm": 0.003019952215254307,
      "learning_rate": 1.0472609499602017e-06,
      "loss": 0.0001,
      "step": 3972
    },
    {
      "epoch": 63.57,
      "grad_norm": 0.0037611902225762606,
      "learning_rate": 1.0452985866205706e-06,
      "loss": 0.0001,
      "step": 3973
    },
    {
      "epoch": 63.58,
      "grad_norm": 0.9188544154167175,
      "learning_rate": 1.0433378489093082e-06,
      "loss": 0.0135,
      "step": 3974
    },
    {
      "epoch": 63.6,
      "grad_norm": 0.0028104502707719803,
      "learning_rate": 1.041378737632402e-06,
      "loss": 0.0001,
      "step": 3975
    },
    {
      "epoch": 63.62,
      "grad_norm": 0.16028259694576263,
      "learning_rate": 1.0394212535951642e-06,
      "loss": 0.0012,
      "step": 3976
    },
    {
      "epoch": 63.63,
      "grad_norm": 0.004916119854897261,
      "learning_rate": 1.037465397602246e-06,
      "loss": 0.0002,
      "step": 3977
    },
    {
      "epoch": 63.65,
      "grad_norm": 0.5380521416664124,
      "learning_rate": 1.0355111704576237e-06,
      "loss": 0.0047,
      "step": 3978
    },
    {
      "epoch": 63.66,
      "grad_norm": 0.8237220644950867,
      "learning_rate": 1.0335585729646081e-06,
      "loss": 0.0112,
      "step": 3979
    },
    {
      "epoch": 63.68,
      "grad_norm": 0.005123618990182877,
      "learning_rate": 1.031607605925839e-06,
      "loss": 0.0002,
      "step": 3980
    },
    {
      "epoch": 63.7,
      "grad_norm": 0.8956588506698608,
      "learning_rate": 1.0296582701432823e-06,
      "loss": 0.0092,
      "step": 3981
    },
    {
      "epoch": 63.71,
      "grad_norm": 0.8508036732673645,
      "learning_rate": 1.0277105664182375e-06,
      "loss": 0.0061,
      "step": 3982
    },
    {
      "epoch": 63.73,
      "grad_norm": 0.9961603283882141,
      "learning_rate": 1.025764495551333e-06,
      "loss": 0.0123,
      "step": 3983
    },
    {
      "epoch": 63.74,
      "grad_norm": 0.0033800113014876842,
      "learning_rate": 1.023820058342524e-06,
      "loss": 0.0001,
      "step": 3984
    },
    {
      "epoch": 63.76,
      "grad_norm": 0.004414415918290615,
      "learning_rate": 1.0218772555910955e-06,
      "loss": 0.0001,
      "step": 3985
    },
    {
      "epoch": 63.78,
      "grad_norm": 0.6776500344276428,
      "learning_rate": 1.0199360880956605e-06,
      "loss": 0.0093,
      "step": 3986
    },
    {
      "epoch": 63.79,
      "grad_norm": 0.002515137428417802,
      "learning_rate": 1.0179965566541593e-06,
      "loss": 0.0,
      "step": 3987
    },
    {
      "epoch": 63.81,
      "grad_norm": 1.0250967741012573,
      "learning_rate": 1.016058662063862e-06,
      "loss": 0.0178,
      "step": 3988
    },
    {
      "epoch": 63.82,
      "grad_norm": 0.0033807756844908,
      "learning_rate": 1.01412240512136e-06,
      "loss": 0.0001,
      "step": 3989
    },
    {
      "epoch": 63.84,
      "grad_norm": 0.5417264699935913,
      "learning_rate": 1.0121877866225783e-06,
      "loss": 0.0044,
      "step": 3990
    },
    {
      "epoch": 63.86,
      "grad_norm": 0.3812725245952606,
      "learning_rate": 1.0102548073627645e-06,
      "loss": 0.003,
      "step": 3991
    },
    {
      "epoch": 63.87,
      "grad_norm": 0.002855179365724325,
      "learning_rate": 1.0083234681364934e-06,
      "loss": 0.0001,
      "step": 3992
    },
    {
      "epoch": 63.89,
      "grad_norm": 0.0033980808220803738,
      "learning_rate": 1.0063937697376657e-06,
      "loss": 0.0001,
      "step": 3993
    },
    {
      "epoch": 63.9,
      "grad_norm": 0.4286258816719055,
      "learning_rate": 1.0044657129595075e-06,
      "loss": 0.005,
      "step": 3994
    },
    {
      "epoch": 63.92,
      "grad_norm": 0.00356689584441483,
      "learning_rate": 1.0025392985945703e-06,
      "loss": 0.0001,
      "step": 3995
    },
    {
      "epoch": 63.94,
      "grad_norm": 1.1091340780258179,
      "learning_rate": 1.0006145274347306e-06,
      "loss": 0.0172,
      "step": 3996
    },
    {
      "epoch": 63.95,
      "grad_norm": 0.018198572099208832,
      "learning_rate": 9.986914002711878e-07,
      "loss": 0.0002,
      "step": 3997
    },
    {
      "epoch": 63.97,
      "grad_norm": 0.003609269857406616,
      "learning_rate": 9.967699178944695e-07,
      "loss": 0.0001,
      "step": 3998
    },
    {
      "epoch": 63.98,
      "grad_norm": 0.6458761692047119,
      "learning_rate": 9.948500810944218e-07,
      "loss": 0.0043,
      "step": 3999
    },
    {
      "epoch": 64.0,
      "grad_norm": 0.7712424993515015,
      "learning_rate": 9.929318906602176e-07,
      "loss": 0.0083,
      "step": 4000
    },
    {
      "epoch": 64.02,
      "grad_norm": 0.003728860756382346,
      "learning_rate": 9.91015347380353e-07,
      "loss": 0.0001,
      "step": 4001
    },
    {
      "epoch": 64.03,
      "grad_norm": 0.004399438854306936,
      "learning_rate": 9.891004520426461e-07,
      "loss": 0.0002,
      "step": 4002
    },
    {
      "epoch": 64.05,
      "grad_norm": 0.00382782774977386,
      "learning_rate": 9.87187205434239e-07,
      "loss": 0.0001,
      "step": 4003
    },
    {
      "epoch": 64.06,
      "grad_norm": 0.5665079355239868,
      "learning_rate": 9.852756083415944e-07,
      "loss": 0.0055,
      "step": 4004
    },
    {
      "epoch": 64.08,
      "grad_norm": 0.7214797735214233,
      "learning_rate": 9.833656615504978e-07,
      "loss": 0.009,
      "step": 4005
    },
    {
      "epoch": 64.1,
      "grad_norm": 0.49291202425956726,
      "learning_rate": 9.814573658460564e-07,
      "loss": 0.0052,
      "step": 4006
    },
    {
      "epoch": 64.11,
      "grad_norm": 0.0054122586734592915,
      "learning_rate": 9.795507220126975e-07,
      "loss": 0.0001,
      "step": 4007
    },
    {
      "epoch": 64.13,
      "grad_norm": 0.0323314405977726,
      "learning_rate": 9.776457308341735e-07,
      "loss": 0.0002,
      "step": 4008
    },
    {
      "epoch": 64.14,
      "grad_norm": 0.0037577373441308737,
      "learning_rate": 9.7574239309355e-07,
      "loss": 0.0001,
      "step": 4009
    },
    {
      "epoch": 64.16,
      "grad_norm": 0.003945897798985243,
      "learning_rate": 9.738407095732195e-07,
      "loss": 0.0001,
      "step": 4010
    },
    {
      "epoch": 64.18,
      "grad_norm": 0.79367595911026,
      "learning_rate": 9.719406810548914e-07,
      "loss": 0.0051,
      "step": 4011
    },
    {
      "epoch": 64.19,
      "grad_norm": 0.4734543561935425,
      "learning_rate": 9.70042308319597e-07,
      "loss": 0.0026,
      "step": 4012
    },
    {
      "epoch": 64.21,
      "grad_norm": 0.020559314638376236,
      "learning_rate": 9.68145592147684e-07,
      "loss": 0.0002,
      "step": 4013
    },
    {
      "epoch": 64.22,
      "grad_norm": 0.7231500148773193,
      "learning_rate": 9.662505333188221e-07,
      "loss": 0.0059,
      "step": 4014
    },
    {
      "epoch": 64.24,
      "grad_norm": 0.04948726296424866,
      "learning_rate": 9.643571326119982e-07,
      "loss": 0.0003,
      "step": 4015
    },
    {
      "epoch": 64.26,
      "grad_norm": 0.0030237638857215643,
      "learning_rate": 9.62465390805517e-07,
      "loss": 0.0001,
      "step": 4016
    },
    {
      "epoch": 64.27,
      "grad_norm": 0.6959714293479919,
      "learning_rate": 9.605753086770031e-07,
      "loss": 0.0088,
      "step": 4017
    },
    {
      "epoch": 64.29,
      "grad_norm": 0.5383080840110779,
      "learning_rate": 9.586868870033972e-07,
      "loss": 0.0033,
      "step": 4018
    },
    {
      "epoch": 64.3,
      "grad_norm": 0.0036781318485736847,
      "learning_rate": 9.568001265609593e-07,
      "loss": 0.0001,
      "step": 4019
    },
    {
      "epoch": 64.32,
      "grad_norm": 0.018725963309407234,
      "learning_rate": 9.549150281252633e-07,
      "loss": 0.0003,
      "step": 4020
    },
    {
      "epoch": 64.34,
      "grad_norm": 0.004239361267536879,
      "learning_rate": 9.53031592471203e-07,
      "loss": 0.0001,
      "step": 4021
    },
    {
      "epoch": 64.35,
      "grad_norm": 0.0026680154260247946,
      "learning_rate": 9.511498203729874e-07,
      "loss": 0.0001,
      "step": 4022
    },
    {
      "epoch": 64.37,
      "grad_norm": 0.786803662776947,
      "learning_rate": 9.492697126041429e-07,
      "loss": 0.0075,
      "step": 4023
    },
    {
      "epoch": 64.38,
      "grad_norm": 0.23578593134880066,
      "learning_rate": 9.473912699375093e-07,
      "loss": 0.0013,
      "step": 4024
    },
    {
      "epoch": 64.4,
      "grad_norm": 0.6405383944511414,
      "learning_rate": 9.455144931452459e-07,
      "loss": 0.0047,
      "step": 4025
    },
    {
      "epoch": 64.42,
      "grad_norm": 0.002969952067360282,
      "learning_rate": 9.436393829988217e-07,
      "loss": 0.0001,
      "step": 4026
    },
    {
      "epoch": 64.43,
      "grad_norm": 0.0021240650676190853,
      "learning_rate": 9.417659402690254e-07,
      "loss": 0.0,
      "step": 4027
    },
    {
      "epoch": 64.45,
      "grad_norm": 0.6477342844009399,
      "learning_rate": 9.398941657259575e-07,
      "loss": 0.0078,
      "step": 4028
    },
    {
      "epoch": 64.46,
      "grad_norm": 0.0036389122251421213,
      "learning_rate": 9.38024060139035e-07,
      "loss": 0.0001,
      "step": 4029
    },
    {
      "epoch": 64.48,
      "grad_norm": 0.6680753827095032,
      "learning_rate": 9.361556242769871e-07,
      "loss": 0.0041,
      "step": 4030
    },
    {
      "epoch": 64.5,
      "grad_norm": 0.5665062665939331,
      "learning_rate": 9.34288858907858e-07,
      "loss": 0.004,
      "step": 4031
    },
    {
      "epoch": 64.51,
      "grad_norm": 0.5860422253608704,
      "learning_rate": 9.324237647990026e-07,
      "loss": 0.0067,
      "step": 4032
    },
    {
      "epoch": 64.53,
      "grad_norm": 0.004197116009891033,
      "learning_rate": 9.305603427170917e-07,
      "loss": 0.0001,
      "step": 4033
    },
    {
      "epoch": 64.54,
      "grad_norm": 0.7546508312225342,
      "learning_rate": 9.286985934281079e-07,
      "loss": 0.0097,
      "step": 4034
    },
    {
      "epoch": 64.56,
      "grad_norm": 0.5191752314567566,
      "learning_rate": 9.26838517697346e-07,
      "loss": 0.0036,
      "step": 4035
    },
    {
      "epoch": 64.58,
      "grad_norm": 0.1199197918176651,
      "learning_rate": 9.249801162894123e-07,
      "loss": 0.0016,
      "step": 4036
    },
    {
      "epoch": 64.59,
      "grad_norm": 0.0023564030416309834,
      "learning_rate": 9.231233899682262e-07,
      "loss": 0.0,
      "step": 4037
    },
    {
      "epoch": 64.61,
      "grad_norm": 0.0031390758231282234,
      "learning_rate": 9.212683394970173e-07,
      "loss": 0.0001,
      "step": 4038
    },
    {
      "epoch": 64.62,
      "grad_norm": 0.6825510263442993,
      "learning_rate": 9.194149656383267e-07,
      "loss": 0.0092,
      "step": 4039
    },
    {
      "epoch": 64.64,
      "grad_norm": 0.038701899349689484,
      "learning_rate": 9.175632691540065e-07,
      "loss": 0.0002,
      "step": 4040
    },
    {
      "epoch": 64.66,
      "grad_norm": 0.0047636134549975395,
      "learning_rate": 9.157132508052208e-07,
      "loss": 0.0002,
      "step": 4041
    },
    {
      "epoch": 64.67,
      "grad_norm": 1.1739122867584229,
      "learning_rate": 9.138649113524389e-07,
      "loss": 0.0187,
      "step": 4042
    },
    {
      "epoch": 64.69,
      "grad_norm": 0.5878828167915344,
      "learning_rate": 9.12018251555446e-07,
      "loss": 0.0035,
      "step": 4043
    },
    {
      "epoch": 64.7,
      "grad_norm": 0.003080827184021473,
      "learning_rate": 9.101732721733325e-07,
      "loss": 0.0001,
      "step": 4044
    },
    {
      "epoch": 64.72,
      "grad_norm": 1.1331838369369507,
      "learning_rate": 9.083299739645007e-07,
      "loss": 0.0215,
      "step": 4045
    },
    {
      "epoch": 64.74,
      "grad_norm": 0.004479533061385155,
      "learning_rate": 9.064883576866612e-07,
      "loss": 0.0001,
      "step": 4046
    },
    {
      "epoch": 64.75,
      "grad_norm": 0.004910847172141075,
      "learning_rate": 9.046484240968317e-07,
      "loss": 0.0002,
      "step": 4047
    },
    {
      "epoch": 64.77,
      "grad_norm": 0.0035396020393818617,
      "learning_rate": 9.028101739513406e-07,
      "loss": 0.0001,
      "step": 4048
    },
    {
      "epoch": 64.78,
      "grad_norm": 0.7831310033798218,
      "learning_rate": 9.009736080058223e-07,
      "loss": 0.01,
      "step": 4049
    },
    {
      "epoch": 64.8,
      "grad_norm": 0.002826486714184284,
      "learning_rate": 8.991387270152202e-07,
      "loss": 0.0001,
      "step": 4050
    },
    {
      "epoch": 64.82,
      "grad_norm": 1.4684621095657349,
      "learning_rate": 8.973055317337848e-07,
      "loss": 0.0151,
      "step": 4051
    },
    {
      "epoch": 64.83,
      "grad_norm": 0.06768236309289932,
      "learning_rate": 8.954740229150732e-07,
      "loss": 0.0003,
      "step": 4052
    },
    {
      "epoch": 64.85,
      "grad_norm": 0.7056366801261902,
      "learning_rate": 8.936442013119489e-07,
      "loss": 0.0061,
      "step": 4053
    },
    {
      "epoch": 64.86,
      "grad_norm": 0.004421372897922993,
      "learning_rate": 8.918160676765824e-07,
      "loss": 0.0001,
      "step": 4054
    },
    {
      "epoch": 64.88,
      "grad_norm": 0.004485521465539932,
      "learning_rate": 8.899896227604509e-07,
      "loss": 0.0001,
      "step": 4055
    },
    {
      "epoch": 64.9,
      "grad_norm": 0.2907845377922058,
      "learning_rate": 8.881648673143367e-07,
      "loss": 0.0029,
      "step": 4056
    },
    {
      "epoch": 64.91,
      "grad_norm": 0.5723572969436646,
      "learning_rate": 8.86341802088328e-07,
      "loss": 0.0032,
      "step": 4057
    },
    {
      "epoch": 64.93,
      "grad_norm": 0.602088212966919,
      "learning_rate": 8.845204278318182e-07,
      "loss": 0.0035,
      "step": 4058
    },
    {
      "epoch": 64.94,
      "grad_norm": 0.06310062110424042,
      "learning_rate": 8.82700745293505e-07,
      "loss": 0.0004,
      "step": 4059
    },
    {
      "epoch": 64.96,
      "grad_norm": 0.06348360329866409,
      "learning_rate": 8.808827552213917e-07,
      "loss": 0.0004,
      "step": 4060
    },
    {
      "epoch": 64.98,
      "grad_norm": 0.00408380338922143,
      "learning_rate": 8.79066458362785e-07,
      "loss": 0.0001,
      "step": 4061
    },
    {
      "epoch": 64.99,
      "grad_norm": 0.816996693611145,
      "learning_rate": 8.772518554642973e-07,
      "loss": 0.0051,
      "step": 4062
    },
    {
      "epoch": 65.01,
      "grad_norm": 0.004147068131715059,
      "learning_rate": 8.754389472718406e-07,
      "loss": 0.0001,
      "step": 4063
    },
    {
      "epoch": 65.02,
      "grad_norm": 0.5820544958114624,
      "learning_rate": 8.736277345306343e-07,
      "loss": 0.0046,
      "step": 4064
    },
    {
      "epoch": 65.04,
      "grad_norm": 0.984135091304779,
      "learning_rate": 8.718182179851998e-07,
      "loss": 0.0133,
      "step": 4065
    },
    {
      "epoch": 65.06,
      "grad_norm": 0.8932080268859863,
      "learning_rate": 8.700103983793606e-07,
      "loss": 0.0115,
      "step": 4066
    },
    {
      "epoch": 65.07,
      "grad_norm": 1.2678414583206177,
      "learning_rate": 8.682042764562431e-07,
      "loss": 0.0212,
      "step": 4067
    },
    {
      "epoch": 65.09,
      "grad_norm": 0.0030250127892941236,
      "learning_rate": 8.663998529582768e-07,
      "loss": 0.0001,
      "step": 4068
    },
    {
      "epoch": 65.1,
      "grad_norm": 0.7466049194335938,
      "learning_rate": 8.645971286271903e-07,
      "loss": 0.0096,
      "step": 4069
    },
    {
      "epoch": 65.12,
      "grad_norm": 0.024958204478025436,
      "learning_rate": 8.627961042040183e-07,
      "loss": 0.0005,
      "step": 4070
    },
    {
      "epoch": 65.14,
      "grad_norm": 0.34796640276908875,
      "learning_rate": 8.609967804290903e-07,
      "loss": 0.0021,
      "step": 4071
    },
    {
      "epoch": 65.15,
      "grad_norm": 0.5799095034599304,
      "learning_rate": 8.591991580420422e-07,
      "loss": 0.0066,
      "step": 4072
    },
    {
      "epoch": 65.17,
      "grad_norm": 0.9484052062034607,
      "learning_rate": 8.574032377818087e-07,
      "loss": 0.0079,
      "step": 4073
    },
    {
      "epoch": 65.18,
      "grad_norm": 0.03540205955505371,
      "learning_rate": 8.556090203866246e-07,
      "loss": 0.0002,
      "step": 4074
    },
    {
      "epoch": 65.2,
      "grad_norm": 0.6205607056617737,
      "learning_rate": 8.538165065940263e-07,
      "loss": 0.0078,
      "step": 4075
    },
    {
      "epoch": 65.22,
      "grad_norm": 0.451732337474823,
      "learning_rate": 8.520256971408453e-07,
      "loss": 0.0028,
      "step": 4076
    },
    {
      "epoch": 65.23,
      "grad_norm": 0.002727413084357977,
      "learning_rate": 8.502365927632178e-07,
      "loss": 0.0001,
      "step": 4077
    },
    {
      "epoch": 65.25,
      "grad_norm": 0.003953489009290934,
      "learning_rate": 8.484491941965767e-07,
      "loss": 0.0001,
      "step": 4078
    },
    {
      "epoch": 65.26,
      "grad_norm": 0.0037048938684165478,
      "learning_rate": 8.466635021756542e-07,
      "loss": 0.0001,
      "step": 4079
    },
    {
      "epoch": 65.28,
      "grad_norm": 0.048950113356113434,
      "learning_rate": 8.448795174344803e-07,
      "loss": 0.0002,
      "step": 4080
    },
    {
      "epoch": 65.3,
      "grad_norm": 0.0033427232410758734,
      "learning_rate": 8.430972407063848e-07,
      "loss": 0.0001,
      "step": 4081
    },
    {
      "epoch": 65.31,
      "grad_norm": 0.9072204232215881,
      "learning_rate": 8.41316672723993e-07,
      "loss": 0.0099,
      "step": 4082
    },
    {
      "epoch": 65.33,
      "grad_norm": 0.004561810754239559,
      "learning_rate": 8.395378142192307e-07,
      "loss": 0.0001,
      "step": 4083
    },
    {
      "epoch": 65.34,
      "grad_norm": 0.004019733052700758,
      "learning_rate": 8.377606659233179e-07,
      "loss": 0.0001,
      "step": 4084
    },
    {
      "epoch": 65.36,
      "grad_norm": 0.5771051645278931,
      "learning_rate": 8.359852285667752e-07,
      "loss": 0.0054,
      "step": 4085
    },
    {
      "epoch": 65.38,
      "grad_norm": 0.004096511751413345,
      "learning_rate": 8.342115028794151e-07,
      "loss": 0.0001,
      "step": 4086
    },
    {
      "epoch": 65.39,
      "grad_norm": 0.0033908977638930082,
      "learning_rate": 8.324394895903509e-07,
      "loss": 0.0001,
      "step": 4087
    },
    {
      "epoch": 65.41,
      "grad_norm": 0.004323126282542944,
      "learning_rate": 8.306691894279894e-07,
      "loss": 0.0001,
      "step": 4088
    },
    {
      "epoch": 65.42,
      "grad_norm": 0.47870123386383057,
      "learning_rate": 8.289006031200352e-07,
      "loss": 0.0037,
      "step": 4089
    },
    {
      "epoch": 65.44,
      "grad_norm": 0.7476405501365662,
      "learning_rate": 8.271337313934869e-07,
      "loss": 0.0059,
      "step": 4090
    },
    {
      "epoch": 65.46,
      "grad_norm": 0.4293346405029297,
      "learning_rate": 8.253685749746388e-07,
      "loss": 0.0024,
      "step": 4091
    },
    {
      "epoch": 65.47,
      "grad_norm": 0.0038158660754561424,
      "learning_rate": 8.2360513458908e-07,
      "loss": 0.0001,
      "step": 4092
    },
    {
      "epoch": 65.49,
      "grad_norm": 0.003422992303967476,
      "learning_rate": 8.218434109616935e-07,
      "loss": 0.0001,
      "step": 4093
    },
    {
      "epoch": 65.5,
      "grad_norm": 0.004156666807830334,
      "learning_rate": 8.200834048166584e-07,
      "loss": 0.0001,
      "step": 4094
    },
    {
      "epoch": 65.52,
      "grad_norm": 0.7096226811408997,
      "learning_rate": 8.183251168774476e-07,
      "loss": 0.0045,
      "step": 4095
    },
    {
      "epoch": 65.54,
      "grad_norm": 0.07498205453157425,
      "learning_rate": 8.16568547866824e-07,
      "loss": 0.0005,
      "step": 4096
    },
    {
      "epoch": 65.55,
      "grad_norm": 0.023089304566383362,
      "learning_rate": 8.148136985068489e-07,
      "loss": 0.0004,
      "step": 4097
    },
    {
      "epoch": 65.57,
      "grad_norm": 0.003714838996529579,
      "learning_rate": 8.130605695188736e-07,
      "loss": 0.0001,
      "step": 4098
    },
    {
      "epoch": 65.58,
      "grad_norm": 0.0031285195145756006,
      "learning_rate": 8.113091616235436e-07,
      "loss": 0.0001,
      "step": 4099
    },
    {
      "epoch": 65.6,
      "grad_norm": 0.23228925466537476,
      "learning_rate": 8.095594755407971e-07,
      "loss": 0.0012,
      "step": 4100
    },
    {
      "epoch": 65.62,
      "grad_norm": 0.5249822735786438,
      "learning_rate": 8.078115119898628e-07,
      "loss": 0.0047,
      "step": 4101
    },
    {
      "epoch": 65.63,
      "grad_norm": 0.49707362055778503,
      "learning_rate": 8.060652716892637e-07,
      "loss": 0.0054,
      "step": 4102
    },
    {
      "epoch": 65.65,
      "grad_norm": 0.04764117673039436,
      "learning_rate": 8.043207553568122e-07,
      "loss": 0.0007,
      "step": 4103
    },
    {
      "epoch": 65.66,
      "grad_norm": 0.004078978672623634,
      "learning_rate": 8.025779637096138e-07,
      "loss": 0.0001,
      "step": 4104
    },
    {
      "epoch": 65.68,
      "grad_norm": 0.0042493161745369434,
      "learning_rate": 8.008368974640634e-07,
      "loss": 0.0001,
      "step": 4105
    },
    {
      "epoch": 65.7,
      "grad_norm": 0.018468212336301804,
      "learning_rate": 7.990975573358501e-07,
      "loss": 0.0002,
      "step": 4106
    },
    {
      "epoch": 65.71,
      "grad_norm": 0.0032631014473736286,
      "learning_rate": 7.973599440399476e-07,
      "loss": 0.0001,
      "step": 4107
    },
    {
      "epoch": 65.73,
      "grad_norm": 0.0033888507168740034,
      "learning_rate": 7.956240582906244e-07,
      "loss": 0.0001,
      "step": 4108
    },
    {
      "epoch": 65.74,
      "grad_norm": 0.0035072427708655596,
      "learning_rate": 7.938899008014378e-07,
      "loss": 0.0001,
      "step": 4109
    },
    {
      "epoch": 65.76,
      "grad_norm": 0.6701639890670776,
      "learning_rate": 7.921574722852343e-07,
      "loss": 0.0041,
      "step": 4110
    },
    {
      "epoch": 65.78,
      "grad_norm": 0.0040874299593269825,
      "learning_rate": 7.904267734541499e-07,
      "loss": 0.0001,
      "step": 4111
    },
    {
      "epoch": 65.79,
      "grad_norm": 0.0037753144279122353,
      "learning_rate": 7.886978050196093e-07,
      "loss": 0.0001,
      "step": 4112
    },
    {
      "epoch": 65.81,
      "grad_norm": 0.6953150033950806,
      "learning_rate": 7.869705676923262e-07,
      "loss": 0.0053,
      "step": 4113
    },
    {
      "epoch": 65.82,
      "grad_norm": 1.0593146085739136,
      "learning_rate": 7.852450621823027e-07,
      "loss": 0.0131,
      "step": 4114
    },
    {
      "epoch": 65.84,
      "grad_norm": 0.05625782534480095,
      "learning_rate": 7.835212891988292e-07,
      "loss": 0.0003,
      "step": 4115
    },
    {
      "epoch": 65.86,
      "grad_norm": 0.7472305297851562,
      "learning_rate": 7.817992494504844e-07,
      "loss": 0.0047,
      "step": 4116
    },
    {
      "epoch": 65.87,
      "grad_norm": 0.002784005831927061,
      "learning_rate": 7.800789436451318e-07,
      "loss": 0.0001,
      "step": 4117
    },
    {
      "epoch": 65.89,
      "grad_norm": 0.546521782875061,
      "learning_rate": 7.783603724899258e-07,
      "loss": 0.0032,
      "step": 4118
    },
    {
      "epoch": 65.9,
      "grad_norm": 0.5983792543411255,
      "learning_rate": 7.766435366913044e-07,
      "loss": 0.008,
      "step": 4119
    },
    {
      "epoch": 65.92,
      "grad_norm": 0.05888860300183296,
      "learning_rate": 7.749284369549954e-07,
      "loss": 0.0004,
      "step": 4120
    },
    {
      "epoch": 65.94,
      "grad_norm": 0.6009966135025024,
      "learning_rate": 7.732150739860106e-07,
      "loss": 0.0066,
      "step": 4121
    },
    {
      "epoch": 65.95,
      "grad_norm": 0.00373273016884923,
      "learning_rate": 7.715034484886491e-07,
      "loss": 0.0001,
      "step": 4122
    },
    {
      "epoch": 65.97,
      "grad_norm": 0.2787715196609497,
      "learning_rate": 7.697935611664964e-07,
      "loss": 0.0027,
      "step": 4123
    },
    {
      "epoch": 65.98,
      "grad_norm": 0.024005407467484474,
      "learning_rate": 7.680854127224213e-07,
      "loss": 0.0003,
      "step": 4124
    },
    {
      "epoch": 66.0,
      "grad_norm": 0.9830479621887207,
      "learning_rate": 7.663790038585794e-07,
      "loss": 0.0146,
      "step": 4125
    },
    {
      "epoch": 66.02,
      "grad_norm": 0.2150007039308548,
      "learning_rate": 7.646743352764113e-07,
      "loss": 0.0012,
      "step": 4126
    },
    {
      "epoch": 66.03,
      "grad_norm": 0.004298641346395016,
      "learning_rate": 7.62971407676642e-07,
      "loss": 0.0001,
      "step": 4127
    },
    {
      "epoch": 66.05,
      "grad_norm": 0.004189483821392059,
      "learning_rate": 7.612702217592816e-07,
      "loss": 0.0001,
      "step": 4128
    },
    {
      "epoch": 66.06,
      "grad_norm": 0.0032290839590132236,
      "learning_rate": 7.595707782236211e-07,
      "loss": 0.0001,
      "step": 4129
    },
    {
      "epoch": 66.08,
      "grad_norm": 0.610346794128418,
      "learning_rate": 7.578730777682386e-07,
      "loss": 0.0072,
      "step": 4130
    },
    {
      "epoch": 66.1,
      "grad_norm": 0.002002433408051729,
      "learning_rate": 7.561771210909946e-07,
      "loss": 0.0,
      "step": 4131
    },
    {
      "epoch": 66.11,
      "grad_norm": 0.0028235316276550293,
      "learning_rate": 7.544829088890326e-07,
      "loss": 0.0001,
      "step": 4132
    },
    {
      "epoch": 66.13,
      "grad_norm": 0.003985504619777203,
      "learning_rate": 7.5279044185878e-07,
      "loss": 0.0001,
      "step": 4133
    },
    {
      "epoch": 66.14,
      "grad_norm": 0.0053891027346253395,
      "learning_rate": 7.510997206959453e-07,
      "loss": 0.0002,
      "step": 4134
    },
    {
      "epoch": 66.16,
      "grad_norm": 0.003492495510727167,
      "learning_rate": 7.494107460955207e-07,
      "loss": 0.0001,
      "step": 4135
    },
    {
      "epoch": 66.18,
      "grad_norm": 0.6563334465026855,
      "learning_rate": 7.477235187517795e-07,
      "loss": 0.007,
      "step": 4136
    },
    {
      "epoch": 66.19,
      "grad_norm": 0.7639674544334412,
      "learning_rate": 7.460380393582772e-07,
      "loss": 0.0068,
      "step": 4137
    },
    {
      "epoch": 66.21,
      "grad_norm": 0.003777121426537633,
      "learning_rate": 7.443543086078508e-07,
      "loss": 0.0001,
      "step": 4138
    },
    {
      "epoch": 66.22,
      "grad_norm": 0.004061535000801086,
      "learning_rate": 7.426723271926195e-07,
      "loss": 0.0001,
      "step": 4139
    },
    {
      "epoch": 66.24,
      "grad_norm": 0.3522927761077881,
      "learning_rate": 7.409920958039795e-07,
      "loss": 0.0028,
      "step": 4140
    },
    {
      "epoch": 66.26,
      "grad_norm": 0.74082350730896,
      "learning_rate": 7.39313615132613e-07,
      "loss": 0.0083,
      "step": 4141
    },
    {
      "epoch": 66.27,
      "grad_norm": 0.003699705470353365,
      "learning_rate": 7.376368858684785e-07,
      "loss": 0.0001,
      "step": 4142
    },
    {
      "epoch": 66.29,
      "grad_norm": 0.8944080471992493,
      "learning_rate": 7.359619087008169e-07,
      "loss": 0.009,
      "step": 4143
    },
    {
      "epoch": 66.3,
      "grad_norm": 0.6074298024177551,
      "learning_rate": 7.342886843181479e-07,
      "loss": 0.0053,
      "step": 4144
    },
    {
      "epoch": 66.32,
      "grad_norm": 0.05329392850399017,
      "learning_rate": 7.326172134082704e-07,
      "loss": 0.0004,
      "step": 4145
    },
    {
      "epoch": 66.34,
      "grad_norm": 0.0036533011589199305,
      "learning_rate": 7.309474966582636e-07,
      "loss": 0.0001,
      "step": 4146
    },
    {
      "epoch": 66.35,
      "grad_norm": 0.002370065078139305,
      "learning_rate": 7.292795347544851e-07,
      "loss": 0.0,
      "step": 4147
    },
    {
      "epoch": 66.37,
      "grad_norm": 0.29490289092063904,
      "learning_rate": 7.276133283825698e-07,
      "loss": 0.0016,
      "step": 4148
    },
    {
      "epoch": 66.38,
      "grad_norm": 0.6474591493606567,
      "learning_rate": 7.259488782274349e-07,
      "loss": 0.0067,
      "step": 4149
    },
    {
      "epoch": 66.4,
      "grad_norm": 0.0710640698671341,
      "learning_rate": 7.242861849732696e-07,
      "loss": 0.0004,
      "step": 4150
    },
    {
      "epoch": 66.42,
      "grad_norm": 0.004995508119463921,
      "learning_rate": 7.226252493035463e-07,
      "loss": 0.0002,
      "step": 4151
    },
    {
      "epoch": 66.43,
      "grad_norm": 0.003791520604863763,
      "learning_rate": 7.209660719010119e-07,
      "loss": 0.0001,
      "step": 4152
    },
    {
      "epoch": 66.45,
      "grad_norm": 0.715032696723938,
      "learning_rate": 7.193086534476923e-07,
      "loss": 0.0045,
      "step": 4153
    },
    {
      "epoch": 66.46,
      "grad_norm": 0.003050313564017415,
      "learning_rate": 7.176529946248894e-07,
      "loss": 0.0001,
      "step": 4154
    },
    {
      "epoch": 66.48,
      "grad_norm": 0.6721334457397461,
      "learning_rate": 7.159990961131818e-07,
      "loss": 0.0092,
      "step": 4155
    },
    {
      "epoch": 66.5,
      "grad_norm": 0.0459306463599205,
      "learning_rate": 7.143469585924251e-07,
      "loss": 0.0003,
      "step": 4156
    },
    {
      "epoch": 66.51,
      "grad_norm": 0.003748239716514945,
      "learning_rate": 7.126965827417509e-07,
      "loss": 0.0001,
      "step": 4157
    },
    {
      "epoch": 66.53,
      "grad_norm": 0.720164954662323,
      "learning_rate": 7.110479692395656e-07,
      "loss": 0.0102,
      "step": 4158
    },
    {
      "epoch": 66.54,
      "grad_norm": 0.02628306671977043,
      "learning_rate": 7.09401118763553e-07,
      "loss": 0.0004,
      "step": 4159
    },
    {
      "epoch": 66.56,
      "grad_norm": 0.1817632019519806,
      "learning_rate": 7.077560319906696e-07,
      "loss": 0.0009,
      "step": 4160
    },
    {
      "epoch": 66.58,
      "grad_norm": 0.650391161441803,
      "learning_rate": 7.061127095971504e-07,
      "loss": 0.0037,
      "step": 4161
    },
    {
      "epoch": 66.59,
      "grad_norm": 0.9027940034866333,
      "learning_rate": 7.044711522585007e-07,
      "loss": 0.0178,
      "step": 4162
    },
    {
      "epoch": 66.61,
      "grad_norm": 0.6661292910575867,
      "learning_rate": 7.02831360649504e-07,
      "loss": 0.0038,
      "step": 4163
    },
    {
      "epoch": 66.62,
      "grad_norm": 0.7147142887115479,
      "learning_rate": 7.011933354442168e-07,
      "loss": 0.0108,
      "step": 4164
    },
    {
      "epoch": 66.64,
      "grad_norm": 0.0038360613398253918,
      "learning_rate": 6.995570773159693e-07,
      "loss": 0.0001,
      "step": 4165
    },
    {
      "epoch": 66.66,
      "grad_norm": 0.8252167701721191,
      "learning_rate": 6.979225869373657e-07,
      "loss": 0.0074,
      "step": 4166
    },
    {
      "epoch": 66.67,
      "grad_norm": 0.012128693982958794,
      "learning_rate": 6.962898649802824e-07,
      "loss": 0.0003,
      "step": 4167
    },
    {
      "epoch": 66.69,
      "grad_norm": 0.31319350004196167,
      "learning_rate": 6.946589121158703e-07,
      "loss": 0.002,
      "step": 4168
    },
    {
      "epoch": 66.7,
      "grad_norm": 0.8989406824111938,
      "learning_rate": 6.930297290145527e-07,
      "loss": 0.0058,
      "step": 4169
    },
    {
      "epoch": 66.72,
      "grad_norm": 0.0049228244461119175,
      "learning_rate": 6.914023163460248e-07,
      "loss": 0.0002,
      "step": 4170
    },
    {
      "epoch": 66.74,
      "grad_norm": 0.0036606257781386375,
      "learning_rate": 6.89776674779255e-07,
      "loss": 0.0001,
      "step": 4171
    },
    {
      "epoch": 66.75,
      "grad_norm": 0.08777455985546112,
      "learning_rate": 6.881528049824837e-07,
      "loss": 0.0008,
      "step": 4172
    },
    {
      "epoch": 66.77,
      "grad_norm": 0.481178879737854,
      "learning_rate": 6.865307076232203e-07,
      "loss": 0.0027,
      "step": 4173
    },
    {
      "epoch": 66.78,
      "grad_norm": 0.0039038530085235834,
      "learning_rate": 6.849103833682491e-07,
      "loss": 0.0001,
      "step": 4174
    },
    {
      "epoch": 66.8,
      "grad_norm": 0.004296295344829559,
      "learning_rate": 6.832918328836247e-07,
      "loss": 0.0001,
      "step": 4175
    },
    {
      "epoch": 66.82,
      "grad_norm": 0.642219603061676,
      "learning_rate": 6.816750568346708e-07,
      "loss": 0.01,
      "step": 4176
    },
    {
      "epoch": 66.83,
      "grad_norm": 0.9780223965644836,
      "learning_rate": 6.800600558859838e-07,
      "loss": 0.013,
      "step": 4177
    },
    {
      "epoch": 66.85,
      "grad_norm": 0.003173015546053648,
      "learning_rate": 6.784468307014297e-07,
      "loss": 0.0001,
      "step": 4178
    },
    {
      "epoch": 66.86,
      "grad_norm": 0.6771435737609863,
      "learning_rate": 6.768353819441436e-07,
      "loss": 0.009,
      "step": 4179
    },
    {
      "epoch": 66.88,
      "grad_norm": 0.06481553614139557,
      "learning_rate": 6.752257102765325e-07,
      "loss": 0.0003,
      "step": 4180
    },
    {
      "epoch": 66.9,
      "grad_norm": 0.928124725818634,
      "learning_rate": 6.736178163602703e-07,
      "loss": 0.0058,
      "step": 4181
    },
    {
      "epoch": 66.91,
      "grad_norm": 0.28449270129203796,
      "learning_rate": 6.720117008563032e-07,
      "loss": 0.0036,
      "step": 4182
    },
    {
      "epoch": 66.93,
      "grad_norm": 0.0033940759021788836,
      "learning_rate": 6.704073644248427e-07,
      "loss": 0.0001,
      "step": 4183
    },
    {
      "epoch": 66.94,
      "grad_norm": 0.6123766899108887,
      "learning_rate": 6.688048077253712e-07,
      "loss": 0.0061,
      "step": 4184
    },
    {
      "epoch": 66.96,
      "grad_norm": 0.7852944135665894,
      "learning_rate": 6.6720403141664e-07,
      "loss": 0.0093,
      "step": 4185
    },
    {
      "epoch": 66.98,
      "grad_norm": 0.7364023923873901,
      "learning_rate": 6.656050361566679e-07,
      "loss": 0.0092,
      "step": 4186
    },
    {
      "epoch": 66.99,
      "grad_norm": 0.00342522282153368,
      "learning_rate": 6.640078226027402e-07,
      "loss": 0.0001,
      "step": 4187
    },
    {
      "epoch": 67.01,
      "grad_norm": 0.004194742534309626,
      "learning_rate": 6.624123914114122e-07,
      "loss": 0.0001,
      "step": 4188
    },
    {
      "epoch": 67.02,
      "grad_norm": 0.015493803657591343,
      "learning_rate": 6.608187432385055e-07,
      "loss": 0.0002,
      "step": 4189
    },
    {
      "epoch": 67.04,
      "grad_norm": 1.101728916168213,
      "learning_rate": 6.592268787391077e-07,
      "loss": 0.0139,
      "step": 4190
    },
    {
      "epoch": 67.06,
      "grad_norm": 0.004240931943058968,
      "learning_rate": 6.576367985675752e-07,
      "loss": 0.0001,
      "step": 4191
    },
    {
      "epoch": 67.07,
      "grad_norm": 0.5951599478721619,
      "learning_rate": 6.560485033775299e-07,
      "loss": 0.0052,
      "step": 4192
    },
    {
      "epoch": 67.09,
      "grad_norm": 0.0035356117878109217,
      "learning_rate": 6.544619938218588e-07,
      "loss": 0.0001,
      "step": 4193
    },
    {
      "epoch": 67.1,
      "grad_norm": 0.6693626046180725,
      "learning_rate": 6.528772705527165e-07,
      "loss": 0.0092,
      "step": 4194
    },
    {
      "epoch": 67.12,
      "grad_norm": 0.5623484253883362,
      "learning_rate": 6.512943342215234e-07,
      "loss": 0.0031,
      "step": 4195
    },
    {
      "epoch": 67.14,
      "grad_norm": 0.0038277809508144855,
      "learning_rate": 6.49713185478964e-07,
      "loss": 0.0001,
      "step": 4196
    },
    {
      "epoch": 67.15,
      "grad_norm": 1.0113426446914673,
      "learning_rate": 6.481338249749896e-07,
      "loss": 0.0129,
      "step": 4197
    },
    {
      "epoch": 67.17,
      "grad_norm": 0.022649789229035378,
      "learning_rate": 6.46556253358816e-07,
      "loss": 0.0004,
      "step": 4198
    },
    {
      "epoch": 67.18,
      "grad_norm": 0.6542933583259583,
      "learning_rate": 6.449804712789221e-07,
      "loss": 0.0034,
      "step": 4199
    },
    {
      "epoch": 67.2,
      "grad_norm": 0.5375327467918396,
      "learning_rate": 6.43406479383053e-07,
      "loss": 0.0043,
      "step": 4200
    },
    {
      "epoch": 67.22,
      "grad_norm": 0.05484946444630623,
      "learning_rate": 6.418342783182174e-07,
      "loss": 0.0002,
      "step": 4201
    },
    {
      "epoch": 67.23,
      "grad_norm": 0.7976310849189758,
      "learning_rate": 6.402638687306872e-07,
      "loss": 0.0109,
      "step": 4202
    },
    {
      "epoch": 67.25,
      "grad_norm": 0.8671992421150208,
      "learning_rate": 6.386952512660005e-07,
      "loss": 0.0085,
      "step": 4203
    },
    {
      "epoch": 67.26,
      "grad_norm": 1.0133891105651855,
      "learning_rate": 6.371284265689543e-07,
      "loss": 0.0082,
      "step": 4204
    },
    {
      "epoch": 67.28,
      "grad_norm": 0.04398300498723984,
      "learning_rate": 6.355633952836115e-07,
      "loss": 0.0005,
      "step": 4205
    },
    {
      "epoch": 67.3,
      "grad_norm": 0.8990647196769714,
      "learning_rate": 6.340001580532979e-07,
      "loss": 0.0124,
      "step": 4206
    },
    {
      "epoch": 67.31,
      "grad_norm": 0.8659600019454956,
      "learning_rate": 6.324387155206019e-07,
      "loss": 0.0056,
      "step": 4207
    },
    {
      "epoch": 67.33,
      "grad_norm": 0.5686002373695374,
      "learning_rate": 6.308790683273719e-07,
      "loss": 0.0057,
      "step": 4208
    },
    {
      "epoch": 67.34,
      "grad_norm": 0.6929651498794556,
      "learning_rate": 6.293212171147206e-07,
      "loss": 0.0043,
      "step": 4209
    },
    {
      "epoch": 67.36,
      "grad_norm": 0.003703888040035963,
      "learning_rate": 6.277651625230219e-07,
      "loss": 0.0001,
      "step": 4210
    },
    {
      "epoch": 67.38,
      "grad_norm": 0.8735246062278748,
      "learning_rate": 6.262109051919113e-07,
      "loss": 0.0112,
      "step": 4211
    },
    {
      "epoch": 67.39,
      "grad_norm": 0.5760475397109985,
      "learning_rate": 6.24658445760285e-07,
      "loss": 0.0065,
      "step": 4212
    },
    {
      "epoch": 67.41,
      "grad_norm": 0.003380553564056754,
      "learning_rate": 6.231077848663008e-07,
      "loss": 0.0001,
      "step": 4213
    },
    {
      "epoch": 67.42,
      "grad_norm": 0.005327911116182804,
      "learning_rate": 6.215589231473762e-07,
      "loss": 0.0002,
      "step": 4214
    },
    {
      "epoch": 67.44,
      "grad_norm": 0.5050089955329895,
      "learning_rate": 6.200118612401918e-07,
      "loss": 0.0029,
      "step": 4215
    },
    {
      "epoch": 67.46,
      "grad_norm": 0.003794329706579447,
      "learning_rate": 6.184665997806832e-07,
      "loss": 0.0001,
      "step": 4216
    },
    {
      "epoch": 67.47,
      "grad_norm": 0.32070136070251465,
      "learning_rate": 6.169231394040503e-07,
      "loss": 0.0018,
      "step": 4217
    },
    {
      "epoch": 67.49,
      "grad_norm": 0.00286431098356843,
      "learning_rate": 6.153814807447517e-07,
      "loss": 0.0001,
      "step": 4218
    },
    {
      "epoch": 67.5,
      "grad_norm": 0.004096491727977991,
      "learning_rate": 6.138416244365047e-07,
      "loss": 0.0001,
      "step": 4219
    },
    {
      "epoch": 67.52,
      "grad_norm": 0.0025352737866342068,
      "learning_rate": 6.12303571112286e-07,
      "loss": 0.0,
      "step": 4220
    },
    {
      "epoch": 67.54,
      "grad_norm": 0.0031782779842615128,
      "learning_rate": 6.107673214043319e-07,
      "loss": 0.0001,
      "step": 4221
    },
    {
      "epoch": 67.55,
      "grad_norm": 0.2807055711746216,
      "learning_rate": 6.092328759441357e-07,
      "loss": 0.003,
      "step": 4222
    },
    {
      "epoch": 67.57,
      "grad_norm": 0.003101162612438202,
      "learning_rate": 6.077002353624506e-07,
      "loss": 0.0001,
      "step": 4223
    },
    {
      "epoch": 67.58,
      "grad_norm": 0.0030965604819357395,
      "learning_rate": 6.06169400289287e-07,
      "loss": 0.0001,
      "step": 4224
    },
    {
      "epoch": 67.6,
      "grad_norm": 0.0027754944749176502,
      "learning_rate": 6.04640371353914e-07,
      "loss": 0.0001,
      "step": 4225
    },
    {
      "epoch": 67.62,
      "grad_norm": 0.7715396285057068,
      "learning_rate": 6.031131491848574e-07,
      "loss": 0.0104,
      "step": 4226
    },
    {
      "epoch": 67.63,
      "grad_norm": 0.21530920267105103,
      "learning_rate": 6.015877344098997e-07,
      "loss": 0.0014,
      "step": 4227
    },
    {
      "epoch": 67.65,
      "grad_norm": 0.003586803562939167,
      "learning_rate": 6.000641276560814e-07,
      "loss": 0.0001,
      "step": 4228
    },
    {
      "epoch": 67.66,
      "grad_norm": 0.00391894206404686,
      "learning_rate": 5.985423295497005e-07,
      "loss": 0.0001,
      "step": 4229
    },
    {
      "epoch": 67.68,
      "grad_norm": 0.0036556434351950884,
      "learning_rate": 5.9702234071631e-07,
      "loss": 0.0001,
      "step": 4230
    },
    {
      "epoch": 67.7,
      "grad_norm": 0.053812138736248016,
      "learning_rate": 5.955041617807205e-07,
      "loss": 0.0004,
      "step": 4231
    },
    {
      "epoch": 67.71,
      "grad_norm": 0.6488485932350159,
      "learning_rate": 5.93987793366998e-07,
      "loss": 0.0054,
      "step": 4232
    },
    {
      "epoch": 67.73,
      "grad_norm": 0.0021517022978514433,
      "learning_rate": 5.924732360984636e-07,
      "loss": 0.0,
      "step": 4233
    },
    {
      "epoch": 67.74,
      "grad_norm": 0.06161588802933693,
      "learning_rate": 5.909604905976951e-07,
      "loss": 0.0003,
      "step": 4234
    },
    {
      "epoch": 67.76,
      "grad_norm": 0.004080998711287975,
      "learning_rate": 5.89449557486525e-07,
      "loss": 0.0001,
      "step": 4235
    },
    {
      "epoch": 67.78,
      "grad_norm": 0.0037523650098592043,
      "learning_rate": 5.879404373860415e-07,
      "loss": 0.0001,
      "step": 4236
    },
    {
      "epoch": 67.79,
      "grad_norm": 1.0880303382873535,
      "learning_rate": 5.86433130916585e-07,
      "loss": 0.0113,
      "step": 4237
    },
    {
      "epoch": 67.81,
      "grad_norm": 0.9350242614746094,
      "learning_rate": 5.849276386977538e-07,
      "loss": 0.0128,
      "step": 4238
    },
    {
      "epoch": 67.82,
      "grad_norm": 0.8799154162406921,
      "learning_rate": 5.834239613483983e-07,
      "loss": 0.0053,
      "step": 4239
    },
    {
      "epoch": 67.84,
      "grad_norm": 0.0034752502106130123,
      "learning_rate": 5.819220994866237e-07,
      "loss": 0.0001,
      "step": 4240
    },
    {
      "epoch": 67.86,
      "grad_norm": 0.0038795247673988342,
      "learning_rate": 5.804220537297883e-07,
      "loss": 0.0001,
      "step": 4241
    },
    {
      "epoch": 67.87,
      "grad_norm": 0.003212748793885112,
      "learning_rate": 5.789238246945051e-07,
      "loss": 0.0001,
      "step": 4242
    },
    {
      "epoch": 67.89,
      "grad_norm": 0.003631949657574296,
      "learning_rate": 5.774274129966384e-07,
      "loss": 0.0001,
      "step": 4243
    },
    {
      "epoch": 67.9,
      "grad_norm": 0.6450616717338562,
      "learning_rate": 5.759328192513075e-07,
      "loss": 0.0044,
      "step": 4244
    },
    {
      "epoch": 67.92,
      "grad_norm": 0.003522028448060155,
      "learning_rate": 5.744400440728826e-07,
      "loss": 0.0001,
      "step": 4245
    },
    {
      "epoch": 67.94,
      "grad_norm": 0.003669345984235406,
      "learning_rate": 5.729490880749888e-07,
      "loss": 0.0001,
      "step": 4246
    },
    {
      "epoch": 67.95,
      "grad_norm": 0.0033626484218984842,
      "learning_rate": 5.714599518704994e-07,
      "loss": 0.0001,
      "step": 4247
    },
    {
      "epoch": 67.97,
      "grad_norm": 0.6728805899620056,
      "learning_rate": 5.699726360715435e-07,
      "loss": 0.0089,
      "step": 4248
    },
    {
      "epoch": 67.98,
      "grad_norm": 0.0030830493196845055,
      "learning_rate": 5.684871412894999e-07,
      "loss": 0.0001,
      "step": 4249
    },
    {
      "epoch": 68.0,
      "grad_norm": 0.8299524784088135,
      "learning_rate": 5.670034681349995e-07,
      "loss": 0.0121,
      "step": 4250
    },
    {
      "epoch": 68.02,
      "grad_norm": 0.8062391877174377,
      "learning_rate": 5.655216172179245e-07,
      "loss": 0.0088,
      "step": 4251
    },
    {
      "epoch": 68.03,
      "grad_norm": 0.23226594924926758,
      "learning_rate": 5.640415891474094e-07,
      "loss": 0.0012,
      "step": 4252
    },
    {
      "epoch": 68.05,
      "grad_norm": 0.004193205386400223,
      "learning_rate": 5.625633845318346e-07,
      "loss": 0.0001,
      "step": 4253
    },
    {
      "epoch": 68.06,
      "grad_norm": 0.4602513015270233,
      "learning_rate": 5.610870039788357e-07,
      "loss": 0.0029,
      "step": 4254
    },
    {
      "epoch": 68.08,
      "grad_norm": 0.0033500855788588524,
      "learning_rate": 5.596124480952975e-07,
      "loss": 0.0001,
      "step": 4255
    },
    {
      "epoch": 68.1,
      "grad_norm": 0.004217416048049927,
      "learning_rate": 5.581397174873532e-07,
      "loss": 0.0001,
      "step": 4256
    },
    {
      "epoch": 68.11,
      "grad_norm": 0.005096028093248606,
      "learning_rate": 5.566688127603876e-07,
      "loss": 0.0002,
      "step": 4257
    },
    {
      "epoch": 68.13,
      "grad_norm": 0.046651970595121384,
      "learning_rate": 5.55199734519034e-07,
      "loss": 0.0002,
      "step": 4258
    },
    {
      "epoch": 68.14,
      "grad_norm": 0.1971646547317505,
      "learning_rate": 5.537324833671753e-07,
      "loss": 0.001,
      "step": 4259
    },
    {
      "epoch": 68.16,
      "grad_norm": 0.003607220947742462,
      "learning_rate": 5.522670599079416e-07,
      "loss": 0.0001,
      "step": 4260
    },
    {
      "epoch": 68.18,
      "grad_norm": 0.8009116649627686,
      "learning_rate": 5.508034647437144e-07,
      "loss": 0.0108,
      "step": 4261
    },
    {
      "epoch": 68.19,
      "grad_norm": 0.004063882399350405,
      "learning_rate": 5.493416984761218e-07,
      "loss": 0.0001,
      "step": 4262
    },
    {
      "epoch": 68.21,
      "grad_norm": 0.5445468425750732,
      "learning_rate": 5.478817617060406e-07,
      "loss": 0.0029,
      "step": 4263
    },
    {
      "epoch": 68.22,
      "grad_norm": 0.00398103054612875,
      "learning_rate": 5.464236550335961e-07,
      "loss": 0.0001,
      "step": 4264
    },
    {
      "epoch": 68.24,
      "grad_norm": 0.0034728359896689653,
      "learning_rate": 5.449673790581611e-07,
      "loss": 0.0001,
      "step": 4265
    },
    {
      "epoch": 68.26,
      "grad_norm": 0.0038320093881338835,
      "learning_rate": 5.435129343783547e-07,
      "loss": 0.0001,
      "step": 4266
    },
    {
      "epoch": 68.27,
      "grad_norm": 0.03515736013650894,
      "learning_rate": 5.42060321592045e-07,
      "loss": 0.0002,
      "step": 4267
    },
    {
      "epoch": 68.29,
      "grad_norm": 0.0023522304836660624,
      "learning_rate": 5.406095412963464e-07,
      "loss": 0.0,
      "step": 4268
    },
    {
      "epoch": 68.3,
      "grad_norm": 0.003470615716651082,
      "learning_rate": 5.391605940876199e-07,
      "loss": 0.0001,
      "step": 4269
    },
    {
      "epoch": 68.32,
      "grad_norm": 0.7874788045883179,
      "learning_rate": 5.377134805614714e-07,
      "loss": 0.0063,
      "step": 4270
    },
    {
      "epoch": 68.34,
      "grad_norm": 0.004102753940969706,
      "learning_rate": 5.362682013127562e-07,
      "loss": 0.0001,
      "step": 4271
    },
    {
      "epoch": 68.35,
      "grad_norm": 0.33262044191360474,
      "learning_rate": 5.348247569355736e-07,
      "loss": 0.0036,
      "step": 4272
    },
    {
      "epoch": 68.37,
      "grad_norm": 0.8520622253417969,
      "learning_rate": 5.333831480232687e-07,
      "loss": 0.0054,
      "step": 4273
    },
    {
      "epoch": 68.38,
      "grad_norm": 0.6715224385261536,
      "learning_rate": 5.319433751684328e-07,
      "loss": 0.0063,
      "step": 4274
    },
    {
      "epoch": 68.4,
      "grad_norm": 0.971839964389801,
      "learning_rate": 5.305054389629022e-07,
      "loss": 0.0125,
      "step": 4275
    },
    {
      "epoch": 68.42,
      "grad_norm": 0.2141522616147995,
      "learning_rate": 5.290693399977581e-07,
      "loss": 0.0013,
      "step": 4276
    },
    {
      "epoch": 68.43,
      "grad_norm": 0.5761174559593201,
      "learning_rate": 5.276350788633267e-07,
      "loss": 0.0062,
      "step": 4277
    },
    {
      "epoch": 68.45,
      "grad_norm": 0.0040509398095309734,
      "learning_rate": 5.262026561491779e-07,
      "loss": 0.0001,
      "step": 4278
    },
    {
      "epoch": 68.46,
      "grad_norm": 0.003872124245390296,
      "learning_rate": 5.247720724441285e-07,
      "loss": 0.0001,
      "step": 4279
    },
    {
      "epoch": 68.48,
      "grad_norm": 0.9634115099906921,
      "learning_rate": 5.233433283362349e-07,
      "loss": 0.0125,
      "step": 4280
    },
    {
      "epoch": 68.5,
      "grad_norm": 0.05228092148900032,
      "learning_rate": 5.219164244128006e-07,
      "loss": 0.0003,
      "step": 4281
    },
    {
      "epoch": 68.51,
      "grad_norm": 0.0032583067659288645,
      "learning_rate": 5.204913612603724e-07,
      "loss": 0.0001,
      "step": 4282
    },
    {
      "epoch": 68.53,
      "grad_norm": 0.6510615944862366,
      "learning_rate": 5.190681394647401e-07,
      "loss": 0.0072,
      "step": 4283
    },
    {
      "epoch": 68.54,
      "grad_norm": 0.0043824962340295315,
      "learning_rate": 5.176467596109358e-07,
      "loss": 0.0001,
      "step": 4284
    },
    {
      "epoch": 68.56,
      "grad_norm": 0.0027060937136411667,
      "learning_rate": 5.162272222832349e-07,
      "loss": 0.0001,
      "step": 4285
    },
    {
      "epoch": 68.58,
      "grad_norm": 0.05989678204059601,
      "learning_rate": 5.148095280651566e-07,
      "loss": 0.0004,
      "step": 4286
    },
    {
      "epoch": 68.59,
      "grad_norm": 0.002874495927244425,
      "learning_rate": 5.133936775394604e-07,
      "loss": 0.0001,
      "step": 4287
    },
    {
      "epoch": 68.61,
      "grad_norm": 0.003175914753228426,
      "learning_rate": 5.119796712881498e-07,
      "loss": 0.0001,
      "step": 4288
    },
    {
      "epoch": 68.62,
      "grad_norm": 0.00949214305728674,
      "learning_rate": 5.10567509892469e-07,
      "loss": 0.0001,
      "step": 4289
    },
    {
      "epoch": 68.64,
      "grad_norm": 0.9845719933509827,
      "learning_rate": 5.091571939329049e-07,
      "loss": 0.0082,
      "step": 4290
    },
    {
      "epoch": 68.66,
      "grad_norm": 0.9994422793388367,
      "learning_rate": 5.077487239891838e-07,
      "loss": 0.0062,
      "step": 4291
    },
    {
      "epoch": 68.67,
      "grad_norm": 1.0314161777496338,
      "learning_rate": 5.063421006402747e-07,
      "loss": 0.0202,
      "step": 4292
    },
    {
      "epoch": 68.69,
      "grad_norm": 0.3908196985721588,
      "learning_rate": 5.049373244643879e-07,
      "loss": 0.0038,
      "step": 4293
    },
    {
      "epoch": 68.7,
      "grad_norm": 0.003377049695700407,
      "learning_rate": 5.035343960389738e-07,
      "loss": 0.0001,
      "step": 4294
    },
    {
      "epoch": 68.72,
      "grad_norm": 0.28908443450927734,
      "learning_rate": 5.021333159407232e-07,
      "loss": 0.0019,
      "step": 4295
    },
    {
      "epoch": 68.74,
      "grad_norm": 0.0028302196878939867,
      "learning_rate": 5.007340847455667e-07,
      "loss": 0.0001,
      "step": 4296
    },
    {
      "epoch": 68.75,
      "grad_norm": 1.0837336778640747,
      "learning_rate": 4.993367030286772e-07,
      "loss": 0.0216,
      "step": 4297
    },
    {
      "epoch": 68.77,
      "grad_norm": 0.004618618171662092,
      "learning_rate": 4.979411713644627e-07,
      "loss": 0.0002,
      "step": 4298
    },
    {
      "epoch": 68.78,
      "grad_norm": 1.0980901718139648,
      "learning_rate": 4.965474903265755e-07,
      "loss": 0.01,
      "step": 4299
    },
    {
      "epoch": 68.8,
      "grad_norm": 0.7562480568885803,
      "learning_rate": 4.951556604879049e-07,
      "loss": 0.0042,
      "step": 4300
    },
    {
      "epoch": 68.82,
      "grad_norm": 0.004092484712600708,
      "learning_rate": 4.937656824205789e-07,
      "loss": 0.0001,
      "step": 4301
    },
    {
      "epoch": 68.83,
      "grad_norm": 0.3230056166648865,
      "learning_rate": 4.923775566959666e-07,
      "loss": 0.0025,
      "step": 4302
    },
    {
      "epoch": 68.85,
      "grad_norm": 0.00276531046256423,
      "learning_rate": 4.90991283884672e-07,
      "loss": 0.0001,
      "step": 4303
    },
    {
      "epoch": 68.86,
      "grad_norm": 0.9356939196586609,
      "learning_rate": 4.896068645565405e-07,
      "loss": 0.0111,
      "step": 4304
    },
    {
      "epoch": 68.88,
      "grad_norm": 0.8547007441520691,
      "learning_rate": 4.882242992806546e-07,
      "loss": 0.0055,
      "step": 4305
    },
    {
      "epoch": 68.9,
      "grad_norm": 0.0029921182431280613,
      "learning_rate": 4.868435886253348e-07,
      "loss": 0.0001,
      "step": 4306
    },
    {
      "epoch": 68.91,
      "grad_norm": 1.12603759765625,
      "learning_rate": 4.854647331581385e-07,
      "loss": 0.0221,
      "step": 4307
    },
    {
      "epoch": 68.93,
      "grad_norm": 0.004360871855169535,
      "learning_rate": 4.840877334458615e-07,
      "loss": 0.0001,
      "step": 4308
    },
    {
      "epoch": 68.94,
      "grad_norm": 0.0036141364835202694,
      "learning_rate": 4.827125900545365e-07,
      "loss": 0.0001,
      "step": 4309
    },
    {
      "epoch": 68.96,
      "grad_norm": 0.0030985602643340826,
      "learning_rate": 4.813393035494329e-07,
      "loss": 0.0001,
      "step": 4310
    },
    {
      "epoch": 68.98,
      "grad_norm": 0.056458912789821625,
      "learning_rate": 4.79967874495057e-07,
      "loss": 0.0004,
      "step": 4311
    },
    {
      "epoch": 68.99,
      "grad_norm": 0.7532750368118286,
      "learning_rate": 4.785983034551523e-07,
      "loss": 0.0045,
      "step": 4312
    },
    {
      "epoch": 69.01,
      "grad_norm": 0.004611887503415346,
      "learning_rate": 4.772305909926956e-07,
      "loss": 0.0002,
      "step": 4313
    },
    {
      "epoch": 69.02,
      "grad_norm": 0.9330906867980957,
      "learning_rate": 4.758647376699033e-07,
      "loss": 0.0125,
      "step": 4314
    },
    {
      "epoch": 69.04,
      "grad_norm": 0.003110673977062106,
      "learning_rate": 4.745007440482252e-07,
      "loss": 0.0001,
      "step": 4315
    },
    {
      "epoch": 69.06,
      "grad_norm": 0.0035754882264882326,
      "learning_rate": 4.731386106883484e-07,
      "loss": 0.0001,
      "step": 4316
    },
    {
      "epoch": 69.07,
      "grad_norm": 0.6725354790687561,
      "learning_rate": 4.7177833815019447e-07,
      "loss": 0.0038,
      "step": 4317
    },
    {
      "epoch": 69.09,
      "grad_norm": 0.19013161957263947,
      "learning_rate": 4.704199269929199e-07,
      "loss": 0.0012,
      "step": 4318
    },
    {
      "epoch": 69.1,
      "grad_norm": 0.003804700681939721,
      "learning_rate": 4.6906337777491594e-07,
      "loss": 0.0001,
      "step": 4319
    },
    {
      "epoch": 69.12,
      "grad_norm": 0.003811237867921591,
      "learning_rate": 4.677086910538092e-07,
      "loss": 0.0001,
      "step": 4320
    },
    {
      "epoch": 69.14,
      "grad_norm": 0.0033920060377568007,
      "learning_rate": 4.663558673864599e-07,
      "loss": 0.0001,
      "step": 4321
    },
    {
      "epoch": 69.15,
      "grad_norm": 0.8084802627563477,
      "learning_rate": 4.650049073289625e-07,
      "loss": 0.0084,
      "step": 4322
    },
    {
      "epoch": 69.17,
      "grad_norm": 0.8605663180351257,
      "learning_rate": 4.636558114366468e-07,
      "loss": 0.0133,
      "step": 4323
    },
    {
      "epoch": 69.18,
      "grad_norm": 0.002189166843891144,
      "learning_rate": 4.6230858026407364e-07,
      "loss": 0.0,
      "step": 4324
    },
    {
      "epoch": 69.2,
      "grad_norm": 0.003309600753709674,
      "learning_rate": 4.6096321436504e-07,
      "loss": 0.0001,
      "step": 4325
    },
    {
      "epoch": 69.22,
      "grad_norm": 0.20202934741973877,
      "learning_rate": 4.59619714292574e-07,
      "loss": 0.0011,
      "step": 4326
    },
    {
      "epoch": 69.23,
      "grad_norm": 0.003828319488093257,
      "learning_rate": 4.582780805989384e-07,
      "loss": 0.0001,
      "step": 4327
    },
    {
      "epoch": 69.25,
      "grad_norm": 0.0037190832663327456,
      "learning_rate": 4.569383138356276e-07,
      "loss": 0.0001,
      "step": 4328
    },
    {
      "epoch": 69.26,
      "grad_norm": 0.003955360502004623,
      "learning_rate": 4.5560041455337043e-07,
      "loss": 0.0001,
      "step": 4329
    },
    {
      "epoch": 69.28,
      "grad_norm": 0.0029526660218834877,
      "learning_rate": 4.542643833021254e-07,
      "loss": 0.0001,
      "step": 4330
    },
    {
      "epoch": 69.3,
      "grad_norm": 0.6716956496238708,
      "learning_rate": 4.5293022063108483e-07,
      "loss": 0.0041,
      "step": 4331
    },
    {
      "epoch": 69.31,
      "grad_norm": 0.004131036810576916,
      "learning_rate": 4.515979270886728e-07,
      "loss": 0.0001,
      "step": 4332
    },
    {
      "epoch": 69.33,
      "grad_norm": 0.6524345278739929,
      "learning_rate": 4.5026750322254564e-07,
      "loss": 0.0043,
      "step": 4333
    },
    {
      "epoch": 69.34,
      "grad_norm": 0.003656700486317277,
      "learning_rate": 4.4893894957958875e-07,
      "loss": 0.0001,
      "step": 4334
    },
    {
      "epoch": 69.36,
      "grad_norm": 0.7551444172859192,
      "learning_rate": 4.4761226670592074e-07,
      "loss": 0.0045,
      "step": 4335
    },
    {
      "epoch": 69.38,
      "grad_norm": 1.096866488456726,
      "learning_rate": 4.4628745514689154e-07,
      "loss": 0.012,
      "step": 4336
    },
    {
      "epoch": 69.39,
      "grad_norm": 0.003403570270165801,
      "learning_rate": 4.449645154470805e-07,
      "loss": 0.0001,
      "step": 4337
    },
    {
      "epoch": 69.41,
      "grad_norm": 0.0035095373168587685,
      "learning_rate": 4.4364344815029825e-07,
      "loss": 0.0001,
      "step": 4338
    },
    {
      "epoch": 69.42,
      "grad_norm": 0.003861762350425124,
      "learning_rate": 4.423242537995864e-07,
      "loss": 0.0001,
      "step": 4339
    },
    {
      "epoch": 69.44,
      "grad_norm": 0.0049701170064508915,
      "learning_rate": 4.410069329372152e-07,
      "loss": 0.0002,
      "step": 4340
    },
    {
      "epoch": 69.46,
      "grad_norm": 0.003806001041084528,
      "learning_rate": 4.3969148610468526e-07,
      "loss": 0.0001,
      "step": 4341
    },
    {
      "epoch": 69.47,
      "grad_norm": 0.0037403772585093975,
      "learning_rate": 4.383779138427274e-07,
      "loss": 0.0001,
      "step": 4342
    },
    {
      "epoch": 69.49,
      "grad_norm": 0.7601026296615601,
      "learning_rate": 4.370662166913031e-07,
      "loss": 0.0045,
      "step": 4343
    },
    {
      "epoch": 69.5,
      "grad_norm": 0.05235607922077179,
      "learning_rate": 4.357563951895988e-07,
      "loss": 0.0002,
      "step": 4344
    },
    {
      "epoch": 69.52,
      "grad_norm": 0.0031441589817404747,
      "learning_rate": 4.344484498760343e-07,
      "loss": 0.0001,
      "step": 4345
    },
    {
      "epoch": 69.54,
      "grad_norm": 0.004138941876590252,
      "learning_rate": 4.3314238128825625e-07,
      "loss": 0.0001,
      "step": 4346
    },
    {
      "epoch": 69.55,
      "grad_norm": 0.003076879307627678,
      "learning_rate": 4.3183818996313965e-07,
      "loss": 0.0001,
      "step": 4347
    },
    {
      "epoch": 69.57,
      "grad_norm": 0.002704451559111476,
      "learning_rate": 4.305358764367884e-07,
      "loss": 0.0001,
      "step": 4348
    },
    {
      "epoch": 69.58,
      "grad_norm": 0.7497279644012451,
      "learning_rate": 4.2923544124453484e-07,
      "loss": 0.0092,
      "step": 4349
    },
    {
      "epoch": 69.6,
      "grad_norm": 0.21277542412281036,
      "learning_rate": 4.279368849209381e-07,
      "loss": 0.0011,
      "step": 4350
    },
    {
      "epoch": 69.62,
      "grad_norm": 0.003662983188405633,
      "learning_rate": 4.266402079997861e-07,
      "loss": 0.0001,
      "step": 4351
    },
    {
      "epoch": 69.63,
      "grad_norm": 0.003610248677432537,
      "learning_rate": 4.253454110140942e-07,
      "loss": 0.0001,
      "step": 4352
    },
    {
      "epoch": 69.65,
      "grad_norm": 0.0031069722026586533,
      "learning_rate": 4.240524944961033e-07,
      "loss": 0.0001,
      "step": 4353
    },
    {
      "epoch": 69.66,
      "grad_norm": 0.0047724610194563866,
      "learning_rate": 4.227614589772838e-07,
      "loss": 0.0002,
      "step": 4354
    },
    {
      "epoch": 69.68,
      "grad_norm": 0.8260969519615173,
      "learning_rate": 4.214723049883307e-07,
      "loss": 0.0105,
      "step": 4355
    },
    {
      "epoch": 69.7,
      "grad_norm": 0.0032041557133197784,
      "learning_rate": 4.201850330591678e-07,
      "loss": 0.0001,
      "step": 4356
    },
    {
      "epoch": 69.71,
      "grad_norm": 0.8706026077270508,
      "learning_rate": 4.188996437189424e-07,
      "loss": 0.0109,
      "step": 4357
    },
    {
      "epoch": 69.73,
      "grad_norm": 0.7667250037193298,
      "learning_rate": 4.1761613749603024e-07,
      "loss": 0.0112,
      "step": 4358
    },
    {
      "epoch": 69.74,
      "grad_norm": 0.8472740054130554,
      "learning_rate": 4.1633451491803203e-07,
      "loss": 0.0105,
      "step": 4359
    },
    {
      "epoch": 69.76,
      "grad_norm": 0.5781278014183044,
      "learning_rate": 4.150547765117746e-07,
      "loss": 0.0051,
      "step": 4360
    },
    {
      "epoch": 69.78,
      "grad_norm": 0.003755431156605482,
      "learning_rate": 4.1377692280331005e-07,
      "loss": 0.0001,
      "step": 4361
    },
    {
      "epoch": 69.79,
      "grad_norm": 0.0041290270164608955,
      "learning_rate": 4.125009543179159e-07,
      "loss": 0.0001,
      "step": 4362
    },
    {
      "epoch": 69.81,
      "grad_norm": 1.0058799982070923,
      "learning_rate": 4.112268715800943e-07,
      "loss": 0.014,
      "step": 4363
    },
    {
      "epoch": 69.82,
      "grad_norm": 0.7475985884666443,
      "learning_rate": 4.0995467511357246e-07,
      "loss": 0.0077,
      "step": 4364
    },
    {
      "epoch": 69.84,
      "grad_norm": 0.864213764667511,
      "learning_rate": 4.086843654413031e-07,
      "loss": 0.0113,
      "step": 4365
    },
    {
      "epoch": 69.86,
      "grad_norm": 0.7682949304580688,
      "learning_rate": 4.074159430854624e-07,
      "loss": 0.0059,
      "step": 4366
    },
    {
      "epoch": 69.87,
      "grad_norm": 0.01606057770550251,
      "learning_rate": 4.0614940856744944e-07,
      "loss": 0.0001,
      "step": 4367
    },
    {
      "epoch": 69.89,
      "grad_norm": 0.9024644494056702,
      "learning_rate": 4.0488476240789e-07,
      "loss": 0.0122,
      "step": 4368
    },
    {
      "epoch": 69.9,
      "grad_norm": 1.0845125913619995,
      "learning_rate": 4.036220051266321e-07,
      "loss": 0.0166,
      "step": 4369
    },
    {
      "epoch": 69.92,
      "grad_norm": 0.42936471104621887,
      "learning_rate": 4.0236113724274716e-07,
      "loss": 0.0026,
      "step": 4370
    },
    {
      "epoch": 69.94,
      "grad_norm": 0.764310896396637,
      "learning_rate": 4.011021592745307e-07,
      "loss": 0.0059,
      "step": 4371
    },
    {
      "epoch": 69.95,
      "grad_norm": 0.2765490412712097,
      "learning_rate": 3.9984507173950136e-07,
      "loss": 0.0018,
      "step": 4372
    },
    {
      "epoch": 69.97,
      "grad_norm": 0.8642393350601196,
      "learning_rate": 3.9858987515439986e-07,
      "loss": 0.0095,
      "step": 4373
    },
    {
      "epoch": 69.98,
      "grad_norm": 1.3098732233047485,
      "learning_rate": 3.9733657003519e-07,
      "loss": 0.0107,
      "step": 4374
    },
    {
      "epoch": 70.0,
      "grad_norm": 0.004168229643255472,
      "learning_rate": 3.960851568970586e-07,
      "loss": 0.0001,
      "step": 4375
    },
    {
      "epoch": 70.02,
      "grad_norm": 0.004018062725663185,
      "learning_rate": 3.9483563625441424e-07,
      "loss": 0.0001,
      "step": 4376
    },
    {
      "epoch": 70.03,
      "grad_norm": 0.0036351201124489307,
      "learning_rate": 3.935880086208882e-07,
      "loss": 0.0001,
      "step": 4377
    },
    {
      "epoch": 70.05,
      "grad_norm": 0.0030792567413300276,
      "learning_rate": 3.9234227450933136e-07,
      "loss": 0.0001,
      "step": 4378
    },
    {
      "epoch": 70.06,
      "grad_norm": 0.003138497471809387,
      "learning_rate": 3.910984344318192e-07,
      "loss": 0.0001,
      "step": 4379
    },
    {
      "epoch": 70.08,
      "grad_norm": 0.003984462935477495,
      "learning_rate": 3.8985648889964755e-07,
      "loss": 0.0001,
      "step": 4380
    },
    {
      "epoch": 70.1,
      "grad_norm": 0.0033333131577819586,
      "learning_rate": 3.886164384233326e-07,
      "loss": 0.0001,
      "step": 4381
    },
    {
      "epoch": 70.11,
      "grad_norm": 0.0030181852634996176,
      "learning_rate": 3.8737828351261273e-07,
      "loss": 0.0001,
      "step": 4382
    },
    {
      "epoch": 70.13,
      "grad_norm": 0.003686804324388504,
      "learning_rate": 3.8614202467644636e-07,
      "loss": 0.0001,
      "step": 4383
    },
    {
      "epoch": 70.14,
      "grad_norm": 0.02971760928630829,
      "learning_rate": 3.8490766242301356e-07,
      "loss": 0.0005,
      "step": 4384
    },
    {
      "epoch": 70.16,
      "grad_norm": 1.1498321294784546,
      "learning_rate": 3.83675197259713e-07,
      "loss": 0.0176,
      "step": 4385
    },
    {
      "epoch": 70.18,
      "grad_norm": 0.0034518397878855467,
      "learning_rate": 3.82444629693165e-07,
      "loss": 0.0001,
      "step": 4386
    },
    {
      "epoch": 70.19,
      "grad_norm": 0.542637050151825,
      "learning_rate": 3.812159602292104e-07,
      "loss": 0.0033,
      "step": 4387
    },
    {
      "epoch": 70.21,
      "grad_norm": 0.051590774208307266,
      "learning_rate": 3.7998918937290686e-07,
      "loss": 0.0003,
      "step": 4388
    },
    {
      "epoch": 70.22,
      "grad_norm": 0.004152422770857811,
      "learning_rate": 3.787643176285355e-07,
      "loss": 0.0001,
      "step": 4389
    },
    {
      "epoch": 70.24,
      "grad_norm": 0.8773294687271118,
      "learning_rate": 3.77541345499593e-07,
      "loss": 0.0069,
      "step": 4390
    },
    {
      "epoch": 70.26,
      "grad_norm": 0.9495593905448914,
      "learning_rate": 3.7632027348879773e-07,
      "loss": 0.0155,
      "step": 4391
    },
    {
      "epoch": 70.27,
      "grad_norm": 0.00404844805598259,
      "learning_rate": 3.7510110209808657e-07,
      "loss": 0.0001,
      "step": 4392
    },
    {
      "epoch": 70.29,
      "grad_norm": 0.05495304986834526,
      "learning_rate": 3.738838318286142e-07,
      "loss": 0.0004,
      "step": 4393
    },
    {
      "epoch": 70.3,
      "grad_norm": 0.0036991245578974485,
      "learning_rate": 3.7266846318075535e-07,
      "loss": 0.0001,
      "step": 4394
    },
    {
      "epoch": 70.32,
      "grad_norm": 0.800308883190155,
      "learning_rate": 3.7145499665410147e-07,
      "loss": 0.0076,
      "step": 4395
    },
    {
      "epoch": 70.34,
      "grad_norm": 0.8883403539657593,
      "learning_rate": 3.70243432747463e-07,
      "loss": 0.0129,
      "step": 4396
    },
    {
      "epoch": 70.35,
      "grad_norm": 0.20433077216148376,
      "learning_rate": 3.6903377195886823e-07,
      "loss": 0.0012,
      "step": 4397
    },
    {
      "epoch": 70.37,
      "grad_norm": 0.048885222524404526,
      "learning_rate": 3.678260147855628e-07,
      "loss": 0.0002,
      "step": 4398
    },
    {
      "epoch": 70.38,
      "grad_norm": 0.45909518003463745,
      "learning_rate": 3.6662016172401114e-07,
      "loss": 0.0036,
      "step": 4399
    },
    {
      "epoch": 70.4,
      "grad_norm": 0.2784820795059204,
      "learning_rate": 3.6541621326989183e-07,
      "loss": 0.0018,
      "step": 4400
    },
    {
      "epoch": 70.42,
      "grad_norm": 0.003990051802247763,
      "learning_rate": 3.64214169918104e-07,
      "loss": 0.0001,
      "step": 4401
    },
    {
      "epoch": 70.43,
      "grad_norm": 0.8710179924964905,
      "learning_rate": 3.6301403216276176e-07,
      "loss": 0.0099,
      "step": 4402
    },
    {
      "epoch": 70.45,
      "grad_norm": 0.0036451038904488087,
      "learning_rate": 3.6181580049719664e-07,
      "loss": 0.0001,
      "step": 4403
    },
    {
      "epoch": 70.46,
      "grad_norm": 0.8342331051826477,
      "learning_rate": 3.606194754139569e-07,
      "loss": 0.0134,
      "step": 4404
    },
    {
      "epoch": 70.48,
      "grad_norm": 0.7975918650627136,
      "learning_rate": 3.5942505740480583e-07,
      "loss": 0.0089,
      "step": 4405
    },
    {
      "epoch": 70.5,
      "grad_norm": 0.003612662199884653,
      "learning_rate": 3.5823254696072343e-07,
      "loss": 0.0001,
      "step": 4406
    },
    {
      "epoch": 70.51,
      "grad_norm": 0.9284967184066772,
      "learning_rate": 3.5704194457190647e-07,
      "loss": 0.0093,
      "step": 4407
    },
    {
      "epoch": 70.53,
      "grad_norm": 0.003482036292552948,
      "learning_rate": 3.5585325072776625e-07,
      "loss": 0.0001,
      "step": 4408
    },
    {
      "epoch": 70.54,
      "grad_norm": 0.002719418378546834,
      "learning_rate": 3.5466646591692966e-07,
      "loss": 0.0001,
      "step": 4409
    },
    {
      "epoch": 70.56,
      "grad_norm": 0.0029208636842668056,
      "learning_rate": 3.534815906272404e-07,
      "loss": 0.0001,
      "step": 4410
    },
    {
      "epoch": 70.58,
      "grad_norm": 0.5063790082931519,
      "learning_rate": 3.5229862534575386e-07,
      "loss": 0.0053,
      "step": 4411
    },
    {
      "epoch": 70.59,
      "grad_norm": 0.06153605133295059,
      "learning_rate": 3.511175705587433e-07,
      "loss": 0.0003,
      "step": 4412
    },
    {
      "epoch": 70.61,
      "grad_norm": 0.004213388077914715,
      "learning_rate": 3.4993842675169587e-07,
      "loss": 0.0001,
      "step": 4413
    },
    {
      "epoch": 70.62,
      "grad_norm": 0.00248133996501565,
      "learning_rate": 3.487611944093133e-07,
      "loss": 0.0001,
      "step": 4414
    },
    {
      "epoch": 70.64,
      "grad_norm": 0.3970385789871216,
      "learning_rate": 3.475858740155108e-07,
      "loss": 0.0022,
      "step": 4415
    },
    {
      "epoch": 70.66,
      "grad_norm": 0.8338722586631775,
      "learning_rate": 3.464124660534191e-07,
      "loss": 0.0081,
      "step": 4416
    },
    {
      "epoch": 70.67,
      "grad_norm": 0.0031861793249845505,
      "learning_rate": 3.452409710053806e-07,
      "loss": 0.0001,
      "step": 4417
    },
    {
      "epoch": 70.69,
      "grad_norm": 1.027330756187439,
      "learning_rate": 3.440713893529535e-07,
      "loss": 0.0085,
      "step": 4418
    },
    {
      "epoch": 70.7,
      "grad_norm": 0.003013019682839513,
      "learning_rate": 3.429037215769082e-07,
      "loss": 0.0001,
      "step": 4419
    },
    {
      "epoch": 70.72,
      "grad_norm": 0.2726031541824341,
      "learning_rate": 3.417379681572297e-07,
      "loss": 0.0018,
      "step": 4420
    },
    {
      "epoch": 70.74,
      "grad_norm": 0.3708304166793823,
      "learning_rate": 3.4057412957311407e-07,
      "loss": 0.0028,
      "step": 4421
    },
    {
      "epoch": 70.75,
      "grad_norm": 0.4827602803707123,
      "learning_rate": 3.39412206302972e-07,
      "loss": 0.003,
      "step": 4422
    },
    {
      "epoch": 70.77,
      "grad_norm": 0.3341616094112396,
      "learning_rate": 3.382521988244264e-07,
      "loss": 0.0019,
      "step": 4423
    },
    {
      "epoch": 70.78,
      "grad_norm": 0.03731870278716087,
      "learning_rate": 3.3709410761431136e-07,
      "loss": 0.0002,
      "step": 4424
    },
    {
      "epoch": 70.8,
      "grad_norm": 0.003864417551085353,
      "learning_rate": 3.359379331486762e-07,
      "loss": 0.0001,
      "step": 4425
    },
    {
      "epoch": 70.82,
      "grad_norm": 0.004291387274861336,
      "learning_rate": 3.347836759027789e-07,
      "loss": 0.0001,
      "step": 4426
    },
    {
      "epoch": 70.83,
      "grad_norm": 0.8000802993774414,
      "learning_rate": 3.3363133635109233e-07,
      "loss": 0.0108,
      "step": 4427
    },
    {
      "epoch": 70.85,
      "grad_norm": 0.0047528063878417015,
      "learning_rate": 3.324809149672992e-07,
      "loss": 0.0001,
      "step": 4428
    },
    {
      "epoch": 70.86,
      "grad_norm": 0.06787601113319397,
      "learning_rate": 3.313324122242945e-07,
      "loss": 0.0003,
      "step": 4429
    },
    {
      "epoch": 70.88,
      "grad_norm": 0.7343874573707581,
      "learning_rate": 3.301858285941845e-07,
      "loss": 0.0091,
      "step": 4430
    },
    {
      "epoch": 70.9,
      "grad_norm": 0.9080950021743774,
      "learning_rate": 3.290411645482855e-07,
      "loss": 0.0076,
      "step": 4431
    },
    {
      "epoch": 70.91,
      "grad_norm": 0.5245703458786011,
      "learning_rate": 3.278984205571262e-07,
      "loss": 0.0044,
      "step": 4432
    },
    {
      "epoch": 70.93,
      "grad_norm": 0.16967949271202087,
      "learning_rate": 3.2675759709044577e-07,
      "loss": 0.0009,
      "step": 4433
    },
    {
      "epoch": 70.94,
      "grad_norm": 0.5394313335418701,
      "learning_rate": 3.25618694617193e-07,
      "loss": 0.0071,
      "step": 4434
    },
    {
      "epoch": 70.96,
      "grad_norm": 0.004017073195427656,
      "learning_rate": 3.2448171360552837e-07,
      "loss": 0.0001,
      "step": 4435
    },
    {
      "epoch": 70.98,
      "grad_norm": 0.05778839811682701,
      "learning_rate": 3.2334665452282143e-07,
      "loss": 0.0004,
      "step": 4436
    },
    {
      "epoch": 70.99,
      "grad_norm": 0.0033001198899000883,
      "learning_rate": 3.222135178356517e-07,
      "loss": 0.0001,
      "step": 4437
    },
    {
      "epoch": 71.01,
      "grad_norm": 0.0036219193134456873,
      "learning_rate": 3.2108230400980987e-07,
      "loss": 0.0001,
      "step": 4438
    },
    {
      "epoch": 71.02,
      "grad_norm": 0.6257134675979614,
      "learning_rate": 3.1995301351029407e-07,
      "loss": 0.0069,
      "step": 4439
    },
    {
      "epoch": 71.04,
      "grad_norm": 0.0023969216272234917,
      "learning_rate": 3.18825646801314e-07,
      "loss": 0.0,
      "step": 4440
    },
    {
      "epoch": 71.06,
      "grad_norm": 0.45728710293769836,
      "learning_rate": 3.1770020434628734e-07,
      "loss": 0.0027,
      "step": 4441
    },
    {
      "epoch": 71.07,
      "grad_norm": 0.6679458022117615,
      "learning_rate": 3.1657668660784015e-07,
      "loss": 0.0085,
      "step": 4442
    },
    {
      "epoch": 71.09,
      "grad_norm": 0.0025135257747024298,
      "learning_rate": 3.154550940478102e-07,
      "loss": 0.0001,
      "step": 4443
    },
    {
      "epoch": 71.1,
      "grad_norm": 0.9553769826889038,
      "learning_rate": 3.143354271272392e-07,
      "loss": 0.0151,
      "step": 4444
    },
    {
      "epoch": 71.12,
      "grad_norm": 0.003036817302927375,
      "learning_rate": 3.1321768630638073e-07,
      "loss": 0.0001,
      "step": 4445
    },
    {
      "epoch": 71.14,
      "grad_norm": 0.2195020467042923,
      "learning_rate": 3.1210187204469667e-07,
      "loss": 0.0016,
      "step": 4446
    },
    {
      "epoch": 71.15,
      "grad_norm": 0.636677622795105,
      "learning_rate": 3.109879848008557e-07,
      "loss": 0.0067,
      "step": 4447
    },
    {
      "epoch": 71.17,
      "grad_norm": 0.046082574874162674,
      "learning_rate": 3.098760250327343e-07,
      "loss": 0.0002,
      "step": 4448
    },
    {
      "epoch": 71.18,
      "grad_norm": 1.0908340215682983,
      "learning_rate": 3.0876599319741797e-07,
      "loss": 0.0148,
      "step": 4449
    },
    {
      "epoch": 71.2,
      "grad_norm": 0.003485205816105008,
      "learning_rate": 3.076578897511978e-07,
      "loss": 0.0001,
      "step": 4450
    },
    {
      "epoch": 71.22,
      "grad_norm": 0.6038417220115662,
      "learning_rate": 3.065517151495745e-07,
      "loss": 0.0048,
      "step": 4451
    },
    {
      "epoch": 71.23,
      "grad_norm": 0.655609130859375,
      "learning_rate": 3.054474698472537e-07,
      "loss": 0.0038,
      "step": 4452
    },
    {
      "epoch": 71.25,
      "grad_norm": 0.004419525619596243,
      "learning_rate": 3.043451542981507e-07,
      "loss": 0.0001,
      "step": 4453
    },
    {
      "epoch": 71.26,
      "grad_norm": 0.003376864828169346,
      "learning_rate": 3.03244768955383e-07,
      "loss": 0.0001,
      "step": 4454
    },
    {
      "epoch": 71.28,
      "grad_norm": 0.004743161611258984,
      "learning_rate": 3.0214631427127883e-07,
      "loss": 0.0002,
      "step": 4455
    },
    {
      "epoch": 71.3,
      "grad_norm": 0.7188089489936829,
      "learning_rate": 3.010497906973714e-07,
      "loss": 0.0089,
      "step": 4456
    },
    {
      "epoch": 71.31,
      "grad_norm": 0.5203551650047302,
      "learning_rate": 2.999551986844001e-07,
      "loss": 0.003,
      "step": 4457
    },
    {
      "epoch": 71.33,
      "grad_norm": 0.0049370755441486835,
      "learning_rate": 2.9886253868231073e-07,
      "loss": 0.0002,
      "step": 4458
    },
    {
      "epoch": 71.34,
      "grad_norm": 0.004251303616911173,
      "learning_rate": 2.977718111402544e-07,
      "loss": 0.0001,
      "step": 4459
    },
    {
      "epoch": 71.36,
      "grad_norm": 0.002496507717296481,
      "learning_rate": 2.966830165065876e-07,
      "loss": 0.0001,
      "step": 4460
    },
    {
      "epoch": 71.38,
      "grad_norm": 0.6152645349502563,
      "learning_rate": 2.9559615522887275e-07,
      "loss": 0.0047,
      "step": 4461
    },
    {
      "epoch": 71.39,
      "grad_norm": 0.0036949587520211935,
      "learning_rate": 2.9451122775387755e-07,
      "loss": 0.0001,
      "step": 4462
    },
    {
      "epoch": 71.41,
      "grad_norm": 0.3760852515697479,
      "learning_rate": 2.9342823452757463e-07,
      "loss": 0.0031,
      "step": 4463
    },
    {
      "epoch": 71.42,
      "grad_norm": 0.40742117166519165,
      "learning_rate": 2.92347175995143e-07,
      "loss": 0.0027,
      "step": 4464
    },
    {
      "epoch": 71.44,
      "grad_norm": 0.7479218244552612,
      "learning_rate": 2.912680526009626e-07,
      "loss": 0.0094,
      "step": 4465
    },
    {
      "epoch": 71.46,
      "grad_norm": 0.0033927191980183125,
      "learning_rate": 2.9019086478862144e-07,
      "loss": 0.0001,
      "step": 4466
    },
    {
      "epoch": 71.47,
      "grad_norm": 0.0025229686871170998,
      "learning_rate": 2.8911561300091094e-07,
      "loss": 0.0,
      "step": 4467
    },
    {
      "epoch": 71.49,
      "grad_norm": 0.003912447951734066,
      "learning_rate": 2.8804229767982637e-07,
      "loss": 0.0001,
      "step": 4468
    },
    {
      "epoch": 71.5,
      "grad_norm": 0.002556369174271822,
      "learning_rate": 2.869709192665665e-07,
      "loss": 0.0001,
      "step": 4469
    },
    {
      "epoch": 71.52,
      "grad_norm": 0.002809148980304599,
      "learning_rate": 2.8590147820153513e-07,
      "loss": 0.0001,
      "step": 4470
    },
    {
      "epoch": 71.54,
      "grad_norm": 0.004391959402710199,
      "learning_rate": 2.8483397492433953e-07,
      "loss": 0.0001,
      "step": 4471
    },
    {
      "epoch": 71.55,
      "grad_norm": 0.5165583491325378,
      "learning_rate": 2.837684098737892e-07,
      "loss": 0.0054,
      "step": 4472
    },
    {
      "epoch": 71.57,
      "grad_norm": 0.004021752160042524,
      "learning_rate": 2.8270478348789764e-07,
      "loss": 0.0001,
      "step": 4473
    },
    {
      "epoch": 71.58,
      "grad_norm": 0.6670063734054565,
      "learning_rate": 2.81643096203883e-07,
      "loss": 0.0062,
      "step": 4474
    },
    {
      "epoch": 71.6,
      "grad_norm": 0.002931695431470871,
      "learning_rate": 2.8058334845816214e-07,
      "loss": 0.0001,
      "step": 4475
    },
    {
      "epoch": 71.62,
      "grad_norm": 0.005330878775566816,
      "learning_rate": 2.795255406863595e-07,
      "loss": 0.0001,
      "step": 4476
    },
    {
      "epoch": 71.63,
      "grad_norm": 0.8133252263069153,
      "learning_rate": 2.784696733232989e-07,
      "loss": 0.0107,
      "step": 4477
    },
    {
      "epoch": 71.65,
      "grad_norm": 0.9971750378608704,
      "learning_rate": 2.7741574680300754e-07,
      "loss": 0.0133,
      "step": 4478
    },
    {
      "epoch": 71.66,
      "grad_norm": 0.5335434675216675,
      "learning_rate": 2.763637615587161e-07,
      "loss": 0.0065,
      "step": 4479
    },
    {
      "epoch": 71.68,
      "grad_norm": 0.4685591161251068,
      "learning_rate": 2.7531371802285436e-07,
      "loss": 0.0026,
      "step": 4480
    },
    {
      "epoch": 71.7,
      "grad_norm": 0.4305118918418884,
      "learning_rate": 2.7426561662705575e-07,
      "loss": 0.0025,
      "step": 4481
    },
    {
      "epoch": 71.71,
      "grad_norm": 0.003056333400309086,
      "learning_rate": 2.7321945780215576e-07,
      "loss": 0.0001,
      "step": 4482
    },
    {
      "epoch": 71.73,
      "grad_norm": 0.9285663962364197,
      "learning_rate": 2.721752419781903e-07,
      "loss": 0.0146,
      "step": 4483
    },
    {
      "epoch": 71.74,
      "grad_norm": 0.021370116621255875,
      "learning_rate": 2.711329695843978e-07,
      "loss": 0.0002,
      "step": 4484
    },
    {
      "epoch": 71.76,
      "grad_norm": 0.6976588368415833,
      "learning_rate": 2.7009264104921606e-07,
      "loss": 0.006,
      "step": 4485
    },
    {
      "epoch": 71.78,
      "grad_norm": 0.7557855844497681,
      "learning_rate": 2.6905425680028686e-07,
      "loss": 0.0045,
      "step": 4486
    },
    {
      "epoch": 71.79,
      "grad_norm": 0.02843794785439968,
      "learning_rate": 2.6801781726444767e-07,
      "loss": 0.0003,
      "step": 4487
    },
    {
      "epoch": 71.81,
      "grad_norm": 0.003029481042176485,
      "learning_rate": 2.6698332286774153e-07,
      "loss": 0.0001,
      "step": 4488
    },
    {
      "epoch": 71.82,
      "grad_norm": 0.004715347196906805,
      "learning_rate": 2.6595077403541e-07,
      "loss": 0.0002,
      "step": 4489
    },
    {
      "epoch": 71.84,
      "grad_norm": 0.061528392136096954,
      "learning_rate": 2.6492017119189415e-07,
      "loss": 0.0003,
      "step": 4490
    },
    {
      "epoch": 71.86,
      "grad_norm": 0.0044370307587087154,
      "learning_rate": 2.638915147608362e-07,
      "loss": 0.0001,
      "step": 4491
    },
    {
      "epoch": 71.87,
      "grad_norm": 0.44559118151664734,
      "learning_rate": 2.628648051650784e-07,
      "loss": 0.006,
      "step": 4492
    },
    {
      "epoch": 71.89,
      "grad_norm": 0.003875871654599905,
      "learning_rate": 2.618400428266621e-07,
      "loss": 0.0001,
      "step": 4493
    },
    {
      "epoch": 71.9,
      "grad_norm": 0.003434945596382022,
      "learning_rate": 2.608172281668281e-07,
      "loss": 0.0001,
      "step": 4494
    },
    {
      "epoch": 71.92,
      "grad_norm": 0.054171252995729446,
      "learning_rate": 2.5979636160601673e-07,
      "loss": 0.0003,
      "step": 4495
    },
    {
      "epoch": 71.94,
      "grad_norm": 0.6965600252151489,
      "learning_rate": 2.587774435638679e-07,
      "loss": 0.0041,
      "step": 4496
    },
    {
      "epoch": 71.95,
      "grad_norm": 0.5854499936103821,
      "learning_rate": 2.577604744592216e-07,
      "loss": 0.0064,
      "step": 4497
    },
    {
      "epoch": 71.97,
      "grad_norm": 0.03846485912799835,
      "learning_rate": 2.5674545471011335e-07,
      "loss": 0.0002,
      "step": 4498
    },
    {
      "epoch": 71.98,
      "grad_norm": 0.029760286211967468,
      "learning_rate": 2.5573238473378003e-07,
      "loss": 0.0002,
      "step": 4499
    },
    {
      "epoch": 72.0,
      "grad_norm": 0.8203788995742798,
      "learning_rate": 2.547212649466568e-07,
      "loss": 0.0113,
      "step": 4500
    },
    {
      "epoch": 72.02,
      "grad_norm": 0.0040002367459237576,
      "learning_rate": 2.537120957643768e-07,
      "loss": 0.0001,
      "step": 4501
    },
    {
      "epoch": 72.03,
      "grad_norm": 0.0027580265887081623,
      "learning_rate": 2.5270487760177153e-07,
      "loss": 0.0001,
      "step": 4502
    },
    {
      "epoch": 72.05,
      "grad_norm": 0.0592840313911438,
      "learning_rate": 2.5169961087286975e-07,
      "loss": 0.0004,
      "step": 4503
    },
    {
      "epoch": 72.06,
      "grad_norm": 0.5047105550765991,
      "learning_rate": 2.5069629599089874e-07,
      "loss": 0.0027,
      "step": 4504
    },
    {
      "epoch": 72.08,
      "grad_norm": 0.0037178664933890104,
      "learning_rate": 2.4969493336828353e-07,
      "loss": 0.0001,
      "step": 4505
    },
    {
      "epoch": 72.1,
      "grad_norm": 0.004175414331257343,
      "learning_rate": 2.4869552341664715e-07,
      "loss": 0.0001,
      "step": 4506
    },
    {
      "epoch": 72.11,
      "grad_norm": 0.003771528135985136,
      "learning_rate": 2.4769806654680874e-07,
      "loss": 0.0001,
      "step": 4507
    },
    {
      "epoch": 72.13,
      "grad_norm": 0.0026321557816118,
      "learning_rate": 2.467025631687847e-07,
      "loss": 0.0001,
      "step": 4508
    },
    {
      "epoch": 72.14,
      "grad_norm": 0.9241347312927246,
      "learning_rate": 2.457090136917889e-07,
      "loss": 0.0119,
      "step": 4509
    },
    {
      "epoch": 72.16,
      "grad_norm": 0.6489230990409851,
      "learning_rate": 2.447174185242324e-07,
      "loss": 0.008,
      "step": 4510
    },
    {
      "epoch": 72.18,
      "grad_norm": 0.905383825302124,
      "learning_rate": 2.4372777807372237e-07,
      "loss": 0.0083,
      "step": 4511
    },
    {
      "epoch": 72.19,
      "grad_norm": 0.003440741216763854,
      "learning_rate": 2.4274009274706244e-07,
      "loss": 0.0001,
      "step": 4512
    },
    {
      "epoch": 72.21,
      "grad_norm": 0.6689701080322266,
      "learning_rate": 2.4175436295025333e-07,
      "loss": 0.0038,
      "step": 4513
    },
    {
      "epoch": 72.22,
      "grad_norm": 0.004647383466362953,
      "learning_rate": 2.4077058908849093e-07,
      "loss": 0.0001,
      "step": 4514
    },
    {
      "epoch": 72.24,
      "grad_norm": 0.715512752532959,
      "learning_rate": 2.397887715661679e-07,
      "loss": 0.0041,
      "step": 4515
    },
    {
      "epoch": 72.26,
      "grad_norm": 0.6807053685188293,
      "learning_rate": 2.388089107868713e-07,
      "loss": 0.0041,
      "step": 4516
    },
    {
      "epoch": 72.27,
      "grad_norm": 0.0029538902454078197,
      "learning_rate": 2.3783100715338624e-07,
      "loss": 0.0001,
      "step": 4517
    },
    {
      "epoch": 72.29,
      "grad_norm": 0.0031504149083048105,
      "learning_rate": 2.368550610676912e-07,
      "loss": 0.0001,
      "step": 4518
    },
    {
      "epoch": 72.3,
      "grad_norm": 0.07125302404165268,
      "learning_rate": 2.3588107293096075e-07,
      "loss": 0.0004,
      "step": 4519
    },
    {
      "epoch": 72.32,
      "grad_norm": 0.003784387605264783,
      "learning_rate": 2.3490904314356412e-07,
      "loss": 0.0001,
      "step": 4520
    },
    {
      "epoch": 72.34,
      "grad_norm": 0.18198005855083466,
      "learning_rate": 2.3393897210506723e-07,
      "loss": 0.0011,
      "step": 4521
    },
    {
      "epoch": 72.35,
      "grad_norm": 0.024340815842151642,
      "learning_rate": 2.3297086021422887e-07,
      "loss": 0.0005,
      "step": 4522
    },
    {
      "epoch": 72.37,
      "grad_norm": 0.6991637945175171,
      "learning_rate": 2.3200470786900298e-07,
      "loss": 0.0066,
      "step": 4523
    },
    {
      "epoch": 72.38,
      "grad_norm": 0.022612886503338814,
      "learning_rate": 2.3104051546654016e-07,
      "loss": 0.0002,
      "step": 4524
    },
    {
      "epoch": 72.4,
      "grad_norm": 0.6297060251235962,
      "learning_rate": 2.3007828340318117e-07,
      "loss": 0.0035,
      "step": 4525
    },
    {
      "epoch": 72.42,
      "grad_norm": 0.0033443777356296778,
      "learning_rate": 2.2911801207446403e-07,
      "loss": 0.0001,
      "step": 4526
    },
    {
      "epoch": 72.43,
      "grad_norm": 0.6814041137695312,
      "learning_rate": 2.2815970187512027e-07,
      "loss": 0.0053,
      "step": 4527
    },
    {
      "epoch": 72.45,
      "grad_norm": 0.03609778359532356,
      "learning_rate": 2.2720335319907472e-07,
      "loss": 0.0002,
      "step": 4528
    },
    {
      "epoch": 72.46,
      "grad_norm": 0.05397331714630127,
      "learning_rate": 2.2624896643944626e-07,
      "loss": 0.0003,
      "step": 4529
    },
    {
      "epoch": 72.48,
      "grad_norm": 0.0026930237654596567,
      "learning_rate": 2.2529654198854834e-07,
      "loss": 0.0001,
      "step": 4530
    },
    {
      "epoch": 72.5,
      "grad_norm": 0.5263653993606567,
      "learning_rate": 2.2434608023788496e-07,
      "loss": 0.0037,
      "step": 4531
    },
    {
      "epoch": 72.51,
      "grad_norm": 0.5707515478134155,
      "learning_rate": 2.2339758157815583e-07,
      "loss": 0.0077,
      "step": 4532
    },
    {
      "epoch": 72.53,
      "grad_norm": 0.0036180128809064627,
      "learning_rate": 2.2245104639925297e-07,
      "loss": 0.0001,
      "step": 4533
    },
    {
      "epoch": 72.54,
      "grad_norm": 0.0035830263514071703,
      "learning_rate": 2.2150647509026068e-07,
      "loss": 0.0001,
      "step": 4534
    },
    {
      "epoch": 72.56,
      "grad_norm": 0.003643499920144677,
      "learning_rate": 2.205638680394573e-07,
      "loss": 0.0001,
      "step": 4535
    },
    {
      "epoch": 72.58,
      "grad_norm": 0.0034069621469825506,
      "learning_rate": 2.1962322563431283e-07,
      "loss": 0.0001,
      "step": 4536
    },
    {
      "epoch": 72.59,
      "grad_norm": 0.004608084913343191,
      "learning_rate": 2.1868454826148966e-07,
      "loss": 0.0001,
      "step": 4537
    },
    {
      "epoch": 72.61,
      "grad_norm": 0.004123772960156202,
      "learning_rate": 2.177478363068425e-07,
      "loss": 0.0001,
      "step": 4538
    },
    {
      "epoch": 72.62,
      "grad_norm": 0.0031137738842517138,
      "learning_rate": 2.168130901554183e-07,
      "loss": 0.0001,
      "step": 4539
    },
    {
      "epoch": 72.64,
      "grad_norm": 0.3845507502555847,
      "learning_rate": 2.1588031019145638e-07,
      "loss": 0.0028,
      "step": 4540
    },
    {
      "epoch": 72.66,
      "grad_norm": 0.49258509278297424,
      "learning_rate": 2.1494949679838673e-07,
      "loss": 0.0032,
      "step": 4541
    },
    {
      "epoch": 72.67,
      "grad_norm": 1.014410138130188,
      "learning_rate": 2.1402065035883157e-07,
      "loss": 0.0121,
      "step": 4542
    },
    {
      "epoch": 72.69,
      "grad_norm": 1.0388902425765991,
      "learning_rate": 2.1309377125460495e-07,
      "loss": 0.017,
      "step": 4543
    },
    {
      "epoch": 72.7,
      "grad_norm": 0.23765283823013306,
      "learning_rate": 2.1216885986671155e-07,
      "loss": 0.0015,
      "step": 4544
    },
    {
      "epoch": 72.72,
      "grad_norm": 0.9595500230789185,
      "learning_rate": 2.1124591657534776e-07,
      "loss": 0.0187,
      "step": 4545
    },
    {
      "epoch": 72.74,
      "grad_norm": 0.0034769088961184025,
      "learning_rate": 2.103249417599007e-07,
      "loss": 0.0001,
      "step": 4546
    },
    {
      "epoch": 72.75,
      "grad_norm": 0.0483960397541523,
      "learning_rate": 2.0940593579894807e-07,
      "loss": 0.0002,
      "step": 4547
    },
    {
      "epoch": 72.77,
      "grad_norm": 0.5106011033058167,
      "learning_rate": 2.0848889907025883e-07,
      "loss": 0.0051,
      "step": 4548
    },
    {
      "epoch": 72.78,
      "grad_norm": 0.00407160259783268,
      "learning_rate": 2.0757383195079194e-07,
      "loss": 0.0001,
      "step": 4549
    },
    {
      "epoch": 72.8,
      "grad_norm": 0.48642462491989136,
      "learning_rate": 2.0666073481669714e-07,
      "loss": 0.0027,
      "step": 4550
    },
    {
      "epoch": 72.82,
      "grad_norm": 0.5367845296859741,
      "learning_rate": 2.0574960804331412e-07,
      "loss": 0.0066,
      "step": 4551
    },
    {
      "epoch": 72.83,
      "grad_norm": 0.6572564840316772,
      "learning_rate": 2.0484045200517222e-07,
      "loss": 0.0039,
      "step": 4552
    },
    {
      "epoch": 72.85,
      "grad_norm": 0.9377223253250122,
      "learning_rate": 2.0393326707599137e-07,
      "loss": 0.0101,
      "step": 4553
    },
    {
      "epoch": 72.86,
      "grad_norm": 0.1975814402103424,
      "learning_rate": 2.0302805362868105e-07,
      "loss": 0.001,
      "step": 4554
    },
    {
      "epoch": 72.88,
      "grad_norm": 0.8398230075836182,
      "learning_rate": 2.0212481203534083e-07,
      "loss": 0.0101,
      "step": 4555
    },
    {
      "epoch": 72.9,
      "grad_norm": 0.0033704317174851894,
      "learning_rate": 2.0122354266725874e-07,
      "loss": 0.0001,
      "step": 4556
    },
    {
      "epoch": 72.91,
      "grad_norm": 0.0033976933918893337,
      "learning_rate": 2.0032424589491228e-07,
      "loss": 0.0001,
      "step": 4557
    },
    {
      "epoch": 72.93,
      "grad_norm": 0.0028912334237247705,
      "learning_rate": 1.994269220879691e-07,
      "loss": 0.0001,
      "step": 4558
    },
    {
      "epoch": 72.94,
      "grad_norm": 1.4014790058135986,
      "learning_rate": 1.9853157161528468e-07,
      "loss": 0.0187,
      "step": 4559
    },
    {
      "epoch": 72.96,
      "grad_norm": 0.6572753190994263,
      "learning_rate": 1.9763819484490353e-07,
      "loss": 0.004,
      "step": 4560
    },
    {
      "epoch": 72.98,
      "grad_norm": 0.923072099685669,
      "learning_rate": 1.9674679214406023e-07,
      "loss": 0.0098,
      "step": 4561
    },
    {
      "epoch": 72.99,
      "grad_norm": 0.7272310256958008,
      "learning_rate": 1.9585736387917554e-07,
      "loss": 0.0078,
      "step": 4562
    },
    {
      "epoch": 73.01,
      "grad_norm": 0.0036306986585259438,
      "learning_rate": 1.9496991041585977e-07,
      "loss": 0.0001,
      "step": 4563
    },
    {
      "epoch": 73.02,
      "grad_norm": 0.05250517651438713,
      "learning_rate": 1.9408443211891227e-07,
      "loss": 0.0003,
      "step": 4564
    },
    {
      "epoch": 73.04,
      "grad_norm": 0.0036496059037745,
      "learning_rate": 1.932009293523196e-07,
      "loss": 0.0001,
      "step": 4565
    },
    {
      "epoch": 73.06,
      "grad_norm": 0.004132757894694805,
      "learning_rate": 1.9231940247925573e-07,
      "loss": 0.0001,
      "step": 4566
    },
    {
      "epoch": 73.07,
      "grad_norm": 0.2943383753299713,
      "learning_rate": 1.9143985186208357e-07,
      "loss": 0.0022,
      "step": 4567
    },
    {
      "epoch": 73.09,
      "grad_norm": 0.7586920261383057,
      "learning_rate": 1.9056227786235337e-07,
      "loss": 0.0059,
      "step": 4568
    },
    {
      "epoch": 73.1,
      "grad_norm": 0.002744715427979827,
      "learning_rate": 1.896866808408032e-07,
      "loss": 0.0001,
      "step": 4569
    },
    {
      "epoch": 73.12,
      "grad_norm": 0.6755372285842896,
      "learning_rate": 1.8881306115735632e-07,
      "loss": 0.0038,
      "step": 4570
    },
    {
      "epoch": 73.14,
      "grad_norm": 0.9732720851898193,
      "learning_rate": 1.8794141917112542e-07,
      "loss": 0.0188,
      "step": 4571
    },
    {
      "epoch": 73.15,
      "grad_norm": 0.6635651588439941,
      "learning_rate": 1.8707175524040998e-07,
      "loss": 0.0092,
      "step": 4572
    },
    {
      "epoch": 73.17,
      "grad_norm": 0.49665865302085876,
      "learning_rate": 1.8620406972269577e-07,
      "loss": 0.005,
      "step": 4573
    },
    {
      "epoch": 73.18,
      "grad_norm": 0.7404059171676636,
      "learning_rate": 1.853383629746558e-07,
      "loss": 0.0089,
      "step": 4574
    },
    {
      "epoch": 73.2,
      "grad_norm": 0.45471084117889404,
      "learning_rate": 1.8447463535214872e-07,
      "loss": 0.0056,
      "step": 4575
    },
    {
      "epoch": 73.22,
      "grad_norm": 0.0032575211953371763,
      "learning_rate": 1.8361288721022053e-07,
      "loss": 0.0001,
      "step": 4576
    },
    {
      "epoch": 73.23,
      "grad_norm": 0.9894590377807617,
      "learning_rate": 1.82753118903104e-07,
      "loss": 0.0161,
      "step": 4577
    },
    {
      "epoch": 73.25,
      "grad_norm": 0.00307492190040648,
      "learning_rate": 1.8189533078421638e-07,
      "loss": 0.0001,
      "step": 4578
    },
    {
      "epoch": 73.26,
      "grad_norm": 0.022756287828087807,
      "learning_rate": 1.8103952320616348e-07,
      "loss": 0.0002,
      "step": 4579
    },
    {
      "epoch": 73.28,
      "grad_norm": 0.0026834860909730196,
      "learning_rate": 1.801856965207338e-07,
      "loss": 0.0001,
      "step": 4580
    },
    {
      "epoch": 73.3,
      "grad_norm": 0.36467471718788147,
      "learning_rate": 1.793338510789039e-07,
      "loss": 0.0031,
      "step": 4581
    },
    {
      "epoch": 73.31,
      "grad_norm": 0.003446489805355668,
      "learning_rate": 1.7848398723083583e-07,
      "loss": 0.0001,
      "step": 4582
    },
    {
      "epoch": 73.33,
      "grad_norm": 0.7935163378715515,
      "learning_rate": 1.7763610532587573e-07,
      "loss": 0.0089,
      "step": 4583
    },
    {
      "epoch": 73.34,
      "grad_norm": 0.5993890166282654,
      "learning_rate": 1.76790205712557e-07,
      "loss": 0.0047,
      "step": 4584
    },
    {
      "epoch": 73.36,
      "grad_norm": 0.789155125617981,
      "learning_rate": 1.7594628873859488e-07,
      "loss": 0.0045,
      "step": 4585
    },
    {
      "epoch": 73.38,
      "grad_norm": 0.8984707593917847,
      "learning_rate": 1.7510435475089348e-07,
      "loss": 0.0094,
      "step": 4586
    },
    {
      "epoch": 73.39,
      "grad_norm": 0.5397646427154541,
      "learning_rate": 1.742644040955399e-07,
      "loss": 0.0034,
      "step": 4587
    },
    {
      "epoch": 73.41,
      "grad_norm": 0.6490821242332458,
      "learning_rate": 1.7342643711780516e-07,
      "loss": 0.004,
      "step": 4588
    },
    {
      "epoch": 73.42,
      "grad_norm": 0.25187331438064575,
      "learning_rate": 1.7259045416214703e-07,
      "loss": 0.0014,
      "step": 4589
    },
    {
      "epoch": 73.44,
      "grad_norm": 0.04906311258673668,
      "learning_rate": 1.7175645557220567e-07,
      "loss": 0.0004,
      "step": 4590
    },
    {
      "epoch": 73.46,
      "grad_norm": 0.5105141997337341,
      "learning_rate": 1.709244416908068e-07,
      "loss": 0.004,
      "step": 4591
    },
    {
      "epoch": 73.47,
      "grad_norm": 0.003906978759914637,
      "learning_rate": 1.700944128599602e-07,
      "loss": 0.0001,
      "step": 4592
    },
    {
      "epoch": 73.49,
      "grad_norm": 0.0037416527047753334,
      "learning_rate": 1.692663694208585e-07,
      "loss": 0.0001,
      "step": 4593
    },
    {
      "epoch": 73.5,
      "grad_norm": 0.9200323820114136,
      "learning_rate": 1.6844031171388054e-07,
      "loss": 0.0101,
      "step": 4594
    },
    {
      "epoch": 73.52,
      "grad_norm": 0.0032675149850547314,
      "learning_rate": 1.6761624007858524e-07,
      "loss": 0.0001,
      "step": 4595
    },
    {
      "epoch": 73.54,
      "grad_norm": 0.2152584195137024,
      "learning_rate": 1.667941548537194e-07,
      "loss": 0.0013,
      "step": 4596
    },
    {
      "epoch": 73.55,
      "grad_norm": 0.6239453554153442,
      "learning_rate": 1.6597405637720997e-07,
      "loss": 0.0035,
      "step": 4597
    },
    {
      "epoch": 73.57,
      "grad_norm": 0.004010581877082586,
      "learning_rate": 1.651559449861684e-07,
      "loss": 0.0001,
      "step": 4598
    },
    {
      "epoch": 73.58,
      "grad_norm": 0.7775131464004517,
      "learning_rate": 1.6433982101689062e-07,
      "loss": 0.0102,
      "step": 4599
    },
    {
      "epoch": 73.6,
      "grad_norm": 0.0033281773794442415,
      "learning_rate": 1.6352568480485277e-07,
      "loss": 0.0001,
      "step": 4600
    },
    {
      "epoch": 73.62,
      "grad_norm": 1.0088773965835571,
      "learning_rate": 1.6271353668471657e-07,
      "loss": 0.0077,
      "step": 4601
    },
    {
      "epoch": 73.63,
      "grad_norm": 0.0036808475852012634,
      "learning_rate": 1.6190337699032554e-07,
      "loss": 0.0001,
      "step": 4602
    },
    {
      "epoch": 73.65,
      "grad_norm": 0.6342476606369019,
      "learning_rate": 1.6109520605470497e-07,
      "loss": 0.0067,
      "step": 4603
    },
    {
      "epoch": 73.66,
      "grad_norm": 0.0038243085145950317,
      "learning_rate": 1.6028902421006464e-07,
      "loss": 0.0001,
      "step": 4604
    },
    {
      "epoch": 73.68,
      "grad_norm": 0.0024887430481612682,
      "learning_rate": 1.594848317877934e-07,
      "loss": 0.0001,
      "step": 4605
    },
    {
      "epoch": 73.7,
      "grad_norm": 0.5805303454399109,
      "learning_rate": 1.586826291184662e-07,
      "loss": 0.0032,
      "step": 4606
    },
    {
      "epoch": 73.71,
      "grad_norm": 0.6149735450744629,
      "learning_rate": 1.5788241653183768e-07,
      "loss": 0.0039,
      "step": 4607
    },
    {
      "epoch": 73.73,
      "grad_norm": 0.0034575294703245163,
      "learning_rate": 1.5708419435684463e-07,
      "loss": 0.0001,
      "step": 4608
    },
    {
      "epoch": 73.74,
      "grad_norm": 0.004611702170222998,
      "learning_rate": 1.562879629216063e-07,
      "loss": 0.0002,
      "step": 4609
    },
    {
      "epoch": 73.76,
      "grad_norm": 0.43682917952537537,
      "learning_rate": 1.5549372255342367e-07,
      "loss": 0.0027,
      "step": 4610
    },
    {
      "epoch": 73.78,
      "grad_norm": 0.00293388357385993,
      "learning_rate": 1.547014735787783e-07,
      "loss": 0.0001,
      "step": 4611
    },
    {
      "epoch": 73.79,
      "grad_norm": 0.005204102955758572,
      "learning_rate": 1.5391121632333473e-07,
      "loss": 0.0001,
      "step": 4612
    },
    {
      "epoch": 73.81,
      "grad_norm": 0.004797706380486488,
      "learning_rate": 1.53122951111937e-07,
      "loss": 0.0002,
      "step": 4613
    },
    {
      "epoch": 73.82,
      "grad_norm": 0.0035481119994074106,
      "learning_rate": 1.5233667826861143e-07,
      "loss": 0.0001,
      "step": 4614
    },
    {
      "epoch": 73.84,
      "grad_norm": 0.0030640317127108574,
      "learning_rate": 1.5155239811656562e-07,
      "loss": 0.0001,
      "step": 4615
    },
    {
      "epoch": 73.86,
      "grad_norm": 0.0037985092494636774,
      "learning_rate": 1.5077011097818729e-07,
      "loss": 0.0001,
      "step": 4616
    },
    {
      "epoch": 73.87,
      "grad_norm": 0.0038606133311986923,
      "learning_rate": 1.4998981717504469e-07,
      "loss": 0.0001,
      "step": 4617
    },
    {
      "epoch": 73.89,
      "grad_norm": 0.055509570986032486,
      "learning_rate": 1.4921151702788683e-07,
      "loss": 0.0003,
      "step": 4618
    },
    {
      "epoch": 73.9,
      "grad_norm": 0.7131131887435913,
      "learning_rate": 1.4843521085664448e-07,
      "loss": 0.0095,
      "step": 4619
    },
    {
      "epoch": 73.92,
      "grad_norm": 1.064697504043579,
      "learning_rate": 1.4766089898042678e-07,
      "loss": 0.0162,
      "step": 4620
    },
    {
      "epoch": 73.94,
      "grad_norm": 0.003438372630625963,
      "learning_rate": 1.468885817175253e-07,
      "loss": 0.0001,
      "step": 4621
    },
    {
      "epoch": 73.95,
      "grad_norm": 0.3882628083229065,
      "learning_rate": 1.4611825938540936e-07,
      "loss": 0.0027,
      "step": 4622
    },
    {
      "epoch": 73.97,
      "grad_norm": 0.003270969493314624,
      "learning_rate": 1.4534993230073013e-07,
      "loss": 0.0001,
      "step": 4623
    },
    {
      "epoch": 73.98,
      "grad_norm": 0.5919070243835449,
      "learning_rate": 1.4458360077931721e-07,
      "loss": 0.0049,
      "step": 4624
    },
    {
      "epoch": 74.0,
      "grad_norm": 0.3911108672618866,
      "learning_rate": 1.4381926513618139e-07,
      "loss": 0.0024,
      "step": 4625
    },
    {
      "epoch": 74.02,
      "grad_norm": 0.0038575218059122562,
      "learning_rate": 1.4305692568551134e-07,
      "loss": 0.0001,
      "step": 4626
    },
    {
      "epoch": 74.03,
      "grad_norm": 0.0038011441938579082,
      "learning_rate": 1.4229658274067694e-07,
      "loss": 0.0001,
      "step": 4627
    },
    {
      "epoch": 74.05,
      "grad_norm": 0.6244077682495117,
      "learning_rate": 1.4153823661422485e-07,
      "loss": 0.0064,
      "step": 4628
    },
    {
      "epoch": 74.06,
      "grad_norm": 0.0037144552916288376,
      "learning_rate": 1.4078188761788402e-07,
      "loss": 0.0001,
      "step": 4629
    },
    {
      "epoch": 74.08,
      "grad_norm": 0.002458079718053341,
      "learning_rate": 1.4002753606256082e-07,
      "loss": 0.0,
      "step": 4630
    },
    {
      "epoch": 74.1,
      "grad_norm": 0.05460761860013008,
      "learning_rate": 1.3927518225833992e-07,
      "loss": 0.0004,
      "step": 4631
    },
    {
      "epoch": 74.11,
      "grad_norm": 0.00393048906698823,
      "learning_rate": 1.3852482651448618e-07,
      "loss": 0.0001,
      "step": 4632
    },
    {
      "epoch": 74.13,
      "grad_norm": 0.0037807640619575977,
      "learning_rate": 1.3777646913944175e-07,
      "loss": 0.0001,
      "step": 4633
    },
    {
      "epoch": 74.14,
      "grad_norm": 0.6722783446311951,
      "learning_rate": 1.3703011044082893e-07,
      "loss": 0.0055,
      "step": 4634
    },
    {
      "epoch": 74.16,
      "grad_norm": 0.003781216451898217,
      "learning_rate": 1.362857507254478e-07,
      "loss": 0.0001,
      "step": 4635
    },
    {
      "epoch": 74.18,
      "grad_norm": 0.0020437364000827074,
      "learning_rate": 1.3554339029927532e-07,
      "loss": 0.0,
      "step": 4636
    },
    {
      "epoch": 74.19,
      "grad_norm": 0.005127417389303446,
      "learning_rate": 1.3480302946746903e-07,
      "loss": 0.0001,
      "step": 4637
    },
    {
      "epoch": 74.21,
      "grad_norm": 0.6144995093345642,
      "learning_rate": 1.3406466853436274e-07,
      "loss": 0.0039,
      "step": 4638
    },
    {
      "epoch": 74.22,
      "grad_norm": 0.004046353977173567,
      "learning_rate": 1.333283078034686e-07,
      "loss": 0.0001,
      "step": 4639
    },
    {
      "epoch": 74.24,
      "grad_norm": 0.793770432472229,
      "learning_rate": 1.3259394757747678e-07,
      "loss": 0.0084,
      "step": 4640
    },
    {
      "epoch": 74.26,
      "grad_norm": 0.5862863659858704,
      "learning_rate": 1.3186158815825467e-07,
      "loss": 0.0045,
      "step": 4641
    },
    {
      "epoch": 74.27,
      "grad_norm": 0.05253888666629791,
      "learning_rate": 1.3113122984684869e-07,
      "loss": 0.0003,
      "step": 4642
    },
    {
      "epoch": 74.29,
      "grad_norm": 0.003380819922313094,
      "learning_rate": 1.3040287294348032e-07,
      "loss": 0.0001,
      "step": 4643
    },
    {
      "epoch": 74.3,
      "grad_norm": 0.6480329036712646,
      "learning_rate": 1.2967651774755065e-07,
      "loss": 0.0089,
      "step": 4644
    },
    {
      "epoch": 74.32,
      "grad_norm": 0.051064737141132355,
      "learning_rate": 1.2895216455763582e-07,
      "loss": 0.0002,
      "step": 4645
    },
    {
      "epoch": 74.34,
      "grad_norm": 0.003795095020905137,
      "learning_rate": 1.2822981367149102e-07,
      "loss": 0.0001,
      "step": 4646
    },
    {
      "epoch": 74.35,
      "grad_norm": 0.43546825647354126,
      "learning_rate": 1.2750946538604702e-07,
      "loss": 0.0026,
      "step": 4647
    },
    {
      "epoch": 74.37,
      "grad_norm": 0.576495349407196,
      "learning_rate": 1.26791119997412e-07,
      "loss": 0.0033,
      "step": 4648
    },
    {
      "epoch": 74.38,
      "grad_norm": 0.003868112340569496,
      "learning_rate": 1.2607477780086974e-07,
      "loss": 0.0001,
      "step": 4649
    },
    {
      "epoch": 74.4,
      "grad_norm": 0.056878462433815,
      "learning_rate": 1.253604390908819e-07,
      "loss": 0.0003,
      "step": 4650
    },
    {
      "epoch": 74.42,
      "grad_norm": 0.0036849125754088163,
      "learning_rate": 1.2464810416108698e-07,
      "loss": 0.0001,
      "step": 4651
    },
    {
      "epoch": 74.43,
      "grad_norm": 0.5904917120933533,
      "learning_rate": 1.2393777330429791e-07,
      "loss": 0.0043,
      "step": 4652
    },
    {
      "epoch": 74.45,
      "grad_norm": 0.7820241451263428,
      "learning_rate": 1.2322944681250503e-07,
      "loss": 0.0074,
      "step": 4653
    },
    {
      "epoch": 74.46,
      "grad_norm": 1.089184045791626,
      "learning_rate": 1.225231249768749e-07,
      "loss": 0.0087,
      "step": 4654
    },
    {
      "epoch": 74.48,
      "grad_norm": 0.22568154335021973,
      "learning_rate": 1.2181880808775026e-07,
      "loss": 0.0013,
      "step": 4655
    },
    {
      "epoch": 74.5,
      "grad_norm": 0.6569026112556458,
      "learning_rate": 1.2111649643464785e-07,
      "loss": 0.0166,
      "step": 4656
    },
    {
      "epoch": 74.51,
      "grad_norm": 0.25223109126091003,
      "learning_rate": 1.2041619030626283e-07,
      "loss": 0.0014,
      "step": 4657
    },
    {
      "epoch": 74.53,
      "grad_norm": 0.016353992745280266,
      "learning_rate": 1.1971788999046385e-07,
      "loss": 0.0002,
      "step": 4658
    },
    {
      "epoch": 74.54,
      "grad_norm": 0.77638179063797,
      "learning_rate": 1.1902159577429629e-07,
      "loss": 0.0087,
      "step": 4659
    },
    {
      "epoch": 74.56,
      "grad_norm": 0.00393542367964983,
      "learning_rate": 1.1832730794397951e-07,
      "loss": 0.0001,
      "step": 4660
    },
    {
      "epoch": 74.58,
      "grad_norm": 0.0026124485302716494,
      "learning_rate": 1.1763502678490968e-07,
      "loss": 0.0001,
      "step": 4661
    },
    {
      "epoch": 74.59,
      "grad_norm": 0.6370185017585754,
      "learning_rate": 1.1694475258165749e-07,
      "loss": 0.0038,
      "step": 4662
    },
    {
      "epoch": 74.61,
      "grad_norm": 0.0032449173741042614,
      "learning_rate": 1.1625648561796765e-07,
      "loss": 0.0001,
      "step": 4663
    },
    {
      "epoch": 74.62,
      "grad_norm": 0.0034302137792110443,
      "learning_rate": 1.1557022617676217e-07,
      "loss": 0.0001,
      "step": 4664
    },
    {
      "epoch": 74.64,
      "grad_norm": 0.716084361076355,
      "learning_rate": 1.1488597454013539e-07,
      "loss": 0.0091,
      "step": 4665
    },
    {
      "epoch": 74.66,
      "grad_norm": 0.3698464632034302,
      "learning_rate": 1.1420373098935733e-07,
      "loss": 0.0021,
      "step": 4666
    },
    {
      "epoch": 74.67,
      "grad_norm": 0.7424176335334778,
      "learning_rate": 1.135234958048731e-07,
      "loss": 0.0093,
      "step": 4667
    },
    {
      "epoch": 74.69,
      "grad_norm": 0.9513705968856812,
      "learning_rate": 1.1284526926630124e-07,
      "loss": 0.0093,
      "step": 4668
    },
    {
      "epoch": 74.7,
      "grad_norm": 0.0038983679842203856,
      "learning_rate": 1.1216905165243542e-07,
      "loss": 0.0001,
      "step": 4669
    },
    {
      "epoch": 74.72,
      "grad_norm": 1.0564987659454346,
      "learning_rate": 1.1149484324124326e-07,
      "loss": 0.0138,
      "step": 4670
    },
    {
      "epoch": 74.74,
      "grad_norm": 0.0031727654859423637,
      "learning_rate": 1.1082264430986533e-07,
      "loss": 0.0001,
      "step": 4671
    },
    {
      "epoch": 74.75,
      "grad_norm": 0.0033022311981767416,
      "learning_rate": 1.1015245513461837e-07,
      "loss": 0.0001,
      "step": 4672
    },
    {
      "epoch": 74.77,
      "grad_norm": 0.050995856523513794,
      "learning_rate": 1.0948427599099143e-07,
      "loss": 0.0002,
      "step": 4673
    },
    {
      "epoch": 74.78,
      "grad_norm": 0.9030200839042664,
      "learning_rate": 1.088181071536476e-07,
      "loss": 0.0136,
      "step": 4674
    },
    {
      "epoch": 74.8,
      "grad_norm": 0.7851163744926453,
      "learning_rate": 1.0815394889642339e-07,
      "loss": 0.0083,
      "step": 4675
    },
    {
      "epoch": 74.82,
      "grad_norm": 0.0030978673603385687,
      "learning_rate": 1.0749180149233041e-07,
      "loss": 0.0001,
      "step": 4676
    },
    {
      "epoch": 74.83,
      "grad_norm": 0.0036227935925126076,
      "learning_rate": 1.0683166521355093e-07,
      "loss": 0.0001,
      "step": 4677
    },
    {
      "epoch": 74.85,
      "grad_norm": 0.003680106019601226,
      "learning_rate": 1.0617354033144289e-07,
      "loss": 0.0001,
      "step": 4678
    },
    {
      "epoch": 74.86,
      "grad_norm": 0.3231576979160309,
      "learning_rate": 1.0551742711653656e-07,
      "loss": 0.0031,
      "step": 4679
    },
    {
      "epoch": 74.88,
      "grad_norm": 0.17734378576278687,
      "learning_rate": 1.0486332583853565e-07,
      "loss": 0.0009,
      "step": 4680
    },
    {
      "epoch": 74.9,
      "grad_norm": 0.003702423768118024,
      "learning_rate": 1.0421123676631562e-07,
      "loss": 0.0001,
      "step": 4681
    },
    {
      "epoch": 74.91,
      "grad_norm": 0.6801611185073853,
      "learning_rate": 1.0356116016792594e-07,
      "loss": 0.0063,
      "step": 4682
    },
    {
      "epoch": 74.93,
      "grad_norm": 0.0035083452239632607,
      "learning_rate": 1.0291309631058899e-07,
      "loss": 0.0001,
      "step": 4683
    },
    {
      "epoch": 74.94,
      "grad_norm": 0.3583891987800598,
      "learning_rate": 1.0226704546069832e-07,
      "loss": 0.0027,
      "step": 4684
    },
    {
      "epoch": 74.96,
      "grad_norm": 0.0023356315214186907,
      "learning_rate": 1.0162300788382263e-07,
      "loss": 0.0,
      "step": 4685
    },
    {
      "epoch": 74.98,
      "grad_norm": 0.7569925785064697,
      "learning_rate": 1.0098098384469957e-07,
      "loss": 0.0097,
      "step": 4686
    },
    {
      "epoch": 74.99,
      "grad_norm": 0.003906731493771076,
      "learning_rate": 1.0034097360724249e-07,
      "loss": 0.0001,
      "step": 4687
    },
    {
      "epoch": 75.01,
      "grad_norm": 0.004031738732010126,
      "learning_rate": 9.970297743453484e-08,
      "loss": 0.0001,
      "step": 4688
    },
    {
      "epoch": 75.02,
      "grad_norm": 0.7402430772781372,
      "learning_rate": 9.906699558883238e-08,
      "loss": 0.0041,
      "step": 4689
    },
    {
      "epoch": 75.04,
      "grad_norm": 0.8445563316345215,
      "learning_rate": 9.843302833156377e-08,
      "loss": 0.0108,
      "step": 4690
    },
    {
      "epoch": 75.06,
      "grad_norm": 0.0034520765766501427,
      "learning_rate": 9.780107592332944e-08,
      "loss": 0.0001,
      "step": 4691
    },
    {
      "epoch": 75.07,
      "grad_norm": 0.636035144329071,
      "learning_rate": 9.717113862389993e-08,
      "loss": 0.0035,
      "step": 4692
    },
    {
      "epoch": 75.09,
      "grad_norm": 0.6183742880821228,
      "learning_rate": 9.654321669221978e-08,
      "loss": 0.0067,
      "step": 4693
    },
    {
      "epoch": 75.1,
      "grad_norm": 0.002190524945035577,
      "learning_rate": 9.591731038640306e-08,
      "loss": 0.0,
      "step": 4694
    },
    {
      "epoch": 75.12,
      "grad_norm": 0.003424581605941057,
      "learning_rate": 9.529341996373675e-08,
      "loss": 0.0001,
      "step": 4695
    },
    {
      "epoch": 75.14,
      "grad_norm": 0.7610874772071838,
      "learning_rate": 9.467154568067849e-08,
      "loss": 0.0098,
      "step": 4696
    },
    {
      "epoch": 75.15,
      "grad_norm": 0.002056455472484231,
      "learning_rate": 9.405168779285712e-08,
      "loss": 0.0,
      "step": 4697
    },
    {
      "epoch": 75.17,
      "grad_norm": 0.5371524691581726,
      "learning_rate": 9.343384655507326e-08,
      "loss": 0.0066,
      "step": 4698
    },
    {
      "epoch": 75.18,
      "grad_norm": 0.003922617062926292,
      "learning_rate": 9.281802222129766e-08,
      "loss": 0.0001,
      "step": 4699
    },
    {
      "epoch": 75.2,
      "grad_norm": 0.0028231239411979914,
      "learning_rate": 9.22042150446728e-08,
      "loss": 0.0001,
      "step": 4700
    },
    {
      "epoch": 75.22,
      "grad_norm": 0.7722890377044678,
      "learning_rate": 9.159242527751245e-08,
      "loss": 0.0098,
      "step": 4701
    },
    {
      "epoch": 75.23,
      "grad_norm": 0.003172358963638544,
      "learning_rate": 9.098265317129817e-08,
      "loss": 0.0001,
      "step": 4702
    },
    {
      "epoch": 75.25,
      "grad_norm": 0.5232357978820801,
      "learning_rate": 9.037489897668561e-08,
      "loss": 0.0031,
      "step": 4703
    },
    {
      "epoch": 75.26,
      "grad_norm": 0.0035281754098832607,
      "learning_rate": 8.976916294349935e-08,
      "loss": 0.0001,
      "step": 4704
    },
    {
      "epoch": 75.28,
      "grad_norm": 0.7108381986618042,
      "learning_rate": 8.916544532073413e-08,
      "loss": 0.0072,
      "step": 4705
    },
    {
      "epoch": 75.3,
      "grad_norm": 0.9454484581947327,
      "learning_rate": 8.856374635655696e-08,
      "loss": 0.0123,
      "step": 4706
    },
    {
      "epoch": 75.31,
      "grad_norm": 0.17783629894256592,
      "learning_rate": 8.796406629830168e-08,
      "loss": 0.0009,
      "step": 4707
    },
    {
      "epoch": 75.33,
      "grad_norm": 0.36075982451438904,
      "learning_rate": 8.736640539247498e-08,
      "loss": 0.0028,
      "step": 4708
    },
    {
      "epoch": 75.34,
      "grad_norm": 0.004052754957228899,
      "learning_rate": 8.677076388475314e-08,
      "loss": 0.0001,
      "step": 4709
    },
    {
      "epoch": 75.36,
      "grad_norm": 0.8239759206771851,
      "learning_rate": 8.617714201998084e-08,
      "loss": 0.0112,
      "step": 4710
    },
    {
      "epoch": 75.38,
      "grad_norm": 0.7793735265731812,
      "learning_rate": 8.5585540042174e-08,
      "loss": 0.0109,
      "step": 4711
    },
    {
      "epoch": 75.39,
      "grad_norm": 0.0546608529984951,
      "learning_rate": 8.499595819451811e-08,
      "loss": 0.0004,
      "step": 4712
    },
    {
      "epoch": 75.41,
      "grad_norm": 0.6183632612228394,
      "learning_rate": 8.440839671936819e-08,
      "loss": 0.0046,
      "step": 4713
    },
    {
      "epoch": 75.42,
      "grad_norm": 0.519590437412262,
      "learning_rate": 8.382285585824879e-08,
      "loss": 0.0028,
      "step": 4714
    },
    {
      "epoch": 75.44,
      "grad_norm": 0.052019961178302765,
      "learning_rate": 8.323933585185184e-08,
      "loss": 0.0003,
      "step": 4715
    },
    {
      "epoch": 75.46,
      "grad_norm": 0.0034973318688571453,
      "learning_rate": 8.265783694004214e-08,
      "loss": 0.0001,
      "step": 4716
    },
    {
      "epoch": 75.47,
      "grad_norm": 0.018567360937595367,
      "learning_rate": 8.207835936185182e-08,
      "loss": 0.0002,
      "step": 4717
    },
    {
      "epoch": 75.49,
      "grad_norm": 0.6706432700157166,
      "learning_rate": 8.150090335548144e-08,
      "loss": 0.0038,
      "step": 4718
    },
    {
      "epoch": 75.5,
      "grad_norm": 0.004213700536638498,
      "learning_rate": 8.092546915830168e-08,
      "loss": 0.0001,
      "step": 4719
    },
    {
      "epoch": 75.52,
      "grad_norm": 0.250256210565567,
      "learning_rate": 8.035205700685167e-08,
      "loss": 0.0016,
      "step": 4720
    },
    {
      "epoch": 75.54,
      "grad_norm": 0.004230232443660498,
      "learning_rate": 7.978066713684008e-08,
      "loss": 0.0001,
      "step": 4721
    },
    {
      "epoch": 75.55,
      "grad_norm": 0.7207563519477844,
      "learning_rate": 7.92112997831429e-08,
      "loss": 0.0041,
      "step": 4722
    },
    {
      "epoch": 75.57,
      "grad_norm": 0.8806134462356567,
      "learning_rate": 7.864395517980627e-08,
      "loss": 0.0111,
      "step": 4723
    },
    {
      "epoch": 75.58,
      "grad_norm": 0.03417494520545006,
      "learning_rate": 7.80786335600442e-08,
      "loss": 0.0001,
      "step": 4724
    },
    {
      "epoch": 75.6,
      "grad_norm": 0.004232881125062704,
      "learning_rate": 7.7515335156238e-08,
      "loss": 0.0001,
      "step": 4725
    },
    {
      "epoch": 75.62,
      "grad_norm": 0.0036107408814132214,
      "learning_rate": 7.695406019993912e-08,
      "loss": 0.0001,
      "step": 4726
    },
    {
      "epoch": 75.63,
      "grad_norm": 0.37468114495277405,
      "learning_rate": 7.639480892186634e-08,
      "loss": 0.0023,
      "step": 4727
    },
    {
      "epoch": 75.65,
      "grad_norm": 0.0033633399289101362,
      "learning_rate": 7.58375815519069e-08,
      "loss": 0.0001,
      "step": 4728
    },
    {
      "epoch": 75.66,
      "grad_norm": 0.9325335025787354,
      "learning_rate": 7.528237831911588e-08,
      "loss": 0.0123,
      "step": 4729
    },
    {
      "epoch": 75.68,
      "grad_norm": 0.1908680945634842,
      "learning_rate": 7.47291994517163e-08,
      "loss": 0.0012,
      "step": 4730
    },
    {
      "epoch": 75.7,
      "grad_norm": 0.6876662373542786,
      "learning_rate": 7.417804517709903e-08,
      "loss": 0.0065,
      "step": 4731
    },
    {
      "epoch": 75.71,
      "grad_norm": 0.003551997011527419,
      "learning_rate": 7.362891572182284e-08,
      "loss": 0.0001,
      "step": 4732
    },
    {
      "epoch": 75.73,
      "grad_norm": 0.7930721044540405,
      "learning_rate": 7.308181131161385e-08,
      "loss": 0.0084,
      "step": 4733
    },
    {
      "epoch": 75.74,
      "grad_norm": 0.003180173924192786,
      "learning_rate": 7.253673217136659e-08,
      "loss": 0.0001,
      "step": 4734
    },
    {
      "epoch": 75.76,
      "grad_norm": 1.1333589553833008,
      "learning_rate": 7.199367852514239e-08,
      "loss": 0.0157,
      "step": 4735
    },
    {
      "epoch": 75.78,
      "grad_norm": 0.44084128737449646,
      "learning_rate": 7.145265059616934e-08,
      "loss": 0.0024,
      "step": 4736
    },
    {
      "epoch": 75.79,
      "grad_norm": 0.6312284469604492,
      "learning_rate": 7.091364860684403e-08,
      "loss": 0.0034,
      "step": 4737
    },
    {
      "epoch": 75.81,
      "grad_norm": 0.7889590263366699,
      "learning_rate": 7.037667277873029e-08,
      "loss": 0.0074,
      "step": 4738
    },
    {
      "epoch": 75.82,
      "grad_norm": 0.017618680372834206,
      "learning_rate": 6.984172333255824e-08,
      "loss": 0.0002,
      "step": 4739
    },
    {
      "epoch": 75.84,
      "grad_norm": 0.7523512244224548,
      "learning_rate": 6.930880048822531e-08,
      "loss": 0.0077,
      "step": 4740
    },
    {
      "epoch": 75.86,
      "grad_norm": 0.0038781468756496906,
      "learning_rate": 6.87779044647957e-08,
      "loss": 0.0001,
      "step": 4741
    },
    {
      "epoch": 75.87,
      "grad_norm": 0.0022227750159800053,
      "learning_rate": 6.824903548050098e-08,
      "loss": 0.0,
      "step": 4742
    },
    {
      "epoch": 75.89,
      "grad_norm": 0.8998619318008423,
      "learning_rate": 6.772219375273947e-08,
      "loss": 0.0094,
      "step": 4743
    },
    {
      "epoch": 75.9,
      "grad_norm": 0.002768778009340167,
      "learning_rate": 6.71973794980757e-08,
      "loss": 0.0001,
      "step": 4744
    },
    {
      "epoch": 75.92,
      "grad_norm": 0.6773270964622498,
      "learning_rate": 6.667459293224155e-08,
      "loss": 0.0056,
      "step": 4745
    },
    {
      "epoch": 75.94,
      "grad_norm": 0.5640875697135925,
      "learning_rate": 6.615383427013345e-08,
      "loss": 0.0056,
      "step": 4746
    },
    {
      "epoch": 75.95,
      "grad_norm": 0.0037094661965966225,
      "learning_rate": 6.563510372581682e-08,
      "loss": 0.0001,
      "step": 4747
    },
    {
      "epoch": 75.97,
      "grad_norm": 0.004508443176746368,
      "learning_rate": 6.511840151252169e-08,
      "loss": 0.0002,
      "step": 4748
    },
    {
      "epoch": 75.98,
      "grad_norm": 0.0033496462274342775,
      "learning_rate": 6.460372784264479e-08,
      "loss": 0.0001,
      "step": 4749
    },
    {
      "epoch": 76.0,
      "grad_norm": 0.5817825794219971,
      "learning_rate": 6.409108292774912e-08,
      "loss": 0.0032,
      "step": 4750
    },
    {
      "epoch": 76.02,
      "grad_norm": 0.0050543504767119884,
      "learning_rate": 6.358046697856446e-08,
      "loss": 0.0002,
      "step": 4751
    },
    {
      "epoch": 76.03,
      "grad_norm": 0.003598154755309224,
      "learning_rate": 6.307188020498401e-08,
      "loss": 0.0001,
      "step": 4752
    },
    {
      "epoch": 76.05,
      "grad_norm": 0.7091186046600342,
      "learning_rate": 6.256532281606997e-08,
      "loss": 0.0092,
      "step": 4753
    },
    {
      "epoch": 76.06,
      "grad_norm": 0.6689684987068176,
      "learning_rate": 6.206079502004803e-08,
      "loss": 0.0071,
      "step": 4754
    },
    {
      "epoch": 76.08,
      "grad_norm": 0.7948499321937561,
      "learning_rate": 6.15582970243117e-08,
      "loss": 0.0085,
      "step": 4755
    },
    {
      "epoch": 76.1,
      "grad_norm": 0.0035916250199079514,
      "learning_rate": 6.105782903541746e-08,
      "loss": 0.0001,
      "step": 4756
    },
    {
      "epoch": 76.11,
      "grad_norm": 0.0030763198155909777,
      "learning_rate": 6.055939125909016e-08,
      "loss": 0.0001,
      "step": 4757
    },
    {
      "epoch": 76.13,
      "grad_norm": 0.0037203317042440176,
      "learning_rate": 6.006298390021814e-08,
      "loss": 0.0001,
      "step": 4758
    },
    {
      "epoch": 76.14,
      "grad_norm": 0.6755132675170898,
      "learning_rate": 5.95686071628554e-08,
      "loss": 0.0049,
      "step": 4759
    },
    {
      "epoch": 76.16,
      "grad_norm": 0.0035542715340852737,
      "learning_rate": 5.907626125022159e-08,
      "loss": 0.0001,
      "step": 4760
    },
    {
      "epoch": 76.18,
      "grad_norm": 0.0033750340808182955,
      "learning_rate": 5.858594636470205e-08,
      "loss": 0.0001,
      "step": 4761
    },
    {
      "epoch": 76.19,
      "grad_norm": 0.0030771151650696993,
      "learning_rate": 5.809766270784667e-08,
      "loss": 0.0001,
      "step": 4762
    },
    {
      "epoch": 76.21,
      "grad_norm": 0.0032098027877509594,
      "learning_rate": 5.761141048036989e-08,
      "loss": 0.0001,
      "step": 4763
    },
    {
      "epoch": 76.22,
      "grad_norm": 0.6338181495666504,
      "learning_rate": 5.712718988215182e-08,
      "loss": 0.0037,
      "step": 4764
    },
    {
      "epoch": 76.24,
      "grad_norm": 0.5089889168739319,
      "learning_rate": 5.6645001112237694e-08,
      "loss": 0.0028,
      "step": 4765
    },
    {
      "epoch": 76.26,
      "grad_norm": 0.6187143325805664,
      "learning_rate": 5.616484436883618e-08,
      "loss": 0.0079,
      "step": 4766
    },
    {
      "epoch": 76.27,
      "grad_norm": 0.0033530863001942635,
      "learning_rate": 5.568671984932272e-08,
      "loss": 0.0001,
      "step": 4767
    },
    {
      "epoch": 76.29,
      "grad_norm": 0.43233591318130493,
      "learning_rate": 5.521062775023567e-08,
      "loss": 0.0029,
      "step": 4768
    },
    {
      "epoch": 76.3,
      "grad_norm": 0.6604876518249512,
      "learning_rate": 5.4736568267278464e-08,
      "loss": 0.0083,
      "step": 4769
    },
    {
      "epoch": 76.32,
      "grad_norm": 0.004276484716683626,
      "learning_rate": 5.426454159531913e-08,
      "loss": 0.0001,
      "step": 4770
    },
    {
      "epoch": 76.34,
      "grad_norm": 0.6146193742752075,
      "learning_rate": 5.379454792838967e-08,
      "loss": 0.0038,
      "step": 4771
    },
    {
      "epoch": 76.35,
      "grad_norm": 0.004870239645242691,
      "learning_rate": 5.3326587459687774e-08,
      "loss": 0.0002,
      "step": 4772
    },
    {
      "epoch": 76.37,
      "grad_norm": 0.3986973464488983,
      "learning_rate": 5.2860660381572894e-08,
      "loss": 0.0021,
      "step": 4773
    },
    {
      "epoch": 76.38,
      "grad_norm": 0.2523188591003418,
      "learning_rate": 5.239676688557072e-08,
      "loss": 0.0013,
      "step": 4774
    },
    {
      "epoch": 76.4,
      "grad_norm": 0.003550734603777528,
      "learning_rate": 5.1934907162370374e-08,
      "loss": 0.0001,
      "step": 4775
    },
    {
      "epoch": 76.42,
      "grad_norm": 0.7898990511894226,
      "learning_rate": 5.1475081401825553e-08,
      "loss": 0.0045,
      "step": 4776
    },
    {
      "epoch": 76.43,
      "grad_norm": 0.33842337131500244,
      "learning_rate": 5.101728979295173e-08,
      "loss": 0.002,
      "step": 4777
    },
    {
      "epoch": 76.45,
      "grad_norm": 0.05538696423172951,
      "learning_rate": 5.05615325239317e-08,
      "loss": 0.0003,
      "step": 4778
    },
    {
      "epoch": 76.46,
      "grad_norm": 0.6242480874061584,
      "learning_rate": 5.0107809782108384e-08,
      "loss": 0.0034,
      "step": 4779
    },
    {
      "epoch": 76.48,
      "grad_norm": 0.7425143122673035,
      "learning_rate": 4.9656121753990924e-08,
      "loss": 0.009,
      "step": 4780
    },
    {
      "epoch": 76.5,
      "grad_norm": 0.5668571591377258,
      "learning_rate": 4.9206468625250804e-08,
      "loss": 0.0058,
      "step": 4781
    },
    {
      "epoch": 76.51,
      "grad_norm": 0.9486672878265381,
      "learning_rate": 4.875885058072349e-08,
      "loss": 0.016,
      "step": 4782
    },
    {
      "epoch": 76.53,
      "grad_norm": 0.004065996967256069,
      "learning_rate": 4.8313267804407926e-08,
      "loss": 0.0001,
      "step": 4783
    },
    {
      "epoch": 76.54,
      "grad_norm": 0.5819445252418518,
      "learning_rate": 4.7869720479466475e-08,
      "loss": 0.0032,
      "step": 4784
    },
    {
      "epoch": 76.56,
      "grad_norm": 0.0028766700997948647,
      "learning_rate": 4.742820878822496e-08,
      "loss": 0.0001,
      "step": 4785
    },
    {
      "epoch": 76.58,
      "grad_norm": 0.5042052865028381,
      "learning_rate": 4.698873291217154e-08,
      "loss": 0.0041,
      "step": 4786
    },
    {
      "epoch": 76.59,
      "grad_norm": 0.0031960189808160067,
      "learning_rate": 4.6551293031958376e-08,
      "loss": 0.0001,
      "step": 4787
    },
    {
      "epoch": 76.61,
      "grad_norm": 0.4849846065044403,
      "learning_rate": 4.611588932740107e-08,
      "loss": 0.0031,
      "step": 4788
    },
    {
      "epoch": 76.62,
      "grad_norm": 0.49455589056015015,
      "learning_rate": 4.568252197747647e-08,
      "loss": 0.0052,
      "step": 4789
    },
    {
      "epoch": 76.64,
      "grad_norm": 0.0033021909184753895,
      "learning_rate": 4.52511911603265e-08,
      "loss": 0.0001,
      "step": 4790
    },
    {
      "epoch": 76.66,
      "grad_norm": 1.0395280122756958,
      "learning_rate": 4.482189705325435e-08,
      "loss": 0.0082,
      "step": 4791
    },
    {
      "epoch": 76.67,
      "grad_norm": 0.0028993235900998116,
      "learning_rate": 4.439463983272663e-08,
      "loss": 0.0001,
      "step": 4792
    },
    {
      "epoch": 76.69,
      "grad_norm": 0.0036772997118532658,
      "learning_rate": 4.3969419674373407e-08,
      "loss": 0.0001,
      "step": 4793
    },
    {
      "epoch": 76.7,
      "grad_norm": 0.004730104934424162,
      "learning_rate": 4.354623675298597e-08,
      "loss": 0.0001,
      "step": 4794
    },
    {
      "epoch": 76.72,
      "grad_norm": 0.21390368044376373,
      "learning_rate": 4.312509124251907e-08,
      "loss": 0.0011,
      "step": 4795
    },
    {
      "epoch": 76.74,
      "grad_norm": 0.003852029098197818,
      "learning_rate": 4.270598331608977e-08,
      "loss": 0.0001,
      "step": 4796
    },
    {
      "epoch": 76.75,
      "grad_norm": 1.0846463441848755,
      "learning_rate": 4.228891314597694e-08,
      "loss": 0.0208,
      "step": 4797
    },
    {
      "epoch": 76.77,
      "grad_norm": 0.4324057102203369,
      "learning_rate": 4.187388090362288e-08,
      "loss": 0.0027,
      "step": 4798
    },
    {
      "epoch": 76.78,
      "grad_norm": 0.0031731848139315844,
      "learning_rate": 4.146088675963167e-08,
      "loss": 0.0001,
      "step": 4799
    },
    {
      "epoch": 76.8,
      "grad_norm": 0.0035106302239000797,
      "learning_rate": 4.104993088376974e-08,
      "loss": 0.0001,
      "step": 4800
    },
    {
      "epoch": 76.82,
      "grad_norm": 0.0026409083511680365,
      "learning_rate": 4.0641013444965294e-08,
      "loss": 0.0001,
      "step": 4801
    },
    {
      "epoch": 76.83,
      "grad_norm": 0.0034405041951686144,
      "learning_rate": 4.0234134611308315e-08,
      "loss": 0.0001,
      "step": 4802
    },
    {
      "epoch": 76.85,
      "grad_norm": 0.003395498264580965,
      "learning_rate": 3.9829294550052244e-08,
      "loss": 0.0001,
      "step": 4803
    },
    {
      "epoch": 76.86,
      "grad_norm": 1.0043126344680786,
      "learning_rate": 3.9426493427611177e-08,
      "loss": 0.0139,
      "step": 4804
    },
    {
      "epoch": 76.88,
      "grad_norm": 0.013910561800003052,
      "learning_rate": 3.902573140956101e-08,
      "loss": 0.0002,
      "step": 4805
    },
    {
      "epoch": 76.9,
      "grad_norm": 0.0032723317854106426,
      "learning_rate": 3.862700866064051e-08,
      "loss": 0.0001,
      "step": 4806
    },
    {
      "epoch": 76.91,
      "grad_norm": 0.002849471988156438,
      "learning_rate": 3.823032534474969e-08,
      "loss": 0.0001,
      "step": 4807
    },
    {
      "epoch": 76.93,
      "grad_norm": 0.05650151148438454,
      "learning_rate": 3.7835681624949216e-08,
      "loss": 0.0004,
      "step": 4808
    },
    {
      "epoch": 76.94,
      "grad_norm": 0.9603609442710876,
      "learning_rate": 3.744307766346267e-08,
      "loss": 0.0118,
      "step": 4809
    },
    {
      "epoch": 76.96,
      "grad_norm": 1.3301633596420288,
      "learning_rate": 3.705251362167484e-08,
      "loss": 0.0225,
      "step": 4810
    },
    {
      "epoch": 76.98,
      "grad_norm": 0.0033138373401015997,
      "learning_rate": 3.666398966013229e-08,
      "loss": 0.0001,
      "step": 4811
    },
    {
      "epoch": 76.99,
      "grad_norm": 0.756000816822052,
      "learning_rate": 3.6277505938541735e-08,
      "loss": 0.0046,
      "step": 4812
    },
    {
      "epoch": 77.01,
      "grad_norm": 0.04696871340274811,
      "learning_rate": 3.589306261577219e-08,
      "loss": 0.0004,
      "step": 4813
    },
    {
      "epoch": 77.02,
      "grad_norm": 0.59394770860672,
      "learning_rate": 3.5510659849853355e-08,
      "loss": 0.0043,
      "step": 4814
    },
    {
      "epoch": 77.04,
      "grad_norm": 0.052489522844552994,
      "learning_rate": 3.513029779797783e-08,
      "loss": 0.0003,
      "step": 4815
    },
    {
      "epoch": 77.06,
      "grad_norm": 0.3987400233745575,
      "learning_rate": 3.475197661649665e-08,
      "loss": 0.0021,
      "step": 4816
    },
    {
      "epoch": 77.07,
      "grad_norm": 0.1425369530916214,
      "learning_rate": 3.437569646092487e-08,
      "loss": 0.0008,
      "step": 4817
    },
    {
      "epoch": 77.09,
      "grad_norm": 0.0037516863085329533,
      "learning_rate": 3.400145748593542e-08,
      "loss": 0.0001,
      "step": 4818
    },
    {
      "epoch": 77.1,
      "grad_norm": 0.7526535391807556,
      "learning_rate": 3.362925984536525e-08,
      "loss": 0.0102,
      "step": 4819
    },
    {
      "epoch": 77.12,
      "grad_norm": 1.042537808418274,
      "learning_rate": 3.325910369220975e-08,
      "loss": 0.0194,
      "step": 4820
    },
    {
      "epoch": 77.14,
      "grad_norm": 0.5131798982620239,
      "learning_rate": 3.289098917862721e-08,
      "loss": 0.0041,
      "step": 4821
    },
    {
      "epoch": 77.15,
      "grad_norm": 0.05170583724975586,
      "learning_rate": 3.2524916455934365e-08,
      "loss": 0.0002,
      "step": 4822
    },
    {
      "epoch": 77.17,
      "grad_norm": 0.003987073432654142,
      "learning_rate": 3.2160885674610284e-08,
      "loss": 0.0001,
      "step": 4823
    },
    {
      "epoch": 77.18,
      "grad_norm": 0.9519677758216858,
      "learning_rate": 3.179889698429473e-08,
      "loss": 0.0117,
      "step": 4824
    },
    {
      "epoch": 77.2,
      "grad_norm": 0.8979682326316833,
      "learning_rate": 3.143895053378698e-08,
      "loss": 0.0113,
      "step": 4825
    },
    {
      "epoch": 77.22,
      "grad_norm": 0.003567768493667245,
      "learning_rate": 3.108104647104815e-08,
      "loss": 0.0001,
      "step": 4826
    },
    {
      "epoch": 77.23,
      "grad_norm": 0.5077527165412903,
      "learning_rate": 3.0725184943198316e-08,
      "loss": 0.0028,
      "step": 4827
    },
    {
      "epoch": 77.25,
      "grad_norm": 0.005094180814921856,
      "learning_rate": 3.037136609651881e-08,
      "loss": 0.0002,
      "step": 4828
    },
    {
      "epoch": 77.26,
      "grad_norm": 0.0034857592545449734,
      "learning_rate": 3.001959007645161e-08,
      "loss": 0.0001,
      "step": 4829
    },
    {
      "epoch": 77.28,
      "grad_norm": 0.47159871459007263,
      "learning_rate": 2.966985702759828e-08,
      "loss": 0.0035,
      "step": 4830
    },
    {
      "epoch": 77.3,
      "grad_norm": 0.002935366239398718,
      "learning_rate": 2.9322167093721598e-08,
      "loss": 0.0001,
      "step": 4831
    },
    {
      "epoch": 77.31,
      "grad_norm": 0.004156294744461775,
      "learning_rate": 2.8976520417742794e-08,
      "loss": 0.0001,
      "step": 4832
    },
    {
      "epoch": 77.33,
      "grad_norm": 0.004424830432981253,
      "learning_rate": 2.8632917141743766e-08,
      "loss": 0.0001,
      "step": 4833
    },
    {
      "epoch": 77.34,
      "grad_norm": 0.004203042481094599,
      "learning_rate": 2.82913574069682e-08,
      "loss": 0.0001,
      "step": 4834
    },
    {
      "epoch": 77.36,
      "grad_norm": 0.003116703126579523,
      "learning_rate": 2.7951841353817676e-08,
      "loss": 0.0001,
      "step": 4835
    },
    {
      "epoch": 77.38,
      "grad_norm": 0.0042755333706736565,
      "learning_rate": 2.7614369121854444e-08,
      "loss": 0.0001,
      "step": 4836
    },
    {
      "epoch": 77.39,
      "grad_norm": 0.0028509381227195263,
      "learning_rate": 2.7278940849800316e-08,
      "loss": 0.0001,
      "step": 4837
    },
    {
      "epoch": 77.41,
      "grad_norm": 0.0034151708241552114,
      "learning_rate": 2.6945556675537776e-08,
      "loss": 0.0001,
      "step": 4838
    },
    {
      "epoch": 77.42,
      "grad_norm": 0.003515477990731597,
      "learning_rate": 2.661421673610831e-08,
      "loss": 0.0001,
      "step": 4839
    },
    {
      "epoch": 77.44,
      "grad_norm": 0.054686401039361954,
      "learning_rate": 2.6284921167712975e-08,
      "loss": 0.0003,
      "step": 4840
    },
    {
      "epoch": 77.46,
      "grad_norm": 0.8007662892341614,
      "learning_rate": 2.595767010571293e-08,
      "loss": 0.007,
      "step": 4841
    },
    {
      "epoch": 77.47,
      "grad_norm": 0.0035315232817083597,
      "learning_rate": 2.5632463684628904e-08,
      "loss": 0.0001,
      "step": 4842
    },
    {
      "epoch": 77.49,
      "grad_norm": 0.9269428253173828,
      "learning_rate": 2.530930203814064e-08,
      "loss": 0.0067,
      "step": 4843
    },
    {
      "epoch": 77.5,
      "grad_norm": 0.7258654832839966,
      "learning_rate": 2.4988185299087973e-08,
      "loss": 0.0105,
      "step": 4844
    },
    {
      "epoch": 77.52,
      "grad_norm": 0.8668041825294495,
      "learning_rate": 2.4669113599469774e-08,
      "loss": 0.0067,
      "step": 4845
    },
    {
      "epoch": 77.54,
      "grad_norm": 0.46604201197624207,
      "learning_rate": 2.43520870704439e-08,
      "loss": 0.0028,
      "step": 4846
    },
    {
      "epoch": 77.55,
      "grad_norm": 0.003539445111528039,
      "learning_rate": 2.4037105842328324e-08,
      "loss": 0.0001,
      "step": 4847
    },
    {
      "epoch": 77.57,
      "grad_norm": 1.0672982931137085,
      "learning_rate": 2.3724170044600036e-08,
      "loss": 0.0159,
      "step": 4848
    },
    {
      "epoch": 77.58,
      "grad_norm": 0.0028692837804555893,
      "learning_rate": 2.3413279805895583e-08,
      "loss": 0.0001,
      "step": 4849
    },
    {
      "epoch": 77.6,
      "grad_norm": 0.004352358635514975,
      "learning_rate": 2.3104435254008852e-08,
      "loss": 0.0001,
      "step": 4850
    },
    {
      "epoch": 77.62,
      "grad_norm": 0.6273301839828491,
      "learning_rate": 2.2797636515895505e-08,
      "loss": 0.0064,
      "step": 4851
    },
    {
      "epoch": 77.63,
      "grad_norm": 0.00412770826369524,
      "learning_rate": 2.2492883717668557e-08,
      "loss": 0.0001,
      "step": 4852
    },
    {
      "epoch": 77.65,
      "grad_norm": 0.00453985296189785,
      "learning_rate": 2.219017698460002e-08,
      "loss": 0.0002,
      "step": 4853
    },
    {
      "epoch": 77.66,
      "grad_norm": 0.5977563261985779,
      "learning_rate": 2.1889516441121472e-08,
      "loss": 0.0061,
      "step": 4854
    },
    {
      "epoch": 77.68,
      "grad_norm": 0.0528898723423481,
      "learning_rate": 2.159090221082294e-08,
      "loss": 0.0003,
      "step": 4855
    },
    {
      "epoch": 77.7,
      "grad_norm": 0.8388066291809082,
      "learning_rate": 2.1294334416453456e-08,
      "loss": 0.0047,
      "step": 4856
    },
    {
      "epoch": 77.71,
      "grad_norm": 0.596877932548523,
      "learning_rate": 2.0999813179921057e-08,
      "loss": 0.005,
      "step": 4857
    },
    {
      "epoch": 77.73,
      "grad_norm": 0.003670511534437537,
      "learning_rate": 2.070733862229224e-08,
      "loss": 0.0001,
      "step": 4858
    },
    {
      "epoch": 77.74,
      "grad_norm": 0.0024971370585262775,
      "learning_rate": 2.0416910863792495e-08,
      "loss": 0.0,
      "step": 4859
    },
    {
      "epoch": 77.76,
      "grad_norm": 0.004043255001306534,
      "learning_rate": 2.012853002380466e-08,
      "loss": 0.0001,
      "step": 4860
    },
    {
      "epoch": 77.78,
      "grad_norm": 0.784076452255249,
      "learning_rate": 1.98421962208728e-08,
      "loss": 0.0081,
      "step": 4861
    },
    {
      "epoch": 77.79,
      "grad_norm": 0.3945222496986389,
      "learning_rate": 1.9557909572696098e-08,
      "loss": 0.004,
      "step": 4862
    },
    {
      "epoch": 77.81,
      "grad_norm": 1.080280065536499,
      "learning_rate": 1.9275670196135522e-08,
      "loss": 0.0166,
      "step": 4863
    },
    {
      "epoch": 77.82,
      "grad_norm": 0.8244506120681763,
      "learning_rate": 1.899547820720882e-08,
      "loss": 0.0104,
      "step": 4864
    },
    {
      "epoch": 77.84,
      "grad_norm": 0.6659946441650391,
      "learning_rate": 1.8717333721091634e-08,
      "loss": 0.015,
      "step": 4865
    },
    {
      "epoch": 77.86,
      "grad_norm": 0.7955968976020813,
      "learning_rate": 1.8441236852119184e-08,
      "loss": 0.0104,
      "step": 4866
    },
    {
      "epoch": 77.87,
      "grad_norm": 0.002931344322860241,
      "learning_rate": 1.8167187713784563e-08,
      "loss": 0.0001,
      "step": 4867
    },
    {
      "epoch": 77.89,
      "grad_norm": 0.03724273294210434,
      "learning_rate": 1.7895186418738773e-08,
      "loss": 0.0002,
      "step": 4868
    },
    {
      "epoch": 77.9,
      "grad_norm": 0.0033735439646989107,
      "learning_rate": 1.7625233078790716e-08,
      "loss": 0.0001,
      "step": 4869
    },
    {
      "epoch": 77.92,
      "grad_norm": 0.7583828568458557,
      "learning_rate": 1.735732780490884e-08,
      "loss": 0.0041,
      "step": 4870
    },
    {
      "epoch": 77.94,
      "grad_norm": 0.5827130675315857,
      "learning_rate": 1.7091470707218948e-08,
      "loss": 0.0033,
      "step": 4871
    },
    {
      "epoch": 77.95,
      "grad_norm": 0.5296137928962708,
      "learning_rate": 1.6827661895004176e-08,
      "loss": 0.003,
      "step": 4872
    },
    {
      "epoch": 77.97,
      "grad_norm": 0.0035645372699946165,
      "learning_rate": 1.6565901476707224e-08,
      "loss": 0.0001,
      "step": 4873
    },
    {
      "epoch": 77.98,
      "grad_norm": 0.003317072754725814,
      "learning_rate": 1.630618955992702e-08,
      "loss": 0.0001,
      "step": 4874
    },
    {
      "epoch": 78.0,
      "grad_norm": 0.20276644825935364,
      "learning_rate": 1.6048526251421502e-08,
      "loss": 0.001,
      "step": 4875
    },
    {
      "epoch": 78.02,
      "grad_norm": 0.4664107859134674,
      "learning_rate": 1.5792911657107057e-08,
      "loss": 0.0047,
      "step": 4876
    },
    {
      "epoch": 78.03,
      "grad_norm": 0.7966952919960022,
      "learning_rate": 1.55393458820563e-08,
      "loss": 0.0085,
      "step": 4877
    },
    {
      "epoch": 78.05,
      "grad_norm": 0.8075676560401917,
      "learning_rate": 1.5287829030500855e-08,
      "loss": 0.0062,
      "step": 4878
    },
    {
      "epoch": 78.06,
      "grad_norm": 0.2708837389945984,
      "learning_rate": 1.503836120583024e-08,
      "loss": 0.0016,
      "step": 4879
    },
    {
      "epoch": 78.08,
      "grad_norm": 0.3389376997947693,
      "learning_rate": 1.4790942510590767e-08,
      "loss": 0.002,
      "step": 4880
    },
    {
      "epoch": 78.1,
      "grad_norm": 0.0041770897805690765,
      "learning_rate": 1.4545573046486627e-08,
      "loss": 0.0001,
      "step": 4881
    },
    {
      "epoch": 78.11,
      "grad_norm": 0.6768640279769897,
      "learning_rate": 1.4302252914380476e-08,
      "loss": 0.0037,
      "step": 4882
    },
    {
      "epoch": 78.13,
      "grad_norm": 0.49333688616752625,
      "learning_rate": 1.4060982214292306e-08,
      "loss": 0.0028,
      "step": 4883
    },
    {
      "epoch": 78.14,
      "grad_norm": 0.00361681473441422,
      "learning_rate": 1.382176104539834e-08,
      "loss": 0.0001,
      "step": 4884
    },
    {
      "epoch": 78.16,
      "grad_norm": 0.003587264334782958,
      "learning_rate": 1.3584589506034362e-08,
      "loss": 0.0001,
      "step": 4885
    },
    {
      "epoch": 78.18,
      "grad_norm": 0.004164732992649078,
      "learning_rate": 1.3349467693692387e-08,
      "loss": 0.0001,
      "step": 4886
    },
    {
      "epoch": 78.19,
      "grad_norm": 0.7594650387763977,
      "learning_rate": 1.3116395705021767e-08,
      "loss": 0.0096,
      "step": 4887
    },
    {
      "epoch": 78.21,
      "grad_norm": 0.857243001461029,
      "learning_rate": 1.2885373635829756e-08,
      "loss": 0.0049,
      "step": 4888
    },
    {
      "epoch": 78.22,
      "grad_norm": 0.0037941928021609783,
      "learning_rate": 1.2656401581080947e-08,
      "loss": 0.0001,
      "step": 4889
    },
    {
      "epoch": 78.24,
      "grad_norm": 0.004275663290172815,
      "learning_rate": 1.2429479634897268e-08,
      "loss": 0.0001,
      "step": 4890
    },
    {
      "epoch": 78.26,
      "grad_norm": 0.33483076095581055,
      "learning_rate": 1.220460789055744e-08,
      "loss": 0.0025,
      "step": 4891
    },
    {
      "epoch": 78.27,
      "grad_norm": 0.003989220131188631,
      "learning_rate": 1.1981786440497523e-08,
      "loss": 0.0001,
      "step": 4892
    },
    {
      "epoch": 78.29,
      "grad_norm": 0.48684126138687134,
      "learning_rate": 1.1761015376310914e-08,
      "loss": 0.0034,
      "step": 4893
    },
    {
      "epoch": 78.3,
      "grad_norm": 1.0153837203979492,
      "learning_rate": 1.1542294788749463e-08,
      "loss": 0.0145,
      "step": 4894
    },
    {
      "epoch": 78.32,
      "grad_norm": 0.5131427049636841,
      "learning_rate": 1.132562476771959e-08,
      "loss": 0.0041,
      "step": 4895
    },
    {
      "epoch": 78.34,
      "grad_norm": 0.656721830368042,
      "learning_rate": 1.1111005402286712e-08,
      "loss": 0.0038,
      "step": 4896
    },
    {
      "epoch": 78.35,
      "grad_norm": 0.004840234760195017,
      "learning_rate": 1.0898436780672483e-08,
      "loss": 0.0002,
      "step": 4897
    },
    {
      "epoch": 78.37,
      "grad_norm": 0.003482908010482788,
      "learning_rate": 1.0687918990256451e-08,
      "loss": 0.0001,
      "step": 4898
    },
    {
      "epoch": 78.38,
      "grad_norm": 0.0025823384057730436,
      "learning_rate": 1.0479452117574396e-08,
      "loss": 0.0001,
      "step": 4899
    },
    {
      "epoch": 78.4,
      "grad_norm": 0.003399639157578349,
      "learning_rate": 1.0273036248318325e-08,
      "loss": 0.0001,
      "step": 4900
    },
    {
      "epoch": 78.42,
      "grad_norm": 0.03896777704358101,
      "learning_rate": 1.0068671467338698e-08,
      "loss": 0.0005,
      "step": 4901
    },
    {
      "epoch": 78.43,
      "grad_norm": 0.0036431557964533567,
      "learning_rate": 9.866357858642206e-09,
      "loss": 0.0001,
      "step": 4902
    },
    {
      "epoch": 78.45,
      "grad_norm": 0.5433841943740845,
      "learning_rate": 9.666095505392325e-09,
      "loss": 0.003,
      "step": 4903
    },
    {
      "epoch": 78.46,
      "grad_norm": 0.004083502572029829,
      "learning_rate": 9.467884489908763e-09,
      "loss": 0.0001,
      "step": 4904
    },
    {
      "epoch": 78.48,
      "grad_norm": 0.0023652324452996254,
      "learning_rate": 9.27172489366912e-09,
      "loss": 0.0,
      "step": 4905
    },
    {
      "epoch": 78.5,
      "grad_norm": 0.0031781308352947235,
      "learning_rate": 9.077616797307232e-09,
      "loss": 0.0001,
      "step": 4906
    },
    {
      "epoch": 78.51,
      "grad_norm": 0.0032296956051141024,
      "learning_rate": 8.88556028061316e-09,
      "loss": 0.0001,
      "step": 4907
    },
    {
      "epoch": 78.53,
      "grad_norm": 0.00249975873157382,
      "learning_rate": 8.695555422534863e-09,
      "loss": 0.0001,
      "step": 4908
    },
    {
      "epoch": 78.54,
      "grad_norm": 0.004042326472699642,
      "learning_rate": 8.507602301175422e-09,
      "loss": 0.0001,
      "step": 4909
    },
    {
      "epoch": 78.56,
      "grad_norm": 0.05259507894515991,
      "learning_rate": 8.321700993795812e-09,
      "loss": 0.0003,
      "step": 4910
    },
    {
      "epoch": 78.58,
      "grad_norm": 0.3699313998222351,
      "learning_rate": 8.137851576812128e-09,
      "loss": 0.0021,
      "step": 4911
    },
    {
      "epoch": 78.59,
      "grad_norm": 1.0926300287246704,
      "learning_rate": 7.956054125798917e-09,
      "loss": 0.0123,
      "step": 4912
    },
    {
      "epoch": 78.61,
      "grad_norm": 0.7743871212005615,
      "learning_rate": 7.776308715485847e-09,
      "loss": 0.0098,
      "step": 4913
    },
    {
      "epoch": 78.62,
      "grad_norm": 0.0035745087079703808,
      "learning_rate": 7.59861541975937e-09,
      "loss": 0.0001,
      "step": 4914
    },
    {
      "epoch": 78.64,
      "grad_norm": 0.004090622998774052,
      "learning_rate": 7.422974311662723e-09,
      "loss": 0.0001,
      "step": 4915
    },
    {
      "epoch": 78.66,
      "grad_norm": 0.003889682935550809,
      "learning_rate": 7.249385463395375e-09,
      "loss": 0.0001,
      "step": 4916
    },
    {
      "epoch": 78.67,
      "grad_norm": 0.004239271394908428,
      "learning_rate": 7.0778489463124714e-09,
      "loss": 0.0001,
      "step": 4917
    },
    {
      "epoch": 78.69,
      "grad_norm": 0.7196879386901855,
      "learning_rate": 6.90836483092705e-09,
      "loss": 0.0089,
      "step": 4918
    },
    {
      "epoch": 78.7,
      "grad_norm": 0.4613583981990814,
      "learning_rate": 6.740933186907828e-09,
      "loss": 0.0027,
      "step": 4919
    },
    {
      "epoch": 78.72,
      "grad_norm": 0.5098345279693604,
      "learning_rate": 6.575554083078084e-09,
      "loss": 0.0035,
      "step": 4920
    },
    {
      "epoch": 78.74,
      "grad_norm": 0.20110124349594116,
      "learning_rate": 6.412227587420661e-09,
      "loss": 0.004,
      "step": 4921
    },
    {
      "epoch": 78.75,
      "grad_norm": 0.5905274748802185,
      "learning_rate": 6.250953767071855e-09,
      "loss": 0.0032,
      "step": 4922
    },
    {
      "epoch": 78.77,
      "grad_norm": 0.002691654721274972,
      "learning_rate": 6.091732688325303e-09,
      "loss": 0.0001,
      "step": 4923
    },
    {
      "epoch": 78.78,
      "grad_norm": 0.9561665654182434,
      "learning_rate": 5.934564416631427e-09,
      "loss": 0.0078,
      "step": 4924
    },
    {
      "epoch": 78.8,
      "grad_norm": 0.0034735752269625664,
      "learning_rate": 5.779449016595773e-09,
      "loss": 0.0001,
      "step": 4925
    },
    {
      "epoch": 78.82,
      "grad_norm": 0.003702752059325576,
      "learning_rate": 5.626386551980112e-09,
      "loss": 0.0001,
      "step": 4926
    },
    {
      "epoch": 78.83,
      "grad_norm": 0.0036797409411519766,
      "learning_rate": 5.475377085703559e-09,
      "loss": 0.0001,
      "step": 4927
    },
    {
      "epoch": 78.85,
      "grad_norm": 0.003998389933258295,
      "learning_rate": 5.3264206798392395e-09,
      "loss": 0.0001,
      "step": 4928
    },
    {
      "epoch": 78.86,
      "grad_norm": 0.04908645898103714,
      "learning_rate": 5.179517395618172e-09,
      "loss": 0.0002,
      "step": 4929
    },
    {
      "epoch": 78.88,
      "grad_norm": 0.9345648288726807,
      "learning_rate": 5.034667293427053e-09,
      "loss": 0.0116,
      "step": 4930
    },
    {
      "epoch": 78.9,
      "grad_norm": 0.003674987005069852,
      "learning_rate": 4.8918704328071446e-09,
      "loss": 0.0001,
      "step": 4931
    },
    {
      "epoch": 78.91,
      "grad_norm": 1.0727773904800415,
      "learning_rate": 4.751126872458156e-09,
      "loss": 0.0202,
      "step": 4932
    },
    {
      "epoch": 78.93,
      "grad_norm": 0.3984590172767639,
      "learning_rate": 4.612436670233811e-09,
      "loss": 0.0022,
      "step": 4933
    },
    {
      "epoch": 78.94,
      "grad_norm": 0.003308393992483616,
      "learning_rate": 4.475799883144061e-09,
      "loss": 0.0001,
      "step": 4934
    },
    {
      "epoch": 78.96,
      "grad_norm": 0.003467314410954714,
      "learning_rate": 4.341216567355644e-09,
      "loss": 0.0001,
      "step": 4935
    },
    {
      "epoch": 78.98,
      "grad_norm": 0.1871277242898941,
      "learning_rate": 4.208686778190974e-09,
      "loss": 0.0009,
      "step": 4936
    },
    {
      "epoch": 78.99,
      "grad_norm": 0.9259210228919983,
      "learning_rate": 4.078210570127028e-09,
      "loss": 0.0157,
      "step": 4937
    },
    {
      "epoch": 79.01,
      "grad_norm": 1.2813670635223389,
      "learning_rate": 3.949787996798682e-09,
      "loss": 0.0228,
      "step": 4938
    },
    {
      "epoch": 79.02,
      "grad_norm": 0.7569195032119751,
      "learning_rate": 3.8234191109948195e-09,
      "loss": 0.0044,
      "step": 4939
    },
    {
      "epoch": 79.04,
      "grad_norm": 0.6770967841148376,
      "learning_rate": 3.6991039646616657e-09,
      "loss": 0.0063,
      "step": 4940
    },
    {
      "epoch": 79.06,
      "grad_norm": 0.004189318977296352,
      "learning_rate": 3.5768426089000107e-09,
      "loss": 0.0001,
      "step": 4941
    },
    {
      "epoch": 79.07,
      "grad_norm": 0.004347600508481264,
      "learning_rate": 3.4566350939668757e-09,
      "loss": 0.0001,
      "step": 4942
    },
    {
      "epoch": 79.09,
      "grad_norm": 0.003002369776368141,
      "learning_rate": 3.3384814692749567e-09,
      "loss": 0.0001,
      "step": 4943
    },
    {
      "epoch": 79.1,
      "grad_norm": 0.6747424602508545,
      "learning_rate": 3.2223817833931803e-09,
      "loss": 0.0049,
      "step": 4944
    },
    {
      "epoch": 79.12,
      "grad_norm": 0.5925779342651367,
      "learning_rate": 3.1083360840455935e-09,
      "loss": 0.0046,
      "step": 4945
    },
    {
      "epoch": 79.14,
      "grad_norm": 0.06214286386966705,
      "learning_rate": 2.9963444181113633e-09,
      "loss": 0.0004,
      "step": 4946
    },
    {
      "epoch": 79.15,
      "grad_norm": 0.37866005301475525,
      "learning_rate": 2.8864068316269976e-09,
      "loss": 0.0023,
      "step": 4947
    },
    {
      "epoch": 79.17,
      "grad_norm": 0.0036321133375167847,
      "learning_rate": 2.7785233697835702e-09,
      "loss": 0.0001,
      "step": 4948
    },
    {
      "epoch": 79.18,
      "grad_norm": 0.3120814561843872,
      "learning_rate": 2.6726940769272736e-09,
      "loss": 0.0018,
      "step": 4949
    },
    {
      "epoch": 79.2,
      "grad_norm": 0.7433876395225525,
      "learning_rate": 2.568918996560532e-09,
      "loss": 0.0091,
      "step": 4950
    },
    {
      "epoch": 79.22,
      "grad_norm": 0.08339840173721313,
      "learning_rate": 2.4671981713420003e-09,
      "loss": 0.0011,
      "step": 4951
    },
    {
      "epoch": 79.23,
      "grad_norm": 0.0038050280418246984,
      "learning_rate": 2.367531643085452e-09,
      "loss": 0.0001,
      "step": 4952
    },
    {
      "epoch": 79.25,
      "grad_norm": 0.25079724192619324,
      "learning_rate": 2.2699194527592282e-09,
      "loss": 0.0015,
      "step": 4953
    },
    {
      "epoch": 79.26,
      "grad_norm": 0.23024927079677582,
      "learning_rate": 2.1743616404878986e-09,
      "loss": 0.0018,
      "step": 4954
    },
    {
      "epoch": 79.28,
      "grad_norm": 0.9655898213386536,
      "learning_rate": 2.0808582455528194e-09,
      "loss": 0.0143,
      "step": 4955
    },
    {
      "epoch": 79.3,
      "grad_norm": 0.0028220643289387226,
      "learning_rate": 1.989409306388801e-09,
      "loss": 0.0001,
      "step": 4956
    },
    {
      "epoch": 79.31,
      "grad_norm": 0.8529778122901917,
      "learning_rate": 1.900014860586885e-09,
      "loss": 0.0077,
      "step": 4957
    },
    {
      "epoch": 79.33,
      "grad_norm": 0.004067177884280682,
      "learning_rate": 1.8126749448943437e-09,
      "loss": 0.0001,
      "step": 4958
    },
    {
      "epoch": 79.34,
      "grad_norm": 0.5796271562576294,
      "learning_rate": 1.7273895952130138e-09,
      "loss": 0.0045,
      "step": 4959
    },
    {
      "epoch": 79.36,
      "grad_norm": 0.003098665503785014,
      "learning_rate": 1.6441588466009627e-09,
      "loss": 0.0001,
      "step": 4960
    },
    {
      "epoch": 79.38,
      "grad_norm": 0.5575268268585205,
      "learning_rate": 1.5629827332702686e-09,
      "loss": 0.0073,
      "step": 4961
    },
    {
      "epoch": 79.39,
      "grad_norm": 0.0037489670794457197,
      "learning_rate": 1.4838612885903492e-09,
      "loss": 0.0001,
      "step": 4962
    },
    {
      "epoch": 79.41,
      "grad_norm": 0.004102275241166353,
      "learning_rate": 1.406794545084078e-09,
      "loss": 0.0001,
      "step": 4963
    },
    {
      "epoch": 79.42,
      "grad_norm": 0.6565587520599365,
      "learning_rate": 1.3317825344316692e-09,
      "loss": 0.0037,
      "step": 4964
    },
    {
      "epoch": 79.44,
      "grad_norm": 0.5085563063621521,
      "learning_rate": 1.2588252874673469e-09,
      "loss": 0.005,
      "step": 4965
    },
    {
      "epoch": 79.46,
      "grad_norm": 0.032212574034929276,
      "learning_rate": 1.187922834180455e-09,
      "loss": 0.0002,
      "step": 4966
    },
    {
      "epoch": 79.47,
      "grad_norm": 0.03527331352233887,
      "learning_rate": 1.1190752037171238e-09,
      "loss": 0.0003,
      "step": 4967
    },
    {
      "epoch": 79.49,
      "grad_norm": 0.0032305882778018713,
      "learning_rate": 1.0522824243774932e-09,
      "loss": 0.0001,
      "step": 4968
    },
    {
      "epoch": 79.5,
      "grad_norm": 0.0031751433853060007,
      "learning_rate": 9.87544523618489e-10,
      "loss": 0.0001,
      "step": 4969
    },
    {
      "epoch": 79.52,
      "grad_norm": 0.8200982213020325,
      "learning_rate": 9.248615280499362e-10,
      "loss": 0.0111,
      "step": 4970
    },
    {
      "epoch": 79.54,
      "grad_norm": 0.005401349626481533,
      "learning_rate": 8.642334634395566e-10,
      "loss": 0.0002,
      "step": 4971
    },
    {
      "epoch": 79.55,
      "grad_norm": 0.3411950469017029,
      "learning_rate": 8.056603547090813e-10,
      "loss": 0.0019,
      "step": 4972
    },
    {
      "epoch": 79.57,
      "grad_norm": 0.004077072720974684,
      "learning_rate": 7.49142225935362e-10,
      "loss": 0.0001,
      "step": 4973
    },
    {
      "epoch": 79.58,
      "grad_norm": 0.9884421229362488,
      "learning_rate": 6.946791003509256e-10,
      "loss": 0.0115,
      "step": 4974
    },
    {
      "epoch": 79.6,
      "grad_norm": 0.0037264067213982344,
      "learning_rate": 6.422710003439747e-10,
      "loss": 0.0001,
      "step": 4975
    },
    {
      "epoch": 79.62,
      "grad_norm": 0.003992852289229631,
      "learning_rate": 5.919179474567216e-10,
      "loss": 0.0001,
      "step": 4976
    },
    {
      "epoch": 79.63,
      "grad_norm": 0.6021273136138916,
      "learning_rate": 5.436199623881644e-10,
      "loss": 0.0075,
      "step": 4977
    },
    {
      "epoch": 79.65,
      "grad_norm": 0.0036099811550229788,
      "learning_rate": 4.973770649918664e-10,
      "loss": 0.0001,
      "step": 4978
    },
    {
      "epoch": 79.66,
      "grad_norm": 0.19789732992649078,
      "learning_rate": 4.5318927427540073e-10,
      "loss": 0.0012,
      "step": 4979
    },
    {
      "epoch": 79.68,
      "grad_norm": 0.59161776304245,
      "learning_rate": 4.1105660840368154e-10,
      "loss": 0.0063,
      "step": 4980
    },
    {
      "epoch": 79.7,
      "grad_norm": 0.6573178172111511,
      "learning_rate": 3.7097908469563295e-10,
      "loss": 0.0038,
      "step": 4981
    },
    {
      "epoch": 79.71,
      "grad_norm": 0.13673041760921478,
      "learning_rate": 3.329567196258543e-10,
      "loss": 0.0008,
      "step": 4982
    },
    {
      "epoch": 79.73,
      "grad_norm": 0.5965244770050049,
      "learning_rate": 2.969895288229552e-10,
      "loss": 0.005,
      "step": 4983
    },
    {
      "epoch": 79.74,
      "grad_norm": 1.1348646879196167,
      "learning_rate": 2.630775270728858e-10,
      "loss": 0.0132,
      "step": 4984
    },
    {
      "epoch": 79.76,
      "grad_norm": 0.5980064272880554,
      "learning_rate": 2.3122072831505137e-10,
      "loss": 0.0061,
      "step": 4985
    },
    {
      "epoch": 79.78,
      "grad_norm": 0.21053238213062286,
      "learning_rate": 2.0141914564453247e-10,
      "loss": 0.0013,
      "step": 4986
    },
    {
      "epoch": 79.79,
      "grad_norm": 0.003682938637211919,
      "learning_rate": 1.7367279131152991e-10,
      "loss": 0.0001,
      "step": 4987
    },
    {
      "epoch": 79.81,
      "grad_norm": 0.42480674386024475,
      "learning_rate": 1.4798167672192e-10,
      "loss": 0.0024,
      "step": 4988
    },
    {
      "epoch": 79.82,
      "grad_norm": 0.0032952537294477224,
      "learning_rate": 1.2434581243614409e-10,
      "loss": 0.0001,
      "step": 4989
    },
    {
      "epoch": 79.84,
      "grad_norm": 0.8581495881080627,
      "learning_rate": 1.0276520816976388e-10,
      "loss": 0.0053,
      "step": 4990
    },
    {
      "epoch": 79.86,
      "grad_norm": 0.002911267802119255,
      "learning_rate": 8.323987279401647e-11,
      "loss": 0.0001,
      "step": 4991
    },
    {
      "epoch": 79.87,
      "grad_norm": 0.005152587313205004,
      "learning_rate": 6.57698143352592e-11,
      "loss": 0.0002,
      "step": 4992
    },
    {
      "epoch": 79.89,
      "grad_norm": 0.002502578077837825,
      "learning_rate": 5.035503997385949e-11,
      "loss": 0.0,
      "step": 4993
    },
    {
      "epoch": 79.9,
      "grad_norm": 0.6408706903457642,
      "learning_rate": 3.699555604752547e-11,
      "loss": 0.0079,
      "step": 4994
    },
    {
      "epoch": 79.92,
      "grad_norm": 0.6316435933113098,
      "learning_rate": 2.5691368046865118e-11,
      "loss": 0.0035,
      "step": 4995
    },
    {
      "epoch": 79.94,
      "grad_norm": 0.8149144649505615,
      "learning_rate": 1.6442480619272007e-11,
      "loss": 0.0045,
      "step": 4996
    },
    {
      "epoch": 79.95,
      "grad_norm": 0.003865065285935998,
      "learning_rate": 9.248897566149773e-12,
      "loss": 0.0001,
      "step": 4997
    },
    {
      "epoch": 79.97,
      "grad_norm": 0.002943474566563964,
      "learning_rate": 4.1106218445774445e-12,
      "loss": 0.0001,
      "step": 4998
    },
    {
      "epoch": 79.98,
      "grad_norm": 0.7096608877182007,
      "learning_rate": 1.0276555667543264e-12,
      "loss": 0.0095,
      "step": 4999
    },
    {
      "epoch": 80.0,
      "grad_norm": 0.2520555853843689,
      "learning_rate": 0.0,
      "loss": 0.0026,
      "step": 5000
    },
    {
      "epoch": 80.0,
      "step": 5000,
      "total_flos": 6.092097129647309e+16,
      "train_loss": 0.04684914803495412,
      "train_runtime": 14324.9889,
      "train_samples_per_second": 22.339,
      "train_steps_per_second": 0.349
    }
  ],
  "logging_steps": 1.0,
  "max_steps": 5000,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 81,
  "save_steps": 500,
  "total_flos": 6.092097129647309e+16,
  "train_batch_size": 2,
  "trial_name": null,
  "trial_params": null
}
