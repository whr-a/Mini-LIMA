{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 207.7922077922078,
  "eval_steps": 500,
  "global_step": 5000,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.04,
      "grad_norm": 7.976983547210693,
      "learning_rate": 1.0000000000000001e-07,
      "loss": 1.3325,
      "step": 1
    },
    {
      "epoch": 0.08,
      "grad_norm": 7.630534648895264,
      "learning_rate": 2.0000000000000002e-07,
      "loss": 1.2647,
      "step": 2
    },
    {
      "epoch": 0.12,
      "grad_norm": 7.509949207305908,
      "learning_rate": 3.0000000000000004e-07,
      "loss": 1.3109,
      "step": 3
    },
    {
      "epoch": 0.17,
      "grad_norm": 7.8908209800720215,
      "learning_rate": 4.0000000000000003e-07,
      "loss": 1.2662,
      "step": 4
    },
    {
      "epoch": 0.21,
      "grad_norm": 8.469590187072754,
      "learning_rate": 5.000000000000001e-07,
      "loss": 1.4372,
      "step": 5
    },
    {
      "epoch": 0.25,
      "grad_norm": 7.731649398803711,
      "learning_rate": 6.000000000000001e-07,
      "loss": 1.2629,
      "step": 6
    },
    {
      "epoch": 0.29,
      "grad_norm": 7.882887363433838,
      "learning_rate": 7.000000000000001e-07,
      "loss": 1.3573,
      "step": 7
    },
    {
      "epoch": 0.33,
      "grad_norm": 12.536077499389648,
      "learning_rate": 8.000000000000001e-07,
      "loss": 1.3215,
      "step": 8
    },
    {
      "epoch": 0.37,
      "grad_norm": 7.198605537414551,
      "learning_rate": 9.000000000000001e-07,
      "loss": 1.3705,
      "step": 9
    },
    {
      "epoch": 0.42,
      "grad_norm": 7.187680244445801,
      "learning_rate": 1.0000000000000002e-06,
      "loss": 1.2813,
      "step": 10
    },
    {
      "epoch": 0.46,
      "grad_norm": 7.163009166717529,
      "learning_rate": 1.1e-06,
      "loss": 1.3788,
      "step": 11
    },
    {
      "epoch": 0.5,
      "grad_norm": 7.913073539733887,
      "learning_rate": 1.2000000000000002e-06,
      "loss": 1.3633,
      "step": 12
    },
    {
      "epoch": 0.54,
      "grad_norm": 7.087693214416504,
      "learning_rate": 1.3e-06,
      "loss": 1.3752,
      "step": 13
    },
    {
      "epoch": 0.58,
      "grad_norm": 7.4086432456970215,
      "learning_rate": 1.4000000000000001e-06,
      "loss": 1.283,
      "step": 14
    },
    {
      "epoch": 0.62,
      "grad_norm": 6.278581619262695,
      "learning_rate": 1.5e-06,
      "loss": 1.2551,
      "step": 15
    },
    {
      "epoch": 0.66,
      "grad_norm": 6.704833507537842,
      "learning_rate": 1.6000000000000001e-06,
      "loss": 1.2561,
      "step": 16
    },
    {
      "epoch": 0.71,
      "grad_norm": 6.659677982330322,
      "learning_rate": 1.7000000000000002e-06,
      "loss": 1.2875,
      "step": 17
    },
    {
      "epoch": 0.75,
      "grad_norm": 8.066048622131348,
      "learning_rate": 1.8000000000000001e-06,
      "loss": 1.2785,
      "step": 18
    },
    {
      "epoch": 0.79,
      "grad_norm": 6.080500602722168,
      "learning_rate": 1.9000000000000002e-06,
      "loss": 1.2837,
      "step": 19
    },
    {
      "epoch": 0.83,
      "grad_norm": 5.599006175994873,
      "learning_rate": 2.0000000000000003e-06,
      "loss": 1.3166,
      "step": 20
    },
    {
      "epoch": 0.87,
      "grad_norm": 4.986660957336426,
      "learning_rate": 2.1000000000000002e-06,
      "loss": 1.2599,
      "step": 21
    },
    {
      "epoch": 0.91,
      "grad_norm": 5.060179233551025,
      "learning_rate": 2.2e-06,
      "loss": 1.1992,
      "step": 22
    },
    {
      "epoch": 0.96,
      "grad_norm": 5.605656623840332,
      "learning_rate": 2.3000000000000004e-06,
      "loss": 1.3444,
      "step": 23
    },
    {
      "epoch": 1.0,
      "grad_norm": 5.2936248779296875,
      "learning_rate": 2.4000000000000003e-06,
      "loss": 1.2928,
      "step": 24
    },
    {
      "epoch": 1.04,
      "grad_norm": 4.9621992111206055,
      "learning_rate": 2.5e-06,
      "loss": 1.2299,
      "step": 25
    },
    {
      "epoch": 1.08,
      "grad_norm": 4.890783309936523,
      "learning_rate": 2.6e-06,
      "loss": 1.3201,
      "step": 26
    },
    {
      "epoch": 1.12,
      "grad_norm": 4.957542419433594,
      "learning_rate": 2.7000000000000004e-06,
      "loss": 1.211,
      "step": 27
    },
    {
      "epoch": 1.16,
      "grad_norm": 4.321884632110596,
      "learning_rate": 2.8000000000000003e-06,
      "loss": 1.2665,
      "step": 28
    },
    {
      "epoch": 1.21,
      "grad_norm": 5.040740013122559,
      "learning_rate": 2.9e-06,
      "loss": 1.1737,
      "step": 29
    },
    {
      "epoch": 1.25,
      "grad_norm": 4.607063293457031,
      "learning_rate": 3e-06,
      "loss": 1.1773,
      "step": 30
    },
    {
      "epoch": 1.29,
      "grad_norm": 5.296383857727051,
      "learning_rate": 3.1000000000000004e-06,
      "loss": 1.2542,
      "step": 31
    },
    {
      "epoch": 1.33,
      "grad_norm": 4.913989543914795,
      "learning_rate": 3.2000000000000003e-06,
      "loss": 1.2279,
      "step": 32
    },
    {
      "epoch": 1.37,
      "grad_norm": 5.084137916564941,
      "learning_rate": 3.3000000000000006e-06,
      "loss": 1.2856,
      "step": 33
    },
    {
      "epoch": 1.41,
      "grad_norm": 5.034333229064941,
      "learning_rate": 3.4000000000000005e-06,
      "loss": 1.2409,
      "step": 34
    },
    {
      "epoch": 1.45,
      "grad_norm": 4.767370700836182,
      "learning_rate": 3.5e-06,
      "loss": 1.2472,
      "step": 35
    },
    {
      "epoch": 1.5,
      "grad_norm": 5.933049201965332,
      "learning_rate": 3.6000000000000003e-06,
      "loss": 1.3788,
      "step": 36
    },
    {
      "epoch": 1.54,
      "grad_norm": 4.675217628479004,
      "learning_rate": 3.7e-06,
      "loss": 1.236,
      "step": 37
    },
    {
      "epoch": 1.58,
      "grad_norm": 8.261747360229492,
      "learning_rate": 3.8000000000000005e-06,
      "loss": 1.1229,
      "step": 38
    },
    {
      "epoch": 1.62,
      "grad_norm": 4.767804145812988,
      "learning_rate": 3.900000000000001e-06,
      "loss": 1.2835,
      "step": 39
    },
    {
      "epoch": 1.66,
      "grad_norm": 4.029632091522217,
      "learning_rate": 4.000000000000001e-06,
      "loss": 1.1565,
      "step": 40
    },
    {
      "epoch": 1.7,
      "grad_norm": 4.144458770751953,
      "learning_rate": 4.1e-06,
      "loss": 1.1896,
      "step": 41
    },
    {
      "epoch": 1.75,
      "grad_norm": 3.9743235111236572,
      "learning_rate": 4.2000000000000004e-06,
      "loss": 1.1859,
      "step": 42
    },
    {
      "epoch": 1.79,
      "grad_norm": 3.707460641860962,
      "learning_rate": 4.3e-06,
      "loss": 1.1259,
      "step": 43
    },
    {
      "epoch": 1.83,
      "grad_norm": 10.135454177856445,
      "learning_rate": 4.4e-06,
      "loss": 1.2023,
      "step": 44
    },
    {
      "epoch": 1.87,
      "grad_norm": 4.40723180770874,
      "learning_rate": 4.5e-06,
      "loss": 1.1816,
      "step": 45
    },
    {
      "epoch": 1.91,
      "grad_norm": 4.803529262542725,
      "learning_rate": 4.600000000000001e-06,
      "loss": 1.0918,
      "step": 46
    },
    {
      "epoch": 1.95,
      "grad_norm": 4.608156204223633,
      "learning_rate": 4.7e-06,
      "loss": 1.1125,
      "step": 47
    },
    {
      "epoch": 1.99,
      "grad_norm": 3.87138032913208,
      "learning_rate": 4.800000000000001e-06,
      "loss": 1.216,
      "step": 48
    },
    {
      "epoch": 2.04,
      "grad_norm": 4.042657852172852,
      "learning_rate": 4.9000000000000005e-06,
      "loss": 1.1521,
      "step": 49
    },
    {
      "epoch": 2.08,
      "grad_norm": 3.9674134254455566,
      "learning_rate": 5e-06,
      "loss": 1.1287,
      "step": 50
    },
    {
      "epoch": 2.12,
      "grad_norm": 3.9138269424438477,
      "learning_rate": 5.1e-06,
      "loss": 1.0664,
      "step": 51
    },
    {
      "epoch": 2.16,
      "grad_norm": 4.079162120819092,
      "learning_rate": 5.2e-06,
      "loss": 1.0904,
      "step": 52
    },
    {
      "epoch": 2.2,
      "grad_norm": 4.368868350982666,
      "learning_rate": 5.300000000000001e-06,
      "loss": 1.0799,
      "step": 53
    },
    {
      "epoch": 2.24,
      "grad_norm": 4.187422752380371,
      "learning_rate": 5.400000000000001e-06,
      "loss": 1.1148,
      "step": 54
    },
    {
      "epoch": 2.29,
      "grad_norm": 3.79020094871521,
      "learning_rate": 5.500000000000001e-06,
      "loss": 1.0452,
      "step": 55
    },
    {
      "epoch": 2.33,
      "grad_norm": 4.234717845916748,
      "learning_rate": 5.600000000000001e-06,
      "loss": 1.1745,
      "step": 56
    },
    {
      "epoch": 2.37,
      "grad_norm": 4.67336368560791,
      "learning_rate": 5.7e-06,
      "loss": 1.0661,
      "step": 57
    },
    {
      "epoch": 2.41,
      "grad_norm": 4.261873245239258,
      "learning_rate": 5.8e-06,
      "loss": 1.03,
      "step": 58
    },
    {
      "epoch": 2.45,
      "grad_norm": 6.037449836730957,
      "learning_rate": 5.9e-06,
      "loss": 1.047,
      "step": 59
    },
    {
      "epoch": 2.49,
      "grad_norm": 3.83086895942688,
      "learning_rate": 6e-06,
      "loss": 1.084,
      "step": 60
    },
    {
      "epoch": 2.54,
      "grad_norm": 3.9883549213409424,
      "learning_rate": 6.1e-06,
      "loss": 0.9706,
      "step": 61
    },
    {
      "epoch": 2.58,
      "grad_norm": 4.2140889167785645,
      "learning_rate": 6.200000000000001e-06,
      "loss": 1.0982,
      "step": 62
    },
    {
      "epoch": 2.62,
      "grad_norm": 3.9160361289978027,
      "learning_rate": 6.300000000000001e-06,
      "loss": 1.0278,
      "step": 63
    },
    {
      "epoch": 2.66,
      "grad_norm": 3.7857980728149414,
      "learning_rate": 6.4000000000000006e-06,
      "loss": 1.0478,
      "step": 64
    },
    {
      "epoch": 2.7,
      "grad_norm": 3.9821958541870117,
      "learning_rate": 6.5000000000000004e-06,
      "loss": 1.1154,
      "step": 65
    },
    {
      "epoch": 2.74,
      "grad_norm": 4.851480007171631,
      "learning_rate": 6.600000000000001e-06,
      "loss": 1.0039,
      "step": 66
    },
    {
      "epoch": 2.78,
      "grad_norm": 4.359899997711182,
      "learning_rate": 6.700000000000001e-06,
      "loss": 1.0365,
      "step": 67
    },
    {
      "epoch": 2.83,
      "grad_norm": 4.274301052093506,
      "learning_rate": 6.800000000000001e-06,
      "loss": 1.0274,
      "step": 68
    },
    {
      "epoch": 2.87,
      "grad_norm": 3.9949681758880615,
      "learning_rate": 6.9e-06,
      "loss": 0.992,
      "step": 69
    },
    {
      "epoch": 2.91,
      "grad_norm": 3.9089505672454834,
      "learning_rate": 7e-06,
      "loss": 0.9754,
      "step": 70
    },
    {
      "epoch": 2.95,
      "grad_norm": 4.277055740356445,
      "learning_rate": 7.100000000000001e-06,
      "loss": 1.0571,
      "step": 71
    },
    {
      "epoch": 2.99,
      "grad_norm": 4.29582405090332,
      "learning_rate": 7.2000000000000005e-06,
      "loss": 1.0775,
      "step": 72
    },
    {
      "epoch": 3.03,
      "grad_norm": 4.422377109527588,
      "learning_rate": 7.3e-06,
      "loss": 1.0247,
      "step": 73
    },
    {
      "epoch": 3.08,
      "grad_norm": 3.7506747245788574,
      "learning_rate": 7.4e-06,
      "loss": 0.8758,
      "step": 74
    },
    {
      "epoch": 3.12,
      "grad_norm": 3.741405487060547,
      "learning_rate": 7.500000000000001e-06,
      "loss": 0.9178,
      "step": 75
    },
    {
      "epoch": 3.16,
      "grad_norm": 4.090646266937256,
      "learning_rate": 7.600000000000001e-06,
      "loss": 0.9159,
      "step": 76
    },
    {
      "epoch": 3.2,
      "grad_norm": 4.086868762969971,
      "learning_rate": 7.7e-06,
      "loss": 0.8723,
      "step": 77
    },
    {
      "epoch": 3.24,
      "grad_norm": 4.291702747344971,
      "learning_rate": 7.800000000000002e-06,
      "loss": 0.9392,
      "step": 78
    },
    {
      "epoch": 3.28,
      "grad_norm": 3.8566629886627197,
      "learning_rate": 7.9e-06,
      "loss": 0.8492,
      "step": 79
    },
    {
      "epoch": 3.32,
      "grad_norm": 3.874673366546631,
      "learning_rate": 8.000000000000001e-06,
      "loss": 0.8626,
      "step": 80
    },
    {
      "epoch": 3.37,
      "grad_norm": 3.892777681350708,
      "learning_rate": 8.1e-06,
      "loss": 0.8767,
      "step": 81
    },
    {
      "epoch": 3.41,
      "grad_norm": 3.794991970062256,
      "learning_rate": 8.2e-06,
      "loss": 0.9131,
      "step": 82
    },
    {
      "epoch": 3.45,
      "grad_norm": 4.050119876861572,
      "learning_rate": 8.3e-06,
      "loss": 0.8103,
      "step": 83
    },
    {
      "epoch": 3.49,
      "grad_norm": 4.527878761291504,
      "learning_rate": 8.400000000000001e-06,
      "loss": 0.89,
      "step": 84
    },
    {
      "epoch": 3.53,
      "grad_norm": 3.6109349727630615,
      "learning_rate": 8.5e-06,
      "loss": 0.8133,
      "step": 85
    },
    {
      "epoch": 3.57,
      "grad_norm": 3.734037399291992,
      "learning_rate": 8.6e-06,
      "loss": 0.8623,
      "step": 86
    },
    {
      "epoch": 3.62,
      "grad_norm": 3.923166036605835,
      "learning_rate": 8.700000000000001e-06,
      "loss": 0.8597,
      "step": 87
    },
    {
      "epoch": 3.66,
      "grad_norm": 6.528100967407227,
      "learning_rate": 8.8e-06,
      "loss": 0.9105,
      "step": 88
    },
    {
      "epoch": 3.7,
      "grad_norm": 3.858189344406128,
      "learning_rate": 8.900000000000001e-06,
      "loss": 0.8481,
      "step": 89
    },
    {
      "epoch": 3.74,
      "grad_norm": 4.7339768409729,
      "learning_rate": 9e-06,
      "loss": 0.8759,
      "step": 90
    },
    {
      "epoch": 3.78,
      "grad_norm": 4.738313674926758,
      "learning_rate": 9.100000000000001e-06,
      "loss": 0.8705,
      "step": 91
    },
    {
      "epoch": 3.82,
      "grad_norm": 4.653385162353516,
      "learning_rate": 9.200000000000002e-06,
      "loss": 0.8608,
      "step": 92
    },
    {
      "epoch": 3.86,
      "grad_norm": 3.8640170097351074,
      "learning_rate": 9.3e-06,
      "loss": 0.9795,
      "step": 93
    },
    {
      "epoch": 3.91,
      "grad_norm": 4.1783013343811035,
      "learning_rate": 9.4e-06,
      "loss": 0.8542,
      "step": 94
    },
    {
      "epoch": 3.95,
      "grad_norm": 3.6374754905700684,
      "learning_rate": 9.5e-06,
      "loss": 0.8036,
      "step": 95
    },
    {
      "epoch": 3.99,
      "grad_norm": 3.790652275085449,
      "learning_rate": 9.600000000000001e-06,
      "loss": 0.8582,
      "step": 96
    },
    {
      "epoch": 4.03,
      "grad_norm": 4.046635627746582,
      "learning_rate": 9.7e-06,
      "loss": 0.776,
      "step": 97
    },
    {
      "epoch": 4.07,
      "grad_norm": 4.2189459800720215,
      "learning_rate": 9.800000000000001e-06,
      "loss": 0.7132,
      "step": 98
    },
    {
      "epoch": 4.11,
      "grad_norm": 3.3982906341552734,
      "learning_rate": 9.9e-06,
      "loss": 0.6417,
      "step": 99
    },
    {
      "epoch": 4.16,
      "grad_norm": 5.56322717666626,
      "learning_rate": 1e-05,
      "loss": 0.7157,
      "step": 100
    },
    {
      "epoch": 4.2,
      "grad_norm": 3.8110711574554443,
      "learning_rate": 9.999998972344433e-06,
      "loss": 0.7263,
      "step": 101
    },
    {
      "epoch": 4.24,
      "grad_norm": 3.8974084854125977,
      "learning_rate": 9.999995889378156e-06,
      "loss": 0.7125,
      "step": 102
    },
    {
      "epoch": 4.28,
      "grad_norm": 4.618264675140381,
      "learning_rate": 9.999990751102436e-06,
      "loss": 0.7342,
      "step": 103
    },
    {
      "epoch": 4.32,
      "grad_norm": 4.243959426879883,
      "learning_rate": 9.999983557519382e-06,
      "loss": 0.6202,
      "step": 104
    },
    {
      "epoch": 4.36,
      "grad_norm": 4.668614387512207,
      "learning_rate": 9.999974308631955e-06,
      "loss": 0.674,
      "step": 105
    },
    {
      "epoch": 4.41,
      "grad_norm": 3.6472809314727783,
      "learning_rate": 9.999963004443953e-06,
      "loss": 0.5984,
      "step": 106
    },
    {
      "epoch": 4.45,
      "grad_norm": 3.561723232269287,
      "learning_rate": 9.999949644960027e-06,
      "loss": 0.7131,
      "step": 107
    },
    {
      "epoch": 4.49,
      "grad_norm": 4.125387191772461,
      "learning_rate": 9.999934230185666e-06,
      "loss": 0.6694,
      "step": 108
    },
    {
      "epoch": 4.53,
      "grad_norm": 4.166313648223877,
      "learning_rate": 9.999916760127207e-06,
      "loss": 0.7566,
      "step": 109
    },
    {
      "epoch": 4.57,
      "grad_norm": 3.9410414695739746,
      "learning_rate": 9.999897234791831e-06,
      "loss": 0.6539,
      "step": 110
    },
    {
      "epoch": 4.61,
      "grad_norm": 4.067495822906494,
      "learning_rate": 9.999875654187566e-06,
      "loss": 0.6805,
      "step": 111
    },
    {
      "epoch": 4.65,
      "grad_norm": 4.110783100128174,
      "learning_rate": 9.99985201832328e-06,
      "loss": 0.727,
      "step": 112
    },
    {
      "epoch": 4.7,
      "grad_norm": 4.362107276916504,
      "learning_rate": 9.99982632720869e-06,
      "loss": 0.6942,
      "step": 113
    },
    {
      "epoch": 4.74,
      "grad_norm": 3.593970775604248,
      "learning_rate": 9.999798580854356e-06,
      "loss": 0.6617,
      "step": 114
    },
    {
      "epoch": 4.78,
      "grad_norm": 5.141175270080566,
      "learning_rate": 9.999768779271687e-06,
      "loss": 0.6824,
      "step": 115
    },
    {
      "epoch": 4.82,
      "grad_norm": 4.634884357452393,
      "learning_rate": 9.999736922472927e-06,
      "loss": 0.6385,
      "step": 116
    },
    {
      "epoch": 4.86,
      "grad_norm": 4.224024295806885,
      "learning_rate": 9.999703010471177e-06,
      "loss": 0.6976,
      "step": 117
    },
    {
      "epoch": 4.9,
      "grad_norm": 4.071551322937012,
      "learning_rate": 9.999667043280375e-06,
      "loss": 0.6837,
      "step": 118
    },
    {
      "epoch": 4.95,
      "grad_norm": 4.581287384033203,
      "learning_rate": 9.999629020915304e-06,
      "loss": 0.6764,
      "step": 119
    },
    {
      "epoch": 4.99,
      "grad_norm": 4.715158462524414,
      "learning_rate": 9.999588943391597e-06,
      "loss": 0.6321,
      "step": 120
    },
    {
      "epoch": 5.03,
      "grad_norm": 5.596242427825928,
      "learning_rate": 9.999546810725726e-06,
      "loss": 0.5769,
      "step": 121
    },
    {
      "epoch": 5.07,
      "grad_norm": 4.341891765594482,
      "learning_rate": 9.99950262293501e-06,
      "loss": 0.5653,
      "step": 122
    },
    {
      "epoch": 5.11,
      "grad_norm": 3.8485851287841797,
      "learning_rate": 9.999456380037613e-06,
      "loss": 0.5382,
      "step": 123
    },
    {
      "epoch": 5.15,
      "grad_norm": 3.6263608932495117,
      "learning_rate": 9.999408082052544e-06,
      "loss": 0.5419,
      "step": 124
    },
    {
      "epoch": 5.19,
      "grad_norm": 5.573211193084717,
      "learning_rate": 9.999357728999657e-06,
      "loss": 0.5323,
      "step": 125
    },
    {
      "epoch": 5.24,
      "grad_norm": 4.720517635345459,
      "learning_rate": 9.99930532089965e-06,
      "loss": 0.4765,
      "step": 126
    },
    {
      "epoch": 5.28,
      "grad_norm": 4.526909828186035,
      "learning_rate": 9.999250857774066e-06,
      "loss": 0.5651,
      "step": 127
    },
    {
      "epoch": 5.32,
      "grad_norm": 4.355658054351807,
      "learning_rate": 9.999194339645292e-06,
      "loss": 0.6188,
      "step": 128
    },
    {
      "epoch": 5.36,
      "grad_norm": 3.7777304649353027,
      "learning_rate": 9.999135766536562e-06,
      "loss": 0.4845,
      "step": 129
    },
    {
      "epoch": 5.4,
      "grad_norm": 4.001148223876953,
      "learning_rate": 9.99907513847195e-06,
      "loss": 0.5388,
      "step": 130
    },
    {
      "epoch": 5.44,
      "grad_norm": 3.507422685623169,
      "learning_rate": 9.999012455476382e-06,
      "loss": 0.5153,
      "step": 131
    },
    {
      "epoch": 5.49,
      "grad_norm": 3.9602410793304443,
      "learning_rate": 9.998947717575624e-06,
      "loss": 0.4784,
      "step": 132
    },
    {
      "epoch": 5.53,
      "grad_norm": 3.823359251022339,
      "learning_rate": 9.998880924796283e-06,
      "loss": 0.5485,
      "step": 133
    },
    {
      "epoch": 5.57,
      "grad_norm": 4.566432952880859,
      "learning_rate": 9.99881207716582e-06,
      "loss": 0.505,
      "step": 134
    },
    {
      "epoch": 5.61,
      "grad_norm": 4.05863618850708,
      "learning_rate": 9.998741174712534e-06,
      "loss": 0.506,
      "step": 135
    },
    {
      "epoch": 5.65,
      "grad_norm": 4.0782036781311035,
      "learning_rate": 9.998668217465569e-06,
      "loss": 0.5465,
      "step": 136
    },
    {
      "epoch": 5.69,
      "grad_norm": 4.1482625007629395,
      "learning_rate": 9.998593205454916e-06,
      "loss": 0.4718,
      "step": 137
    },
    {
      "epoch": 5.74,
      "grad_norm": 4.254421710968018,
      "learning_rate": 9.998516138711411e-06,
      "loss": 0.5526,
      "step": 138
    },
    {
      "epoch": 5.78,
      "grad_norm": 3.831739902496338,
      "learning_rate": 9.998437017266731e-06,
      "loss": 0.5147,
      "step": 139
    },
    {
      "epoch": 5.82,
      "grad_norm": 4.051061153411865,
      "learning_rate": 9.9983558411534e-06,
      "loss": 0.5624,
      "step": 140
    },
    {
      "epoch": 5.86,
      "grad_norm": 4.0381269454956055,
      "learning_rate": 9.998272610404789e-06,
      "loss": 0.5352,
      "step": 141
    },
    {
      "epoch": 5.9,
      "grad_norm": 4.039897918701172,
      "learning_rate": 9.998187325055107e-06,
      "loss": 0.4817,
      "step": 142
    },
    {
      "epoch": 5.94,
      "grad_norm": 3.8196611404418945,
      "learning_rate": 9.998099985139413e-06,
      "loss": 0.4671,
      "step": 143
    },
    {
      "epoch": 5.98,
      "grad_norm": 3.638505697250366,
      "learning_rate": 9.998010590693612e-06,
      "loss": 0.4416,
      "step": 144
    },
    {
      "epoch": 6.03,
      "grad_norm": 4.030430793762207,
      "learning_rate": 9.997919141754448e-06,
      "loss": 0.4781,
      "step": 145
    },
    {
      "epoch": 6.07,
      "grad_norm": 4.2242255210876465,
      "learning_rate": 9.997825638359512e-06,
      "loss": 0.4122,
      "step": 146
    },
    {
      "epoch": 6.11,
      "grad_norm": 3.489896535873413,
      "learning_rate": 9.99773008054724e-06,
      "loss": 0.4029,
      "step": 147
    },
    {
      "epoch": 6.15,
      "grad_norm": 3.646599769592285,
      "learning_rate": 9.997632468356915e-06,
      "loss": 0.336,
      "step": 148
    },
    {
      "epoch": 6.19,
      "grad_norm": 3.9143521785736084,
      "learning_rate": 9.997532801828659e-06,
      "loss": 0.4147,
      "step": 149
    },
    {
      "epoch": 6.23,
      "grad_norm": 5.0328497886657715,
      "learning_rate": 9.99743108100344e-06,
      "loss": 0.3707,
      "step": 150
    },
    {
      "epoch": 6.28,
      "grad_norm": 4.466871738433838,
      "learning_rate": 9.997327305923073e-06,
      "loss": 0.3628,
      "step": 151
    },
    {
      "epoch": 6.32,
      "grad_norm": 4.221166133880615,
      "learning_rate": 9.997221476630217e-06,
      "loss": 0.3869,
      "step": 152
    },
    {
      "epoch": 6.36,
      "grad_norm": 3.954454183578491,
      "learning_rate": 9.997113593168374e-06,
      "loss": 0.4064,
      "step": 153
    },
    {
      "epoch": 6.4,
      "grad_norm": 3.6858954429626465,
      "learning_rate": 9.99700365558189e-06,
      "loss": 0.3544,
      "step": 154
    },
    {
      "epoch": 6.44,
      "grad_norm": 3.7346181869506836,
      "learning_rate": 9.996891663915955e-06,
      "loss": 0.3627,
      "step": 155
    },
    {
      "epoch": 6.48,
      "grad_norm": 4.077933311462402,
      "learning_rate": 9.996777618216608e-06,
      "loss": 0.3989,
      "step": 156
    },
    {
      "epoch": 6.52,
      "grad_norm": 3.675703525543213,
      "learning_rate": 9.996661518530726e-06,
      "loss": 0.3909,
      "step": 157
    },
    {
      "epoch": 6.57,
      "grad_norm": 3.803162097930908,
      "learning_rate": 9.996543364906033e-06,
      "loss": 0.3654,
      "step": 158
    },
    {
      "epoch": 6.61,
      "grad_norm": 3.882801055908203,
      "learning_rate": 9.996423157391101e-06,
      "loss": 0.3661,
      "step": 159
    },
    {
      "epoch": 6.65,
      "grad_norm": 3.591606378555298,
      "learning_rate": 9.99630089603534e-06,
      "loss": 0.3262,
      "step": 160
    },
    {
      "epoch": 6.69,
      "grad_norm": 4.3036580085754395,
      "learning_rate": 9.996176580889005e-06,
      "loss": 0.3668,
      "step": 161
    },
    {
      "epoch": 6.73,
      "grad_norm": 3.8614656925201416,
      "learning_rate": 9.996050212003203e-06,
      "loss": 0.3646,
      "step": 162
    },
    {
      "epoch": 6.77,
      "grad_norm": 4.0132527351379395,
      "learning_rate": 9.995921789429875e-06,
      "loss": 0.421,
      "step": 163
    },
    {
      "epoch": 6.82,
      "grad_norm": 3.9246714115142822,
      "learning_rate": 9.99579131322181e-06,
      "loss": 0.3721,
      "step": 164
    },
    {
      "epoch": 6.86,
      "grad_norm": 3.540142774581909,
      "learning_rate": 9.995658783432645e-06,
      "loss": 0.3716,
      "step": 165
    },
    {
      "epoch": 6.9,
      "grad_norm": 3.820617198944092,
      "learning_rate": 9.995524200116857e-06,
      "loss": 0.3496,
      "step": 166
    },
    {
      "epoch": 6.94,
      "grad_norm": 21.226318359375,
      "learning_rate": 9.995387563329768e-06,
      "loss": 0.3706,
      "step": 167
    },
    {
      "epoch": 6.98,
      "grad_norm": 3.5028178691864014,
      "learning_rate": 9.995248873127544e-06,
      "loss": 0.3645,
      "step": 168
    },
    {
      "epoch": 7.02,
      "grad_norm": 4.035603046417236,
      "learning_rate": 9.995108129567193e-06,
      "loss": 0.3272,
      "step": 169
    },
    {
      "epoch": 7.06,
      "grad_norm": 3.5734570026397705,
      "learning_rate": 9.994965332706574e-06,
      "loss": 0.2583,
      "step": 170
    },
    {
      "epoch": 7.11,
      "grad_norm": 3.6558051109313965,
      "learning_rate": 9.994820482604383e-06,
      "loss": 0.2875,
      "step": 171
    },
    {
      "epoch": 7.15,
      "grad_norm": 3.7039613723754883,
      "learning_rate": 9.994673579320162e-06,
      "loss": 0.2738,
      "step": 172
    },
    {
      "epoch": 7.19,
      "grad_norm": 3.3993749618530273,
      "learning_rate": 9.994524622914297e-06,
      "loss": 0.272,
      "step": 173
    },
    {
      "epoch": 7.23,
      "grad_norm": 4.954463481903076,
      "learning_rate": 9.994373613448021e-06,
      "loss": 0.2609,
      "step": 174
    },
    {
      "epoch": 7.27,
      "grad_norm": 5.373043537139893,
      "learning_rate": 9.994220550983404e-06,
      "loss": 0.2835,
      "step": 175
    },
    {
      "epoch": 7.31,
      "grad_norm": 4.886481761932373,
      "learning_rate": 9.994065435583368e-06,
      "loss": 0.279,
      "step": 176
    },
    {
      "epoch": 7.36,
      "grad_norm": 3.3887505531311035,
      "learning_rate": 9.993908267311677e-06,
      "loss": 0.2599,
      "step": 177
    },
    {
      "epoch": 7.4,
      "grad_norm": 3.312431812286377,
      "learning_rate": 9.99374904623293e-06,
      "loss": 0.2707,
      "step": 178
    },
    {
      "epoch": 7.44,
      "grad_norm": 3.446387529373169,
      "learning_rate": 9.99358777241258e-06,
      "loss": 0.2868,
      "step": 179
    },
    {
      "epoch": 7.48,
      "grad_norm": 3.5979671478271484,
      "learning_rate": 9.993424445916923e-06,
      "loss": 0.284,
      "step": 180
    },
    {
      "epoch": 7.52,
      "grad_norm": 3.223734140396118,
      "learning_rate": 9.993259066813094e-06,
      "loss": 0.2273,
      "step": 181
    },
    {
      "epoch": 7.56,
      "grad_norm": 3.736030340194702,
      "learning_rate": 9.993091635169075e-06,
      "loss": 0.2752,
      "step": 182
    },
    {
      "epoch": 7.61,
      "grad_norm": 3.9598145484924316,
      "learning_rate": 9.992922151053688e-06,
      "loss": 0.317,
      "step": 183
    },
    {
      "epoch": 7.65,
      "grad_norm": 3.420762062072754,
      "learning_rate": 9.992750614536606e-06,
      "loss": 0.25,
      "step": 184
    },
    {
      "epoch": 7.69,
      "grad_norm": 3.4905152320861816,
      "learning_rate": 9.992577025688338e-06,
      "loss": 0.2397,
      "step": 185
    },
    {
      "epoch": 7.73,
      "grad_norm": 3.4440805912017822,
      "learning_rate": 9.992401384580241e-06,
      "loss": 0.2174,
      "step": 186
    },
    {
      "epoch": 7.77,
      "grad_norm": 7.119194507598877,
      "learning_rate": 9.992223691284515e-06,
      "loss": 0.2873,
      "step": 187
    },
    {
      "epoch": 7.81,
      "grad_norm": 3.713858127593994,
      "learning_rate": 9.9920439458742e-06,
      "loss": 0.2556,
      "step": 188
    },
    {
      "epoch": 7.85,
      "grad_norm": 3.893540382385254,
      "learning_rate": 9.991862148423187e-06,
      "loss": 0.2504,
      "step": 189
    },
    {
      "epoch": 7.9,
      "grad_norm": 3.595919609069824,
      "learning_rate": 9.991678299006206e-06,
      "loss": 0.2598,
      "step": 190
    },
    {
      "epoch": 7.94,
      "grad_norm": 3.832662582397461,
      "learning_rate": 9.991492397698825e-06,
      "loss": 0.2588,
      "step": 191
    },
    {
      "epoch": 7.98,
      "grad_norm": 3.5387611389160156,
      "learning_rate": 9.991304444577465e-06,
      "loss": 0.3085,
      "step": 192
    },
    {
      "epoch": 8.02,
      "grad_norm": 3.238077163696289,
      "learning_rate": 9.991114439719387e-06,
      "loss": 0.1914,
      "step": 193
    },
    {
      "epoch": 8.06,
      "grad_norm": 6.086616516113281,
      "learning_rate": 9.990922383202694e-06,
      "loss": 0.2064,
      "step": 194
    },
    {
      "epoch": 8.1,
      "grad_norm": 3.3369805812835693,
      "learning_rate": 9.990728275106332e-06,
      "loss": 0.2234,
      "step": 195
    },
    {
      "epoch": 8.15,
      "grad_norm": 3.3056623935699463,
      "learning_rate": 9.990532115510093e-06,
      "loss": 0.1796,
      "step": 196
    },
    {
      "epoch": 8.19,
      "grad_norm": 3.1350536346435547,
      "learning_rate": 9.990333904494609e-06,
      "loss": 0.1596,
      "step": 197
    },
    {
      "epoch": 8.23,
      "grad_norm": 3.0660245418548584,
      "learning_rate": 9.990133642141359e-06,
      "loss": 0.186,
      "step": 198
    },
    {
      "epoch": 8.27,
      "grad_norm": 3.883399248123169,
      "learning_rate": 9.989931328532663e-06,
      "loss": 0.2112,
      "step": 199
    },
    {
      "epoch": 8.31,
      "grad_norm": 4.4430108070373535,
      "learning_rate": 9.989726963751683e-06,
      "loss": 0.1925,
      "step": 200
    },
    {
      "epoch": 8.35,
      "grad_norm": 4.06942892074585,
      "learning_rate": 9.989520547882426e-06,
      "loss": 0.2118,
      "step": 201
    },
    {
      "epoch": 8.39,
      "grad_norm": 3.790048599243164,
      "learning_rate": 9.989312081009744e-06,
      "loss": 0.1632,
      "step": 202
    },
    {
      "epoch": 8.44,
      "grad_norm": 2.9387261867523193,
      "learning_rate": 9.989101563219327e-06,
      "loss": 0.2036,
      "step": 203
    },
    {
      "epoch": 8.48,
      "grad_norm": 2.629387140274048,
      "learning_rate": 9.988888994597714e-06,
      "loss": 0.1554,
      "step": 204
    },
    {
      "epoch": 8.52,
      "grad_norm": 3.176211357116699,
      "learning_rate": 9.98867437523228e-06,
      "loss": 0.1727,
      "step": 205
    },
    {
      "epoch": 8.56,
      "grad_norm": 3.302748203277588,
      "learning_rate": 9.988457705211251e-06,
      "loss": 0.1673,
      "step": 206
    },
    {
      "epoch": 8.6,
      "grad_norm": 3.182809829711914,
      "learning_rate": 9.988238984623691e-06,
      "loss": 0.1839,
      "step": 207
    },
    {
      "epoch": 8.64,
      "grad_norm": 4.568685531616211,
      "learning_rate": 9.988018213559504e-06,
      "loss": 0.1719,
      "step": 208
    },
    {
      "epoch": 8.69,
      "grad_norm": 3.179128646850586,
      "learning_rate": 9.987795392109443e-06,
      "loss": 0.1963,
      "step": 209
    },
    {
      "epoch": 8.73,
      "grad_norm": 3.155284881591797,
      "learning_rate": 9.987570520365105e-06,
      "loss": 0.172,
      "step": 210
    },
    {
      "epoch": 8.77,
      "grad_norm": 3.7964673042297363,
      "learning_rate": 9.987343598418919e-06,
      "loss": 0.1957,
      "step": 211
    },
    {
      "epoch": 8.81,
      "grad_norm": 3.3540937900543213,
      "learning_rate": 9.987114626364172e-06,
      "loss": 0.1689,
      "step": 212
    },
    {
      "epoch": 8.85,
      "grad_norm": 3.7508275508880615,
      "learning_rate": 9.98688360429498e-06,
      "loss": 0.2201,
      "step": 213
    },
    {
      "epoch": 8.89,
      "grad_norm": 3.6630187034606934,
      "learning_rate": 9.986650532306308e-06,
      "loss": 0.21,
      "step": 214
    },
    {
      "epoch": 8.94,
      "grad_norm": 3.262923240661621,
      "learning_rate": 9.986415410493966e-06,
      "loss": 0.1651,
      "step": 215
    },
    {
      "epoch": 8.98,
      "grad_norm": 3.321244239807129,
      "learning_rate": 9.986178238954602e-06,
      "loss": 0.1693,
      "step": 216
    },
    {
      "epoch": 9.02,
      "grad_norm": 3.4190926551818848,
      "learning_rate": 9.985939017785709e-06,
      "loss": 0.1669,
      "step": 217
    },
    {
      "epoch": 9.06,
      "grad_norm": 2.6190009117126465,
      "learning_rate": 9.985697747085621e-06,
      "loss": 0.1405,
      "step": 218
    },
    {
      "epoch": 9.1,
      "grad_norm": 3.7396152019500732,
      "learning_rate": 9.985454426953513e-06,
      "loss": 0.12,
      "step": 219
    },
    {
      "epoch": 9.14,
      "grad_norm": 3.6806159019470215,
      "learning_rate": 9.98520905748941e-06,
      "loss": 0.1497,
      "step": 220
    },
    {
      "epoch": 9.18,
      "grad_norm": 2.8771486282348633,
      "learning_rate": 9.984961638794171e-06,
      "loss": 0.1147,
      "step": 221
    },
    {
      "epoch": 9.23,
      "grad_norm": 2.809965133666992,
      "learning_rate": 9.9847121709695e-06,
      "loss": 0.1077,
      "step": 222
    },
    {
      "epoch": 9.27,
      "grad_norm": 2.9141056537628174,
      "learning_rate": 9.984460654117944e-06,
      "loss": 0.1227,
      "step": 223
    },
    {
      "epoch": 9.31,
      "grad_norm": 2.845125675201416,
      "learning_rate": 9.984207088342895e-06,
      "loss": 0.1358,
      "step": 224
    },
    {
      "epoch": 9.35,
      "grad_norm": 2.5696046352386475,
      "learning_rate": 9.983951473748579e-06,
      "loss": 0.1023,
      "step": 225
    },
    {
      "epoch": 9.39,
      "grad_norm": 2.959871768951416,
      "learning_rate": 9.983693810440073e-06,
      "loss": 0.1324,
      "step": 226
    },
    {
      "epoch": 9.43,
      "grad_norm": 2.988957643508911,
      "learning_rate": 9.983434098523294e-06,
      "loss": 0.1349,
      "step": 227
    },
    {
      "epoch": 9.48,
      "grad_norm": 3.208979368209839,
      "learning_rate": 9.983172338104996e-06,
      "loss": 0.1438,
      "step": 228
    },
    {
      "epoch": 9.52,
      "grad_norm": 6.370164394378662,
      "learning_rate": 9.982908529292782e-06,
      "loss": 0.1312,
      "step": 229
    },
    {
      "epoch": 9.56,
      "grad_norm": 3.625462532043457,
      "learning_rate": 9.982642672195093e-06,
      "loss": 0.1408,
      "step": 230
    },
    {
      "epoch": 9.6,
      "grad_norm": 3.7386417388916016,
      "learning_rate": 9.98237476692121e-06,
      "loss": 0.118,
      "step": 231
    },
    {
      "epoch": 9.64,
      "grad_norm": 2.629812717437744,
      "learning_rate": 9.982104813581263e-06,
      "loss": 0.1342,
      "step": 232
    },
    {
      "epoch": 9.68,
      "grad_norm": 3.517119884490967,
      "learning_rate": 9.981832812286217e-06,
      "loss": 0.129,
      "step": 233
    },
    {
      "epoch": 9.72,
      "grad_norm": 2.6192004680633545,
      "learning_rate": 9.98155876314788e-06,
      "loss": 0.1035,
      "step": 234
    },
    {
      "epoch": 9.77,
      "grad_norm": 2.9066810607910156,
      "learning_rate": 9.98128266627891e-06,
      "loss": 0.1327,
      "step": 235
    },
    {
      "epoch": 9.81,
      "grad_norm": 3.357680320739746,
      "learning_rate": 9.981004521792793e-06,
      "loss": 0.1236,
      "step": 236
    },
    {
      "epoch": 9.85,
      "grad_norm": 3.378086566925049,
      "learning_rate": 9.980724329803865e-06,
      "loss": 0.1372,
      "step": 237
    },
    {
      "epoch": 9.89,
      "grad_norm": 2.9504220485687256,
      "learning_rate": 9.980442090427305e-06,
      "loss": 0.1653,
      "step": 238
    },
    {
      "epoch": 9.93,
      "grad_norm": 2.944674015045166,
      "learning_rate": 9.980157803779127e-06,
      "loss": 0.1238,
      "step": 239
    },
    {
      "epoch": 9.97,
      "grad_norm": 3.8784828186035156,
      "learning_rate": 9.979871469976197e-06,
      "loss": 0.128,
      "step": 240
    },
    {
      "epoch": 10.02,
      "grad_norm": 3.0875425338745117,
      "learning_rate": 9.979583089136208e-06,
      "loss": 0.1129,
      "step": 241
    },
    {
      "epoch": 10.06,
      "grad_norm": 2.6656603813171387,
      "learning_rate": 9.979292661377709e-06,
      "loss": 0.0845,
      "step": 242
    },
    {
      "epoch": 10.1,
      "grad_norm": 2.454578161239624,
      "learning_rate": 9.97900018682008e-06,
      "loss": 0.0999,
      "step": 243
    },
    {
      "epoch": 10.14,
      "grad_norm": 3.9660298824310303,
      "learning_rate": 9.978705665583548e-06,
      "loss": 0.1123,
      "step": 244
    },
    {
      "epoch": 10.18,
      "grad_norm": 3.4985721111297607,
      "learning_rate": 9.978409097789178e-06,
      "loss": 0.0784,
      "step": 245
    },
    {
      "epoch": 10.22,
      "grad_norm": 2.422011613845825,
      "learning_rate": 9.97811048355888e-06,
      "loss": 0.0822,
      "step": 246
    },
    {
      "epoch": 10.26,
      "grad_norm": 3.632354497909546,
      "learning_rate": 9.9778098230154e-06,
      "loss": 0.1044,
      "step": 247
    },
    {
      "epoch": 10.31,
      "grad_norm": 3.2362074851989746,
      "learning_rate": 9.977507116282333e-06,
      "loss": 0.1,
      "step": 248
    },
    {
      "epoch": 10.35,
      "grad_norm": 3.7692065238952637,
      "learning_rate": 9.977202363484105e-06,
      "loss": 0.0941,
      "step": 249
    },
    {
      "epoch": 10.39,
      "grad_norm": 2.6170103549957275,
      "learning_rate": 9.976895564745993e-06,
      "loss": 0.1013,
      "step": 250
    },
    {
      "epoch": 10.43,
      "grad_norm": 2.80134916305542,
      "learning_rate": 9.976586720194106e-06,
      "loss": 0.0759,
      "step": 251
    },
    {
      "epoch": 10.47,
      "grad_norm": 3.135725259780884,
      "learning_rate": 9.9762758299554e-06,
      "loss": 0.0925,
      "step": 252
    },
    {
      "epoch": 10.51,
      "grad_norm": 2.97684383392334,
      "learning_rate": 9.975962894157673e-06,
      "loss": 0.0822,
      "step": 253
    },
    {
      "epoch": 10.56,
      "grad_norm": 2.9515552520751953,
      "learning_rate": 9.975647912929558e-06,
      "loss": 0.0683,
      "step": 254
    },
    {
      "epoch": 10.6,
      "grad_norm": 2.6791799068450928,
      "learning_rate": 9.975330886400531e-06,
      "loss": 0.0886,
      "step": 255
    },
    {
      "epoch": 10.64,
      "grad_norm": 2.740931749343872,
      "learning_rate": 9.975011814700912e-06,
      "loss": 0.094,
      "step": 256
    },
    {
      "epoch": 10.68,
      "grad_norm": 2.580803155899048,
      "learning_rate": 9.97469069796186e-06,
      "loss": 0.0925,
      "step": 257
    },
    {
      "epoch": 10.72,
      "grad_norm": 2.6085026264190674,
      "learning_rate": 9.974367536315372e-06,
      "loss": 0.0983,
      "step": 258
    },
    {
      "epoch": 10.76,
      "grad_norm": 3.399540901184082,
      "learning_rate": 9.974042329894287e-06,
      "loss": 0.0719,
      "step": 259
    },
    {
      "epoch": 10.81,
      "grad_norm": 2.608975410461426,
      "learning_rate": 9.973715078832288e-06,
      "loss": 0.0919,
      "step": 260
    },
    {
      "epoch": 10.85,
      "grad_norm": 2.7483272552490234,
      "learning_rate": 9.973385783263893e-06,
      "loss": 0.1059,
      "step": 261
    },
    {
      "epoch": 10.89,
      "grad_norm": 2.878378391265869,
      "learning_rate": 9.973054443324463e-06,
      "loss": 0.0985,
      "step": 262
    },
    {
      "epoch": 10.93,
      "grad_norm": 3.48641037940979,
      "learning_rate": 9.9727210591502e-06,
      "loss": 0.1461,
      "step": 263
    },
    {
      "epoch": 10.97,
      "grad_norm": 3.519969940185547,
      "learning_rate": 9.972385630878147e-06,
      "loss": 0.0951,
      "step": 264
    },
    {
      "epoch": 11.01,
      "grad_norm": 3.195232391357422,
      "learning_rate": 9.972048158646184e-06,
      "loss": 0.1206,
      "step": 265
    },
    {
      "epoch": 11.05,
      "grad_norm": 2.053880214691162,
      "learning_rate": 9.971708642593033e-06,
      "loss": 0.0592,
      "step": 266
    },
    {
      "epoch": 11.1,
      "grad_norm": 2.338088274002075,
      "learning_rate": 9.971367082858256e-06,
      "loss": 0.0646,
      "step": 267
    },
    {
      "epoch": 11.14,
      "grad_norm": 2.2269415855407715,
      "learning_rate": 9.971023479582258e-06,
      "loss": 0.0656,
      "step": 268
    },
    {
      "epoch": 11.18,
      "grad_norm": 2.2351317405700684,
      "learning_rate": 9.970677832906279e-06,
      "loss": 0.0616,
      "step": 269
    },
    {
      "epoch": 11.22,
      "grad_norm": 2.0274786949157715,
      "learning_rate": 9.970330142972403e-06,
      "loss": 0.1232,
      "step": 270
    },
    {
      "epoch": 11.26,
      "grad_norm": 3.0418176651000977,
      "learning_rate": 9.96998040992355e-06,
      "loss": 0.0932,
      "step": 271
    },
    {
      "epoch": 11.3,
      "grad_norm": 3.3192522525787354,
      "learning_rate": 9.969628633903483e-06,
      "loss": 0.0633,
      "step": 272
    },
    {
      "epoch": 11.35,
      "grad_norm": 3.677824020385742,
      "learning_rate": 9.969274815056801e-06,
      "loss": 0.0564,
      "step": 273
    },
    {
      "epoch": 11.39,
      "grad_norm": 4.771346092224121,
      "learning_rate": 9.968918953528953e-06,
      "loss": 0.0804,
      "step": 274
    },
    {
      "epoch": 11.43,
      "grad_norm": 3.1274516582489014,
      "learning_rate": 9.968561049466214e-06,
      "loss": 0.0782,
      "step": 275
    },
    {
      "epoch": 11.47,
      "grad_norm": 2.848417282104492,
      "learning_rate": 9.968201103015707e-06,
      "loss": 0.0664,
      "step": 276
    },
    {
      "epoch": 11.51,
      "grad_norm": 2.5610640048980713,
      "learning_rate": 9.96783911432539e-06,
      "loss": 0.0669,
      "step": 277
    },
    {
      "epoch": 11.55,
      "grad_norm": 2.176969289779663,
      "learning_rate": 9.967475083544066e-06,
      "loss": 0.0582,
      "step": 278
    },
    {
      "epoch": 11.59,
      "grad_norm": 2.8442158699035645,
      "learning_rate": 9.967109010821374e-06,
      "loss": 0.0703,
      "step": 279
    },
    {
      "epoch": 11.64,
      "grad_norm": 2.3376522064208984,
      "learning_rate": 9.966740896307791e-06,
      "loss": 0.0633,
      "step": 280
    },
    {
      "epoch": 11.68,
      "grad_norm": 2.1747653484344482,
      "learning_rate": 9.966370740154636e-06,
      "loss": 0.0714,
      "step": 281
    },
    {
      "epoch": 11.72,
      "grad_norm": 2.552476406097412,
      "learning_rate": 9.965998542514066e-06,
      "loss": 0.0871,
      "step": 282
    },
    {
      "epoch": 11.76,
      "grad_norm": 3.6925387382507324,
      "learning_rate": 9.965624303539077e-06,
      "loss": 0.0639,
      "step": 283
    },
    {
      "epoch": 11.8,
      "grad_norm": 2.5359058380126953,
      "learning_rate": 9.965248023383505e-06,
      "loss": 0.0704,
      "step": 284
    },
    {
      "epoch": 11.84,
      "grad_norm": 2.305264711380005,
      "learning_rate": 9.964869702202023e-06,
      "loss": 0.0681,
      "step": 285
    },
    {
      "epoch": 11.89,
      "grad_norm": 3.2342545986175537,
      "learning_rate": 9.964489340150147e-06,
      "loss": 0.068,
      "step": 286
    },
    {
      "epoch": 11.93,
      "grad_norm": 2.6825318336486816,
      "learning_rate": 9.964106937384229e-06,
      "loss": 0.0663,
      "step": 287
    },
    {
      "epoch": 11.97,
      "grad_norm": 3.7453219890594482,
      "learning_rate": 9.96372249406146e-06,
      "loss": 0.0744,
      "step": 288
    },
    {
      "epoch": 12.01,
      "grad_norm": 2.9598231315612793,
      "learning_rate": 9.963336010339869e-06,
      "loss": 0.0789,
      "step": 289
    },
    {
      "epoch": 12.05,
      "grad_norm": 1.9720228910446167,
      "learning_rate": 9.962947486378325e-06,
      "loss": 0.0448,
      "step": 290
    },
    {
      "epoch": 12.09,
      "grad_norm": 1.7343083620071411,
      "learning_rate": 9.962556922336538e-06,
      "loss": 0.0453,
      "step": 291
    },
    {
      "epoch": 12.14,
      "grad_norm": 2.821934461593628,
      "learning_rate": 9.962164318375052e-06,
      "loss": 0.0593,
      "step": 292
    },
    {
      "epoch": 12.18,
      "grad_norm": 1.6540932655334473,
      "learning_rate": 9.961769674655251e-06,
      "loss": 0.0429,
      "step": 293
    },
    {
      "epoch": 12.22,
      "grad_norm": 3.311414957046509,
      "learning_rate": 9.96137299133936e-06,
      "loss": 0.0668,
      "step": 294
    },
    {
      "epoch": 12.26,
      "grad_norm": 2.2904014587402344,
      "learning_rate": 9.96097426859044e-06,
      "loss": 0.05,
      "step": 295
    },
    {
      "epoch": 12.3,
      "grad_norm": 3.0871059894561768,
      "learning_rate": 9.960573506572391e-06,
      "loss": 0.0744,
      "step": 296
    },
    {
      "epoch": 12.34,
      "grad_norm": 3.087928056716919,
      "learning_rate": 9.960170705449948e-06,
      "loss": 0.0539,
      "step": 297
    },
    {
      "epoch": 12.38,
      "grad_norm": 2.9847922325134277,
      "learning_rate": 9.959765865388693e-06,
      "loss": 0.0548,
      "step": 298
    },
    {
      "epoch": 12.43,
      "grad_norm": 2.396268367767334,
      "learning_rate": 9.959358986555037e-06,
      "loss": 0.0506,
      "step": 299
    },
    {
      "epoch": 12.47,
      "grad_norm": 2.8721766471862793,
      "learning_rate": 9.95895006911623e-06,
      "loss": 0.0434,
      "step": 300
    },
    {
      "epoch": 12.51,
      "grad_norm": 2.2071475982666016,
      "learning_rate": 9.958539113240368e-06,
      "loss": 0.0515,
      "step": 301
    },
    {
      "epoch": 12.55,
      "grad_norm": 2.7063722610473633,
      "learning_rate": 9.958126119096378e-06,
      "loss": 0.0565,
      "step": 302
    },
    {
      "epoch": 12.59,
      "grad_norm": 2.7612662315368652,
      "learning_rate": 9.957711086854025e-06,
      "loss": 0.0607,
      "step": 303
    },
    {
      "epoch": 12.63,
      "grad_norm": 2.477078676223755,
      "learning_rate": 9.957294016683912e-06,
      "loss": 0.0556,
      "step": 304
    },
    {
      "epoch": 12.68,
      "grad_norm": 2.747528076171875,
      "learning_rate": 9.956874908757482e-06,
      "loss": 0.0512,
      "step": 305
    },
    {
      "epoch": 12.72,
      "grad_norm": 2.171206474304199,
      "learning_rate": 9.956453763247015e-06,
      "loss": 0.0606,
      "step": 306
    },
    {
      "epoch": 12.76,
      "grad_norm": 2.8665783405303955,
      "learning_rate": 9.956030580325628e-06,
      "loss": 0.0605,
      "step": 307
    },
    {
      "epoch": 12.8,
      "grad_norm": 2.1943204402923584,
      "learning_rate": 9.955605360167275e-06,
      "loss": 0.0588,
      "step": 308
    },
    {
      "epoch": 12.84,
      "grad_norm": 2.6338136196136475,
      "learning_rate": 9.955178102946747e-06,
      "loss": 0.0768,
      "step": 309
    },
    {
      "epoch": 12.88,
      "grad_norm": 2.4422836303710938,
      "learning_rate": 9.954748808839675e-06,
      "loss": 0.0583,
      "step": 310
    },
    {
      "epoch": 12.92,
      "grad_norm": 1.84110689163208,
      "learning_rate": 9.954317478022524e-06,
      "loss": 0.0439,
      "step": 311
    },
    {
      "epoch": 12.97,
      "grad_norm": 2.588667631149292,
      "learning_rate": 9.9538841106726e-06,
      "loss": 0.0631,
      "step": 312
    },
    {
      "epoch": 13.01,
      "grad_norm": 2.476720094680786,
      "learning_rate": 9.953448706968043e-06,
      "loss": 0.0537,
      "step": 313
    },
    {
      "epoch": 13.05,
      "grad_norm": 2.1084978580474854,
      "learning_rate": 9.95301126708783e-06,
      "loss": 0.0402,
      "step": 314
    },
    {
      "epoch": 13.09,
      "grad_norm": 1.7394931316375732,
      "learning_rate": 9.952571791211776e-06,
      "loss": 0.04,
      "step": 315
    },
    {
      "epoch": 13.13,
      "grad_norm": 2.2838821411132812,
      "learning_rate": 9.952130279520535e-06,
      "loss": 0.0373,
      "step": 316
    },
    {
      "epoch": 13.17,
      "grad_norm": 1.3656402826309204,
      "learning_rate": 9.951686732195593e-06,
      "loss": 0.037,
      "step": 317
    },
    {
      "epoch": 13.22,
      "grad_norm": 1.8983135223388672,
      "learning_rate": 9.951241149419278e-06,
      "loss": 0.0413,
      "step": 318
    },
    {
      "epoch": 13.26,
      "grad_norm": 1.842639446258545,
      "learning_rate": 9.95079353137475e-06,
      "loss": 0.0316,
      "step": 319
    },
    {
      "epoch": 13.3,
      "grad_norm": 2.4132699966430664,
      "learning_rate": 9.950343878246011e-06,
      "loss": 0.0498,
      "step": 320
    },
    {
      "epoch": 13.34,
      "grad_norm": 2.289524793624878,
      "learning_rate": 9.949892190217892e-06,
      "loss": 0.0413,
      "step": 321
    },
    {
      "epoch": 13.38,
      "grad_norm": 1.4879275560379028,
      "learning_rate": 9.94943846747607e-06,
      "loss": 0.0363,
      "step": 322
    },
    {
      "epoch": 13.42,
      "grad_norm": 1.5249100923538208,
      "learning_rate": 9.94898271020705e-06,
      "loss": 0.0365,
      "step": 323
    },
    {
      "epoch": 13.46,
      "grad_norm": 2.203565835952759,
      "learning_rate": 9.948524918598175e-06,
      "loss": 0.0418,
      "step": 324
    },
    {
      "epoch": 13.51,
      "grad_norm": 1.9235634803771973,
      "learning_rate": 9.948065092837631e-06,
      "loss": 0.0343,
      "step": 325
    },
    {
      "epoch": 13.55,
      "grad_norm": 2.6725640296936035,
      "learning_rate": 9.94760323311443e-06,
      "loss": 0.0369,
      "step": 326
    },
    {
      "epoch": 13.59,
      "grad_norm": 5.080569267272949,
      "learning_rate": 9.947139339618428e-06,
      "loss": 0.0607,
      "step": 327
    },
    {
      "epoch": 13.63,
      "grad_norm": 2.9588263034820557,
      "learning_rate": 9.946673412540313e-06,
      "loss": 0.0643,
      "step": 328
    },
    {
      "epoch": 13.67,
      "grad_norm": 2.848219156265259,
      "learning_rate": 9.946205452071611e-06,
      "loss": 0.0444,
      "step": 329
    },
    {
      "epoch": 13.71,
      "grad_norm": 1.75403892993927,
      "learning_rate": 9.945735458404681e-06,
      "loss": 0.0373,
      "step": 330
    },
    {
      "epoch": 13.76,
      "grad_norm": 2.0020694732666016,
      "learning_rate": 9.945263431732722e-06,
      "loss": 0.0513,
      "step": 331
    },
    {
      "epoch": 13.8,
      "grad_norm": 2.153024196624756,
      "learning_rate": 9.944789372249765e-06,
      "loss": 0.0543,
      "step": 332
    },
    {
      "epoch": 13.84,
      "grad_norm": 2.9010977745056152,
      "learning_rate": 9.944313280150678e-06,
      "loss": 0.0612,
      "step": 333
    },
    {
      "epoch": 13.88,
      "grad_norm": 3.158857822418213,
      "learning_rate": 9.943835155631166e-06,
      "loss": 0.0447,
      "step": 334
    },
    {
      "epoch": 13.92,
      "grad_norm": 3.0015487670898438,
      "learning_rate": 9.943354998887763e-06,
      "loss": 0.0441,
      "step": 335
    },
    {
      "epoch": 13.96,
      "grad_norm": 2.022921085357666,
      "learning_rate": 9.94287281011785e-06,
      "loss": 0.0437,
      "step": 336
    },
    {
      "epoch": 14.01,
      "grad_norm": 2.0472803115844727,
      "learning_rate": 9.942388589519631e-06,
      "loss": 0.0343,
      "step": 337
    },
    {
      "epoch": 14.05,
      "grad_norm": 2.169529914855957,
      "learning_rate": 9.941902337292156e-06,
      "loss": 0.0516,
      "step": 338
    },
    {
      "epoch": 14.09,
      "grad_norm": 2.691234827041626,
      "learning_rate": 9.9414140536353e-06,
      "loss": 0.0388,
      "step": 339
    },
    {
      "epoch": 14.13,
      "grad_norm": 2.912230968475342,
      "learning_rate": 9.94092373874978e-06,
      "loss": 0.0363,
      "step": 340
    },
    {
      "epoch": 14.17,
      "grad_norm": 2.2003185749053955,
      "learning_rate": 9.940431392837147e-06,
      "loss": 0.0312,
      "step": 341
    },
    {
      "epoch": 14.21,
      "grad_norm": 3.038078546524048,
      "learning_rate": 9.939937016099783e-06,
      "loss": 0.0365,
      "step": 342
    },
    {
      "epoch": 14.25,
      "grad_norm": 2.3429527282714844,
      "learning_rate": 9.939440608740912e-06,
      "loss": 0.0343,
      "step": 343
    },
    {
      "epoch": 14.3,
      "grad_norm": 2.8855643272399902,
      "learning_rate": 9.938942170964583e-06,
      "loss": 0.0406,
      "step": 344
    },
    {
      "epoch": 14.34,
      "grad_norm": 1.9712369441986084,
      "learning_rate": 9.938441702975689e-06,
      "loss": 0.0304,
      "step": 345
    },
    {
      "epoch": 14.38,
      "grad_norm": 2.0227742195129395,
      "learning_rate": 9.937939204979952e-06,
      "loss": 0.0411,
      "step": 346
    },
    {
      "epoch": 14.42,
      "grad_norm": 3.2121164798736572,
      "learning_rate": 9.937434677183931e-06,
      "loss": 0.0434,
      "step": 347
    },
    {
      "epoch": 14.46,
      "grad_norm": 3.1735596656799316,
      "learning_rate": 9.936928119795017e-06,
      "loss": 0.0366,
      "step": 348
    },
    {
      "epoch": 14.5,
      "grad_norm": 1.5375618934631348,
      "learning_rate": 9.936419533021437e-06,
      "loss": 0.0303,
      "step": 349
    },
    {
      "epoch": 14.55,
      "grad_norm": 1.6677252054214478,
      "learning_rate": 9.935908917072253e-06,
      "loss": 0.0283,
      "step": 350
    },
    {
      "epoch": 14.59,
      "grad_norm": 1.4863203763961792,
      "learning_rate": 9.935396272157357e-06,
      "loss": 0.0289,
      "step": 351
    },
    {
      "epoch": 14.63,
      "grad_norm": 2.1092820167541504,
      "learning_rate": 9.934881598487478e-06,
      "loss": 0.0317,
      "step": 352
    },
    {
      "epoch": 14.67,
      "grad_norm": 2.7109310626983643,
      "learning_rate": 9.934364896274185e-06,
      "loss": 0.0422,
      "step": 353
    },
    {
      "epoch": 14.71,
      "grad_norm": 2.083139181137085,
      "learning_rate": 9.933846165729867e-06,
      "loss": 0.0357,
      "step": 354
    },
    {
      "epoch": 14.75,
      "grad_norm": 1.6619012355804443,
      "learning_rate": 9.93332540706776e-06,
      "loss": 0.033,
      "step": 355
    },
    {
      "epoch": 14.79,
      "grad_norm": 2.2874574661254883,
      "learning_rate": 9.932802620501925e-06,
      "loss": 0.0393,
      "step": 356
    },
    {
      "epoch": 14.84,
      "grad_norm": 2.759704351425171,
      "learning_rate": 9.93227780624726e-06,
      "loss": 0.0412,
      "step": 357
    },
    {
      "epoch": 14.88,
      "grad_norm": 1.9362393617630005,
      "learning_rate": 9.9317509645195e-06,
      "loss": 0.0359,
      "step": 358
    },
    {
      "epoch": 14.92,
      "grad_norm": 2.4941346645355225,
      "learning_rate": 9.931222095535204e-06,
      "loss": 0.0368,
      "step": 359
    },
    {
      "epoch": 14.96,
      "grad_norm": 2.1386959552764893,
      "learning_rate": 9.930691199511775e-06,
      "loss": 0.0336,
      "step": 360
    },
    {
      "epoch": 15.0,
      "grad_norm": 2.4430716037750244,
      "learning_rate": 9.930158276667443e-06,
      "loss": 0.0339,
      "step": 361
    },
    {
      "epoch": 15.04,
      "grad_norm": 1.415106177330017,
      "learning_rate": 9.92962332722127e-06,
      "loss": 0.0274,
      "step": 362
    },
    {
      "epoch": 15.09,
      "grad_norm": 1.5274604558944702,
      "learning_rate": 9.929086351393158e-06,
      "loss": 0.0243,
      "step": 363
    },
    {
      "epoch": 15.13,
      "grad_norm": 1.7159472703933716,
      "learning_rate": 9.928547349403832e-06,
      "loss": 0.0305,
      "step": 364
    },
    {
      "epoch": 15.17,
      "grad_norm": 2.341430902481079,
      "learning_rate": 9.928006321474859e-06,
      "loss": 0.0371,
      "step": 365
    },
    {
      "epoch": 15.21,
      "grad_norm": 2.506337881088257,
      "learning_rate": 9.927463267828635e-06,
      "loss": 0.0297,
      "step": 366
    },
    {
      "epoch": 15.25,
      "grad_norm": 1.7415536642074585,
      "learning_rate": 9.926918188688387e-06,
      "loss": 0.0277,
      "step": 367
    },
    {
      "epoch": 15.29,
      "grad_norm": 3.091092586517334,
      "learning_rate": 9.926371084278178e-06,
      "loss": 0.0375,
      "step": 368
    },
    {
      "epoch": 15.34,
      "grad_norm": 1.795575499534607,
      "learning_rate": 9.925821954822902e-06,
      "loss": 0.028,
      "step": 369
    },
    {
      "epoch": 15.38,
      "grad_norm": 1.835422158241272,
      "learning_rate": 9.925270800548285e-06,
      "loss": 0.0321,
      "step": 370
    },
    {
      "epoch": 15.42,
      "grad_norm": 1.731566309928894,
      "learning_rate": 9.924717621680885e-06,
      "loss": 0.0375,
      "step": 371
    },
    {
      "epoch": 15.46,
      "grad_norm": 1.8799350261688232,
      "learning_rate": 9.924162418448093e-06,
      "loss": 0.0324,
      "step": 372
    },
    {
      "epoch": 15.5,
      "grad_norm": 2.6991186141967773,
      "learning_rate": 9.923605191078134e-06,
      "loss": 0.0404,
      "step": 373
    },
    {
      "epoch": 15.54,
      "grad_norm": 2.0400726795196533,
      "learning_rate": 9.923045939800063e-06,
      "loss": 0.0283,
      "step": 374
    },
    {
      "epoch": 15.58,
      "grad_norm": 2.7859251499176025,
      "learning_rate": 9.922484664843763e-06,
      "loss": 0.0426,
      "step": 375
    },
    {
      "epoch": 15.63,
      "grad_norm": 1.8309327363967896,
      "learning_rate": 9.921921366439958e-06,
      "loss": 0.0306,
      "step": 376
    },
    {
      "epoch": 15.67,
      "grad_norm": 2.2801671028137207,
      "learning_rate": 9.921356044820194e-06,
      "loss": 0.0358,
      "step": 377
    },
    {
      "epoch": 15.71,
      "grad_norm": 1.5481621026992798,
      "learning_rate": 9.920788700216857e-06,
      "loss": 0.026,
      "step": 378
    },
    {
      "epoch": 15.75,
      "grad_norm": 1.8509008884429932,
      "learning_rate": 9.920219332863161e-06,
      "loss": 0.031,
      "step": 379
    },
    {
      "epoch": 15.79,
      "grad_norm": 2.0844902992248535,
      "learning_rate": 9.91964794299315e-06,
      "loss": 0.0326,
      "step": 380
    },
    {
      "epoch": 15.83,
      "grad_norm": 2.0843353271484375,
      "learning_rate": 9.9190745308417e-06,
      "loss": 0.035,
      "step": 381
    },
    {
      "epoch": 15.88,
      "grad_norm": 2.825051784515381,
      "learning_rate": 9.91849909664452e-06,
      "loss": 0.0412,
      "step": 382
    },
    {
      "epoch": 15.92,
      "grad_norm": 2.053762912750244,
      "learning_rate": 9.91792164063815e-06,
      "loss": 0.035,
      "step": 383
    },
    {
      "epoch": 15.96,
      "grad_norm": 2.6175007820129395,
      "learning_rate": 9.917342163059959e-06,
      "loss": 0.0389,
      "step": 384
    },
    {
      "epoch": 16.0,
      "grad_norm": 1.9400025606155396,
      "learning_rate": 9.916760664148148e-06,
      "loss": 0.0363,
      "step": 385
    },
    {
      "epoch": 16.04,
      "grad_norm": 1.8369193077087402,
      "learning_rate": 9.916177144141752e-06,
      "loss": 0.0268,
      "step": 386
    },
    {
      "epoch": 16.08,
      "grad_norm": 1.6794781684875488,
      "learning_rate": 9.915591603280632e-06,
      "loss": 0.027,
      "step": 387
    },
    {
      "epoch": 16.12,
      "grad_norm": 1.3823609352111816,
      "learning_rate": 9.915004041805482e-06,
      "loss": 0.0266,
      "step": 388
    },
    {
      "epoch": 16.17,
      "grad_norm": 2.6887924671173096,
      "learning_rate": 9.914414459957826e-06,
      "loss": 0.0304,
      "step": 389
    },
    {
      "epoch": 16.21,
      "grad_norm": 1.3351466655731201,
      "learning_rate": 9.91382285798002e-06,
      "loss": 0.0292,
      "step": 390
    },
    {
      "epoch": 16.25,
      "grad_norm": 1.1669325828552246,
      "learning_rate": 9.913229236115248e-06,
      "loss": 0.0201,
      "step": 391
    },
    {
      "epoch": 16.29,
      "grad_norm": 2.03071665763855,
      "learning_rate": 9.912633594607526e-06,
      "loss": 0.0238,
      "step": 392
    },
    {
      "epoch": 16.33,
      "grad_norm": 2.04936146736145,
      "learning_rate": 9.912035933701699e-06,
      "loss": 0.027,
      "step": 393
    },
    {
      "epoch": 16.37,
      "grad_norm": 2.159613847732544,
      "learning_rate": 9.911436253643445e-06,
      "loss": 0.0303,
      "step": 394
    },
    {
      "epoch": 16.42,
      "grad_norm": 1.9379570484161377,
      "learning_rate": 9.910834554679266e-06,
      "loss": 0.0249,
      "step": 395
    },
    {
      "epoch": 16.46,
      "grad_norm": 1.3244850635528564,
      "learning_rate": 9.910230837056501e-06,
      "loss": 0.0194,
      "step": 396
    },
    {
      "epoch": 16.5,
      "grad_norm": 3.498192548751831,
      "learning_rate": 9.909625101023315e-06,
      "loss": 0.0378,
      "step": 397
    },
    {
      "epoch": 16.54,
      "grad_norm": 2.134613275527954,
      "learning_rate": 9.909017346828702e-06,
      "loss": 0.0282,
      "step": 398
    },
    {
      "epoch": 16.58,
      "grad_norm": 2.057426691055298,
      "learning_rate": 9.908407574722489e-06,
      "loss": 0.0274,
      "step": 399
    },
    {
      "epoch": 16.62,
      "grad_norm": 2.4701786041259766,
      "learning_rate": 9.907795784955327e-06,
      "loss": 0.0259,
      "step": 400
    },
    {
      "epoch": 16.66,
      "grad_norm": 1.5023711919784546,
      "learning_rate": 9.907181977778702e-06,
      "loss": 0.0234,
      "step": 401
    },
    {
      "epoch": 16.71,
      "grad_norm": 1.8669651746749878,
      "learning_rate": 9.906566153444927e-06,
      "loss": 0.0249,
      "step": 402
    },
    {
      "epoch": 16.75,
      "grad_norm": 2.0588936805725098,
      "learning_rate": 9.905948312207143e-06,
      "loss": 0.0323,
      "step": 403
    },
    {
      "epoch": 16.79,
      "grad_norm": 2.1780660152435303,
      "learning_rate": 9.905328454319323e-06,
      "loss": 0.0354,
      "step": 404
    },
    {
      "epoch": 16.83,
      "grad_norm": 2.1933493614196777,
      "learning_rate": 9.904706580036265e-06,
      "loss": 0.0386,
      "step": 405
    },
    {
      "epoch": 16.87,
      "grad_norm": 1.6419522762298584,
      "learning_rate": 9.904082689613598e-06,
      "loss": 0.0308,
      "step": 406
    },
    {
      "epoch": 16.91,
      "grad_norm": 2.0658910274505615,
      "learning_rate": 9.903456783307781e-06,
      "loss": 0.0303,
      "step": 407
    },
    {
      "epoch": 16.96,
      "grad_norm": 2.003610372543335,
      "learning_rate": 9.902828861376101e-06,
      "loss": 0.028,
      "step": 408
    },
    {
      "epoch": 17.0,
      "grad_norm": 2.2149181365966797,
      "learning_rate": 9.902198924076671e-06,
      "loss": 0.0327,
      "step": 409
    },
    {
      "epoch": 17.04,
      "grad_norm": 1.3474197387695312,
      "learning_rate": 9.901566971668437e-06,
      "loss": 0.0179,
      "step": 410
    },
    {
      "epoch": 17.08,
      "grad_norm": 2.4308226108551025,
      "learning_rate": 9.900933004411169e-06,
      "loss": 0.0323,
      "step": 411
    },
    {
      "epoch": 17.12,
      "grad_norm": 1.9528087377548218,
      "learning_rate": 9.900297022565467e-06,
      "loss": 0.0225,
      "step": 412
    },
    {
      "epoch": 17.16,
      "grad_norm": 1.5334008932113647,
      "learning_rate": 9.899659026392758e-06,
      "loss": 0.0232,
      "step": 413
    },
    {
      "epoch": 17.21,
      "grad_norm": 1.3862117528915405,
      "learning_rate": 9.899019016155302e-06,
      "loss": 0.02,
      "step": 414
    },
    {
      "epoch": 17.25,
      "grad_norm": 2.549808979034424,
      "learning_rate": 9.898376992116179e-06,
      "loss": 0.0336,
      "step": 415
    },
    {
      "epoch": 17.29,
      "grad_norm": 1.879227638244629,
      "learning_rate": 9.897732954539303e-06,
      "loss": 0.0239,
      "step": 416
    },
    {
      "epoch": 17.33,
      "grad_norm": 1.4690443277359009,
      "learning_rate": 9.897086903689413e-06,
      "loss": 0.0176,
      "step": 417
    },
    {
      "epoch": 17.37,
      "grad_norm": 2.015439510345459,
      "learning_rate": 9.896438839832074e-06,
      "loss": 0.0288,
      "step": 418
    },
    {
      "epoch": 17.41,
      "grad_norm": 1.6173285245895386,
      "learning_rate": 9.895788763233684e-06,
      "loss": 0.0273,
      "step": 419
    },
    {
      "epoch": 17.45,
      "grad_norm": 2.015014410018921,
      "learning_rate": 9.895136674161466e-06,
      "loss": 0.0282,
      "step": 420
    },
    {
      "epoch": 17.5,
      "grad_norm": 1.7419638633728027,
      "learning_rate": 9.894482572883464e-06,
      "loss": 0.0243,
      "step": 421
    },
    {
      "epoch": 17.54,
      "grad_norm": 1.7075955867767334,
      "learning_rate": 9.89382645966856e-06,
      "loss": 0.0256,
      "step": 422
    },
    {
      "epoch": 17.58,
      "grad_norm": 2.1389071941375732,
      "learning_rate": 9.89316833478645e-06,
      "loss": 0.0275,
      "step": 423
    },
    {
      "epoch": 17.62,
      "grad_norm": 1.742737889289856,
      "learning_rate": 9.892508198507671e-06,
      "loss": 0.0236,
      "step": 424
    },
    {
      "epoch": 17.66,
      "grad_norm": 1.9500808715820312,
      "learning_rate": 9.891846051103578e-06,
      "loss": 0.0249,
      "step": 425
    },
    {
      "epoch": 17.7,
      "grad_norm": 1.240721344947815,
      "learning_rate": 9.891181892846353e-06,
      "loss": 0.0252,
      "step": 426
    },
    {
      "epoch": 17.75,
      "grad_norm": 2.466168165206909,
      "learning_rate": 9.89051572400901e-06,
      "loss": 0.0306,
      "step": 427
    },
    {
      "epoch": 17.79,
      "grad_norm": 1.6468380689620972,
      "learning_rate": 9.889847544865383e-06,
      "loss": 0.0235,
      "step": 428
    },
    {
      "epoch": 17.83,
      "grad_norm": 1.6101264953613281,
      "learning_rate": 9.889177355690136e-06,
      "loss": 0.0236,
      "step": 429
    },
    {
      "epoch": 17.87,
      "grad_norm": 1.2777742147445679,
      "learning_rate": 9.888505156758758e-06,
      "loss": 0.0199,
      "step": 430
    },
    {
      "epoch": 17.91,
      "grad_norm": 2.172670364379883,
      "learning_rate": 9.887830948347565e-06,
      "loss": 0.0353,
      "step": 431
    },
    {
      "epoch": 17.95,
      "grad_norm": 1.5695316791534424,
      "learning_rate": 9.887154730733699e-06,
      "loss": 0.025,
      "step": 432
    },
    {
      "epoch": 17.99,
      "grad_norm": 2.5956010818481445,
      "learning_rate": 9.886476504195129e-06,
      "loss": 0.0279,
      "step": 433
    },
    {
      "epoch": 18.04,
      "grad_norm": 1.9868745803833008,
      "learning_rate": 9.885796269010644e-06,
      "loss": 0.0217,
      "step": 434
    },
    {
      "epoch": 18.08,
      "grad_norm": 1.6626479625701904,
      "learning_rate": 9.885114025459865e-06,
      "loss": 0.0207,
      "step": 435
    },
    {
      "epoch": 18.12,
      "grad_norm": 1.31485915184021,
      "learning_rate": 9.884429773823238e-06,
      "loss": 0.0176,
      "step": 436
    },
    {
      "epoch": 18.16,
      "grad_norm": 1.4523029327392578,
      "learning_rate": 9.883743514382034e-06,
      "loss": 0.0235,
      "step": 437
    },
    {
      "epoch": 18.2,
      "grad_norm": 2.0536439418792725,
      "learning_rate": 9.883055247418343e-06,
      "loss": 0.0207,
      "step": 438
    },
    {
      "epoch": 18.24,
      "grad_norm": 1.3987176418304443,
      "learning_rate": 9.882364973215092e-06,
      "loss": 0.0179,
      "step": 439
    },
    {
      "epoch": 18.29,
      "grad_norm": 1.939835548400879,
      "learning_rate": 9.881672692056022e-06,
      "loss": 0.0295,
      "step": 440
    },
    {
      "epoch": 18.33,
      "grad_norm": 1.1074745655059814,
      "learning_rate": 9.880978404225705e-06,
      "loss": 0.0152,
      "step": 441
    },
    {
      "epoch": 18.37,
      "grad_norm": 1.30391263961792,
      "learning_rate": 9.880282110009537e-06,
      "loss": 0.0155,
      "step": 442
    },
    {
      "epoch": 18.41,
      "grad_norm": 1.4299744367599487,
      "learning_rate": 9.879583809693737e-06,
      "loss": 0.0188,
      "step": 443
    },
    {
      "epoch": 18.45,
      "grad_norm": 1.0672177076339722,
      "learning_rate": 9.878883503565353e-06,
      "loss": 0.0219,
      "step": 444
    },
    {
      "epoch": 18.49,
      "grad_norm": 2.7898173332214355,
      "learning_rate": 9.878181191912251e-06,
      "loss": 0.0263,
      "step": 445
    },
    {
      "epoch": 18.54,
      "grad_norm": 2.269109010696411,
      "learning_rate": 9.877476875023125e-06,
      "loss": 0.0252,
      "step": 446
    },
    {
      "epoch": 18.58,
      "grad_norm": 1.716168999671936,
      "learning_rate": 9.876770553187495e-06,
      "loss": 0.0315,
      "step": 447
    },
    {
      "epoch": 18.62,
      "grad_norm": 2.2185850143432617,
      "learning_rate": 9.876062226695703e-06,
      "loss": 0.0315,
      "step": 448
    },
    {
      "epoch": 18.66,
      "grad_norm": 1.6877391338348389,
      "learning_rate": 9.875351895838914e-06,
      "loss": 0.0273,
      "step": 449
    },
    {
      "epoch": 18.7,
      "grad_norm": 1.3985309600830078,
      "learning_rate": 9.874639560909118e-06,
      "loss": 0.0203,
      "step": 450
    },
    {
      "epoch": 18.74,
      "grad_norm": 1.0805623531341553,
      "learning_rate": 9.873925222199131e-06,
      "loss": 0.0168,
      "step": 451
    },
    {
      "epoch": 18.78,
      "grad_norm": 2.2295074462890625,
      "learning_rate": 9.87320888000259e-06,
      "loss": 0.0299,
      "step": 452
    },
    {
      "epoch": 18.83,
      "grad_norm": 1.6548182964324951,
      "learning_rate": 9.872490534613954e-06,
      "loss": 0.0242,
      "step": 453
    },
    {
      "epoch": 18.87,
      "grad_norm": 2.918421983718872,
      "learning_rate": 9.87177018632851e-06,
      "loss": 0.033,
      "step": 454
    },
    {
      "epoch": 18.91,
      "grad_norm": 3.673631191253662,
      "learning_rate": 9.871047835442365e-06,
      "loss": 0.0447,
      "step": 455
    },
    {
      "epoch": 18.95,
      "grad_norm": 2.131314754486084,
      "learning_rate": 9.870323482252451e-06,
      "loss": 0.0273,
      "step": 456
    },
    {
      "epoch": 18.99,
      "grad_norm": 2.2069878578186035,
      "learning_rate": 9.86959712705652e-06,
      "loss": 0.0253,
      "step": 457
    },
    {
      "epoch": 19.03,
      "grad_norm": 1.1046397686004639,
      "learning_rate": 9.868868770153153e-06,
      "loss": 0.0214,
      "step": 458
    },
    {
      "epoch": 19.08,
      "grad_norm": 1.1094341278076172,
      "learning_rate": 9.868138411841746e-06,
      "loss": 0.0154,
      "step": 459
    },
    {
      "epoch": 19.12,
      "grad_norm": 1.1485086679458618,
      "learning_rate": 9.867406052422525e-06,
      "loss": 0.0219,
      "step": 460
    },
    {
      "epoch": 19.16,
      "grad_norm": 1.4722627401351929,
      "learning_rate": 9.866671692196533e-06,
      "loss": 0.0175,
      "step": 461
    },
    {
      "epoch": 19.2,
      "grad_norm": 1.1267305612564087,
      "learning_rate": 9.86593533146564e-06,
      "loss": 0.015,
      "step": 462
    },
    {
      "epoch": 19.24,
      "grad_norm": 1.270119309425354,
      "learning_rate": 9.865196970532532e-06,
      "loss": 0.0164,
      "step": 463
    },
    {
      "epoch": 19.28,
      "grad_norm": 1.709779977798462,
      "learning_rate": 9.864456609700726e-06,
      "loss": 0.0223,
      "step": 464
    },
    {
      "epoch": 19.32,
      "grad_norm": 0.8048851490020752,
      "learning_rate": 9.863714249274553e-06,
      "loss": 0.0144,
      "step": 465
    },
    {
      "epoch": 19.37,
      "grad_norm": 1.3326743841171265,
      "learning_rate": 9.862969889559172e-06,
      "loss": 0.0192,
      "step": 466
    },
    {
      "epoch": 19.41,
      "grad_norm": 1.8098483085632324,
      "learning_rate": 9.862223530860559e-06,
      "loss": 0.0214,
      "step": 467
    },
    {
      "epoch": 19.45,
      "grad_norm": 2.341162919998169,
      "learning_rate": 9.861475173485516e-06,
      "loss": 0.0257,
      "step": 468
    },
    {
      "epoch": 19.49,
      "grad_norm": 1.3756924867630005,
      "learning_rate": 9.860724817741661e-06,
      "loss": 0.0156,
      "step": 469
    },
    {
      "epoch": 19.53,
      "grad_norm": 2.0362071990966797,
      "learning_rate": 9.85997246393744e-06,
      "loss": 0.0232,
      "step": 470
    },
    {
      "epoch": 19.57,
      "grad_norm": 2.670856237411499,
      "learning_rate": 9.859218112382117e-06,
      "loss": 0.0259,
      "step": 471
    },
    {
      "epoch": 19.62,
      "grad_norm": 1.4636114835739136,
      "learning_rate": 9.858461763385776e-06,
      "loss": 0.0174,
      "step": 472
    },
    {
      "epoch": 19.66,
      "grad_norm": 2.5577783584594727,
      "learning_rate": 9.857703417259325e-06,
      "loss": 0.0328,
      "step": 473
    },
    {
      "epoch": 19.7,
      "grad_norm": 1.6291624307632446,
      "learning_rate": 9.85694307431449e-06,
      "loss": 0.0207,
      "step": 474
    },
    {
      "epoch": 19.74,
      "grad_norm": 3.267590045928955,
      "learning_rate": 9.85618073486382e-06,
      "loss": 0.0191,
      "step": 475
    },
    {
      "epoch": 19.78,
      "grad_norm": 1.8481110334396362,
      "learning_rate": 9.855416399220683e-06,
      "loss": 0.0188,
      "step": 476
    },
    {
      "epoch": 19.82,
      "grad_norm": 2.120171546936035,
      "learning_rate": 9.854650067699272e-06,
      "loss": 0.0313,
      "step": 477
    },
    {
      "epoch": 19.86,
      "grad_norm": 1.8505444526672363,
      "learning_rate": 9.853881740614591e-06,
      "loss": 0.0229,
      "step": 478
    },
    {
      "epoch": 19.91,
      "grad_norm": 1.8958022594451904,
      "learning_rate": 9.853111418282476e-06,
      "loss": 0.0353,
      "step": 479
    },
    {
      "epoch": 19.95,
      "grad_norm": 2.3128175735473633,
      "learning_rate": 9.852339101019574e-06,
      "loss": 0.0309,
      "step": 480
    },
    {
      "epoch": 19.99,
      "grad_norm": 1.5476323366165161,
      "learning_rate": 9.851564789143357e-06,
      "loss": 0.0235,
      "step": 481
    },
    {
      "epoch": 20.03,
      "grad_norm": 1.6428319215774536,
      "learning_rate": 9.850788482972114e-06,
      "loss": 0.0179,
      "step": 482
    },
    {
      "epoch": 20.07,
      "grad_norm": 1.019000768661499,
      "learning_rate": 9.850010182824956e-06,
      "loss": 0.0132,
      "step": 483
    },
    {
      "epoch": 20.11,
      "grad_norm": 1.0759729146957397,
      "learning_rate": 9.849229889021814e-06,
      "loss": 0.015,
      "step": 484
    },
    {
      "epoch": 20.16,
      "grad_norm": 0.9193121194839478,
      "learning_rate": 9.848447601883436e-06,
      "loss": 0.0152,
      "step": 485
    },
    {
      "epoch": 20.2,
      "grad_norm": 1.4386664628982544,
      "learning_rate": 9.847663321731388e-06,
      "loss": 0.0172,
      "step": 486
    },
    {
      "epoch": 20.24,
      "grad_norm": 1.2688305377960205,
      "learning_rate": 9.846877048888064e-06,
      "loss": 0.0155,
      "step": 487
    },
    {
      "epoch": 20.28,
      "grad_norm": 2.4122462272644043,
      "learning_rate": 9.846088783676666e-06,
      "loss": 0.0223,
      "step": 488
    },
    {
      "epoch": 20.32,
      "grad_norm": 2.1340651512145996,
      "learning_rate": 9.845298526421224e-06,
      "loss": 0.0163,
      "step": 489
    },
    {
      "epoch": 20.36,
      "grad_norm": 1.7957786321640015,
      "learning_rate": 9.844506277446577e-06,
      "loss": 0.0211,
      "step": 490
    },
    {
      "epoch": 20.41,
      "grad_norm": 1.581291913986206,
      "learning_rate": 9.843712037078394e-06,
      "loss": 0.0185,
      "step": 491
    },
    {
      "epoch": 20.45,
      "grad_norm": 1.336010456085205,
      "learning_rate": 9.842915805643156e-06,
      "loss": 0.0165,
      "step": 492
    },
    {
      "epoch": 20.49,
      "grad_norm": 2.217350959777832,
      "learning_rate": 9.842117583468165e-06,
      "loss": 0.0254,
      "step": 493
    },
    {
      "epoch": 20.53,
      "grad_norm": 1.8147966861724854,
      "learning_rate": 9.841317370881535e-06,
      "loss": 0.0234,
      "step": 494
    },
    {
      "epoch": 20.57,
      "grad_norm": 1.7471894025802612,
      "learning_rate": 9.840515168212208e-06,
      "loss": 0.0234,
      "step": 495
    },
    {
      "epoch": 20.61,
      "grad_norm": 1.255450963973999,
      "learning_rate": 9.839710975789937e-06,
      "loss": 0.0148,
      "step": 496
    },
    {
      "epoch": 20.65,
      "grad_norm": 1.610554575920105,
      "learning_rate": 9.838904793945297e-06,
      "loss": 0.0215,
      "step": 497
    },
    {
      "epoch": 20.7,
      "grad_norm": 1.3659415245056152,
      "learning_rate": 9.838096623009676e-06,
      "loss": 0.0143,
      "step": 498
    },
    {
      "epoch": 20.74,
      "grad_norm": 1.2201590538024902,
      "learning_rate": 9.837286463315284e-06,
      "loss": 0.0191,
      "step": 499
    },
    {
      "epoch": 20.78,
      "grad_norm": 1.6058489084243774,
      "learning_rate": 9.836474315195148e-06,
      "loss": 0.0163,
      "step": 500
    },
    {
      "epoch": 20.82,
      "grad_norm": 1.9624208211898804,
      "learning_rate": 9.83566017898311e-06,
      "loss": 0.0203,
      "step": 501
    },
    {
      "epoch": 20.86,
      "grad_norm": 1.4596922397613525,
      "learning_rate": 9.834844055013833e-06,
      "loss": 0.0197,
      "step": 502
    },
    {
      "epoch": 20.9,
      "grad_norm": 1.1742264032363892,
      "learning_rate": 9.834025943622792e-06,
      "loss": 0.0169,
      "step": 503
    },
    {
      "epoch": 20.95,
      "grad_norm": 1.2824679613113403,
      "learning_rate": 9.833205845146283e-06,
      "loss": 0.0209,
      "step": 504
    },
    {
      "epoch": 20.99,
      "grad_norm": 1.5611176490783691,
      "learning_rate": 9.832383759921415e-06,
      "loss": 0.0188,
      "step": 505
    },
    {
      "epoch": 21.03,
      "grad_norm": 2.006805181503296,
      "learning_rate": 9.83155968828612e-06,
      "loss": 0.0195,
      "step": 506
    },
    {
      "epoch": 21.07,
      "grad_norm": 1.3561519384384155,
      "learning_rate": 9.830733630579142e-06,
      "loss": 0.0126,
      "step": 507
    },
    {
      "epoch": 21.11,
      "grad_norm": 1.2587878704071045,
      "learning_rate": 9.829905587140041e-06,
      "loss": 0.0143,
      "step": 508
    },
    {
      "epoch": 21.15,
      "grad_norm": 2.025333881378174,
      "learning_rate": 9.829075558309194e-06,
      "loss": 0.0123,
      "step": 509
    },
    {
      "epoch": 21.19,
      "grad_norm": 2.782839298248291,
      "learning_rate": 9.828243544427795e-06,
      "loss": 0.0255,
      "step": 510
    },
    {
      "epoch": 21.24,
      "grad_norm": 2.5124893188476562,
      "learning_rate": 9.827409545837853e-06,
      "loss": 0.0213,
      "step": 511
    },
    {
      "epoch": 21.28,
      "grad_norm": 1.3521658182144165,
      "learning_rate": 9.826573562882195e-06,
      "loss": 0.0141,
      "step": 512
    },
    {
      "epoch": 21.32,
      "grad_norm": 1.6916242837905884,
      "learning_rate": 9.825735595904462e-06,
      "loss": 0.0189,
      "step": 513
    },
    {
      "epoch": 21.36,
      "grad_norm": 2.9430224895477295,
      "learning_rate": 9.824895645249107e-06,
      "loss": 0.0229,
      "step": 514
    },
    {
      "epoch": 21.4,
      "grad_norm": 1.575308084487915,
      "learning_rate": 9.824053711261405e-06,
      "loss": 0.0217,
      "step": 515
    },
    {
      "epoch": 21.44,
      "grad_norm": 1.4486247301101685,
      "learning_rate": 9.823209794287446e-06,
      "loss": 0.014,
      "step": 516
    },
    {
      "epoch": 21.49,
      "grad_norm": 1.3748173713684082,
      "learning_rate": 9.822363894674125e-06,
      "loss": 0.0156,
      "step": 517
    },
    {
      "epoch": 21.53,
      "grad_norm": 1.3152363300323486,
      "learning_rate": 9.821516012769165e-06,
      "loss": 0.0179,
      "step": 518
    },
    {
      "epoch": 21.57,
      "grad_norm": 1.4565523862838745,
      "learning_rate": 9.820666148921097e-06,
      "loss": 0.0191,
      "step": 519
    },
    {
      "epoch": 21.61,
      "grad_norm": 1.5164717435836792,
      "learning_rate": 9.819814303479268e-06,
      "loss": 0.0189,
      "step": 520
    },
    {
      "epoch": 21.65,
      "grad_norm": 1.3321702480316162,
      "learning_rate": 9.818960476793837e-06,
      "loss": 0.0196,
      "step": 521
    },
    {
      "epoch": 21.69,
      "grad_norm": 1.2680552005767822,
      "learning_rate": 9.818104669215784e-06,
      "loss": 0.0143,
      "step": 522
    },
    {
      "epoch": 21.74,
      "grad_norm": 1.695988655090332,
      "learning_rate": 9.817246881096898e-06,
      "loss": 0.0147,
      "step": 523
    },
    {
      "epoch": 21.78,
      "grad_norm": 2.313166618347168,
      "learning_rate": 9.81638711278978e-06,
      "loss": 0.018,
      "step": 524
    },
    {
      "epoch": 21.82,
      "grad_norm": 1.5952444076538086,
      "learning_rate": 9.815525364647853e-06,
      "loss": 0.0129,
      "step": 525
    },
    {
      "epoch": 21.86,
      "grad_norm": 2.171248197555542,
      "learning_rate": 9.814661637025346e-06,
      "loss": 0.0162,
      "step": 526
    },
    {
      "epoch": 21.9,
      "grad_norm": 2.093165159225464,
      "learning_rate": 9.813795930277306e-06,
      "loss": 0.0208,
      "step": 527
    },
    {
      "epoch": 21.94,
      "grad_norm": 1.377444863319397,
      "learning_rate": 9.812928244759591e-06,
      "loss": 0.0139,
      "step": 528
    },
    {
      "epoch": 21.98,
      "grad_norm": 1.1809170246124268,
      "learning_rate": 9.812058580828877e-06,
      "loss": 0.0123,
      "step": 529
    },
    {
      "epoch": 22.03,
      "grad_norm": 1.6602662801742554,
      "learning_rate": 9.811186938842645e-06,
      "loss": 0.015,
      "step": 530
    },
    {
      "epoch": 22.07,
      "grad_norm": 1.2318538427352905,
      "learning_rate": 9.810313319159198e-06,
      "loss": 0.0139,
      "step": 531
    },
    {
      "epoch": 22.11,
      "grad_norm": 1.3687058687210083,
      "learning_rate": 9.809437722137647e-06,
      "loss": 0.0145,
      "step": 532
    },
    {
      "epoch": 22.15,
      "grad_norm": 1.5528533458709717,
      "learning_rate": 9.808560148137916e-06,
      "loss": 0.0156,
      "step": 533
    },
    {
      "epoch": 22.19,
      "grad_norm": 1.9880590438842773,
      "learning_rate": 9.807680597520746e-06,
      "loss": 0.0145,
      "step": 534
    },
    {
      "epoch": 22.23,
      "grad_norm": 2.2347865104675293,
      "learning_rate": 9.80679907064768e-06,
      "loss": 0.0232,
      "step": 535
    },
    {
      "epoch": 22.28,
      "grad_norm": 1.3977619409561157,
      "learning_rate": 9.805915567881088e-06,
      "loss": 0.0151,
      "step": 536
    },
    {
      "epoch": 22.32,
      "grad_norm": 1.2335580587387085,
      "learning_rate": 9.805030089584141e-06,
      "loss": 0.0151,
      "step": 537
    },
    {
      "epoch": 22.36,
      "grad_norm": 1.3754693269729614,
      "learning_rate": 9.804142636120827e-06,
      "loss": 0.0153,
      "step": 538
    },
    {
      "epoch": 22.4,
      "grad_norm": 1.182434320449829,
      "learning_rate": 9.80325320785594e-06,
      "loss": 0.0141,
      "step": 539
    },
    {
      "epoch": 22.44,
      "grad_norm": 1.62021005153656,
      "learning_rate": 9.802361805155097e-06,
      "loss": 0.0182,
      "step": 540
    },
    {
      "epoch": 22.48,
      "grad_norm": 1.3455549478530884,
      "learning_rate": 9.801468428384716e-06,
      "loss": 0.0134,
      "step": 541
    },
    {
      "epoch": 22.52,
      "grad_norm": 1.1176600456237793,
      "learning_rate": 9.800573077912032e-06,
      "loss": 0.0144,
      "step": 542
    },
    {
      "epoch": 22.57,
      "grad_norm": 1.861369013786316,
      "learning_rate": 9.799675754105088e-06,
      "loss": 0.0171,
      "step": 543
    },
    {
      "epoch": 22.61,
      "grad_norm": 1.4851101636886597,
      "learning_rate": 9.798776457332742e-06,
      "loss": 0.0115,
      "step": 544
    },
    {
      "epoch": 22.65,
      "grad_norm": 1.8287948369979858,
      "learning_rate": 9.797875187964661e-06,
      "loss": 0.0207,
      "step": 545
    },
    {
      "epoch": 22.69,
      "grad_norm": 2.5988271236419678,
      "learning_rate": 9.79697194637132e-06,
      "loss": 0.0246,
      "step": 546
    },
    {
      "epoch": 22.73,
      "grad_norm": 1.259818196296692,
      "learning_rate": 9.79606673292401e-06,
      "loss": 0.0152,
      "step": 547
    },
    {
      "epoch": 22.77,
      "grad_norm": 2.0091001987457275,
      "learning_rate": 9.79515954799483e-06,
      "loss": 0.0231,
      "step": 548
    },
    {
      "epoch": 22.82,
      "grad_norm": 1.6523723602294922,
      "learning_rate": 9.794250391956687e-06,
      "loss": 0.0157,
      "step": 549
    },
    {
      "epoch": 22.86,
      "grad_norm": 0.9857004880905151,
      "learning_rate": 9.793339265183303e-06,
      "loss": 0.0099,
      "step": 550
    },
    {
      "epoch": 22.9,
      "grad_norm": 1.2768840789794922,
      "learning_rate": 9.79242616804921e-06,
      "loss": 0.0176,
      "step": 551
    },
    {
      "epoch": 22.94,
      "grad_norm": 2.025540351867676,
      "learning_rate": 9.791511100929743e-06,
      "loss": 0.0222,
      "step": 552
    },
    {
      "epoch": 22.98,
      "grad_norm": 1.709256649017334,
      "learning_rate": 9.790594064201053e-06,
      "loss": 0.0203,
      "step": 553
    },
    {
      "epoch": 23.02,
      "grad_norm": 1.261169195175171,
      "learning_rate": 9.7896750582401e-06,
      "loss": 0.0182,
      "step": 554
    },
    {
      "epoch": 23.06,
      "grad_norm": 1.3042669296264648,
      "learning_rate": 9.788754083424654e-06,
      "loss": 0.013,
      "step": 555
    },
    {
      "epoch": 23.11,
      "grad_norm": 1.2250356674194336,
      "learning_rate": 9.78783114013329e-06,
      "loss": 0.0129,
      "step": 556
    },
    {
      "epoch": 23.15,
      "grad_norm": 1.3807754516601562,
      "learning_rate": 9.786906228745397e-06,
      "loss": 0.0135,
      "step": 557
    },
    {
      "epoch": 23.19,
      "grad_norm": 1.4278075695037842,
      "learning_rate": 9.78597934964117e-06,
      "loss": 0.0189,
      "step": 558
    },
    {
      "epoch": 23.23,
      "grad_norm": 1.7838526964187622,
      "learning_rate": 9.785050503201614e-06,
      "loss": 0.0113,
      "step": 559
    },
    {
      "epoch": 23.27,
      "grad_norm": 1.294252872467041,
      "learning_rate": 9.784119689808545e-06,
      "loss": 0.0106,
      "step": 560
    },
    {
      "epoch": 23.31,
      "grad_norm": 1.0824915170669556,
      "learning_rate": 9.783186909844582e-06,
      "loss": 0.0136,
      "step": 561
    },
    {
      "epoch": 23.36,
      "grad_norm": 1.0590628385543823,
      "learning_rate": 9.782252163693159e-06,
      "loss": 0.0123,
      "step": 562
    },
    {
      "epoch": 23.4,
      "grad_norm": 2.335660696029663,
      "learning_rate": 9.78131545173851e-06,
      "loss": 0.017,
      "step": 563
    },
    {
      "epoch": 23.44,
      "grad_norm": 1.4731491804122925,
      "learning_rate": 9.780376774365687e-06,
      "loss": 0.0159,
      "step": 564
    },
    {
      "epoch": 23.48,
      "grad_norm": 1.7935240268707275,
      "learning_rate": 9.779436131960544e-06,
      "loss": 0.017,
      "step": 565
    },
    {
      "epoch": 23.52,
      "grad_norm": 1.4461148977279663,
      "learning_rate": 9.77849352490974e-06,
      "loss": 0.0172,
      "step": 566
    },
    {
      "epoch": 23.56,
      "grad_norm": 1.1038603782653809,
      "learning_rate": 9.777548953600748e-06,
      "loss": 0.013,
      "step": 567
    },
    {
      "epoch": 23.61,
      "grad_norm": 1.4733235836029053,
      "learning_rate": 9.776602418421846e-06,
      "loss": 0.017,
      "step": 568
    },
    {
      "epoch": 23.65,
      "grad_norm": 1.8339452743530273,
      "learning_rate": 9.775653919762117e-06,
      "loss": 0.0228,
      "step": 569
    },
    {
      "epoch": 23.69,
      "grad_norm": 2.5785446166992188,
      "learning_rate": 9.774703458011453e-06,
      "loss": 0.0259,
      "step": 570
    },
    {
      "epoch": 23.73,
      "grad_norm": 1.8733267784118652,
      "learning_rate": 9.773751033560554e-06,
      "loss": 0.021,
      "step": 571
    },
    {
      "epoch": 23.77,
      "grad_norm": 1.1832705736160278,
      "learning_rate": 9.772796646800926e-06,
      "loss": 0.0148,
      "step": 572
    },
    {
      "epoch": 23.81,
      "grad_norm": 1.7580177783966064,
      "learning_rate": 9.771840298124882e-06,
      "loss": 0.0176,
      "step": 573
    },
    {
      "epoch": 23.85,
      "grad_norm": 1.5729432106018066,
      "learning_rate": 9.770881987925538e-06,
      "loss": 0.0142,
      "step": 574
    },
    {
      "epoch": 23.9,
      "grad_norm": 1.4690943956375122,
      "learning_rate": 9.76992171659682e-06,
      "loss": 0.0148,
      "step": 575
    },
    {
      "epoch": 23.94,
      "grad_norm": 2.107720136642456,
      "learning_rate": 9.768959484533461e-06,
      "loss": 0.0271,
      "step": 576
    },
    {
      "epoch": 23.98,
      "grad_norm": 1.896767020225525,
      "learning_rate": 9.767995292130998e-06,
      "loss": 0.0212,
      "step": 577
    },
    {
      "epoch": 24.02,
      "grad_norm": 1.060327172279358,
      "learning_rate": 9.767029139785772e-06,
      "loss": 0.0152,
      "step": 578
    },
    {
      "epoch": 24.06,
      "grad_norm": 0.8874773979187012,
      "learning_rate": 9.766061027894932e-06,
      "loss": 0.0139,
      "step": 579
    },
    {
      "epoch": 24.1,
      "grad_norm": 1.296108365058899,
      "learning_rate": 9.765090956856437e-06,
      "loss": 0.0117,
      "step": 580
    },
    {
      "epoch": 24.15,
      "grad_norm": 1.982295274734497,
      "learning_rate": 9.76411892706904e-06,
      "loss": 0.0187,
      "step": 581
    },
    {
      "epoch": 24.19,
      "grad_norm": 0.6740370988845825,
      "learning_rate": 9.76314493893231e-06,
      "loss": 0.0097,
      "step": 582
    },
    {
      "epoch": 24.23,
      "grad_norm": 1.0652464628219604,
      "learning_rate": 9.762168992846615e-06,
      "loss": 0.0099,
      "step": 583
    },
    {
      "epoch": 24.27,
      "grad_norm": 1.2530184984207153,
      "learning_rate": 9.76119108921313e-06,
      "loss": 0.0093,
      "step": 584
    },
    {
      "epoch": 24.31,
      "grad_norm": 0.867599368095398,
      "learning_rate": 9.760211228433834e-06,
      "loss": 0.0091,
      "step": 585
    },
    {
      "epoch": 24.35,
      "grad_norm": 1.2560555934906006,
      "learning_rate": 9.759229410911511e-06,
      "loss": 0.011,
      "step": 586
    },
    {
      "epoch": 24.39,
      "grad_norm": 1.487899661064148,
      "learning_rate": 9.758245637049748e-06,
      "loss": 0.0178,
      "step": 587
    },
    {
      "epoch": 24.44,
      "grad_norm": 1.3526244163513184,
      "learning_rate": 9.757259907252938e-06,
      "loss": 0.0117,
      "step": 588
    },
    {
      "epoch": 24.48,
      "grad_norm": 2.6987948417663574,
      "learning_rate": 9.756272221926278e-06,
      "loss": 0.0175,
      "step": 589
    },
    {
      "epoch": 24.52,
      "grad_norm": 1.6906884908676147,
      "learning_rate": 9.755282581475769e-06,
      "loss": 0.0126,
      "step": 590
    },
    {
      "epoch": 24.56,
      "grad_norm": 2.2085623741149902,
      "learning_rate": 9.754290986308212e-06,
      "loss": 0.0143,
      "step": 591
    },
    {
      "epoch": 24.6,
      "grad_norm": 1.4100291728973389,
      "learning_rate": 9.753297436831217e-06,
      "loss": 0.0182,
      "step": 592
    },
    {
      "epoch": 24.64,
      "grad_norm": 1.8836398124694824,
      "learning_rate": 9.752301933453192e-06,
      "loss": 0.0259,
      "step": 593
    },
    {
      "epoch": 24.69,
      "grad_norm": 0.7958755493164062,
      "learning_rate": 9.751304476583354e-06,
      "loss": 0.0094,
      "step": 594
    },
    {
      "epoch": 24.73,
      "grad_norm": 2.18403959274292,
      "learning_rate": 9.750305066631717e-06,
      "loss": 0.0162,
      "step": 595
    },
    {
      "epoch": 24.77,
      "grad_norm": 1.2110352516174316,
      "learning_rate": 9.749303704009103e-06,
      "loss": 0.0155,
      "step": 596
    },
    {
      "epoch": 24.81,
      "grad_norm": 1.8701049089431763,
      "learning_rate": 9.748300389127132e-06,
      "loss": 0.0226,
      "step": 597
    },
    {
      "epoch": 24.85,
      "grad_norm": 2.267115831375122,
      "learning_rate": 9.74729512239823e-06,
      "loss": 0.0254,
      "step": 598
    },
    {
      "epoch": 24.89,
      "grad_norm": 2.1383440494537354,
      "learning_rate": 9.746287904235625e-06,
      "loss": 0.0165,
      "step": 599
    },
    {
      "epoch": 24.94,
      "grad_norm": 1.3220481872558594,
      "learning_rate": 9.745278735053345e-06,
      "loss": 0.0138,
      "step": 600
    },
    {
      "epoch": 24.98,
      "grad_norm": 1.5948050022125244,
      "learning_rate": 9.74426761526622e-06,
      "loss": 0.0154,
      "step": 601
    },
    {
      "epoch": 25.02,
      "grad_norm": 2.033447742462158,
      "learning_rate": 9.743254545289889e-06,
      "loss": 0.026,
      "step": 602
    },
    {
      "epoch": 25.06,
      "grad_norm": 1.1540682315826416,
      "learning_rate": 9.74223952554078e-06,
      "loss": 0.0107,
      "step": 603
    },
    {
      "epoch": 25.1,
      "grad_norm": 0.9953367710113525,
      "learning_rate": 9.741222556436132e-06,
      "loss": 0.0124,
      "step": 604
    },
    {
      "epoch": 25.14,
      "grad_norm": 1.0140894651412964,
      "learning_rate": 9.740203638393984e-06,
      "loss": 0.0087,
      "step": 605
    },
    {
      "epoch": 25.18,
      "grad_norm": 1.9855012893676758,
      "learning_rate": 9.739182771833173e-06,
      "loss": 0.0155,
      "step": 606
    },
    {
      "epoch": 25.23,
      "grad_norm": 1.1636043787002563,
      "learning_rate": 9.738159957173338e-06,
      "loss": 0.0103,
      "step": 607
    },
    {
      "epoch": 25.27,
      "grad_norm": 2.1795201301574707,
      "learning_rate": 9.737135194834923e-06,
      "loss": 0.0331,
      "step": 608
    },
    {
      "epoch": 25.31,
      "grad_norm": 1.4532884359359741,
      "learning_rate": 9.736108485239165e-06,
      "loss": 0.017,
      "step": 609
    },
    {
      "epoch": 25.35,
      "grad_norm": 1.671129584312439,
      "learning_rate": 9.735079828808107e-06,
      "loss": 0.0131,
      "step": 610
    },
    {
      "epoch": 25.39,
      "grad_norm": 1.4553560018539429,
      "learning_rate": 9.734049225964591e-06,
      "loss": 0.014,
      "step": 611
    },
    {
      "epoch": 25.43,
      "grad_norm": 0.7736483216285706,
      "learning_rate": 9.73301667713226e-06,
      "loss": 0.0088,
      "step": 612
    },
    {
      "epoch": 25.48,
      "grad_norm": 1.4587726593017578,
      "learning_rate": 9.731982182735553e-06,
      "loss": 0.0139,
      "step": 613
    },
    {
      "epoch": 25.52,
      "grad_norm": 2.0848641395568848,
      "learning_rate": 9.730945743199715e-06,
      "loss": 0.0116,
      "step": 614
    },
    {
      "epoch": 25.56,
      "grad_norm": 1.3619890213012695,
      "learning_rate": 9.729907358950785e-06,
      "loss": 0.015,
      "step": 615
    },
    {
      "epoch": 25.6,
      "grad_norm": 1.9373246431350708,
      "learning_rate": 9.728867030415604e-06,
      "loss": 0.0132,
      "step": 616
    },
    {
      "epoch": 25.64,
      "grad_norm": 1.1845250129699707,
      "learning_rate": 9.727824758021812e-06,
      "loss": 0.0117,
      "step": 617
    },
    {
      "epoch": 25.68,
      "grad_norm": 1.1788760423660278,
      "learning_rate": 9.726780542197845e-06,
      "loss": 0.0123,
      "step": 618
    },
    {
      "epoch": 25.72,
      "grad_norm": 1.4072145223617554,
      "learning_rate": 9.725734383372945e-06,
      "loss": 0.0169,
      "step": 619
    },
    {
      "epoch": 25.77,
      "grad_norm": 1.528531789779663,
      "learning_rate": 9.724686281977146e-06,
      "loss": 0.0132,
      "step": 620
    },
    {
      "epoch": 25.81,
      "grad_norm": 1.2350269556045532,
      "learning_rate": 9.723636238441285e-06,
      "loss": 0.0136,
      "step": 621
    },
    {
      "epoch": 25.85,
      "grad_norm": 1.8793730735778809,
      "learning_rate": 9.722584253196993e-06,
      "loss": 0.0176,
      "step": 622
    },
    {
      "epoch": 25.89,
      "grad_norm": 1.1725118160247803,
      "learning_rate": 9.721530326676703e-06,
      "loss": 0.0119,
      "step": 623
    },
    {
      "epoch": 25.93,
      "grad_norm": 1.2882431745529175,
      "learning_rate": 9.720474459313641e-06,
      "loss": 0.0186,
      "step": 624
    },
    {
      "epoch": 25.97,
      "grad_norm": 1.6133400201797485,
      "learning_rate": 9.719416651541839e-06,
      "loss": 0.0167,
      "step": 625
    },
    {
      "epoch": 26.02,
      "grad_norm": 1.3991116285324097,
      "learning_rate": 9.718356903796118e-06,
      "loss": 0.0122,
      "step": 626
    },
    {
      "epoch": 26.06,
      "grad_norm": 0.9884077906608582,
      "learning_rate": 9.717295216512103e-06,
      "loss": 0.0114,
      "step": 627
    },
    {
      "epoch": 26.1,
      "grad_norm": 1.150733232498169,
      "learning_rate": 9.716231590126211e-06,
      "loss": 0.0183,
      "step": 628
    },
    {
      "epoch": 26.14,
      "grad_norm": 1.0423929691314697,
      "learning_rate": 9.715166025075662e-06,
      "loss": 0.0115,
      "step": 629
    },
    {
      "epoch": 26.18,
      "grad_norm": 1.440423846244812,
      "learning_rate": 9.714098521798466e-06,
      "loss": 0.0153,
      "step": 630
    },
    {
      "epoch": 26.22,
      "grad_norm": 0.8605368137359619,
      "learning_rate": 9.713029080733434e-06,
      "loss": 0.0091,
      "step": 631
    },
    {
      "epoch": 26.26,
      "grad_norm": 1.38444185256958,
      "learning_rate": 9.711957702320176e-06,
      "loss": 0.0105,
      "step": 632
    },
    {
      "epoch": 26.31,
      "grad_norm": 1.0223057270050049,
      "learning_rate": 9.71088438699909e-06,
      "loss": 0.011,
      "step": 633
    },
    {
      "epoch": 26.35,
      "grad_norm": 2.0747156143188477,
      "learning_rate": 9.70980913521138e-06,
      "loss": 0.017,
      "step": 634
    },
    {
      "epoch": 26.39,
      "grad_norm": 1.2788701057434082,
      "learning_rate": 9.708731947399039e-06,
      "loss": 0.0105,
      "step": 635
    },
    {
      "epoch": 26.43,
      "grad_norm": 1.4615358114242554,
      "learning_rate": 9.707652824004858e-06,
      "loss": 0.0128,
      "step": 636
    },
    {
      "epoch": 26.47,
      "grad_norm": 1.2284668684005737,
      "learning_rate": 9.706571765472426e-06,
      "loss": 0.012,
      "step": 637
    },
    {
      "epoch": 26.51,
      "grad_norm": 0.9540973901748657,
      "learning_rate": 9.705488772246124e-06,
      "loss": 0.0117,
      "step": 638
    },
    {
      "epoch": 26.56,
      "grad_norm": 1.4607784748077393,
      "learning_rate": 9.704403844771128e-06,
      "loss": 0.0141,
      "step": 639
    },
    {
      "epoch": 26.6,
      "grad_norm": 1.0056153535842896,
      "learning_rate": 9.703316983493414e-06,
      "loss": 0.008,
      "step": 640
    },
    {
      "epoch": 26.64,
      "grad_norm": 1.5910645723342896,
      "learning_rate": 9.702228188859746e-06,
      "loss": 0.017,
      "step": 641
    },
    {
      "epoch": 26.68,
      "grad_norm": 1.1659204959869385,
      "learning_rate": 9.70113746131769e-06,
      "loss": 0.0097,
      "step": 642
    },
    {
      "epoch": 26.72,
      "grad_norm": 1.1770734786987305,
      "learning_rate": 9.7000448013156e-06,
      "loss": 0.016,
      "step": 643
    },
    {
      "epoch": 26.76,
      "grad_norm": 1.538647174835205,
      "learning_rate": 9.698950209302629e-06,
      "loss": 0.02,
      "step": 644
    },
    {
      "epoch": 26.81,
      "grad_norm": 1.8532816171646118,
      "learning_rate": 9.697853685728721e-06,
      "loss": 0.0175,
      "step": 645
    },
    {
      "epoch": 26.85,
      "grad_norm": 1.1289597749710083,
      "learning_rate": 9.696755231044618e-06,
      "loss": 0.0113,
      "step": 646
    },
    {
      "epoch": 26.89,
      "grad_norm": 1.5345735549926758,
      "learning_rate": 9.695654845701852e-06,
      "loss": 0.0189,
      "step": 647
    },
    {
      "epoch": 26.93,
      "grad_norm": 1.9738218784332275,
      "learning_rate": 9.694552530152747e-06,
      "loss": 0.0191,
      "step": 648
    },
    {
      "epoch": 26.97,
      "grad_norm": 1.3926448822021484,
      "learning_rate": 9.693448284850427e-06,
      "loss": 0.0197,
      "step": 649
    },
    {
      "epoch": 27.01,
      "grad_norm": 1.4429880380630493,
      "learning_rate": 9.692342110248802e-06,
      "loss": 0.0207,
      "step": 650
    },
    {
      "epoch": 27.05,
      "grad_norm": 0.8048099875450134,
      "learning_rate": 9.691234006802584e-06,
      "loss": 0.0102,
      "step": 651
    },
    {
      "epoch": 27.1,
      "grad_norm": 1.2976267337799072,
      "learning_rate": 9.690123974967267e-06,
      "loss": 0.0127,
      "step": 652
    },
    {
      "epoch": 27.14,
      "grad_norm": 1.3435474634170532,
      "learning_rate": 9.689012015199147e-06,
      "loss": 0.0112,
      "step": 653
    },
    {
      "epoch": 27.18,
      "grad_norm": 1.274719476699829,
      "learning_rate": 9.687898127955305e-06,
      "loss": 0.0154,
      "step": 654
    },
    {
      "epoch": 27.22,
      "grad_norm": 0.9309212565422058,
      "learning_rate": 9.686782313693622e-06,
      "loss": 0.0104,
      "step": 655
    },
    {
      "epoch": 27.26,
      "grad_norm": 0.8821429014205933,
      "learning_rate": 9.68566457287276e-06,
      "loss": 0.0105,
      "step": 656
    },
    {
      "epoch": 27.3,
      "grad_norm": 1.2468456029891968,
      "learning_rate": 9.684544905952192e-06,
      "loss": 0.0186,
      "step": 657
    },
    {
      "epoch": 27.35,
      "grad_norm": 1.7899876832962036,
      "learning_rate": 9.68342331339216e-06,
      "loss": 0.0105,
      "step": 658
    },
    {
      "epoch": 27.39,
      "grad_norm": 1.1544466018676758,
      "learning_rate": 9.682299795653714e-06,
      "loss": 0.0072,
      "step": 659
    },
    {
      "epoch": 27.43,
      "grad_norm": 1.3161728382110596,
      "learning_rate": 9.681174353198687e-06,
      "loss": 0.0096,
      "step": 660
    },
    {
      "epoch": 27.47,
      "grad_norm": 1.6898788213729858,
      "learning_rate": 9.680046986489707e-06,
      "loss": 0.0244,
      "step": 661
    },
    {
      "epoch": 27.51,
      "grad_norm": 0.9835695028305054,
      "learning_rate": 9.678917695990191e-06,
      "loss": 0.0113,
      "step": 662
    },
    {
      "epoch": 27.55,
      "grad_norm": 0.9090026617050171,
      "learning_rate": 9.67778648216435e-06,
      "loss": 0.012,
      "step": 663
    },
    {
      "epoch": 27.59,
      "grad_norm": 2.4559617042541504,
      "learning_rate": 9.67665334547718e-06,
      "loss": 0.0236,
      "step": 664
    },
    {
      "epoch": 27.64,
      "grad_norm": 1.2295053005218506,
      "learning_rate": 9.675518286394474e-06,
      "loss": 0.0133,
      "step": 665
    },
    {
      "epoch": 27.68,
      "grad_norm": 2.3001155853271484,
      "learning_rate": 9.674381305382808e-06,
      "loss": 0.0223,
      "step": 666
    },
    {
      "epoch": 27.72,
      "grad_norm": 1.517849087715149,
      "learning_rate": 9.673242402909555e-06,
      "loss": 0.0135,
      "step": 667
    },
    {
      "epoch": 27.76,
      "grad_norm": 2.030163049697876,
      "learning_rate": 9.672101579442875e-06,
      "loss": 0.0245,
      "step": 668
    },
    {
      "epoch": 27.8,
      "grad_norm": 1.3739105463027954,
      "learning_rate": 9.670958835451716e-06,
      "loss": 0.0137,
      "step": 669
    },
    {
      "epoch": 27.84,
      "grad_norm": 1.3506567478179932,
      "learning_rate": 9.669814171405818e-06,
      "loss": 0.0163,
      "step": 670
    },
    {
      "epoch": 27.89,
      "grad_norm": 1.2599499225616455,
      "learning_rate": 9.668667587775706e-06,
      "loss": 0.0113,
      "step": 671
    },
    {
      "epoch": 27.93,
      "grad_norm": 1.4239976406097412,
      "learning_rate": 9.667519085032701e-06,
      "loss": 0.0213,
      "step": 672
    },
    {
      "epoch": 27.97,
      "grad_norm": 0.6558859348297119,
      "learning_rate": 9.666368663648908e-06,
      "loss": 0.0089,
      "step": 673
    },
    {
      "epoch": 28.01,
      "grad_norm": 1.3079904317855835,
      "learning_rate": 9.665216324097222e-06,
      "loss": 0.014,
      "step": 674
    },
    {
      "epoch": 28.05,
      "grad_norm": 0.6399962902069092,
      "learning_rate": 9.664062066851325e-06,
      "loss": 0.0071,
      "step": 675
    },
    {
      "epoch": 28.09,
      "grad_norm": 0.8891623616218567,
      "learning_rate": 9.66290589238569e-06,
      "loss": 0.0095,
      "step": 676
    },
    {
      "epoch": 28.14,
      "grad_norm": 0.9989529252052307,
      "learning_rate": 9.661747801175576e-06,
      "loss": 0.0126,
      "step": 677
    },
    {
      "epoch": 28.18,
      "grad_norm": 1.790611982345581,
      "learning_rate": 9.660587793697029e-06,
      "loss": 0.013,
      "step": 678
    },
    {
      "epoch": 28.22,
      "grad_norm": 1.3646697998046875,
      "learning_rate": 9.659425870426885e-06,
      "loss": 0.0118,
      "step": 679
    },
    {
      "epoch": 28.26,
      "grad_norm": 1.0375019311904907,
      "learning_rate": 9.658262031842772e-06,
      "loss": 0.0107,
      "step": 680
    },
    {
      "epoch": 28.3,
      "grad_norm": 0.6827393174171448,
      "learning_rate": 9.657096278423093e-06,
      "loss": 0.0077,
      "step": 681
    },
    {
      "epoch": 28.34,
      "grad_norm": 0.9570422768592834,
      "learning_rate": 9.655928610647048e-06,
      "loss": 0.0089,
      "step": 682
    },
    {
      "epoch": 28.38,
      "grad_norm": 1.9821072816848755,
      "learning_rate": 9.65475902899462e-06,
      "loss": 0.0166,
      "step": 683
    },
    {
      "epoch": 28.43,
      "grad_norm": 2.278095006942749,
      "learning_rate": 9.653587533946583e-06,
      "loss": 0.0198,
      "step": 684
    },
    {
      "epoch": 28.47,
      "grad_norm": 2.624213695526123,
      "learning_rate": 9.65241412598449e-06,
      "loss": 0.0172,
      "step": 685
    },
    {
      "epoch": 28.51,
      "grad_norm": 1.8534846305847168,
      "learning_rate": 9.651238805590689e-06,
      "loss": 0.0118,
      "step": 686
    },
    {
      "epoch": 28.55,
      "grad_norm": 1.0202008485794067,
      "learning_rate": 9.650061573248305e-06,
      "loss": 0.0127,
      "step": 687
    },
    {
      "epoch": 28.59,
      "grad_norm": 1.9552991390228271,
      "learning_rate": 9.648882429441258e-06,
      "loss": 0.0268,
      "step": 688
    },
    {
      "epoch": 28.63,
      "grad_norm": 1.029455542564392,
      "learning_rate": 9.647701374654248e-06,
      "loss": 0.0094,
      "step": 689
    },
    {
      "epoch": 28.68,
      "grad_norm": 1.4034144878387451,
      "learning_rate": 9.64651840937276e-06,
      "loss": 0.013,
      "step": 690
    },
    {
      "epoch": 28.72,
      "grad_norm": 0.8455649018287659,
      "learning_rate": 9.645333534083072e-06,
      "loss": 0.0101,
      "step": 691
    },
    {
      "epoch": 28.76,
      "grad_norm": 0.9751182794570923,
      "learning_rate": 9.644146749272234e-06,
      "loss": 0.0107,
      "step": 692
    },
    {
      "epoch": 28.8,
      "grad_norm": 1.5705313682556152,
      "learning_rate": 9.642958055428094e-06,
      "loss": 0.0146,
      "step": 693
    },
    {
      "epoch": 28.84,
      "grad_norm": 1.102044939994812,
      "learning_rate": 9.641767453039276e-06,
      "loss": 0.0137,
      "step": 694
    },
    {
      "epoch": 28.88,
      "grad_norm": 0.9745062589645386,
      "learning_rate": 9.640574942595195e-06,
      "loss": 0.0115,
      "step": 695
    },
    {
      "epoch": 28.92,
      "grad_norm": 0.9177864789962769,
      "learning_rate": 9.639380524586044e-06,
      "loss": 0.0087,
      "step": 696
    },
    {
      "epoch": 28.97,
      "grad_norm": 1.088767647743225,
      "learning_rate": 9.638184199502805e-06,
      "loss": 0.0104,
      "step": 697
    },
    {
      "epoch": 29.01,
      "grad_norm": 1.2535587549209595,
      "learning_rate": 9.63698596783724e-06,
      "loss": 0.0104,
      "step": 698
    },
    {
      "epoch": 29.05,
      "grad_norm": 0.8040958046913147,
      "learning_rate": 9.635785830081898e-06,
      "loss": 0.0089,
      "step": 699
    },
    {
      "epoch": 29.09,
      "grad_norm": 0.5362353324890137,
      "learning_rate": 9.63458378673011e-06,
      "loss": 0.0083,
      "step": 700
    },
    {
      "epoch": 29.13,
      "grad_norm": 1.128970742225647,
      "learning_rate": 9.633379838275991e-06,
      "loss": 0.011,
      "step": 701
    },
    {
      "epoch": 29.17,
      "grad_norm": 0.9619750380516052,
      "learning_rate": 9.632173985214438e-06,
      "loss": 0.0106,
      "step": 702
    },
    {
      "epoch": 29.22,
      "grad_norm": 0.9359307289123535,
      "learning_rate": 9.630966228041132e-06,
      "loss": 0.0086,
      "step": 703
    },
    {
      "epoch": 29.26,
      "grad_norm": 1.4208073616027832,
      "learning_rate": 9.629756567252539e-06,
      "loss": 0.0208,
      "step": 704
    },
    {
      "epoch": 29.3,
      "grad_norm": 2.6859636306762695,
      "learning_rate": 9.6285450033459e-06,
      "loss": 0.0118,
      "step": 705
    },
    {
      "epoch": 29.34,
      "grad_norm": 2.5067977905273438,
      "learning_rate": 9.627331536819246e-06,
      "loss": 0.0151,
      "step": 706
    },
    {
      "epoch": 29.38,
      "grad_norm": 1.2864809036254883,
      "learning_rate": 9.626116168171386e-06,
      "loss": 0.0117,
      "step": 707
    },
    {
      "epoch": 29.42,
      "grad_norm": 1.1415596008300781,
      "learning_rate": 9.624898897901915e-06,
      "loss": 0.0136,
      "step": 708
    },
    {
      "epoch": 29.46,
      "grad_norm": 1.210037350654602,
      "learning_rate": 9.623679726511204e-06,
      "loss": 0.0147,
      "step": 709
    },
    {
      "epoch": 29.51,
      "grad_norm": 1.156491994857788,
      "learning_rate": 9.622458654500408e-06,
      "loss": 0.0128,
      "step": 710
    },
    {
      "epoch": 29.55,
      "grad_norm": 0.84107506275177,
      "learning_rate": 9.621235682371466e-06,
      "loss": 0.0077,
      "step": 711
    },
    {
      "epoch": 29.59,
      "grad_norm": 0.7424963712692261,
      "learning_rate": 9.620010810627093e-06,
      "loss": 0.0085,
      "step": 712
    },
    {
      "epoch": 29.63,
      "grad_norm": 0.6631203889846802,
      "learning_rate": 9.61878403977079e-06,
      "loss": 0.0106,
      "step": 713
    },
    {
      "epoch": 29.67,
      "grad_norm": 1.4885227680206299,
      "learning_rate": 9.617555370306834e-06,
      "loss": 0.0179,
      "step": 714
    },
    {
      "epoch": 29.71,
      "grad_norm": 1.7425193786621094,
      "learning_rate": 9.616324802740287e-06,
      "loss": 0.0142,
      "step": 715
    },
    {
      "epoch": 29.76,
      "grad_norm": 1.5519394874572754,
      "learning_rate": 9.615092337576987e-06,
      "loss": 0.0156,
      "step": 716
    },
    {
      "epoch": 29.8,
      "grad_norm": 0.713211178779602,
      "learning_rate": 9.613857975323553e-06,
      "loss": 0.0099,
      "step": 717
    },
    {
      "epoch": 29.84,
      "grad_norm": 1.7308355569839478,
      "learning_rate": 9.612621716487388e-06,
      "loss": 0.0152,
      "step": 718
    },
    {
      "epoch": 29.88,
      "grad_norm": 1.0860642194747925,
      "learning_rate": 9.611383561576668e-06,
      "loss": 0.0109,
      "step": 719
    },
    {
      "epoch": 29.92,
      "grad_norm": 1.6104263067245483,
      "learning_rate": 9.610143511100354e-06,
      "loss": 0.0123,
      "step": 720
    },
    {
      "epoch": 29.96,
      "grad_norm": 0.7168225049972534,
      "learning_rate": 9.608901565568181e-06,
      "loss": 0.0102,
      "step": 721
    },
    {
      "epoch": 30.01,
      "grad_norm": 0.7452203035354614,
      "learning_rate": 9.607657725490669e-06,
      "loss": 0.0092,
      "step": 722
    },
    {
      "epoch": 30.05,
      "grad_norm": 2.039771795272827,
      "learning_rate": 9.606411991379113e-06,
      "loss": 0.0141,
      "step": 723
    },
    {
      "epoch": 30.09,
      "grad_norm": 0.8953931331634521,
      "learning_rate": 9.605164363745588e-06,
      "loss": 0.0089,
      "step": 724
    },
    {
      "epoch": 30.13,
      "grad_norm": 0.754508912563324,
      "learning_rate": 9.603914843102941e-06,
      "loss": 0.009,
      "step": 725
    },
    {
      "epoch": 30.17,
      "grad_norm": 0.9882765412330627,
      "learning_rate": 9.602663429964811e-06,
      "loss": 0.0114,
      "step": 726
    },
    {
      "epoch": 30.21,
      "grad_norm": 1.1315590143203735,
      "learning_rate": 9.6014101248456e-06,
      "loss": 0.0103,
      "step": 727
    },
    {
      "epoch": 30.25,
      "grad_norm": 1.1987608671188354,
      "learning_rate": 9.600154928260499e-06,
      "loss": 0.011,
      "step": 728
    },
    {
      "epoch": 30.3,
      "grad_norm": 1.1036146879196167,
      "learning_rate": 9.59889784072547e-06,
      "loss": 0.0109,
      "step": 729
    },
    {
      "epoch": 30.34,
      "grad_norm": 1.1525851488113403,
      "learning_rate": 9.597638862757255e-06,
      "loss": 0.0114,
      "step": 730
    },
    {
      "epoch": 30.38,
      "grad_norm": 0.9747069478034973,
      "learning_rate": 9.596377994873369e-06,
      "loss": 0.0089,
      "step": 731
    },
    {
      "epoch": 30.42,
      "grad_norm": 0.7197263836860657,
      "learning_rate": 9.595115237592112e-06,
      "loss": 0.0063,
      "step": 732
    },
    {
      "epoch": 30.46,
      "grad_norm": 0.9754987359046936,
      "learning_rate": 9.593850591432552e-06,
      "loss": 0.0098,
      "step": 733
    },
    {
      "epoch": 30.5,
      "grad_norm": 0.5655142664909363,
      "learning_rate": 9.592584056914539e-06,
      "loss": 0.0055,
      "step": 734
    },
    {
      "epoch": 30.55,
      "grad_norm": 0.8814784288406372,
      "learning_rate": 9.591315634558698e-06,
      "loss": 0.0084,
      "step": 735
    },
    {
      "epoch": 30.59,
      "grad_norm": 1.8104071617126465,
      "learning_rate": 9.590045324886429e-06,
      "loss": 0.0083,
      "step": 736
    },
    {
      "epoch": 30.63,
      "grad_norm": 1.0722606182098389,
      "learning_rate": 9.588773128419907e-06,
      "loss": 0.0107,
      "step": 737
    },
    {
      "epoch": 30.67,
      "grad_norm": 1.4358139038085938,
      "learning_rate": 9.587499045682084e-06,
      "loss": 0.0097,
      "step": 738
    },
    {
      "epoch": 30.71,
      "grad_norm": 1.1493419408798218,
      "learning_rate": 9.58622307719669e-06,
      "loss": 0.0148,
      "step": 739
    },
    {
      "epoch": 30.75,
      "grad_norm": 1.0183762311935425,
      "learning_rate": 9.584945223488227e-06,
      "loss": 0.0137,
      "step": 740
    },
    {
      "epoch": 30.79,
      "grad_norm": 0.5421620011329651,
      "learning_rate": 9.58366548508197e-06,
      "loss": 0.0069,
      "step": 741
    },
    {
      "epoch": 30.84,
      "grad_norm": 0.49201908707618713,
      "learning_rate": 9.582383862503972e-06,
      "loss": 0.0048,
      "step": 742
    },
    {
      "epoch": 30.88,
      "grad_norm": 1.1267682313919067,
      "learning_rate": 9.581100356281059e-06,
      "loss": 0.0083,
      "step": 743
    },
    {
      "epoch": 30.92,
      "grad_norm": 2.5256001949310303,
      "learning_rate": 9.579814966940833e-06,
      "loss": 0.0169,
      "step": 744
    },
    {
      "epoch": 30.96,
      "grad_norm": 0.9799692630767822,
      "learning_rate": 9.57852769501167e-06,
      "loss": 0.0097,
      "step": 745
    },
    {
      "epoch": 31.0,
      "grad_norm": 1.4923354387283325,
      "learning_rate": 9.577238541022718e-06,
      "loss": 0.0179,
      "step": 746
    },
    {
      "epoch": 31.04,
      "grad_norm": 1.0840104818344116,
      "learning_rate": 9.575947505503897e-06,
      "loss": 0.0069,
      "step": 747
    },
    {
      "epoch": 31.09,
      "grad_norm": 1.3540904521942139,
      "learning_rate": 9.574654588985907e-06,
      "loss": 0.0108,
      "step": 748
    },
    {
      "epoch": 31.13,
      "grad_norm": 1.278752326965332,
      "learning_rate": 9.573359792000214e-06,
      "loss": 0.0098,
      "step": 749
    },
    {
      "epoch": 31.17,
      "grad_norm": 0.8181701898574829,
      "learning_rate": 9.572063115079063e-06,
      "loss": 0.0094,
      "step": 750
    },
    {
      "epoch": 31.21,
      "grad_norm": 0.7418901920318604,
      "learning_rate": 9.570764558755466e-06,
      "loss": 0.0072,
      "step": 751
    },
    {
      "epoch": 31.25,
      "grad_norm": 0.8720927238464355,
      "learning_rate": 9.569464123563212e-06,
      "loss": 0.0106,
      "step": 752
    },
    {
      "epoch": 31.29,
      "grad_norm": 1.119044303894043,
      "learning_rate": 9.568161810036861e-06,
      "loss": 0.0073,
      "step": 753
    },
    {
      "epoch": 31.34,
      "grad_norm": 1.0349235534667969,
      "learning_rate": 9.566857618711744e-06,
      "loss": 0.0106,
      "step": 754
    },
    {
      "epoch": 31.38,
      "grad_norm": 1.5950462818145752,
      "learning_rate": 9.565551550123967e-06,
      "loss": 0.0111,
      "step": 755
    },
    {
      "epoch": 31.42,
      "grad_norm": 0.8816194534301758,
      "learning_rate": 9.564243604810401e-06,
      "loss": 0.0105,
      "step": 756
    },
    {
      "epoch": 31.46,
      "grad_norm": 1.0832172632217407,
      "learning_rate": 9.562933783308698e-06,
      "loss": 0.0123,
      "step": 757
    },
    {
      "epoch": 31.5,
      "grad_norm": 1.1258186101913452,
      "learning_rate": 9.561622086157273e-06,
      "loss": 0.0116,
      "step": 758
    },
    {
      "epoch": 31.54,
      "grad_norm": 1.1548442840576172,
      "learning_rate": 9.560308513895315e-06,
      "loss": 0.0104,
      "step": 759
    },
    {
      "epoch": 31.58,
      "grad_norm": 0.7487155795097351,
      "learning_rate": 9.558993067062785e-06,
      "loss": 0.0058,
      "step": 760
    },
    {
      "epoch": 31.63,
      "grad_norm": 1.027305006980896,
      "learning_rate": 9.557675746200414e-06,
      "loss": 0.0118,
      "step": 761
    },
    {
      "epoch": 31.67,
      "grad_norm": 0.8355854153633118,
      "learning_rate": 9.556356551849702e-06,
      "loss": 0.0087,
      "step": 762
    },
    {
      "epoch": 31.71,
      "grad_norm": 0.6267271041870117,
      "learning_rate": 9.55503548455292e-06,
      "loss": 0.0077,
      "step": 763
    },
    {
      "epoch": 31.75,
      "grad_norm": 0.8113292455673218,
      "learning_rate": 9.553712544853109e-06,
      "loss": 0.0076,
      "step": 764
    },
    {
      "epoch": 31.79,
      "grad_norm": 1.0985575914382935,
      "learning_rate": 9.552387733294081e-06,
      "loss": 0.0109,
      "step": 765
    },
    {
      "epoch": 31.83,
      "grad_norm": 0.8696607947349548,
      "learning_rate": 9.551061050420413e-06,
      "loss": 0.0082,
      "step": 766
    },
    {
      "epoch": 31.88,
      "grad_norm": 1.013948917388916,
      "learning_rate": 9.549732496777455e-06,
      "loss": 0.0129,
      "step": 767
    },
    {
      "epoch": 31.92,
      "grad_norm": 0.9310299754142761,
      "learning_rate": 9.548402072911328e-06,
      "loss": 0.0071,
      "step": 768
    },
    {
      "epoch": 31.96,
      "grad_norm": 1.4424971342086792,
      "learning_rate": 9.547069779368915e-06,
      "loss": 0.0126,
      "step": 769
    },
    {
      "epoch": 32.0,
      "grad_norm": 0.7199421525001526,
      "learning_rate": 9.545735616697875e-06,
      "loss": 0.007,
      "step": 770
    },
    {
      "epoch": 32.04,
      "grad_norm": 0.6327075362205505,
      "learning_rate": 9.54439958544663e-06,
      "loss": 0.0042,
      "step": 771
    },
    {
      "epoch": 32.08,
      "grad_norm": 1.249509334564209,
      "learning_rate": 9.543061686164374e-06,
      "loss": 0.0102,
      "step": 772
    },
    {
      "epoch": 32.12,
      "grad_norm": 1.1624044179916382,
      "learning_rate": 9.541721919401063e-06,
      "loss": 0.0122,
      "step": 773
    },
    {
      "epoch": 32.17,
      "grad_norm": 0.7749924063682556,
      "learning_rate": 9.540380285707426e-06,
      "loss": 0.0068,
      "step": 774
    },
    {
      "epoch": 32.21,
      "grad_norm": 1.3668361902236938,
      "learning_rate": 9.539036785634961e-06,
      "loss": 0.0094,
      "step": 775
    },
    {
      "epoch": 32.25,
      "grad_norm": 0.7427449822425842,
      "learning_rate": 9.537691419735929e-06,
      "loss": 0.0073,
      "step": 776
    },
    {
      "epoch": 32.29,
      "grad_norm": 0.9247550964355469,
      "learning_rate": 9.536344188563355e-06,
      "loss": 0.0109,
      "step": 777
    },
    {
      "epoch": 32.33,
      "grad_norm": 1.2574918270111084,
      "learning_rate": 9.534995092671038e-06,
      "loss": 0.0121,
      "step": 778
    },
    {
      "epoch": 32.37,
      "grad_norm": 1.3353062868118286,
      "learning_rate": 9.533644132613542e-06,
      "loss": 0.0137,
      "step": 779
    },
    {
      "epoch": 32.42,
      "grad_norm": 0.6455280780792236,
      "learning_rate": 9.532291308946191e-06,
      "loss": 0.0063,
      "step": 780
    },
    {
      "epoch": 32.46,
      "grad_norm": 0.8528560400009155,
      "learning_rate": 9.530936622225084e-06,
      "loss": 0.0145,
      "step": 781
    },
    {
      "epoch": 32.5,
      "grad_norm": 0.6218437552452087,
      "learning_rate": 9.52958007300708e-06,
      "loss": 0.0062,
      "step": 782
    },
    {
      "epoch": 32.54,
      "grad_norm": 0.7169654965400696,
      "learning_rate": 9.528221661849806e-06,
      "loss": 0.0071,
      "step": 783
    },
    {
      "epoch": 32.58,
      "grad_norm": 0.9819614291191101,
      "learning_rate": 9.526861389311652e-06,
      "loss": 0.0097,
      "step": 784
    },
    {
      "epoch": 32.62,
      "grad_norm": 2.9582102298736572,
      "learning_rate": 9.525499255951775e-06,
      "loss": 0.0222,
      "step": 785
    },
    {
      "epoch": 32.66,
      "grad_norm": 0.7620760202407837,
      "learning_rate": 9.524135262330098e-06,
      "loss": 0.0076,
      "step": 786
    },
    {
      "epoch": 32.71,
      "grad_norm": 0.485625684261322,
      "learning_rate": 9.522769409007306e-06,
      "loss": 0.0048,
      "step": 787
    },
    {
      "epoch": 32.75,
      "grad_norm": 0.8294665217399597,
      "learning_rate": 9.52140169654485e-06,
      "loss": 0.008,
      "step": 788
    },
    {
      "epoch": 32.79,
      "grad_norm": 1.8379528522491455,
      "learning_rate": 9.520032125504942e-06,
      "loss": 0.0251,
      "step": 789
    },
    {
      "epoch": 32.83,
      "grad_norm": 1.438228726387024,
      "learning_rate": 9.518660696450567e-06,
      "loss": 0.0087,
      "step": 790
    },
    {
      "epoch": 32.87,
      "grad_norm": 0.636202335357666,
      "learning_rate": 9.517287409945463e-06,
      "loss": 0.0064,
      "step": 791
    },
    {
      "epoch": 32.91,
      "grad_norm": 0.967237114906311,
      "learning_rate": 9.51591226655414e-06,
      "loss": 0.0074,
      "step": 792
    },
    {
      "epoch": 32.96,
      "grad_norm": 0.8096508383750916,
      "learning_rate": 9.514535266841863e-06,
      "loss": 0.0069,
      "step": 793
    },
    {
      "epoch": 33.0,
      "grad_norm": 1.2588577270507812,
      "learning_rate": 9.513156411374667e-06,
      "loss": 0.0124,
      "step": 794
    },
    {
      "epoch": 33.04,
      "grad_norm": 0.44200342893600464,
      "learning_rate": 9.511775700719347e-06,
      "loss": 0.0052,
      "step": 795
    },
    {
      "epoch": 33.08,
      "grad_norm": 1.0525305271148682,
      "learning_rate": 9.51039313544346e-06,
      "loss": 0.008,
      "step": 796
    },
    {
      "epoch": 33.12,
      "grad_norm": 0.9538615345954895,
      "learning_rate": 9.509008716115328e-06,
      "loss": 0.0066,
      "step": 797
    },
    {
      "epoch": 33.16,
      "grad_norm": 1.053529977798462,
      "learning_rate": 9.507622443304036e-06,
      "loss": 0.01,
      "step": 798
    },
    {
      "epoch": 33.21,
      "grad_norm": 0.7784187197685242,
      "learning_rate": 9.506234317579422e-06,
      "loss": 0.0056,
      "step": 799
    },
    {
      "epoch": 33.25,
      "grad_norm": 0.31162238121032715,
      "learning_rate": 9.504844339512096e-06,
      "loss": 0.0036,
      "step": 800
    },
    {
      "epoch": 33.29,
      "grad_norm": 2.2046916484832764,
      "learning_rate": 9.503452509673426e-06,
      "loss": 0.0118,
      "step": 801
    },
    {
      "epoch": 33.33,
      "grad_norm": 1.0721851587295532,
      "learning_rate": 9.50205882863554e-06,
      "loss": 0.0088,
      "step": 802
    },
    {
      "epoch": 33.37,
      "grad_norm": 1.0314596891403198,
      "learning_rate": 9.500663296971324e-06,
      "loss": 0.0084,
      "step": 803
    },
    {
      "epoch": 33.41,
      "grad_norm": 1.2152366638183594,
      "learning_rate": 9.499265915254434e-06,
      "loss": 0.0084,
      "step": 804
    },
    {
      "epoch": 33.45,
      "grad_norm": 0.731601357460022,
      "learning_rate": 9.497866684059278e-06,
      "loss": 0.0087,
      "step": 805
    },
    {
      "epoch": 33.5,
      "grad_norm": 0.9377961754798889,
      "learning_rate": 9.496465603961028e-06,
      "loss": 0.0076,
      "step": 806
    },
    {
      "epoch": 33.54,
      "grad_norm": 1.7971141338348389,
      "learning_rate": 9.495062675535614e-06,
      "loss": 0.0127,
      "step": 807
    },
    {
      "epoch": 33.58,
      "grad_norm": 0.5018189549446106,
      "learning_rate": 9.493657899359727e-06,
      "loss": 0.0049,
      "step": 808
    },
    {
      "epoch": 33.62,
      "grad_norm": 1.468942642211914,
      "learning_rate": 9.492251276010817e-06,
      "loss": 0.0128,
      "step": 809
    },
    {
      "epoch": 33.66,
      "grad_norm": 1.1482133865356445,
      "learning_rate": 9.490842806067095e-06,
      "loss": 0.007,
      "step": 810
    },
    {
      "epoch": 33.7,
      "grad_norm": 1.2444615364074707,
      "learning_rate": 9.489432490107531e-06,
      "loss": 0.0079,
      "step": 811
    },
    {
      "epoch": 33.75,
      "grad_norm": 0.9927482604980469,
      "learning_rate": 9.488020328711851e-06,
      "loss": 0.0086,
      "step": 812
    },
    {
      "epoch": 33.79,
      "grad_norm": 0.7343482971191406,
      "learning_rate": 9.48660632246054e-06,
      "loss": 0.0077,
      "step": 813
    },
    {
      "epoch": 33.83,
      "grad_norm": 1.474161148071289,
      "learning_rate": 9.485190471934845e-06,
      "loss": 0.0086,
      "step": 814
    },
    {
      "epoch": 33.87,
      "grad_norm": 0.727411150932312,
      "learning_rate": 9.483772777716767e-06,
      "loss": 0.0077,
      "step": 815
    },
    {
      "epoch": 33.91,
      "grad_norm": 0.813332200050354,
      "learning_rate": 9.482353240389066e-06,
      "loss": 0.0094,
      "step": 816
    },
    {
      "epoch": 33.95,
      "grad_norm": 0.80517578125,
      "learning_rate": 9.480931860535261e-06,
      "loss": 0.0072,
      "step": 817
    },
    {
      "epoch": 33.99,
      "grad_norm": 1.1704535484313965,
      "learning_rate": 9.479508638739629e-06,
      "loss": 0.0128,
      "step": 818
    },
    {
      "epoch": 34.04,
      "grad_norm": 1.2833203077316284,
      "learning_rate": 9.4780835755872e-06,
      "loss": 0.009,
      "step": 819
    },
    {
      "epoch": 34.08,
      "grad_norm": 0.4536212980747223,
      "learning_rate": 9.476656671663766e-06,
      "loss": 0.006,
      "step": 820
    },
    {
      "epoch": 34.12,
      "grad_norm": 1.4020222425460815,
      "learning_rate": 9.475227927555873e-06,
      "loss": 0.0077,
      "step": 821
    },
    {
      "epoch": 34.16,
      "grad_norm": 0.6073265671730042,
      "learning_rate": 9.473797343850822e-06,
      "loss": 0.0057,
      "step": 822
    },
    {
      "epoch": 34.2,
      "grad_norm": 0.6213653683662415,
      "learning_rate": 9.472364921136674e-06,
      "loss": 0.006,
      "step": 823
    },
    {
      "epoch": 34.24,
      "grad_norm": 1.1333403587341309,
      "learning_rate": 9.470930660002241e-06,
      "loss": 0.0113,
      "step": 824
    },
    {
      "epoch": 34.29,
      "grad_norm": 1.0016417503356934,
      "learning_rate": 9.469494561037097e-06,
      "loss": 0.0087,
      "step": 825
    },
    {
      "epoch": 34.33,
      "grad_norm": 0.7193984389305115,
      "learning_rate": 9.468056624831568e-06,
      "loss": 0.0063,
      "step": 826
    },
    {
      "epoch": 34.37,
      "grad_norm": 0.7615434527397156,
      "learning_rate": 9.466616851976734e-06,
      "loss": 0.0065,
      "step": 827
    },
    {
      "epoch": 34.41,
      "grad_norm": 0.9232017397880554,
      "learning_rate": 9.465175243064428e-06,
      "loss": 0.0131,
      "step": 828
    },
    {
      "epoch": 34.45,
      "grad_norm": 1.1338882446289062,
      "learning_rate": 9.463731798687246e-06,
      "loss": 0.0133,
      "step": 829
    },
    {
      "epoch": 34.49,
      "grad_norm": 0.7923070192337036,
      "learning_rate": 9.462286519438531e-06,
      "loss": 0.0086,
      "step": 830
    },
    {
      "epoch": 34.54,
      "grad_norm": 1.2135180234909058,
      "learning_rate": 9.46083940591238e-06,
      "loss": 0.0089,
      "step": 831
    },
    {
      "epoch": 34.58,
      "grad_norm": 1.087720513343811,
      "learning_rate": 9.459390458703654e-06,
      "loss": 0.0088,
      "step": 832
    },
    {
      "epoch": 34.62,
      "grad_norm": 0.8176458477973938,
      "learning_rate": 9.457939678407955e-06,
      "loss": 0.0105,
      "step": 833
    },
    {
      "epoch": 34.66,
      "grad_norm": 1.178382396697998,
      "learning_rate": 9.456487065621646e-06,
      "loss": 0.0076,
      "step": 834
    },
    {
      "epoch": 34.7,
      "grad_norm": 1.1438026428222656,
      "learning_rate": 9.45503262094184e-06,
      "loss": 0.0119,
      "step": 835
    },
    {
      "epoch": 34.74,
      "grad_norm": 1.3194128274917603,
      "learning_rate": 9.453576344966404e-06,
      "loss": 0.0151,
      "step": 836
    },
    {
      "epoch": 34.78,
      "grad_norm": 1.0376862287521362,
      "learning_rate": 9.452118238293961e-06,
      "loss": 0.0081,
      "step": 837
    },
    {
      "epoch": 34.83,
      "grad_norm": 0.6065642833709717,
      "learning_rate": 9.45065830152388e-06,
      "loss": 0.007,
      "step": 838
    },
    {
      "epoch": 34.87,
      "grad_norm": 1.3718341588974,
      "learning_rate": 9.449196535256286e-06,
      "loss": 0.0106,
      "step": 839
    },
    {
      "epoch": 34.91,
      "grad_norm": 2.795884609222412,
      "learning_rate": 9.44773294009206e-06,
      "loss": 0.0129,
      "step": 840
    },
    {
      "epoch": 34.95,
      "grad_norm": 0.4235377013683319,
      "learning_rate": 9.446267516632826e-06,
      "loss": 0.0051,
      "step": 841
    },
    {
      "epoch": 34.99,
      "grad_norm": 0.7945225238800049,
      "learning_rate": 9.444800265480968e-06,
      "loss": 0.0073,
      "step": 842
    },
    {
      "epoch": 35.03,
      "grad_norm": 0.8985181450843811,
      "learning_rate": 9.443331187239613e-06,
      "loss": 0.0102,
      "step": 843
    },
    {
      "epoch": 35.08,
      "grad_norm": 0.5202003121376038,
      "learning_rate": 9.441860282512648e-06,
      "loss": 0.0063,
      "step": 844
    },
    {
      "epoch": 35.12,
      "grad_norm": 0.7869227528572083,
      "learning_rate": 9.440387551904705e-06,
      "loss": 0.006,
      "step": 845
    },
    {
      "epoch": 35.16,
      "grad_norm": 0.6773049235343933,
      "learning_rate": 9.438912996021165e-06,
      "loss": 0.007,
      "step": 846
    },
    {
      "epoch": 35.2,
      "grad_norm": 0.6256711483001709,
      "learning_rate": 9.437436615468167e-06,
      "loss": 0.0058,
      "step": 847
    },
    {
      "epoch": 35.24,
      "grad_norm": 0.7342522144317627,
      "learning_rate": 9.435958410852593e-06,
      "loss": 0.0068,
      "step": 848
    },
    {
      "epoch": 35.28,
      "grad_norm": 0.8841196298599243,
      "learning_rate": 9.434478382782075e-06,
      "loss": 0.0059,
      "step": 849
    },
    {
      "epoch": 35.32,
      "grad_norm": 1.599441409111023,
      "learning_rate": 9.432996531865001e-06,
      "loss": 0.0069,
      "step": 850
    },
    {
      "epoch": 35.37,
      "grad_norm": 1.8472797870635986,
      "learning_rate": 9.431512858710501e-06,
      "loss": 0.016,
      "step": 851
    },
    {
      "epoch": 35.41,
      "grad_norm": 1.0224254131317139,
      "learning_rate": 9.430027363928458e-06,
      "loss": 0.0073,
      "step": 852
    },
    {
      "epoch": 35.45,
      "grad_norm": 1.5509021282196045,
      "learning_rate": 9.428540048129502e-06,
      "loss": 0.011,
      "step": 853
    },
    {
      "epoch": 35.49,
      "grad_norm": 0.42956995964050293,
      "learning_rate": 9.427050911925014e-06,
      "loss": 0.008,
      "step": 854
    },
    {
      "epoch": 35.53,
      "grad_norm": 2.8247532844543457,
      "learning_rate": 9.425559955927118e-06,
      "loss": 0.0265,
      "step": 855
    },
    {
      "epoch": 35.57,
      "grad_norm": 1.133683443069458,
      "learning_rate": 9.424067180748692e-06,
      "loss": 0.0105,
      "step": 856
    },
    {
      "epoch": 35.62,
      "grad_norm": 0.4986560642719269,
      "learning_rate": 9.422572587003362e-06,
      "loss": 0.0062,
      "step": 857
    },
    {
      "epoch": 35.66,
      "grad_norm": 0.554783046245575,
      "learning_rate": 9.421076175305496e-06,
      "loss": 0.0045,
      "step": 858
    },
    {
      "epoch": 35.7,
      "grad_norm": 0.45664623379707336,
      "learning_rate": 9.419577946270213e-06,
      "loss": 0.0041,
      "step": 859
    },
    {
      "epoch": 35.74,
      "grad_norm": 0.8961682915687561,
      "learning_rate": 9.418077900513377e-06,
      "loss": 0.0085,
      "step": 860
    },
    {
      "epoch": 35.78,
      "grad_norm": 1.735295057296753,
      "learning_rate": 9.416576038651602e-06,
      "loss": 0.0088,
      "step": 861
    },
    {
      "epoch": 35.82,
      "grad_norm": 0.947220504283905,
      "learning_rate": 9.415072361302247e-06,
      "loss": 0.0129,
      "step": 862
    },
    {
      "epoch": 35.86,
      "grad_norm": 0.7217110991477966,
      "learning_rate": 9.413566869083417e-06,
      "loss": 0.008,
      "step": 863
    },
    {
      "epoch": 35.91,
      "grad_norm": 1.2819325923919678,
      "learning_rate": 9.41205956261396e-06,
      "loss": 0.0204,
      "step": 864
    },
    {
      "epoch": 35.95,
      "grad_norm": 1.3079439401626587,
      "learning_rate": 9.410550442513475e-06,
      "loss": 0.0134,
      "step": 865
    },
    {
      "epoch": 35.99,
      "grad_norm": 0.6591728925704956,
      "learning_rate": 9.409039509402305e-06,
      "loss": 0.0119,
      "step": 866
    },
    {
      "epoch": 36.03,
      "grad_norm": 0.9332252144813538,
      "learning_rate": 9.407526763901538e-06,
      "loss": 0.0082,
      "step": 867
    },
    {
      "epoch": 36.07,
      "grad_norm": 0.666333019733429,
      "learning_rate": 9.406012206633004e-06,
      "loss": 0.0076,
      "step": 868
    },
    {
      "epoch": 36.11,
      "grad_norm": 0.26882404088974,
      "learning_rate": 9.40449583821928e-06,
      "loss": 0.0034,
      "step": 869
    },
    {
      "epoch": 36.16,
      "grad_norm": 0.7228793501853943,
      "learning_rate": 9.40297765928369e-06,
      "loss": 0.0083,
      "step": 870
    },
    {
      "epoch": 36.2,
      "grad_norm": 0.8631962537765503,
      "learning_rate": 9.4014576704503e-06,
      "loss": 0.0074,
      "step": 871
    },
    {
      "epoch": 36.24,
      "grad_norm": 0.8106330633163452,
      "learning_rate": 9.39993587234392e-06,
      "loss": 0.0072,
      "step": 872
    },
    {
      "epoch": 36.28,
      "grad_norm": 0.7272655367851257,
      "learning_rate": 9.398412265590102e-06,
      "loss": 0.0056,
      "step": 873
    },
    {
      "epoch": 36.32,
      "grad_norm": 1.1923985481262207,
      "learning_rate": 9.396886850815144e-06,
      "loss": 0.0093,
      "step": 874
    },
    {
      "epoch": 36.36,
      "grad_norm": 0.6295662522315979,
      "learning_rate": 9.395359628646087e-06,
      "loss": 0.0057,
      "step": 875
    },
    {
      "epoch": 36.41,
      "grad_norm": 1.6961685419082642,
      "learning_rate": 9.393830599710714e-06,
      "loss": 0.0087,
      "step": 876
    },
    {
      "epoch": 36.45,
      "grad_norm": 1.7092015743255615,
      "learning_rate": 9.39229976463755e-06,
      "loss": 0.0167,
      "step": 877
    },
    {
      "epoch": 36.49,
      "grad_norm": 1.2325984239578247,
      "learning_rate": 9.390767124055866e-06,
      "loss": 0.0094,
      "step": 878
    },
    {
      "epoch": 36.53,
      "grad_norm": 0.6677331924438477,
      "learning_rate": 9.38923267859567e-06,
      "loss": 0.0042,
      "step": 879
    },
    {
      "epoch": 36.57,
      "grad_norm": 0.7840896844863892,
      "learning_rate": 9.387696428887715e-06,
      "loss": 0.0064,
      "step": 880
    },
    {
      "epoch": 36.61,
      "grad_norm": 0.4278734028339386,
      "learning_rate": 9.386158375563497e-06,
      "loss": 0.0056,
      "step": 881
    },
    {
      "epoch": 36.65,
      "grad_norm": 0.5046305060386658,
      "learning_rate": 9.384618519255252e-06,
      "loss": 0.004,
      "step": 882
    },
    {
      "epoch": 36.7,
      "grad_norm": 1.1228621006011963,
      "learning_rate": 9.383076860595952e-06,
      "loss": 0.0071,
      "step": 883
    },
    {
      "epoch": 36.74,
      "grad_norm": 0.8903341889381409,
      "learning_rate": 9.381533400219319e-06,
      "loss": 0.0069,
      "step": 884
    },
    {
      "epoch": 36.78,
      "grad_norm": 1.037611484527588,
      "learning_rate": 9.37998813875981e-06,
      "loss": 0.0156,
      "step": 885
    },
    {
      "epoch": 36.82,
      "grad_norm": 1.308443307876587,
      "learning_rate": 9.378441076852624e-06,
      "loss": 0.0143,
      "step": 886
    },
    {
      "epoch": 36.86,
      "grad_norm": 0.7414575815200806,
      "learning_rate": 9.3768922151337e-06,
      "loss": 0.0042,
      "step": 887
    },
    {
      "epoch": 36.9,
      "grad_norm": 0.5652013421058655,
      "learning_rate": 9.375341554239716e-06,
      "loss": 0.0049,
      "step": 888
    },
    {
      "epoch": 36.95,
      "grad_norm": 1.0897647142410278,
      "learning_rate": 9.37378909480809e-06,
      "loss": 0.0098,
      "step": 889
    },
    {
      "epoch": 36.99,
      "grad_norm": 0.6847667694091797,
      "learning_rate": 9.372234837476979e-06,
      "loss": 0.0083,
      "step": 890
    },
    {
      "epoch": 37.03,
      "grad_norm": 0.7117094397544861,
      "learning_rate": 9.37067878288528e-06,
      "loss": 0.005,
      "step": 891
    },
    {
      "epoch": 37.07,
      "grad_norm": 0.6587062478065491,
      "learning_rate": 9.369120931672631e-06,
      "loss": 0.0075,
      "step": 892
    },
    {
      "epoch": 37.11,
      "grad_norm": 0.3700617551803589,
      "learning_rate": 9.3675612844794e-06,
      "loss": 0.0036,
      "step": 893
    },
    {
      "epoch": 37.15,
      "grad_norm": 0.4236878454685211,
      "learning_rate": 9.365999841946703e-06,
      "loss": 0.0037,
      "step": 894
    },
    {
      "epoch": 37.19,
      "grad_norm": 1.7661051750183105,
      "learning_rate": 9.364436604716389e-06,
      "loss": 0.0085,
      "step": 895
    },
    {
      "epoch": 37.24,
      "grad_norm": 0.7996834516525269,
      "learning_rate": 9.362871573431046e-06,
      "loss": 0.0082,
      "step": 896
    },
    {
      "epoch": 37.28,
      "grad_norm": 0.6906037926673889,
      "learning_rate": 9.361304748734e-06,
      "loss": 0.0052,
      "step": 897
    },
    {
      "epoch": 37.32,
      "grad_norm": 0.77424156665802,
      "learning_rate": 9.359736131269312e-06,
      "loss": 0.0042,
      "step": 898
    },
    {
      "epoch": 37.36,
      "grad_norm": 1.054663896560669,
      "learning_rate": 9.358165721681782e-06,
      "loss": 0.0103,
      "step": 899
    },
    {
      "epoch": 37.4,
      "grad_norm": 0.4215148687362671,
      "learning_rate": 9.356593520616948e-06,
      "loss": 0.0041,
      "step": 900
    },
    {
      "epoch": 37.44,
      "grad_norm": 1.0583938360214233,
      "learning_rate": 9.355019528721079e-06,
      "loss": 0.0115,
      "step": 901
    },
    {
      "epoch": 37.49,
      "grad_norm": 0.709509015083313,
      "learning_rate": 9.353443746641185e-06,
      "loss": 0.0054,
      "step": 902
    },
    {
      "epoch": 37.53,
      "grad_norm": 0.731120765209198,
      "learning_rate": 9.351866175025011e-06,
      "loss": 0.0056,
      "step": 903
    },
    {
      "epoch": 37.57,
      "grad_norm": 1.2613446712493896,
      "learning_rate": 9.350286814521037e-06,
      "loss": 0.0054,
      "step": 904
    },
    {
      "epoch": 37.61,
      "grad_norm": 1.0165766477584839,
      "learning_rate": 9.348705665778479e-06,
      "loss": 0.0055,
      "step": 905
    },
    {
      "epoch": 37.65,
      "grad_norm": 1.1384865045547485,
      "learning_rate": 9.347122729447284e-06,
      "loss": 0.0107,
      "step": 906
    },
    {
      "epoch": 37.69,
      "grad_norm": 1.316459059715271,
      "learning_rate": 9.345538006178143e-06,
      "loss": 0.0089,
      "step": 907
    },
    {
      "epoch": 37.74,
      "grad_norm": 0.8130612373352051,
      "learning_rate": 9.343951496622473e-06,
      "loss": 0.006,
      "step": 908
    },
    {
      "epoch": 37.78,
      "grad_norm": 0.8442455530166626,
      "learning_rate": 9.342363201432425e-06,
      "loss": 0.0055,
      "step": 909
    },
    {
      "epoch": 37.82,
      "grad_norm": 1.780215859413147,
      "learning_rate": 9.340773121260893e-06,
      "loss": 0.0078,
      "step": 910
    },
    {
      "epoch": 37.86,
      "grad_norm": 0.9565002918243408,
      "learning_rate": 9.339181256761495e-06,
      "loss": 0.0095,
      "step": 911
    },
    {
      "epoch": 37.9,
      "grad_norm": 0.6793904900550842,
      "learning_rate": 9.337587608588588e-06,
      "loss": 0.0055,
      "step": 912
    },
    {
      "epoch": 37.94,
      "grad_norm": 1.5327448844909668,
      "learning_rate": 9.335992177397261e-06,
      "loss": 0.0175,
      "step": 913
    },
    {
      "epoch": 37.98,
      "grad_norm": 0.9416285157203674,
      "learning_rate": 9.334394963843334e-06,
      "loss": 0.0043,
      "step": 914
    },
    {
      "epoch": 38.03,
      "grad_norm": 1.7715821266174316,
      "learning_rate": 9.33279596858336e-06,
      "loss": 0.014,
      "step": 915
    },
    {
      "epoch": 38.07,
      "grad_norm": 0.183228999376297,
      "learning_rate": 9.33119519227463e-06,
      "loss": 0.003,
      "step": 916
    },
    {
      "epoch": 38.11,
      "grad_norm": 0.5343000888824463,
      "learning_rate": 9.32959263557516e-06,
      "loss": 0.005,
      "step": 917
    },
    {
      "epoch": 38.15,
      "grad_norm": 1.2401533126831055,
      "learning_rate": 9.327988299143697e-06,
      "loss": 0.0085,
      "step": 918
    },
    {
      "epoch": 38.19,
      "grad_norm": 0.7383843064308167,
      "learning_rate": 9.326382183639731e-06,
      "loss": 0.0078,
      "step": 919
    },
    {
      "epoch": 38.23,
      "grad_norm": 0.5391771793365479,
      "learning_rate": 9.324774289723469e-06,
      "loss": 0.0046,
      "step": 920
    },
    {
      "epoch": 38.28,
      "grad_norm": 0.8553276658058167,
      "learning_rate": 9.323164618055858e-06,
      "loss": 0.0064,
      "step": 921
    },
    {
      "epoch": 38.32,
      "grad_norm": 1.5869052410125732,
      "learning_rate": 9.321553169298571e-06,
      "loss": 0.0202,
      "step": 922
    },
    {
      "epoch": 38.36,
      "grad_norm": 0.43874117732048035,
      "learning_rate": 9.319939944114018e-06,
      "loss": 0.0038,
      "step": 923
    },
    {
      "epoch": 38.4,
      "grad_norm": 1.389291524887085,
      "learning_rate": 9.318324943165331e-06,
      "loss": 0.0084,
      "step": 924
    },
    {
      "epoch": 38.44,
      "grad_norm": 1.1565704345703125,
      "learning_rate": 9.316708167116377e-06,
      "loss": 0.0092,
      "step": 925
    },
    {
      "epoch": 38.48,
      "grad_norm": 1.029412865638733,
      "learning_rate": 9.315089616631752e-06,
      "loss": 0.0103,
      "step": 926
    },
    {
      "epoch": 38.52,
      "grad_norm": 2.017050266265869,
      "learning_rate": 9.313469292376781e-06,
      "loss": 0.0123,
      "step": 927
    },
    {
      "epoch": 38.57,
      "grad_norm": 0.9231042265892029,
      "learning_rate": 9.311847195017518e-06,
      "loss": 0.0064,
      "step": 928
    },
    {
      "epoch": 38.61,
      "grad_norm": 2.2347769737243652,
      "learning_rate": 9.310223325220747e-06,
      "loss": 0.0235,
      "step": 929
    },
    {
      "epoch": 38.65,
      "grad_norm": 0.4662861227989197,
      "learning_rate": 9.308597683653976e-06,
      "loss": 0.0058,
      "step": 930
    },
    {
      "epoch": 38.69,
      "grad_norm": 0.8839818835258484,
      "learning_rate": 9.306970270985449e-06,
      "loss": 0.0101,
      "step": 931
    },
    {
      "epoch": 38.73,
      "grad_norm": 1.33241605758667,
      "learning_rate": 9.30534108788413e-06,
      "loss": 0.0131,
      "step": 932
    },
    {
      "epoch": 38.77,
      "grad_norm": 0.9630036950111389,
      "learning_rate": 9.30371013501972e-06,
      "loss": 0.0058,
      "step": 933
    },
    {
      "epoch": 38.82,
      "grad_norm": 0.6687218546867371,
      "learning_rate": 9.302077413062636e-06,
      "loss": 0.0076,
      "step": 934
    },
    {
      "epoch": 38.86,
      "grad_norm": 0.9307408928871155,
      "learning_rate": 9.300442922684033e-06,
      "loss": 0.0049,
      "step": 935
    },
    {
      "epoch": 38.9,
      "grad_norm": 0.6535394787788391,
      "learning_rate": 9.298806664555783e-06,
      "loss": 0.0067,
      "step": 936
    },
    {
      "epoch": 38.94,
      "grad_norm": 0.84636390209198,
      "learning_rate": 9.297168639350498e-06,
      "loss": 0.0076,
      "step": 937
    },
    {
      "epoch": 38.98,
      "grad_norm": 1.2478293180465698,
      "learning_rate": 9.295528847741501e-06,
      "loss": 0.0114,
      "step": 938
    },
    {
      "epoch": 39.02,
      "grad_norm": 1.1879029273986816,
      "learning_rate": 9.293887290402853e-06,
      "loss": 0.01,
      "step": 939
    },
    {
      "epoch": 39.06,
      "grad_norm": 0.6606095433235168,
      "learning_rate": 9.292243968009332e-06,
      "loss": 0.0077,
      "step": 940
    },
    {
      "epoch": 39.11,
      "grad_norm": 0.7002545595169067,
      "learning_rate": 9.290598881236448e-06,
      "loss": 0.0044,
      "step": 941
    },
    {
      "epoch": 39.15,
      "grad_norm": 0.9115826487541199,
      "learning_rate": 9.288952030760434e-06,
      "loss": 0.0074,
      "step": 942
    },
    {
      "epoch": 39.19,
      "grad_norm": 0.9759711027145386,
      "learning_rate": 9.287303417258248e-06,
      "loss": 0.0065,
      "step": 943
    },
    {
      "epoch": 39.23,
      "grad_norm": 0.48879432678222656,
      "learning_rate": 9.285653041407575e-06,
      "loss": 0.0039,
      "step": 944
    },
    {
      "epoch": 39.27,
      "grad_norm": 0.8832067847251892,
      "learning_rate": 9.284000903886818e-06,
      "loss": 0.0055,
      "step": 945
    },
    {
      "epoch": 39.31,
      "grad_norm": 0.5911577343940735,
      "learning_rate": 9.282347005375111e-06,
      "loss": 0.0054,
      "step": 946
    },
    {
      "epoch": 39.36,
      "grad_norm": 0.5762745141983032,
      "learning_rate": 9.280691346552308e-06,
      "loss": 0.006,
      "step": 947
    },
    {
      "epoch": 39.4,
      "grad_norm": 0.9764127135276794,
      "learning_rate": 9.27903392809899e-06,
      "loss": 0.0102,
      "step": 948
    },
    {
      "epoch": 39.44,
      "grad_norm": 1.081921935081482,
      "learning_rate": 9.277374750696455e-06,
      "loss": 0.0076,
      "step": 949
    },
    {
      "epoch": 39.48,
      "grad_norm": 0.9599935412406921,
      "learning_rate": 9.275713815026732e-06,
      "loss": 0.0076,
      "step": 950
    },
    {
      "epoch": 39.52,
      "grad_norm": 0.9934046864509583,
      "learning_rate": 9.274051121772568e-06,
      "loss": 0.0114,
      "step": 951
    },
    {
      "epoch": 39.56,
      "grad_norm": 0.8323878049850464,
      "learning_rate": 9.272386671617431e-06,
      "loss": 0.0055,
      "step": 952
    },
    {
      "epoch": 39.61,
      "grad_norm": 1.3725991249084473,
      "learning_rate": 9.270720465245515e-06,
      "loss": 0.0157,
      "step": 953
    },
    {
      "epoch": 39.65,
      "grad_norm": 0.794260561466217,
      "learning_rate": 9.269052503341737e-06,
      "loss": 0.0088,
      "step": 954
    },
    {
      "epoch": 39.69,
      "grad_norm": 1.7625880241394043,
      "learning_rate": 9.26738278659173e-06,
      "loss": 0.0152,
      "step": 955
    },
    {
      "epoch": 39.73,
      "grad_norm": 0.5881691575050354,
      "learning_rate": 9.265711315681853e-06,
      "loss": 0.0058,
      "step": 956
    },
    {
      "epoch": 39.77,
      "grad_norm": 0.9072176218032837,
      "learning_rate": 9.264038091299185e-06,
      "loss": 0.0073,
      "step": 957
    },
    {
      "epoch": 39.81,
      "grad_norm": 0.8335102796554565,
      "learning_rate": 9.262363114131523e-06,
      "loss": 0.0121,
      "step": 958
    },
    {
      "epoch": 39.85,
      "grad_norm": 0.9278455376625061,
      "learning_rate": 9.260686384867388e-06,
      "loss": 0.0068,
      "step": 959
    },
    {
      "epoch": 39.9,
      "grad_norm": 1.2237263917922974,
      "learning_rate": 9.259007904196023e-06,
      "loss": 0.0109,
      "step": 960
    },
    {
      "epoch": 39.94,
      "grad_norm": 0.794884443283081,
      "learning_rate": 9.257327672807383e-06,
      "loss": 0.0083,
      "step": 961
    },
    {
      "epoch": 39.98,
      "grad_norm": 0.7988935708999634,
      "learning_rate": 9.25564569139215e-06,
      "loss": 0.008,
      "step": 962
    },
    {
      "epoch": 40.02,
      "grad_norm": 0.9270431995391846,
      "learning_rate": 9.253961960641724e-06,
      "loss": 0.0095,
      "step": 963
    },
    {
      "epoch": 40.06,
      "grad_norm": 0.8908997774124146,
      "learning_rate": 9.25227648124822e-06,
      "loss": 0.0083,
      "step": 964
    },
    {
      "epoch": 40.1,
      "grad_norm": 0.6419752836227417,
      "learning_rate": 9.250589253904481e-06,
      "loss": 0.0047,
      "step": 965
    },
    {
      "epoch": 40.15,
      "grad_norm": 0.6709808707237244,
      "learning_rate": 9.248900279304056e-06,
      "loss": 0.0059,
      "step": 966
    },
    {
      "epoch": 40.19,
      "grad_norm": 1.218061089515686,
      "learning_rate": 9.247209558141222e-06,
      "loss": 0.0075,
      "step": 967
    },
    {
      "epoch": 40.23,
      "grad_norm": 0.534831702709198,
      "learning_rate": 9.24551709111097e-06,
      "loss": 0.0047,
      "step": 968
    },
    {
      "epoch": 40.27,
      "grad_norm": 0.5710359811782837,
      "learning_rate": 9.243822878909007e-06,
      "loss": 0.0055,
      "step": 969
    },
    {
      "epoch": 40.31,
      "grad_norm": 0.9354391694068909,
      "learning_rate": 9.242126922231763e-06,
      "loss": 0.0064,
      "step": 970
    },
    {
      "epoch": 40.35,
      "grad_norm": 0.9697964787483215,
      "learning_rate": 9.240429221776382e-06,
      "loss": 0.0107,
      "step": 971
    },
    {
      "epoch": 40.39,
      "grad_norm": 0.7884173393249512,
      "learning_rate": 9.23872977824072e-06,
      "loss": 0.0063,
      "step": 972
    },
    {
      "epoch": 40.44,
      "grad_norm": 1.3159120082855225,
      "learning_rate": 9.237028592323358e-06,
      "loss": 0.0097,
      "step": 973
    },
    {
      "epoch": 40.48,
      "grad_norm": 0.5440434813499451,
      "learning_rate": 9.23532566472359e-06,
      "loss": 0.0049,
      "step": 974
    },
    {
      "epoch": 40.52,
      "grad_norm": 1.633072853088379,
      "learning_rate": 9.233620996141421e-06,
      "loss": 0.0076,
      "step": 975
    },
    {
      "epoch": 40.56,
      "grad_norm": 1.1729469299316406,
      "learning_rate": 9.231914587277579e-06,
      "loss": 0.0106,
      "step": 976
    },
    {
      "epoch": 40.6,
      "grad_norm": 0.42718783020973206,
      "learning_rate": 9.230206438833506e-06,
      "loss": 0.0038,
      "step": 977
    },
    {
      "epoch": 40.64,
      "grad_norm": 0.3893873691558838,
      "learning_rate": 9.228496551511352e-06,
      "loss": 0.0049,
      "step": 978
    },
    {
      "epoch": 40.69,
      "grad_norm": 0.6977096796035767,
      "learning_rate": 9.226784926013991e-06,
      "loss": 0.0079,
      "step": 979
    },
    {
      "epoch": 40.73,
      "grad_norm": 0.39590197801589966,
      "learning_rate": 9.225071563045007e-06,
      "loss": 0.0036,
      "step": 980
    },
    {
      "epoch": 40.77,
      "grad_norm": 0.6778159141540527,
      "learning_rate": 9.223356463308698e-06,
      "loss": 0.0048,
      "step": 981
    },
    {
      "epoch": 40.81,
      "grad_norm": 0.7883135080337524,
      "learning_rate": 9.221639627510076e-06,
      "loss": 0.0126,
      "step": 982
    },
    {
      "epoch": 40.85,
      "grad_norm": 1.6919302940368652,
      "learning_rate": 9.219921056354871e-06,
      "loss": 0.0072,
      "step": 983
    },
    {
      "epoch": 40.89,
      "grad_norm": 0.6494534611701965,
      "learning_rate": 9.218200750549517e-06,
      "loss": 0.0043,
      "step": 984
    },
    {
      "epoch": 40.94,
      "grad_norm": 0.9515856504440308,
      "learning_rate": 9.216478710801171e-06,
      "loss": 0.0067,
      "step": 985
    },
    {
      "epoch": 40.98,
      "grad_norm": 0.2792876958847046,
      "learning_rate": 9.214754937817697e-06,
      "loss": 0.0037,
      "step": 986
    },
    {
      "epoch": 41.02,
      "grad_norm": 0.4850618541240692,
      "learning_rate": 9.213029432307673e-06,
      "loss": 0.0033,
      "step": 987
    },
    {
      "epoch": 41.06,
      "grad_norm": 1.3403048515319824,
      "learning_rate": 9.211302194980391e-06,
      "loss": 0.0111,
      "step": 988
    },
    {
      "epoch": 41.1,
      "grad_norm": 0.5960517525672913,
      "learning_rate": 9.209573226545852e-06,
      "loss": 0.0057,
      "step": 989
    },
    {
      "epoch": 41.14,
      "grad_norm": 0.12877555191516876,
      "learning_rate": 9.207842527714767e-06,
      "loss": 0.0022,
      "step": 990
    },
    {
      "epoch": 41.18,
      "grad_norm": 0.71108478307724,
      "learning_rate": 9.206110099198563e-06,
      "loss": 0.0047,
      "step": 991
    },
    {
      "epoch": 41.23,
      "grad_norm": 0.3283795416355133,
      "learning_rate": 9.204375941709377e-06,
      "loss": 0.0032,
      "step": 992
    },
    {
      "epoch": 41.27,
      "grad_norm": 1.2576515674591064,
      "learning_rate": 9.202640055960053e-06,
      "loss": 0.01,
      "step": 993
    },
    {
      "epoch": 41.31,
      "grad_norm": 0.551588237285614,
      "learning_rate": 9.20090244266415e-06,
      "loss": 0.0042,
      "step": 994
    },
    {
      "epoch": 41.35,
      "grad_norm": 1.3867038488388062,
      "learning_rate": 9.199163102535937e-06,
      "loss": 0.0073,
      "step": 995
    },
    {
      "epoch": 41.39,
      "grad_norm": 0.6663714647293091,
      "learning_rate": 9.197422036290386e-06,
      "loss": 0.0042,
      "step": 996
    },
    {
      "epoch": 41.43,
      "grad_norm": 1.0405937433242798,
      "learning_rate": 9.195679244643188e-06,
      "loss": 0.0067,
      "step": 997
    },
    {
      "epoch": 41.48,
      "grad_norm": 0.5033742189407349,
      "learning_rate": 9.193934728310737e-06,
      "loss": 0.0052,
      "step": 998
    },
    {
      "epoch": 41.52,
      "grad_norm": 0.7677752375602722,
      "learning_rate": 9.192188488010139e-06,
      "loss": 0.0085,
      "step": 999
    },
    {
      "epoch": 41.56,
      "grad_norm": 0.8385822176933289,
      "learning_rate": 9.190440524459203e-06,
      "loss": 0.0084,
      "step": 1000
    },
    {
      "epoch": 41.6,
      "grad_norm": 0.745892345905304,
      "learning_rate": 9.188690838376457e-06,
      "loss": 0.0054,
      "step": 1001
    },
    {
      "epoch": 41.64,
      "grad_norm": 1.6762750148773193,
      "learning_rate": 9.186939430481128e-06,
      "loss": 0.011,
      "step": 1002
    },
    {
      "epoch": 41.68,
      "grad_norm": 0.4170907735824585,
      "learning_rate": 9.185186301493153e-06,
      "loss": 0.0029,
      "step": 1003
    },
    {
      "epoch": 41.72,
      "grad_norm": 0.39053821563720703,
      "learning_rate": 9.183431452133177e-06,
      "loss": 0.0037,
      "step": 1004
    },
    {
      "epoch": 41.77,
      "grad_norm": 0.4013179838657379,
      "learning_rate": 9.181674883122554e-06,
      "loss": 0.0028,
      "step": 1005
    },
    {
      "epoch": 41.81,
      "grad_norm": 0.48782917857170105,
      "learning_rate": 9.179916595183342e-06,
      "loss": 0.0059,
      "step": 1006
    },
    {
      "epoch": 41.85,
      "grad_norm": 0.9326340556144714,
      "learning_rate": 9.178156589038307e-06,
      "loss": 0.0087,
      "step": 1007
    },
    {
      "epoch": 41.89,
      "grad_norm": 0.7353906035423279,
      "learning_rate": 9.176394865410922e-06,
      "loss": 0.0039,
      "step": 1008
    },
    {
      "epoch": 41.93,
      "grad_norm": 0.9209306836128235,
      "learning_rate": 9.174631425025363e-06,
      "loss": 0.0064,
      "step": 1009
    },
    {
      "epoch": 41.97,
      "grad_norm": 0.8010016083717346,
      "learning_rate": 9.172866268606514e-06,
      "loss": 0.006,
      "step": 1010
    },
    {
      "epoch": 42.02,
      "grad_norm": 0.6527629494667053,
      "learning_rate": 9.171099396879966e-06,
      "loss": 0.0055,
      "step": 1011
    },
    {
      "epoch": 42.06,
      "grad_norm": 0.49409225583076477,
      "learning_rate": 9.169330810572012e-06,
      "loss": 0.0076,
      "step": 1012
    },
    {
      "epoch": 42.1,
      "grad_norm": 0.7051922678947449,
      "learning_rate": 9.167560510409649e-06,
      "loss": 0.0063,
      "step": 1013
    },
    {
      "epoch": 42.14,
      "grad_norm": 0.6241179704666138,
      "learning_rate": 9.165788497120587e-06,
      "loss": 0.0031,
      "step": 1014
    },
    {
      "epoch": 42.18,
      "grad_norm": 0.3116721510887146,
      "learning_rate": 9.164014771433228e-06,
      "loss": 0.0025,
      "step": 1015
    },
    {
      "epoch": 42.22,
      "grad_norm": 0.3573228120803833,
      "learning_rate": 9.162239334076684e-06,
      "loss": 0.0039,
      "step": 1016
    },
    {
      "epoch": 42.26,
      "grad_norm": 0.46071746945381165,
      "learning_rate": 9.16046218578077e-06,
      "loss": 0.006,
      "step": 1017
    },
    {
      "epoch": 42.31,
      "grad_norm": 0.8434019684791565,
      "learning_rate": 9.158683327276008e-06,
      "loss": 0.0057,
      "step": 1018
    },
    {
      "epoch": 42.35,
      "grad_norm": 1.1910905838012695,
      "learning_rate": 9.156902759293616e-06,
      "loss": 0.0077,
      "step": 1019
    },
    {
      "epoch": 42.39,
      "grad_norm": 0.6247171759605408,
      "learning_rate": 9.15512048256552e-06,
      "loss": 0.0036,
      "step": 1020
    },
    {
      "epoch": 42.43,
      "grad_norm": 0.7372381687164307,
      "learning_rate": 9.153336497824348e-06,
      "loss": 0.0066,
      "step": 1021
    },
    {
      "epoch": 42.47,
      "grad_norm": 0.27213844656944275,
      "learning_rate": 9.151550805803424e-06,
      "loss": 0.0022,
      "step": 1022
    },
    {
      "epoch": 42.51,
      "grad_norm": 1.0675302743911743,
      "learning_rate": 9.149763407236785e-06,
      "loss": 0.0066,
      "step": 1023
    },
    {
      "epoch": 42.56,
      "grad_norm": 0.5956276059150696,
      "learning_rate": 9.147974302859158e-06,
      "loss": 0.0061,
      "step": 1024
    },
    {
      "epoch": 42.6,
      "grad_norm": 1.3576412200927734,
      "learning_rate": 9.146183493405976e-06,
      "loss": 0.0054,
      "step": 1025
    },
    {
      "epoch": 42.64,
      "grad_norm": 0.7302291393280029,
      "learning_rate": 9.144390979613376e-06,
      "loss": 0.0044,
      "step": 1026
    },
    {
      "epoch": 42.68,
      "grad_norm": 1.342016339302063,
      "learning_rate": 9.142596762218192e-06,
      "loss": 0.0094,
      "step": 1027
    },
    {
      "epoch": 42.72,
      "grad_norm": 1.0483570098876953,
      "learning_rate": 9.140800841957958e-06,
      "loss": 0.0113,
      "step": 1028
    },
    {
      "epoch": 42.76,
      "grad_norm": 1.1924957036972046,
      "learning_rate": 9.139003219570911e-06,
      "loss": 0.0092,
      "step": 1029
    },
    {
      "epoch": 42.81,
      "grad_norm": 0.31536903977394104,
      "learning_rate": 9.137203895795983e-06,
      "loss": 0.0034,
      "step": 1030
    },
    {
      "epoch": 42.85,
      "grad_norm": 1.6549125909805298,
      "learning_rate": 9.13540287137281e-06,
      "loss": 0.0116,
      "step": 1031
    },
    {
      "epoch": 42.89,
      "grad_norm": 0.2974534034729004,
      "learning_rate": 9.133600147041723e-06,
      "loss": 0.0037,
      "step": 1032
    },
    {
      "epoch": 42.93,
      "grad_norm": 0.9759052395820618,
      "learning_rate": 9.131795723543757e-06,
      "loss": 0.0055,
      "step": 1033
    },
    {
      "epoch": 42.97,
      "grad_norm": 1.5734623670578003,
      "learning_rate": 9.12998960162064e-06,
      "loss": 0.0098,
      "step": 1034
    },
    {
      "epoch": 43.01,
      "grad_norm": 0.6802622675895691,
      "learning_rate": 9.128181782014801e-06,
      "loss": 0.0056,
      "step": 1035
    },
    {
      "epoch": 43.05,
      "grad_norm": 0.19628185033798218,
      "learning_rate": 9.126372265469368e-06,
      "loss": 0.0027,
      "step": 1036
    },
    {
      "epoch": 43.1,
      "grad_norm": 0.9182366132736206,
      "learning_rate": 9.12456105272816e-06,
      "loss": 0.0051,
      "step": 1037
    },
    {
      "epoch": 43.14,
      "grad_norm": 0.6829686760902405,
      "learning_rate": 9.122748144535704e-06,
      "loss": 0.0042,
      "step": 1038
    },
    {
      "epoch": 43.18,
      "grad_norm": 0.6247805953025818,
      "learning_rate": 9.120933541637215e-06,
      "loss": 0.0041,
      "step": 1039
    },
    {
      "epoch": 43.22,
      "grad_norm": 0.14602845907211304,
      "learning_rate": 9.119117244778609e-06,
      "loss": 0.0024,
      "step": 1040
    },
    {
      "epoch": 43.26,
      "grad_norm": 0.7074348330497742,
      "learning_rate": 9.117299254706496e-06,
      "loss": 0.005,
      "step": 1041
    },
    {
      "epoch": 43.3,
      "grad_norm": 0.5397821664810181,
      "learning_rate": 9.115479572168183e-06,
      "loss": 0.0071,
      "step": 1042
    },
    {
      "epoch": 43.35,
      "grad_norm": 0.5655111074447632,
      "learning_rate": 9.113658197911673e-06,
      "loss": 0.0059,
      "step": 1043
    },
    {
      "epoch": 43.39,
      "grad_norm": 0.5361924767494202,
      "learning_rate": 9.111835132685665e-06,
      "loss": 0.0053,
      "step": 1044
    },
    {
      "epoch": 43.43,
      "grad_norm": 0.6251279711723328,
      "learning_rate": 9.110010377239552e-06,
      "loss": 0.0034,
      "step": 1045
    },
    {
      "epoch": 43.47,
      "grad_norm": 1.2640858888626099,
      "learning_rate": 9.10818393232342e-06,
      "loss": 0.007,
      "step": 1046
    },
    {
      "epoch": 43.51,
      "grad_norm": 0.7812306880950928,
      "learning_rate": 9.106355798688052e-06,
      "loss": 0.0038,
      "step": 1047
    },
    {
      "epoch": 43.55,
      "grad_norm": 0.313737154006958,
      "learning_rate": 9.104525977084928e-06,
      "loss": 0.0026,
      "step": 1048
    },
    {
      "epoch": 43.59,
      "grad_norm": 0.5460379719734192,
      "learning_rate": 9.102694468266216e-06,
      "loss": 0.0039,
      "step": 1049
    },
    {
      "epoch": 43.64,
      "grad_norm": 0.684625506401062,
      "learning_rate": 9.10086127298478e-06,
      "loss": 0.0071,
      "step": 1050
    },
    {
      "epoch": 43.68,
      "grad_norm": 0.7863852381706238,
      "learning_rate": 9.09902639199418e-06,
      "loss": 0.0043,
      "step": 1051
    },
    {
      "epoch": 43.72,
      "grad_norm": 0.7218043804168701,
      "learning_rate": 9.09718982604866e-06,
      "loss": 0.0062,
      "step": 1052
    },
    {
      "epoch": 43.76,
      "grad_norm": 0.6710922122001648,
      "learning_rate": 9.095351575903168e-06,
      "loss": 0.0058,
      "step": 1053
    },
    {
      "epoch": 43.8,
      "grad_norm": 0.8518223762512207,
      "learning_rate": 9.09351164231334e-06,
      "loss": 0.0045,
      "step": 1054
    },
    {
      "epoch": 43.84,
      "grad_norm": 2.062335729598999,
      "learning_rate": 9.0916700260355e-06,
      "loss": 0.0082,
      "step": 1055
    },
    {
      "epoch": 43.89,
      "grad_norm": 0.9191877245903015,
      "learning_rate": 9.08982672782667e-06,
      "loss": 0.0059,
      "step": 1056
    },
    {
      "epoch": 43.93,
      "grad_norm": 0.7376312017440796,
      "learning_rate": 9.087981748444556e-06,
      "loss": 0.0053,
      "step": 1057
    },
    {
      "epoch": 43.97,
      "grad_norm": 0.6925739645957947,
      "learning_rate": 9.086135088647563e-06,
      "loss": 0.0054,
      "step": 1058
    },
    {
      "epoch": 44.01,
      "grad_norm": 0.5622327327728271,
      "learning_rate": 9.084286749194783e-06,
      "loss": 0.0028,
      "step": 1059
    },
    {
      "epoch": 44.05,
      "grad_norm": 1.788358449935913,
      "learning_rate": 9.082436730845993e-06,
      "loss": 0.0145,
      "step": 1060
    },
    {
      "epoch": 44.09,
      "grad_norm": 0.4816697835922241,
      "learning_rate": 9.080585034361675e-06,
      "loss": 0.0035,
      "step": 1061
    },
    {
      "epoch": 44.14,
      "grad_norm": 0.2609620988368988,
      "learning_rate": 9.078731660502983e-06,
      "loss": 0.002,
      "step": 1062
    },
    {
      "epoch": 44.18,
      "grad_norm": 0.471008837223053,
      "learning_rate": 9.076876610031776e-06,
      "loss": 0.0025,
      "step": 1063
    },
    {
      "epoch": 44.22,
      "grad_norm": 0.4905068874359131,
      "learning_rate": 9.07501988371059e-06,
      "loss": 0.0036,
      "step": 1064
    },
    {
      "epoch": 44.26,
      "grad_norm": 0.5216756463050842,
      "learning_rate": 9.073161482302656e-06,
      "loss": 0.0037,
      "step": 1065
    },
    {
      "epoch": 44.3,
      "grad_norm": 0.7024052739143372,
      "learning_rate": 9.071301406571893e-06,
      "loss": 0.004,
      "step": 1066
    },
    {
      "epoch": 44.34,
      "grad_norm": 0.47581619024276733,
      "learning_rate": 9.06943965728291e-06,
      "loss": 0.0031,
      "step": 1067
    },
    {
      "epoch": 44.38,
      "grad_norm": 2.020533323287964,
      "learning_rate": 9.067576235200999e-06,
      "loss": 0.0096,
      "step": 1068
    },
    {
      "epoch": 44.43,
      "grad_norm": 0.5366806387901306,
      "learning_rate": 9.065711141092144e-06,
      "loss": 0.0055,
      "step": 1069
    },
    {
      "epoch": 44.47,
      "grad_norm": 0.2679957449436188,
      "learning_rate": 9.063844375723014e-06,
      "loss": 0.0027,
      "step": 1070
    },
    {
      "epoch": 44.51,
      "grad_norm": 1.9535497426986694,
      "learning_rate": 9.061975939860966e-06,
      "loss": 0.0066,
      "step": 1071
    },
    {
      "epoch": 44.55,
      "grad_norm": 1.6079679727554321,
      "learning_rate": 9.060105834274044e-06,
      "loss": 0.0116,
      "step": 1072
    },
    {
      "epoch": 44.59,
      "grad_norm": 0.2682638168334961,
      "learning_rate": 9.058234059730977e-06,
      "loss": 0.0021,
      "step": 1073
    },
    {
      "epoch": 44.63,
      "grad_norm": 0.7486835718154907,
      "learning_rate": 9.05636061700118e-06,
      "loss": 0.0078,
      "step": 1074
    },
    {
      "epoch": 44.68,
      "grad_norm": 0.8899410367012024,
      "learning_rate": 9.054485506854756e-06,
      "loss": 0.0055,
      "step": 1075
    },
    {
      "epoch": 44.72,
      "grad_norm": 2.1335506439208984,
      "learning_rate": 9.05260873006249e-06,
      "loss": 0.0082,
      "step": 1076
    },
    {
      "epoch": 44.76,
      "grad_norm": 1.0010602474212646,
      "learning_rate": 9.050730287395858e-06,
      "loss": 0.006,
      "step": 1077
    },
    {
      "epoch": 44.8,
      "grad_norm": 1.0044606924057007,
      "learning_rate": 9.048850179627013e-06,
      "loss": 0.0118,
      "step": 1078
    },
    {
      "epoch": 44.84,
      "grad_norm": 0.8532744646072388,
      "learning_rate": 9.046968407528797e-06,
      "loss": 0.0045,
      "step": 1079
    },
    {
      "epoch": 44.88,
      "grad_norm": 0.830396831035614,
      "learning_rate": 9.045084971874738e-06,
      "loss": 0.0059,
      "step": 1080
    },
    {
      "epoch": 44.92,
      "grad_norm": 0.8455102443695068,
      "learning_rate": 9.04319987343904e-06,
      "loss": 0.004,
      "step": 1081
    },
    {
      "epoch": 44.97,
      "grad_norm": 0.7475310564041138,
      "learning_rate": 9.041313112996604e-06,
      "loss": 0.005,
      "step": 1082
    },
    {
      "epoch": 45.01,
      "grad_norm": 1.0867691040039062,
      "learning_rate": 9.039424691322998e-06,
      "loss": 0.0069,
      "step": 1083
    },
    {
      "epoch": 45.05,
      "grad_norm": 0.42124924063682556,
      "learning_rate": 9.037534609194482e-06,
      "loss": 0.0028,
      "step": 1084
    },
    {
      "epoch": 45.09,
      "grad_norm": 0.17519231140613556,
      "learning_rate": 9.035642867388003e-06,
      "loss": 0.0024,
      "step": 1085
    },
    {
      "epoch": 45.13,
      "grad_norm": 0.5311237573623657,
      "learning_rate": 9.033749466681178e-06,
      "loss": 0.0054,
      "step": 1086
    },
    {
      "epoch": 45.17,
      "grad_norm": 0.4181007444858551,
      "learning_rate": 9.031854407852317e-06,
      "loss": 0.0048,
      "step": 1087
    },
    {
      "epoch": 45.22,
      "grad_norm": 0.28177884221076965,
      "learning_rate": 9.029957691680404e-06,
      "loss": 0.0025,
      "step": 1088
    },
    {
      "epoch": 45.26,
      "grad_norm": 1.1662282943725586,
      "learning_rate": 9.02805931894511e-06,
      "loss": 0.0045,
      "step": 1089
    },
    {
      "epoch": 45.3,
      "grad_norm": 0.4903424382209778,
      "learning_rate": 9.026159290426782e-06,
      "loss": 0.0042,
      "step": 1090
    },
    {
      "epoch": 45.34,
      "grad_norm": 1.3733116388320923,
      "learning_rate": 9.024257606906452e-06,
      "loss": 0.0074,
      "step": 1091
    },
    {
      "epoch": 45.38,
      "grad_norm": 0.24981677532196045,
      "learning_rate": 9.022354269165828e-06,
      "loss": 0.0037,
      "step": 1092
    },
    {
      "epoch": 45.42,
      "grad_norm": 1.0160284042358398,
      "learning_rate": 9.020449277987302e-06,
      "loss": 0.0075,
      "step": 1093
    },
    {
      "epoch": 45.46,
      "grad_norm": 0.7223360538482666,
      "learning_rate": 9.018542634153944e-06,
      "loss": 0.0037,
      "step": 1094
    },
    {
      "epoch": 45.51,
      "grad_norm": 1.408615231513977,
      "learning_rate": 9.016634338449504e-06,
      "loss": 0.0109,
      "step": 1095
    },
    {
      "epoch": 45.55,
      "grad_norm": 2.0834970474243164,
      "learning_rate": 9.014724391658407e-06,
      "loss": 0.0143,
      "step": 1096
    },
    {
      "epoch": 45.59,
      "grad_norm": 0.7589585781097412,
      "learning_rate": 9.012812794565762e-06,
      "loss": 0.0048,
      "step": 1097
    },
    {
      "epoch": 45.63,
      "grad_norm": 0.5050346255302429,
      "learning_rate": 9.010899547957354e-06,
      "loss": 0.0026,
      "step": 1098
    },
    {
      "epoch": 45.67,
      "grad_norm": 0.8093375563621521,
      "learning_rate": 9.008984652619649e-06,
      "loss": 0.007,
      "step": 1099
    },
    {
      "epoch": 45.71,
      "grad_norm": 0.22998811304569244,
      "learning_rate": 9.007068109339783e-06,
      "loss": 0.0027,
      "step": 1100
    },
    {
      "epoch": 45.76,
      "grad_norm": 1.841107726097107,
      "learning_rate": 9.00514991890558e-06,
      "loss": 0.0075,
      "step": 1101
    },
    {
      "epoch": 45.8,
      "grad_norm": 0.8965396881103516,
      "learning_rate": 9.003230082105532e-06,
      "loss": 0.0062,
      "step": 1102
    },
    {
      "epoch": 45.84,
      "grad_norm": 0.679108202457428,
      "learning_rate": 9.001308599728813e-06,
      "loss": 0.0041,
      "step": 1103
    },
    {
      "epoch": 45.88,
      "grad_norm": 1.268341064453125,
      "learning_rate": 8.999385472565271e-06,
      "loss": 0.0129,
      "step": 1104
    },
    {
      "epoch": 45.92,
      "grad_norm": 0.8604140281677246,
      "learning_rate": 8.997460701405431e-06,
      "loss": 0.0085,
      "step": 1105
    },
    {
      "epoch": 45.96,
      "grad_norm": 0.4803965091705322,
      "learning_rate": 8.995534287040494e-06,
      "loss": 0.0042,
      "step": 1106
    },
    {
      "epoch": 46.01,
      "grad_norm": 1.1079193353652954,
      "learning_rate": 8.993606230262335e-06,
      "loss": 0.0053,
      "step": 1107
    },
    {
      "epoch": 46.05,
      "grad_norm": 0.993233859539032,
      "learning_rate": 8.991676531863507e-06,
      "loss": 0.0086,
      "step": 1108
    },
    {
      "epoch": 46.09,
      "grad_norm": 0.27347061038017273,
      "learning_rate": 8.989745192637237e-06,
      "loss": 0.0032,
      "step": 1109
    },
    {
      "epoch": 46.13,
      "grad_norm": 0.4693109393119812,
      "learning_rate": 8.987812213377423e-06,
      "loss": 0.0046,
      "step": 1110
    },
    {
      "epoch": 46.17,
      "grad_norm": 0.5293859243392944,
      "learning_rate": 8.985877594878642e-06,
      "loss": 0.0048,
      "step": 1111
    },
    {
      "epoch": 46.21,
      "grad_norm": 0.4872833788394928,
      "learning_rate": 8.98394133793614e-06,
      "loss": 0.0067,
      "step": 1112
    },
    {
      "epoch": 46.25,
      "grad_norm": 1.4778965711593628,
      "learning_rate": 8.982003443345841e-06,
      "loss": 0.0093,
      "step": 1113
    },
    {
      "epoch": 46.3,
      "grad_norm": 0.5128074884414673,
      "learning_rate": 8.98006391190434e-06,
      "loss": 0.0044,
      "step": 1114
    },
    {
      "epoch": 46.34,
      "grad_norm": 0.6526855826377869,
      "learning_rate": 8.978122744408905e-06,
      "loss": 0.0036,
      "step": 1115
    },
    {
      "epoch": 46.38,
      "grad_norm": 0.8596347570419312,
      "learning_rate": 8.976179941657478e-06,
      "loss": 0.006,
      "step": 1116
    },
    {
      "epoch": 46.42,
      "grad_norm": 0.4920992851257324,
      "learning_rate": 8.97423550444867e-06,
      "loss": 0.0046,
      "step": 1117
    },
    {
      "epoch": 46.46,
      "grad_norm": 0.6048877239227295,
      "learning_rate": 8.972289433581765e-06,
      "loss": 0.0057,
      "step": 1118
    },
    {
      "epoch": 46.5,
      "grad_norm": 0.5604735016822815,
      "learning_rate": 8.97034172985672e-06,
      "loss": 0.003,
      "step": 1119
    },
    {
      "epoch": 46.55,
      "grad_norm": 0.9289501905441284,
      "learning_rate": 8.968392394074164e-06,
      "loss": 0.003,
      "step": 1120
    },
    {
      "epoch": 46.59,
      "grad_norm": 1.1074302196502686,
      "learning_rate": 8.966441427035392e-06,
      "loss": 0.0046,
      "step": 1121
    },
    {
      "epoch": 46.63,
      "grad_norm": 0.863682210445404,
      "learning_rate": 8.964488829542377e-06,
      "loss": 0.0058,
      "step": 1122
    },
    {
      "epoch": 46.67,
      "grad_norm": 0.49218079447746277,
      "learning_rate": 8.962534602397756e-06,
      "loss": 0.0053,
      "step": 1123
    },
    {
      "epoch": 46.71,
      "grad_norm": 0.8549468517303467,
      "learning_rate": 8.960578746404837e-06,
      "loss": 0.0065,
      "step": 1124
    },
    {
      "epoch": 46.75,
      "grad_norm": 0.5326871871948242,
      "learning_rate": 8.9586212623676e-06,
      "loss": 0.0037,
      "step": 1125
    },
    {
      "epoch": 46.79,
      "grad_norm": 2.927065134048462,
      "learning_rate": 8.95666215109069e-06,
      "loss": 0.0216,
      "step": 1126
    },
    {
      "epoch": 46.84,
      "grad_norm": 0.9060254096984863,
      "learning_rate": 8.95470141337943e-06,
      "loss": 0.0055,
      "step": 1127
    },
    {
      "epoch": 46.88,
      "grad_norm": 1.594855546951294,
      "learning_rate": 8.9527390500398e-06,
      "loss": 0.0084,
      "step": 1128
    },
    {
      "epoch": 46.92,
      "grad_norm": 0.8052709102630615,
      "learning_rate": 8.950775061878453e-06,
      "loss": 0.0066,
      "step": 1129
    },
    {
      "epoch": 46.96,
      "grad_norm": 0.7774277329444885,
      "learning_rate": 8.948809449702712e-06,
      "loss": 0.0065,
      "step": 1130
    },
    {
      "epoch": 47.0,
      "grad_norm": 0.40989065170288086,
      "learning_rate": 8.946842214320566e-06,
      "loss": 0.0031,
      "step": 1131
    },
    {
      "epoch": 47.04,
      "grad_norm": 0.49704232811927795,
      "learning_rate": 8.944873356540671e-06,
      "loss": 0.0039,
      "step": 1132
    },
    {
      "epoch": 47.09,
      "grad_norm": 0.8804678916931152,
      "learning_rate": 8.94290287717235e-06,
      "loss": 0.0055,
      "step": 1133
    },
    {
      "epoch": 47.13,
      "grad_norm": 0.7941491007804871,
      "learning_rate": 8.940930777025594e-06,
      "loss": 0.005,
      "step": 1134
    },
    {
      "epoch": 47.17,
      "grad_norm": 0.625342071056366,
      "learning_rate": 8.938957056911057e-06,
      "loss": 0.0061,
      "step": 1135
    },
    {
      "epoch": 47.21,
      "grad_norm": 0.6669651865959167,
      "learning_rate": 8.936981717640061e-06,
      "loss": 0.0052,
      "step": 1136
    },
    {
      "epoch": 47.25,
      "grad_norm": 0.23703457415103912,
      "learning_rate": 8.935004760024592e-06,
      "loss": 0.0024,
      "step": 1137
    },
    {
      "epoch": 47.29,
      "grad_norm": 0.594703733921051,
      "learning_rate": 8.933026184877308e-06,
      "loss": 0.0051,
      "step": 1138
    },
    {
      "epoch": 47.34,
      "grad_norm": 0.42679035663604736,
      "learning_rate": 8.93104599301152e-06,
      "loss": 0.0076,
      "step": 1139
    },
    {
      "epoch": 47.38,
      "grad_norm": 1.165274977684021,
      "learning_rate": 8.929064185241214e-06,
      "loss": 0.0086,
      "step": 1140
    },
    {
      "epoch": 47.42,
      "grad_norm": 1.088120460510254,
      "learning_rate": 8.927080762381033e-06,
      "loss": 0.0079,
      "step": 1141
    },
    {
      "epoch": 47.46,
      "grad_norm": 0.5039284229278564,
      "learning_rate": 8.925095725246291e-06,
      "loss": 0.0035,
      "step": 1142
    },
    {
      "epoch": 47.5,
      "grad_norm": 0.509641706943512,
      "learning_rate": 8.92310907465296e-06,
      "loss": 0.0039,
      "step": 1143
    },
    {
      "epoch": 47.54,
      "grad_norm": 0.48471078276634216,
      "learning_rate": 8.921120811417678e-06,
      "loss": 0.0039,
      "step": 1144
    },
    {
      "epoch": 47.58,
      "grad_norm": 0.6653332710266113,
      "learning_rate": 8.919130936357743e-06,
      "loss": 0.0037,
      "step": 1145
    },
    {
      "epoch": 47.63,
      "grad_norm": 0.8480440974235535,
      "learning_rate": 8.917139450291119e-06,
      "loss": 0.0033,
      "step": 1146
    },
    {
      "epoch": 47.67,
      "grad_norm": 1.1771411895751953,
      "learning_rate": 8.91514635403643e-06,
      "loss": 0.0052,
      "step": 1147
    },
    {
      "epoch": 47.71,
      "grad_norm": 0.9198469519615173,
      "learning_rate": 8.913151648412963e-06,
      "loss": 0.0062,
      "step": 1148
    },
    {
      "epoch": 47.75,
      "grad_norm": 0.7975867390632629,
      "learning_rate": 8.911155334240667e-06,
      "loss": 0.0051,
      "step": 1149
    },
    {
      "epoch": 47.79,
      "grad_norm": 0.5986433029174805,
      "learning_rate": 8.90915741234015e-06,
      "loss": 0.0048,
      "step": 1150
    },
    {
      "epoch": 47.83,
      "grad_norm": 1.1527036428451538,
      "learning_rate": 8.907157883532682e-06,
      "loss": 0.0089,
      "step": 1151
    },
    {
      "epoch": 47.88,
      "grad_norm": 0.9702758193016052,
      "learning_rate": 8.905156748640194e-06,
      "loss": 0.007,
      "step": 1152
    },
    {
      "epoch": 47.92,
      "grad_norm": 0.8074759840965271,
      "learning_rate": 8.903154008485278e-06,
      "loss": 0.0063,
      "step": 1153
    },
    {
      "epoch": 47.96,
      "grad_norm": 0.874511182308197,
      "learning_rate": 8.901149663891184e-06,
      "loss": 0.0055,
      "step": 1154
    },
    {
      "epoch": 48.0,
      "grad_norm": 0.8243052363395691,
      "learning_rate": 8.899143715681822e-06,
      "loss": 0.0051,
      "step": 1155
    },
    {
      "epoch": 48.04,
      "grad_norm": 0.7056268453598022,
      "learning_rate": 8.897136164681763e-06,
      "loss": 0.0033,
      "step": 1156
    },
    {
      "epoch": 48.08,
      "grad_norm": 0.32028573751449585,
      "learning_rate": 8.895127011716234e-06,
      "loss": 0.0025,
      "step": 1157
    },
    {
      "epoch": 48.12,
      "grad_norm": 0.389352411031723,
      "learning_rate": 8.893116257611122e-06,
      "loss": 0.0036,
      "step": 1158
    },
    {
      "epoch": 48.17,
      "grad_norm": 0.944442093372345,
      "learning_rate": 8.891103903192972e-06,
      "loss": 0.0076,
      "step": 1159
    },
    {
      "epoch": 48.21,
      "grad_norm": 1.2323611974716187,
      "learning_rate": 8.889089949288986e-06,
      "loss": 0.0056,
      "step": 1160
    },
    {
      "epoch": 48.25,
      "grad_norm": 0.294950008392334,
      "learning_rate": 8.88707439672703e-06,
      "loss": 0.0028,
      "step": 1161
    },
    {
      "epoch": 48.29,
      "grad_norm": 0.6159133315086365,
      "learning_rate": 8.885057246335614e-06,
      "loss": 0.005,
      "step": 1162
    },
    {
      "epoch": 48.33,
      "grad_norm": 0.8890577554702759,
      "learning_rate": 8.883038498943916e-06,
      "loss": 0.0061,
      "step": 1163
    },
    {
      "epoch": 48.37,
      "grad_norm": 0.6800898313522339,
      "learning_rate": 8.881018155381766e-06,
      "loss": 0.0034,
      "step": 1164
    },
    {
      "epoch": 48.42,
      "grad_norm": 1.4471733570098877,
      "learning_rate": 8.878996216479651e-06,
      "loss": 0.0119,
      "step": 1165
    },
    {
      "epoch": 48.46,
      "grad_norm": 0.5839786529541016,
      "learning_rate": 8.876972683068714e-06,
      "loss": 0.0049,
      "step": 1166
    },
    {
      "epoch": 48.5,
      "grad_norm": 0.9872044920921326,
      "learning_rate": 8.874947555980753e-06,
      "loss": 0.0068,
      "step": 1167
    },
    {
      "epoch": 48.54,
      "grad_norm": 0.934277355670929,
      "learning_rate": 8.872920836048222e-06,
      "loss": 0.0047,
      "step": 1168
    },
    {
      "epoch": 48.58,
      "grad_norm": 0.5438260436058044,
      "learning_rate": 8.87089252410423e-06,
      "loss": 0.0037,
      "step": 1169
    },
    {
      "epoch": 48.62,
      "grad_norm": 0.8181443214416504,
      "learning_rate": 8.868862620982534e-06,
      "loss": 0.0055,
      "step": 1170
    },
    {
      "epoch": 48.66,
      "grad_norm": 0.748555600643158,
      "learning_rate": 8.866831127517557e-06,
      "loss": 0.0066,
      "step": 1171
    },
    {
      "epoch": 48.71,
      "grad_norm": 1.3643443584442139,
      "learning_rate": 8.864798044544365e-06,
      "loss": 0.0078,
      "step": 1172
    },
    {
      "epoch": 48.75,
      "grad_norm": 0.37457379698753357,
      "learning_rate": 8.862763372898684e-06,
      "loss": 0.0029,
      "step": 1173
    },
    {
      "epoch": 48.79,
      "grad_norm": 0.7683041095733643,
      "learning_rate": 8.860727113416889e-06,
      "loss": 0.0047,
      "step": 1174
    },
    {
      "epoch": 48.83,
      "grad_norm": 1.0234102010726929,
      "learning_rate": 8.85868926693601e-06,
      "loss": 0.0047,
      "step": 1175
    },
    {
      "epoch": 48.87,
      "grad_norm": 0.8034811615943909,
      "learning_rate": 8.85664983429373e-06,
      "loss": 0.0044,
      "step": 1176
    },
    {
      "epoch": 48.91,
      "grad_norm": 0.6339677572250366,
      "learning_rate": 8.854608816328379e-06,
      "loss": 0.0042,
      "step": 1177
    },
    {
      "epoch": 48.96,
      "grad_norm": 0.7343217730522156,
      "learning_rate": 8.852566213878947e-06,
      "loss": 0.0039,
      "step": 1178
    },
    {
      "epoch": 49.0,
      "grad_norm": 1.311187982559204,
      "learning_rate": 8.850522027785067e-06,
      "loss": 0.0039,
      "step": 1179
    },
    {
      "epoch": 49.04,
      "grad_norm": 0.4363139271736145,
      "learning_rate": 8.84847625888703e-06,
      "loss": 0.0042,
      "step": 1180
    },
    {
      "epoch": 49.08,
      "grad_norm": 1.2625731229782104,
      "learning_rate": 8.846428908025773e-06,
      "loss": 0.0073,
      "step": 1181
    },
    {
      "epoch": 49.12,
      "grad_norm": 0.6070346236228943,
      "learning_rate": 8.844379976042882e-06,
      "loss": 0.0036,
      "step": 1182
    },
    {
      "epoch": 49.16,
      "grad_norm": 0.7203699350357056,
      "learning_rate": 8.842329463780601e-06,
      "loss": 0.0038,
      "step": 1183
    },
    {
      "epoch": 49.21,
      "grad_norm": 0.28545063734054565,
      "learning_rate": 8.840277372081812e-06,
      "loss": 0.0031,
      "step": 1184
    },
    {
      "epoch": 49.25,
      "grad_norm": 0.8345621228218079,
      "learning_rate": 8.838223701790057e-06,
      "loss": 0.0066,
      "step": 1185
    },
    {
      "epoch": 49.29,
      "grad_norm": 0.5361225008964539,
      "learning_rate": 8.83616845374952e-06,
      "loss": 0.0027,
      "step": 1186
    },
    {
      "epoch": 49.33,
      "grad_norm": 1.0958267450332642,
      "learning_rate": 8.834111628805036e-06,
      "loss": 0.0126,
      "step": 1187
    },
    {
      "epoch": 49.37,
      "grad_norm": 0.8412835001945496,
      "learning_rate": 8.832053227802089e-06,
      "loss": 0.0079,
      "step": 1188
    },
    {
      "epoch": 49.41,
      "grad_norm": 1.1701586246490479,
      "learning_rate": 8.829993251586808e-06,
      "loss": 0.01,
      "step": 1189
    },
    {
      "epoch": 49.45,
      "grad_norm": 0.8085410594940186,
      "learning_rate": 8.827931701005974e-06,
      "loss": 0.0037,
      "step": 1190
    },
    {
      "epoch": 49.5,
      "grad_norm": 1.1387337446212769,
      "learning_rate": 8.825868576907012e-06,
      "loss": 0.0094,
      "step": 1191
    },
    {
      "epoch": 49.54,
      "grad_norm": 0.6128835678100586,
      "learning_rate": 8.823803880137993e-06,
      "loss": 0.0038,
      "step": 1192
    },
    {
      "epoch": 49.58,
      "grad_norm": 1.106866478919983,
      "learning_rate": 8.821737611547636e-06,
      "loss": 0.0078,
      "step": 1193
    },
    {
      "epoch": 49.62,
      "grad_norm": 2.1007533073425293,
      "learning_rate": 8.819669771985308e-06,
      "loss": 0.0158,
      "step": 1194
    },
    {
      "epoch": 49.66,
      "grad_norm": 0.841517984867096,
      "learning_rate": 8.817600362301018e-06,
      "loss": 0.0041,
      "step": 1195
    },
    {
      "epoch": 49.7,
      "grad_norm": 0.7791988849639893,
      "learning_rate": 8.815529383345421e-06,
      "loss": 0.0042,
      "step": 1196
    },
    {
      "epoch": 49.75,
      "grad_norm": 0.6219226717948914,
      "learning_rate": 8.81345683596982e-06,
      "loss": 0.0041,
      "step": 1197
    },
    {
      "epoch": 49.79,
      "grad_norm": 0.4664952754974365,
      "learning_rate": 8.81138272102616e-06,
      "loss": 0.0039,
      "step": 1198
    },
    {
      "epoch": 49.83,
      "grad_norm": 1.2956814765930176,
      "learning_rate": 8.809307039367035e-06,
      "loss": 0.0076,
      "step": 1199
    },
    {
      "epoch": 49.87,
      "grad_norm": 0.7397843599319458,
      "learning_rate": 8.807229791845673e-06,
      "loss": 0.0056,
      "step": 1200
    },
    {
      "epoch": 49.91,
      "grad_norm": 0.6596176624298096,
      "learning_rate": 8.805150979315956e-06,
      "loss": 0.0048,
      "step": 1201
    },
    {
      "epoch": 49.95,
      "grad_norm": 0.8148395419120789,
      "learning_rate": 8.803070602632405e-06,
      "loss": 0.0065,
      "step": 1202
    },
    {
      "epoch": 49.99,
      "grad_norm": 1.1995902061462402,
      "learning_rate": 8.800988662650183e-06,
      "loss": 0.0139,
      "step": 1203
    },
    {
      "epoch": 50.04,
      "grad_norm": 0.5701907873153687,
      "learning_rate": 8.7989051602251e-06,
      "loss": 0.0042,
      "step": 1204
    },
    {
      "epoch": 50.08,
      "grad_norm": 0.3426778316497803,
      "learning_rate": 8.7968200962136e-06,
      "loss": 0.0025,
      "step": 1205
    },
    {
      "epoch": 50.12,
      "grad_norm": 1.3924822807312012,
      "learning_rate": 8.794733471472778e-06,
      "loss": 0.0091,
      "step": 1206
    },
    {
      "epoch": 50.16,
      "grad_norm": 0.31744644045829773,
      "learning_rate": 8.792645286860367e-06,
      "loss": 0.0035,
      "step": 1207
    },
    {
      "epoch": 50.2,
      "grad_norm": 0.3427564799785614,
      "learning_rate": 8.790555543234739e-06,
      "loss": 0.0024,
      "step": 1208
    },
    {
      "epoch": 50.24,
      "grad_norm": 0.6294954419136047,
      "learning_rate": 8.788464241454906e-06,
      "loss": 0.0059,
      "step": 1209
    },
    {
      "epoch": 50.29,
      "grad_norm": 0.24925148487091064,
      "learning_rate": 8.786371382380527e-06,
      "loss": 0.002,
      "step": 1210
    },
    {
      "epoch": 50.33,
      "grad_norm": 0.7227379679679871,
      "learning_rate": 8.784276966871898e-06,
      "loss": 0.003,
      "step": 1211
    },
    {
      "epoch": 50.37,
      "grad_norm": 1.4733636379241943,
      "learning_rate": 8.782180995789953e-06,
      "loss": 0.0101,
      "step": 1212
    },
    {
      "epoch": 50.41,
      "grad_norm": 1.0218137502670288,
      "learning_rate": 8.780083469996264e-06,
      "loss": 0.0037,
      "step": 1213
    },
    {
      "epoch": 50.45,
      "grad_norm": 1.3448398113250732,
      "learning_rate": 8.777984390353048e-06,
      "loss": 0.0099,
      "step": 1214
    },
    {
      "epoch": 50.49,
      "grad_norm": 1.4029511213302612,
      "learning_rate": 8.775883757723156e-06,
      "loss": 0.0094,
      "step": 1215
    },
    {
      "epoch": 50.54,
      "grad_norm": 1.113113284111023,
      "learning_rate": 8.773781572970079e-06,
      "loss": 0.005,
      "step": 1216
    },
    {
      "epoch": 50.58,
      "grad_norm": 1.6387443542480469,
      "learning_rate": 8.771677836957944e-06,
      "loss": 0.0146,
      "step": 1217
    },
    {
      "epoch": 50.62,
      "grad_norm": 0.5599250793457031,
      "learning_rate": 8.769572550551522e-06,
      "loss": 0.0034,
      "step": 1218
    },
    {
      "epoch": 50.66,
      "grad_norm": 0.9514482021331787,
      "learning_rate": 8.767465714616212e-06,
      "loss": 0.0111,
      "step": 1219
    },
    {
      "epoch": 50.7,
      "grad_norm": 1.1756750345230103,
      "learning_rate": 8.765357330018056e-06,
      "loss": 0.0116,
      "step": 1220
    },
    {
      "epoch": 50.74,
      "grad_norm": 0.7426458597183228,
      "learning_rate": 8.763247397623731e-06,
      "loss": 0.0048,
      "step": 1221
    },
    {
      "epoch": 50.78,
      "grad_norm": 1.3610745668411255,
      "learning_rate": 8.761135918300552e-06,
      "loss": 0.0123,
      "step": 1222
    },
    {
      "epoch": 50.83,
      "grad_norm": 0.48031094670295715,
      "learning_rate": 8.759022892916468e-06,
      "loss": 0.0037,
      "step": 1223
    },
    {
      "epoch": 50.87,
      "grad_norm": 0.7616623044013977,
      "learning_rate": 8.756908322340063e-06,
      "loss": 0.0055,
      "step": 1224
    },
    {
      "epoch": 50.91,
      "grad_norm": 1.267291784286499,
      "learning_rate": 8.754792207440557e-06,
      "loss": 0.0098,
      "step": 1225
    },
    {
      "epoch": 50.95,
      "grad_norm": 0.6551554799079895,
      "learning_rate": 8.752674549087806e-06,
      "loss": 0.0067,
      "step": 1226
    },
    {
      "epoch": 50.99,
      "grad_norm": 0.6634003520011902,
      "learning_rate": 8.750555348152299e-06,
      "loss": 0.0101,
      "step": 1227
    },
    {
      "epoch": 51.03,
      "grad_norm": 0.801013171672821,
      "learning_rate": 8.748434605505159e-06,
      "loss": 0.0078,
      "step": 1228
    },
    {
      "epoch": 51.08,
      "grad_norm": 0.4975464940071106,
      "learning_rate": 8.746312322018143e-06,
      "loss": 0.0038,
      "step": 1229
    },
    {
      "epoch": 51.12,
      "grad_norm": 0.21092425286769867,
      "learning_rate": 8.74418849856364e-06,
      "loss": 0.0028,
      "step": 1230
    },
    {
      "epoch": 51.16,
      "grad_norm": 1.2237693071365356,
      "learning_rate": 8.74206313601468e-06,
      "loss": 0.0085,
      "step": 1231
    },
    {
      "epoch": 51.2,
      "grad_norm": 0.6425766348838806,
      "learning_rate": 8.739936235244913e-06,
      "loss": 0.0031,
      "step": 1232
    },
    {
      "epoch": 51.24,
      "grad_norm": 1.392905831336975,
      "learning_rate": 8.73780779712863e-06,
      "loss": 0.0119,
      "step": 1233
    },
    {
      "epoch": 51.28,
      "grad_norm": 0.6462253332138062,
      "learning_rate": 8.73567782254075e-06,
      "loss": 0.0058,
      "step": 1234
    },
    {
      "epoch": 51.32,
      "grad_norm": 0.5814747214317322,
      "learning_rate": 8.733546312356826e-06,
      "loss": 0.0041,
      "step": 1235
    },
    {
      "epoch": 51.37,
      "grad_norm": 0.989851176738739,
      "learning_rate": 8.73141326745304e-06,
      "loss": 0.0065,
      "step": 1236
    },
    {
      "epoch": 51.41,
      "grad_norm": 0.5699052214622498,
      "learning_rate": 8.72927868870621e-06,
      "loss": 0.005,
      "step": 1237
    },
    {
      "epoch": 51.45,
      "grad_norm": 0.4508724510669708,
      "learning_rate": 8.727142576993776e-06,
      "loss": 0.0036,
      "step": 1238
    },
    {
      "epoch": 51.49,
      "grad_norm": 0.525702953338623,
      "learning_rate": 8.725004933193818e-06,
      "loss": 0.0027,
      "step": 1239
    },
    {
      "epoch": 51.53,
      "grad_norm": 0.7762078046798706,
      "learning_rate": 8.722865758185036e-06,
      "loss": 0.0062,
      "step": 1240
    },
    {
      "epoch": 51.57,
      "grad_norm": 0.9753125905990601,
      "learning_rate": 8.720725052846766e-06,
      "loss": 0.0103,
      "step": 1241
    },
    {
      "epoch": 51.62,
      "grad_norm": 0.8223097324371338,
      "learning_rate": 8.718582818058971e-06,
      "loss": 0.0081,
      "step": 1242
    },
    {
      "epoch": 51.66,
      "grad_norm": 0.45082390308380127,
      "learning_rate": 8.716439054702242e-06,
      "loss": 0.0034,
      "step": 1243
    },
    {
      "epoch": 51.7,
      "grad_norm": 0.5902648568153381,
      "learning_rate": 8.7142937636578e-06,
      "loss": 0.0064,
      "step": 1244
    },
    {
      "epoch": 51.74,
      "grad_norm": 0.4673459529876709,
      "learning_rate": 8.712146945807494e-06,
      "loss": 0.0042,
      "step": 1245
    },
    {
      "epoch": 51.78,
      "grad_norm": 0.8607320189476013,
      "learning_rate": 8.709998602033796e-06,
      "loss": 0.0072,
      "step": 1246
    },
    {
      "epoch": 51.82,
      "grad_norm": 0.35431811213493347,
      "learning_rate": 8.707848733219815e-06,
      "loss": 0.0041,
      "step": 1247
    },
    {
      "epoch": 51.86,
      "grad_norm": 0.4050891399383545,
      "learning_rate": 8.705697340249275e-06,
      "loss": 0.0029,
      "step": 1248
    },
    {
      "epoch": 51.91,
      "grad_norm": 0.2963046729564667,
      "learning_rate": 8.703544424006536e-06,
      "loss": 0.003,
      "step": 1249
    },
    {
      "epoch": 51.95,
      "grad_norm": 1.5041533708572388,
      "learning_rate": 8.701389985376578e-06,
      "loss": 0.0097,
      "step": 1250
    },
    {
      "epoch": 51.99,
      "grad_norm": 0.5369647741317749,
      "learning_rate": 8.699234025245012e-06,
      "loss": 0.0052,
      "step": 1251
    },
    {
      "epoch": 52.03,
      "grad_norm": 0.24573403596878052,
      "learning_rate": 8.69707654449807e-06,
      "loss": 0.0025,
      "step": 1252
    },
    {
      "epoch": 52.07,
      "grad_norm": 0.6393117904663086,
      "learning_rate": 8.694917544022611e-06,
      "loss": 0.0044,
      "step": 1253
    },
    {
      "epoch": 52.11,
      "grad_norm": 0.5293768644332886,
      "learning_rate": 8.692757024706121e-06,
      "loss": 0.0035,
      "step": 1254
    },
    {
      "epoch": 52.16,
      "grad_norm": 0.33408161997795105,
      "learning_rate": 8.690594987436705e-06,
      "loss": 0.0025,
      "step": 1255
    },
    {
      "epoch": 52.2,
      "grad_norm": 0.8877549171447754,
      "learning_rate": 8.688431433103094e-06,
      "loss": 0.0039,
      "step": 1256
    },
    {
      "epoch": 52.24,
      "grad_norm": 0.6393066048622131,
      "learning_rate": 8.686266362594646e-06,
      "loss": 0.0024,
      "step": 1257
    },
    {
      "epoch": 52.28,
      "grad_norm": 0.6311200857162476,
      "learning_rate": 8.684099776801339e-06,
      "loss": 0.0037,
      "step": 1258
    },
    {
      "epoch": 52.32,
      "grad_norm": 0.13558602333068848,
      "learning_rate": 8.681931676613774e-06,
      "loss": 0.0017,
      "step": 1259
    },
    {
      "epoch": 52.36,
      "grad_norm": 1.951553463935852,
      "learning_rate": 8.679762062923176e-06,
      "loss": 0.008,
      "step": 1260
    },
    {
      "epoch": 52.41,
      "grad_norm": 0.3374760150909424,
      "learning_rate": 8.67759093662139e-06,
      "loss": 0.0034,
      "step": 1261
    },
    {
      "epoch": 52.45,
      "grad_norm": 0.41313523054122925,
      "learning_rate": 8.675418298600884e-06,
      "loss": 0.0018,
      "step": 1262
    },
    {
      "epoch": 52.49,
      "grad_norm": 0.43705374002456665,
      "learning_rate": 8.67324414975475e-06,
      "loss": 0.0028,
      "step": 1263
    },
    {
      "epoch": 52.53,
      "grad_norm": 0.1922355741262436,
      "learning_rate": 8.671068490976695e-06,
      "loss": 0.0026,
      "step": 1264
    },
    {
      "epoch": 52.57,
      "grad_norm": 0.6183437705039978,
      "learning_rate": 8.668891323161053e-06,
      "loss": 0.0044,
      "step": 1265
    },
    {
      "epoch": 52.61,
      "grad_norm": 0.8186808824539185,
      "learning_rate": 8.666712647202775e-06,
      "loss": 0.0045,
      "step": 1266
    },
    {
      "epoch": 52.65,
      "grad_norm": 1.6890250444412231,
      "learning_rate": 8.664532463997429e-06,
      "loss": 0.0051,
      "step": 1267
    },
    {
      "epoch": 52.7,
      "grad_norm": 1.0319961309432983,
      "learning_rate": 8.66235077444121e-06,
      "loss": 0.0042,
      "step": 1268
    },
    {
      "epoch": 52.74,
      "grad_norm": 0.7627688646316528,
      "learning_rate": 8.660167579430926e-06,
      "loss": 0.0058,
      "step": 1269
    },
    {
      "epoch": 52.78,
      "grad_norm": 0.6607836484909058,
      "learning_rate": 8.657982879864007e-06,
      "loss": 0.0045,
      "step": 1270
    },
    {
      "epoch": 52.82,
      "grad_norm": 1.2430113554000854,
      "learning_rate": 8.655796676638502e-06,
      "loss": 0.0162,
      "step": 1271
    },
    {
      "epoch": 52.86,
      "grad_norm": 0.8802666068077087,
      "learning_rate": 8.653608970653072e-06,
      "loss": 0.0024,
      "step": 1272
    },
    {
      "epoch": 52.9,
      "grad_norm": 0.6477659940719604,
      "learning_rate": 8.651419762807005e-06,
      "loss": 0.0025,
      "step": 1273
    },
    {
      "epoch": 52.95,
      "grad_norm": 0.33283913135528564,
      "learning_rate": 8.649229054000198e-06,
      "loss": 0.0039,
      "step": 1274
    },
    {
      "epoch": 52.99,
      "grad_norm": 0.9946944713592529,
      "learning_rate": 8.647036845133171e-06,
      "loss": 0.0079,
      "step": 1275
    },
    {
      "epoch": 53.03,
      "grad_norm": 0.5937759280204773,
      "learning_rate": 8.644843137107058e-06,
      "loss": 0.0056,
      "step": 1276
    },
    {
      "epoch": 53.07,
      "grad_norm": 0.6191762685775757,
      "learning_rate": 8.64264793082361e-06,
      "loss": 0.0063,
      "step": 1277
    },
    {
      "epoch": 53.11,
      "grad_norm": 0.5228250026702881,
      "learning_rate": 8.640451227185191e-06,
      "loss": 0.0035,
      "step": 1278
    },
    {
      "epoch": 53.15,
      "grad_norm": 0.6324213147163391,
      "learning_rate": 8.638253027094785e-06,
      "loss": 0.0028,
      "step": 1279
    },
    {
      "epoch": 53.19,
      "grad_norm": 0.8524994254112244,
      "learning_rate": 8.636053331455986e-06,
      "loss": 0.0074,
      "step": 1280
    },
    {
      "epoch": 53.24,
      "grad_norm": 0.3066350221633911,
      "learning_rate": 8.633852141173012e-06,
      "loss": 0.002,
      "step": 1281
    },
    {
      "epoch": 53.28,
      "grad_norm": 0.48596736788749695,
      "learning_rate": 8.631649457150684e-06,
      "loss": 0.0022,
      "step": 1282
    },
    {
      "epoch": 53.32,
      "grad_norm": 1.3081402778625488,
      "learning_rate": 8.629445280294445e-06,
      "loss": 0.0066,
      "step": 1283
    },
    {
      "epoch": 53.36,
      "grad_norm": 0.1253579556941986,
      "learning_rate": 8.627239611510343e-06,
      "loss": 0.0017,
      "step": 1284
    },
    {
      "epoch": 53.4,
      "grad_norm": 1.2161333560943604,
      "learning_rate": 8.625032451705053e-06,
      "loss": 0.0075,
      "step": 1285
    },
    {
      "epoch": 53.44,
      "grad_norm": 0.6697121262550354,
      "learning_rate": 8.622823801785851e-06,
      "loss": 0.0045,
      "step": 1286
    },
    {
      "epoch": 53.49,
      "grad_norm": 0.6571714878082275,
      "learning_rate": 8.62061366266063e-06,
      "loss": 0.0046,
      "step": 1287
    },
    {
      "epoch": 53.53,
      "grad_norm": 0.44000500440597534,
      "learning_rate": 8.618402035237895e-06,
      "loss": 0.0029,
      "step": 1288
    },
    {
      "epoch": 53.57,
      "grad_norm": 0.5238385200500488,
      "learning_rate": 8.61618892042676e-06,
      "loss": 0.0026,
      "step": 1289
    },
    {
      "epoch": 53.61,
      "grad_norm": 0.2109307199716568,
      "learning_rate": 8.613974319136959e-06,
      "loss": 0.0025,
      "step": 1290
    },
    {
      "epoch": 53.65,
      "grad_norm": 0.3698136508464813,
      "learning_rate": 8.611758232278824e-06,
      "loss": 0.0035,
      "step": 1291
    },
    {
      "epoch": 53.69,
      "grad_norm": 0.5262768268585205,
      "learning_rate": 8.60954066076331e-06,
      "loss": 0.0047,
      "step": 1292
    },
    {
      "epoch": 53.74,
      "grad_norm": 0.39974337816238403,
      "learning_rate": 8.607321605501974e-06,
      "loss": 0.0039,
      "step": 1293
    },
    {
      "epoch": 53.78,
      "grad_norm": 1.4421844482421875,
      "learning_rate": 8.605101067406986e-06,
      "loss": 0.0048,
      "step": 1294
    },
    {
      "epoch": 53.82,
      "grad_norm": 0.2769073247909546,
      "learning_rate": 8.602879047391127e-06,
      "loss": 0.0022,
      "step": 1295
    },
    {
      "epoch": 53.86,
      "grad_norm": 0.821966290473938,
      "learning_rate": 8.600655546367782e-06,
      "loss": 0.0072,
      "step": 1296
    },
    {
      "epoch": 53.9,
      "grad_norm": 0.9459247589111328,
      "learning_rate": 8.598430565250953e-06,
      "loss": 0.0042,
      "step": 1297
    },
    {
      "epoch": 53.94,
      "grad_norm": 1.6490904092788696,
      "learning_rate": 8.596204104955241e-06,
      "loss": 0.0091,
      "step": 1298
    },
    {
      "epoch": 53.98,
      "grad_norm": 1.9412058591842651,
      "learning_rate": 8.593976166395864e-06,
      "loss": 0.0093,
      "step": 1299
    },
    {
      "epoch": 54.03,
      "grad_norm": 0.8545070290565491,
      "learning_rate": 8.591746750488639e-06,
      "loss": 0.0029,
      "step": 1300
    },
    {
      "epoch": 54.07,
      "grad_norm": 0.5186325311660767,
      "learning_rate": 8.589515858149998e-06,
      "loss": 0.0038,
      "step": 1301
    },
    {
      "epoch": 54.11,
      "grad_norm": 0.7277250289916992,
      "learning_rate": 8.587283490296976e-06,
      "loss": 0.0034,
      "step": 1302
    },
    {
      "epoch": 54.15,
      "grad_norm": 0.42290833592414856,
      "learning_rate": 8.585049647847214e-06,
      "loss": 0.0018,
      "step": 1303
    },
    {
      "epoch": 54.19,
      "grad_norm": 0.9147427678108215,
      "learning_rate": 8.582814331718961e-06,
      "loss": 0.0069,
      "step": 1304
    },
    {
      "epoch": 54.23,
      "grad_norm": 0.25865787267684937,
      "learning_rate": 8.580577542831072e-06,
      "loss": 0.0018,
      "step": 1305
    },
    {
      "epoch": 54.28,
      "grad_norm": 0.9719850420951843,
      "learning_rate": 8.578339282103006e-06,
      "loss": 0.0054,
      "step": 1306
    },
    {
      "epoch": 54.32,
      "grad_norm": 0.5602405667304993,
      "learning_rate": 8.576099550454825e-06,
      "loss": 0.0065,
      "step": 1307
    },
    {
      "epoch": 54.36,
      "grad_norm": 0.6384018063545227,
      "learning_rate": 8.5738583488072e-06,
      "loss": 0.0038,
      "step": 1308
    },
    {
      "epoch": 54.4,
      "grad_norm": 0.8248264789581299,
      "learning_rate": 8.571615678081405e-06,
      "loss": 0.006,
      "step": 1309
    },
    {
      "epoch": 54.44,
      "grad_norm": 0.318477600812912,
      "learning_rate": 8.569371539199316e-06,
      "loss": 0.0029,
      "step": 1310
    },
    {
      "epoch": 54.48,
      "grad_norm": 0.6827839612960815,
      "learning_rate": 8.567125933083415e-06,
      "loss": 0.0065,
      "step": 1311
    },
    {
      "epoch": 54.52,
      "grad_norm": 0.4902893304824829,
      "learning_rate": 8.564878860656784e-06,
      "loss": 0.0029,
      "step": 1312
    },
    {
      "epoch": 54.57,
      "grad_norm": 1.0366663932800293,
      "learning_rate": 8.56263032284311e-06,
      "loss": 0.0056,
      "step": 1313
    },
    {
      "epoch": 54.61,
      "grad_norm": 1.4969267845153809,
      "learning_rate": 8.560380320566685e-06,
      "loss": 0.0099,
      "step": 1314
    },
    {
      "epoch": 54.65,
      "grad_norm": 0.27921241521835327,
      "learning_rate": 8.558128854752397e-06,
      "loss": 0.0046,
      "step": 1315
    },
    {
      "epoch": 54.69,
      "grad_norm": 0.7169631719589233,
      "learning_rate": 8.555875926325738e-06,
      "loss": 0.0069,
      "step": 1316
    },
    {
      "epoch": 54.73,
      "grad_norm": 0.43078216910362244,
      "learning_rate": 8.553621536212801e-06,
      "loss": 0.0032,
      "step": 1317
    },
    {
      "epoch": 54.77,
      "grad_norm": 1.8228498697280884,
      "learning_rate": 8.551365685340285e-06,
      "loss": 0.0075,
      "step": 1318
    },
    {
      "epoch": 54.82,
      "grad_norm": 1.300522804260254,
      "learning_rate": 8.549108374635482e-06,
      "loss": 0.0137,
      "step": 1319
    },
    {
      "epoch": 54.86,
      "grad_norm": 0.8669828176498413,
      "learning_rate": 8.54684960502629e-06,
      "loss": 0.0072,
      "step": 1320
    },
    {
      "epoch": 54.9,
      "grad_norm": 0.6617681980133057,
      "learning_rate": 8.5445893774412e-06,
      "loss": 0.0042,
      "step": 1321
    },
    {
      "epoch": 54.94,
      "grad_norm": 4.640839576721191,
      "learning_rate": 8.542327692809307e-06,
      "loss": 0.0077,
      "step": 1322
    },
    {
      "epoch": 54.98,
      "grad_norm": 0.4451526701450348,
      "learning_rate": 8.540064552060304e-06,
      "loss": 0.0036,
      "step": 1323
    },
    {
      "epoch": 55.02,
      "grad_norm": 0.6910313963890076,
      "learning_rate": 8.537799956124486e-06,
      "loss": 0.006,
      "step": 1324
    },
    {
      "epoch": 55.06,
      "grad_norm": 0.5921975374221802,
      "learning_rate": 8.535533905932739e-06,
      "loss": 0.0039,
      "step": 1325
    },
    {
      "epoch": 55.11,
      "grad_norm": 0.34116241335868835,
      "learning_rate": 8.533266402416551e-06,
      "loss": 0.0054,
      "step": 1326
    },
    {
      "epoch": 55.15,
      "grad_norm": 1.0340110063552856,
      "learning_rate": 8.530997446508011e-06,
      "loss": 0.0049,
      "step": 1327
    },
    {
      "epoch": 55.19,
      "grad_norm": 0.4993220567703247,
      "learning_rate": 8.528727039139796e-06,
      "loss": 0.0035,
      "step": 1328
    },
    {
      "epoch": 55.23,
      "grad_norm": 0.7045443058013916,
      "learning_rate": 8.526455181245188e-06,
      "loss": 0.0051,
      "step": 1329
    },
    {
      "epoch": 55.27,
      "grad_norm": 0.7881790995597839,
      "learning_rate": 8.52418187375806e-06,
      "loss": 0.0059,
      "step": 1330
    },
    {
      "epoch": 55.31,
      "grad_norm": 0.48981916904449463,
      "learning_rate": 8.521907117612883e-06,
      "loss": 0.0034,
      "step": 1331
    },
    {
      "epoch": 55.36,
      "grad_norm": 0.4050632119178772,
      "learning_rate": 8.519630913744726e-06,
      "loss": 0.0021,
      "step": 1332
    },
    {
      "epoch": 55.4,
      "grad_norm": 0.6370220184326172,
      "learning_rate": 8.517353263089247e-06,
      "loss": 0.005,
      "step": 1333
    },
    {
      "epoch": 55.44,
      "grad_norm": 0.45973989367485046,
      "learning_rate": 8.515074166582703e-06,
      "loss": 0.0038,
      "step": 1334
    },
    {
      "epoch": 55.48,
      "grad_norm": 0.4939906597137451,
      "learning_rate": 8.512793625161947e-06,
      "loss": 0.0025,
      "step": 1335
    },
    {
      "epoch": 55.52,
      "grad_norm": 0.6238475441932678,
      "learning_rate": 8.51051163976442e-06,
      "loss": 0.0068,
      "step": 1336
    },
    {
      "epoch": 55.56,
      "grad_norm": 0.5153671503067017,
      "learning_rate": 8.508228211328164e-06,
      "loss": 0.0047,
      "step": 1337
    },
    {
      "epoch": 55.61,
      "grad_norm": 0.3961564004421234,
      "learning_rate": 8.505943340791804e-06,
      "loss": 0.0034,
      "step": 1338
    },
    {
      "epoch": 55.65,
      "grad_norm": 0.47352734208106995,
      "learning_rate": 8.503657029094569e-06,
      "loss": 0.0021,
      "step": 1339
    },
    {
      "epoch": 55.69,
      "grad_norm": 0.6008840799331665,
      "learning_rate": 8.501369277176275e-06,
      "loss": 0.004,
      "step": 1340
    },
    {
      "epoch": 55.73,
      "grad_norm": 0.2956819534301758,
      "learning_rate": 8.499080085977329e-06,
      "loss": 0.0026,
      "step": 1341
    },
    {
      "epoch": 55.77,
      "grad_norm": 0.3763463497161865,
      "learning_rate": 8.496789456438731e-06,
      "loss": 0.0044,
      "step": 1342
    },
    {
      "epoch": 55.81,
      "grad_norm": 0.503400981426239,
      "learning_rate": 8.494497389502075e-06,
      "loss": 0.0039,
      "step": 1343
    },
    {
      "epoch": 55.85,
      "grad_norm": 0.2157876193523407,
      "learning_rate": 8.492203886109538e-06,
      "loss": 0.0018,
      "step": 1344
    },
    {
      "epoch": 55.9,
      "grad_norm": 1.0871254205703735,
      "learning_rate": 8.489908947203897e-06,
      "loss": 0.0062,
      "step": 1345
    },
    {
      "epoch": 55.94,
      "grad_norm": 1.2179584503173828,
      "learning_rate": 8.487612573728513e-06,
      "loss": 0.0089,
      "step": 1346
    },
    {
      "epoch": 55.98,
      "grad_norm": 0.34121909737586975,
      "learning_rate": 8.485314766627337e-06,
      "loss": 0.0027,
      "step": 1347
    },
    {
      "epoch": 56.02,
      "grad_norm": 0.21461938321590424,
      "learning_rate": 8.483015526844914e-06,
      "loss": 0.0018,
      "step": 1348
    },
    {
      "epoch": 56.06,
      "grad_norm": 0.336132675409317,
      "learning_rate": 8.480714855326372e-06,
      "loss": 0.0021,
      "step": 1349
    },
    {
      "epoch": 56.1,
      "grad_norm": 0.6521293520927429,
      "learning_rate": 8.478412753017433e-06,
      "loss": 0.0041,
      "step": 1350
    },
    {
      "epoch": 56.15,
      "grad_norm": 0.36963316798210144,
      "learning_rate": 8.476109220864401e-06,
      "loss": 0.0026,
      "step": 1351
    },
    {
      "epoch": 56.19,
      "grad_norm": 0.6367881894111633,
      "learning_rate": 8.473804259814173e-06,
      "loss": 0.0025,
      "step": 1352
    },
    {
      "epoch": 56.23,
      "grad_norm": 0.33711305260658264,
      "learning_rate": 8.47149787081423e-06,
      "loss": 0.0034,
      "step": 1353
    },
    {
      "epoch": 56.27,
      "grad_norm": 0.23613348603248596,
      "learning_rate": 8.469190054812642e-06,
      "loss": 0.0019,
      "step": 1354
    },
    {
      "epoch": 56.31,
      "grad_norm": 0.3548547029495239,
      "learning_rate": 8.466880812758064e-06,
      "loss": 0.0021,
      "step": 1355
    },
    {
      "epoch": 56.35,
      "grad_norm": 0.38014769554138184,
      "learning_rate": 8.464570145599742e-06,
      "loss": 0.0026,
      "step": 1356
    },
    {
      "epoch": 56.39,
      "grad_norm": 0.4771813750267029,
      "learning_rate": 8.4622580542875e-06,
      "loss": 0.0039,
      "step": 1357
    },
    {
      "epoch": 56.44,
      "grad_norm": 0.44392189383506775,
      "learning_rate": 8.459944539771755e-06,
      "loss": 0.0033,
      "step": 1358
    },
    {
      "epoch": 56.48,
      "grad_norm": 0.5894204378128052,
      "learning_rate": 8.457629603003502e-06,
      "loss": 0.0036,
      "step": 1359
    },
    {
      "epoch": 56.52,
      "grad_norm": 1.1722124814987183,
      "learning_rate": 8.455313244934324e-06,
      "loss": 0.0047,
      "step": 1360
    },
    {
      "epoch": 56.56,
      "grad_norm": 0.4929201900959015,
      "learning_rate": 8.45299546651639e-06,
      "loss": 0.0026,
      "step": 1361
    },
    {
      "epoch": 56.6,
      "grad_norm": 0.34467270970344543,
      "learning_rate": 8.45067626870245e-06,
      "loss": 0.0022,
      "step": 1362
    },
    {
      "epoch": 56.64,
      "grad_norm": 0.4157518744468689,
      "learning_rate": 8.448355652445843e-06,
      "loss": 0.0018,
      "step": 1363
    },
    {
      "epoch": 56.69,
      "grad_norm": 0.42285820841789246,
      "learning_rate": 8.44603361870048e-06,
      "loss": 0.0027,
      "step": 1364
    },
    {
      "epoch": 56.73,
      "grad_norm": 1.0900005102157593,
      "learning_rate": 8.443710168420866e-06,
      "loss": 0.0063,
      "step": 1365
    },
    {
      "epoch": 56.77,
      "grad_norm": 0.7384186387062073,
      "learning_rate": 8.44138530256208e-06,
      "loss": 0.0063,
      "step": 1366
    },
    {
      "epoch": 56.81,
      "grad_norm": 0.5632984042167664,
      "learning_rate": 8.439059022079789e-06,
      "loss": 0.0047,
      "step": 1367
    },
    {
      "epoch": 56.85,
      "grad_norm": 0.30081355571746826,
      "learning_rate": 8.43673132793024e-06,
      "loss": 0.0021,
      "step": 1368
    },
    {
      "epoch": 56.89,
      "grad_norm": 0.2456398457288742,
      "learning_rate": 8.434402221070258e-06,
      "loss": 0.0026,
      "step": 1369
    },
    {
      "epoch": 56.94,
      "grad_norm": 1.2122966051101685,
      "learning_rate": 8.432071702457253e-06,
      "loss": 0.0052,
      "step": 1370
    },
    {
      "epoch": 56.98,
      "grad_norm": 0.5266070365905762,
      "learning_rate": 8.42973977304921e-06,
      "loss": 0.0074,
      "step": 1371
    },
    {
      "epoch": 57.02,
      "grad_norm": 0.4518698453903198,
      "learning_rate": 8.4274064338047e-06,
      "loss": 0.0024,
      "step": 1372
    },
    {
      "epoch": 57.06,
      "grad_norm": 0.320586234331131,
      "learning_rate": 8.425071685682868e-06,
      "loss": 0.0031,
      "step": 1373
    },
    {
      "epoch": 57.1,
      "grad_norm": 1.3189719915390015,
      "learning_rate": 8.422735529643445e-06,
      "loss": 0.0032,
      "step": 1374
    },
    {
      "epoch": 57.14,
      "grad_norm": 0.7598975896835327,
      "learning_rate": 8.420397966646732e-06,
      "loss": 0.003,
      "step": 1375
    },
    {
      "epoch": 57.18,
      "grad_norm": 0.7248868346214294,
      "learning_rate": 8.418058997653613e-06,
      "loss": 0.0041,
      "step": 1376
    },
    {
      "epoch": 57.23,
      "grad_norm": 0.5427260994911194,
      "learning_rate": 8.415718623625553e-06,
      "loss": 0.0047,
      "step": 1377
    },
    {
      "epoch": 57.27,
      "grad_norm": 0.7906457185745239,
      "learning_rate": 8.41337684552459e-06,
      "loss": 0.0039,
      "step": 1378
    },
    {
      "epoch": 57.31,
      "grad_norm": 0.13359220325946808,
      "learning_rate": 8.411033664313341e-06,
      "loss": 0.0018,
      "step": 1379
    },
    {
      "epoch": 57.35,
      "grad_norm": 0.13614076375961304,
      "learning_rate": 8.408689080954997e-06,
      "loss": 0.002,
      "step": 1380
    },
    {
      "epoch": 57.39,
      "grad_norm": 0.4237386882305145,
      "learning_rate": 8.406343096413333e-06,
      "loss": 0.0018,
      "step": 1381
    },
    {
      "epoch": 57.43,
      "grad_norm": 0.5176068544387817,
      "learning_rate": 8.403995711652687e-06,
      "loss": 0.0033,
      "step": 1382
    },
    {
      "epoch": 57.48,
      "grad_norm": 0.46843963861465454,
      "learning_rate": 8.401646927637985e-06,
      "loss": 0.0036,
      "step": 1383
    },
    {
      "epoch": 57.52,
      "grad_norm": 0.9009042978286743,
      "learning_rate": 8.399296745334723e-06,
      "loss": 0.0053,
      "step": 1384
    },
    {
      "epoch": 57.56,
      "grad_norm": 0.3926481604576111,
      "learning_rate": 8.396945165708971e-06,
      "loss": 0.0028,
      "step": 1385
    },
    {
      "epoch": 57.6,
      "grad_norm": 0.5310323238372803,
      "learning_rate": 8.394592189727375e-06,
      "loss": 0.0023,
      "step": 1386
    },
    {
      "epoch": 57.64,
      "grad_norm": 0.48956018686294556,
      "learning_rate": 8.392237818357157e-06,
      "loss": 0.0041,
      "step": 1387
    },
    {
      "epoch": 57.68,
      "grad_norm": 0.23753061890602112,
      "learning_rate": 8.389882052566106e-06,
      "loss": 0.002,
      "step": 1388
    },
    {
      "epoch": 57.72,
      "grad_norm": 0.9691587686538696,
      "learning_rate": 8.38752489332259e-06,
      "loss": 0.0051,
      "step": 1389
    },
    {
      "epoch": 57.77,
      "grad_norm": 0.8677119612693787,
      "learning_rate": 8.38516634159555e-06,
      "loss": 0.0066,
      "step": 1390
    },
    {
      "epoch": 57.81,
      "grad_norm": 0.16950291395187378,
      "learning_rate": 8.382806398354493e-06,
      "loss": 0.0022,
      "step": 1391
    },
    {
      "epoch": 57.85,
      "grad_norm": 0.10101105272769928,
      "learning_rate": 8.380445064569506e-06,
      "loss": 0.0017,
      "step": 1392
    },
    {
      "epoch": 57.89,
      "grad_norm": 0.6798502802848816,
      "learning_rate": 8.378082341211245e-06,
      "loss": 0.0032,
      "step": 1393
    },
    {
      "epoch": 57.93,
      "grad_norm": 1.1201341152191162,
      "learning_rate": 8.375718229250935e-06,
      "loss": 0.0073,
      "step": 1394
    },
    {
      "epoch": 57.97,
      "grad_norm": 0.5697822570800781,
      "learning_rate": 8.373352729660373e-06,
      "loss": 0.0042,
      "step": 1395
    },
    {
      "epoch": 58.02,
      "grad_norm": 0.6468866467475891,
      "learning_rate": 8.370985843411924e-06,
      "loss": 0.0035,
      "step": 1396
    },
    {
      "epoch": 58.06,
      "grad_norm": 0.3288482427597046,
      "learning_rate": 8.368617571478533e-06,
      "loss": 0.0021,
      "step": 1397
    },
    {
      "epoch": 58.1,
      "grad_norm": 0.26765406131744385,
      "learning_rate": 8.366247914833698e-06,
      "loss": 0.0014,
      "step": 1398
    },
    {
      "epoch": 58.14,
      "grad_norm": 0.36317673325538635,
      "learning_rate": 8.3638768744515e-06,
      "loss": 0.0025,
      "step": 1399
    },
    {
      "epoch": 58.18,
      "grad_norm": 0.43029558658599854,
      "learning_rate": 8.361504451306585e-06,
      "loss": 0.002,
      "step": 1400
    },
    {
      "epoch": 58.22,
      "grad_norm": 0.7313687205314636,
      "learning_rate": 8.359130646374164e-06,
      "loss": 0.0066,
      "step": 1401
    },
    {
      "epoch": 58.26,
      "grad_norm": 1.3367141485214233,
      "learning_rate": 8.35675546063002e-06,
      "loss": 0.0075,
      "step": 1402
    },
    {
      "epoch": 58.31,
      "grad_norm": 1.034269094467163,
      "learning_rate": 8.354378895050502e-06,
      "loss": 0.0041,
      "step": 1403
    },
    {
      "epoch": 58.35,
      "grad_norm": 0.5575722455978394,
      "learning_rate": 8.352000950612526e-06,
      "loss": 0.0024,
      "step": 1404
    },
    {
      "epoch": 58.39,
      "grad_norm": 0.2624622881412506,
      "learning_rate": 8.349621628293578e-06,
      "loss": 0.0021,
      "step": 1405
    },
    {
      "epoch": 58.43,
      "grad_norm": 0.5623326897621155,
      "learning_rate": 8.347240929071701e-06,
      "loss": 0.0039,
      "step": 1406
    },
    {
      "epoch": 58.47,
      "grad_norm": 1.002193808555603,
      "learning_rate": 8.344858853925517e-06,
      "loss": 0.0072,
      "step": 1407
    },
    {
      "epoch": 58.51,
      "grad_norm": 0.4651375412940979,
      "learning_rate": 8.342475403834203e-06,
      "loss": 0.0021,
      "step": 1408
    },
    {
      "epoch": 58.56,
      "grad_norm": 0.5857270956039429,
      "learning_rate": 8.340090579777507e-06,
      "loss": 0.0049,
      "step": 1409
    },
    {
      "epoch": 58.6,
      "grad_norm": 0.12580008804798126,
      "learning_rate": 8.337704382735741e-06,
      "loss": 0.0024,
      "step": 1410
    },
    {
      "epoch": 58.64,
      "grad_norm": 0.21736973524093628,
      "learning_rate": 8.335316813689778e-06,
      "loss": 0.0031,
      "step": 1411
    },
    {
      "epoch": 58.68,
      "grad_norm": 0.5100632309913635,
      "learning_rate": 8.332927873621059e-06,
      "loss": 0.005,
      "step": 1412
    },
    {
      "epoch": 58.72,
      "grad_norm": 0.7640446424484253,
      "learning_rate": 8.330537563511588e-06,
      "loss": 0.0048,
      "step": 1413
    },
    {
      "epoch": 58.76,
      "grad_norm": 0.35307443141937256,
      "learning_rate": 8.32814588434393e-06,
      "loss": 0.0025,
      "step": 1414
    },
    {
      "epoch": 58.81,
      "grad_norm": 0.9159979224205017,
      "learning_rate": 8.325752837101213e-06,
      "loss": 0.0054,
      "step": 1415
    },
    {
      "epoch": 58.85,
      "grad_norm": 0.2115827202796936,
      "learning_rate": 8.32335842276713e-06,
      "loss": 0.0026,
      "step": 1416
    },
    {
      "epoch": 58.89,
      "grad_norm": 0.49544334411621094,
      "learning_rate": 8.320962642325931e-06,
      "loss": 0.0019,
      "step": 1417
    },
    {
      "epoch": 58.93,
      "grad_norm": 0.5957121849060059,
      "learning_rate": 8.318565496762436e-06,
      "loss": 0.0025,
      "step": 1418
    },
    {
      "epoch": 58.97,
      "grad_norm": 0.8536818027496338,
      "learning_rate": 8.31616698706202e-06,
      "loss": 0.0034,
      "step": 1419
    },
    {
      "epoch": 59.01,
      "grad_norm": 0.9444447159767151,
      "learning_rate": 8.313767114210615e-06,
      "loss": 0.0078,
      "step": 1420
    },
    {
      "epoch": 59.05,
      "grad_norm": 0.6312692165374756,
      "learning_rate": 8.311365879194725e-06,
      "loss": 0.0022,
      "step": 1421
    },
    {
      "epoch": 59.1,
      "grad_norm": 0.634848415851593,
      "learning_rate": 8.308963283001399e-06,
      "loss": 0.0035,
      "step": 1422
    },
    {
      "epoch": 59.14,
      "grad_norm": 1.0440700054168701,
      "learning_rate": 8.30655932661826e-06,
      "loss": 0.0048,
      "step": 1423
    },
    {
      "epoch": 59.18,
      "grad_norm": 2.228647232055664,
      "learning_rate": 8.30415401103348e-06,
      "loss": 0.0178,
      "step": 1424
    },
    {
      "epoch": 59.22,
      "grad_norm": 0.28724467754364014,
      "learning_rate": 8.301747337235798e-06,
      "loss": 0.0025,
      "step": 1425
    },
    {
      "epoch": 59.26,
      "grad_norm": 0.47835013270378113,
      "learning_rate": 8.299339306214501e-06,
      "loss": 0.0025,
      "step": 1426
    },
    {
      "epoch": 59.3,
      "grad_norm": 0.08275994658470154,
      "learning_rate": 8.296929918959444e-06,
      "loss": 0.0014,
      "step": 1427
    },
    {
      "epoch": 59.35,
      "grad_norm": 0.5753483772277832,
      "learning_rate": 8.29451917646103e-06,
      "loss": 0.0047,
      "step": 1428
    },
    {
      "epoch": 59.39,
      "grad_norm": 0.7134208679199219,
      "learning_rate": 8.292107079710231e-06,
      "loss": 0.0059,
      "step": 1429
    },
    {
      "epoch": 59.43,
      "grad_norm": 0.7145932912826538,
      "learning_rate": 8.289693629698564e-06,
      "loss": 0.0043,
      "step": 1430
    },
    {
      "epoch": 59.47,
      "grad_norm": 0.8776552677154541,
      "learning_rate": 8.287278827418108e-06,
      "loss": 0.0084,
      "step": 1431
    },
    {
      "epoch": 59.51,
      "grad_norm": 0.4880371689796448,
      "learning_rate": 8.284862673861498e-06,
      "loss": 0.0027,
      "step": 1432
    },
    {
      "epoch": 59.55,
      "grad_norm": 0.7749248743057251,
      "learning_rate": 8.282445170021922e-06,
      "loss": 0.0058,
      "step": 1433
    },
    {
      "epoch": 59.59,
      "grad_norm": 0.42411166429519653,
      "learning_rate": 8.280026316893126e-06,
      "loss": 0.0065,
      "step": 1434
    },
    {
      "epoch": 59.64,
      "grad_norm": 0.39089277386665344,
      "learning_rate": 8.27760611546941e-06,
      "loss": 0.0033,
      "step": 1435
    },
    {
      "epoch": 59.68,
      "grad_norm": 0.6023206114768982,
      "learning_rate": 8.275184566745625e-06,
      "loss": 0.0037,
      "step": 1436
    },
    {
      "epoch": 59.72,
      "grad_norm": 0.5456635355949402,
      "learning_rate": 8.272761671717178e-06,
      "loss": 0.004,
      "step": 1437
    },
    {
      "epoch": 59.76,
      "grad_norm": 0.30101707577705383,
      "learning_rate": 8.270337431380032e-06,
      "loss": 0.0022,
      "step": 1438
    },
    {
      "epoch": 59.8,
      "grad_norm": 0.20347632467746735,
      "learning_rate": 8.267911846730698e-06,
      "loss": 0.0026,
      "step": 1439
    },
    {
      "epoch": 59.84,
      "grad_norm": 0.7975518107414246,
      "learning_rate": 8.265484918766243e-06,
      "loss": 0.0049,
      "step": 1440
    },
    {
      "epoch": 59.89,
      "grad_norm": 0.2715282142162323,
      "learning_rate": 8.26305664848429e-06,
      "loss": 0.0021,
      "step": 1441
    },
    {
      "epoch": 59.93,
      "grad_norm": 0.6518247723579407,
      "learning_rate": 8.260627036883002e-06,
      "loss": 0.0046,
      "step": 1442
    },
    {
      "epoch": 59.97,
      "grad_norm": 0.9467716217041016,
      "learning_rate": 8.258196084961103e-06,
      "loss": 0.0048,
      "step": 1443
    },
    {
      "epoch": 60.01,
      "grad_norm": 0.8156931400299072,
      "learning_rate": 8.255763793717868e-06,
      "loss": 0.0048,
      "step": 1444
    },
    {
      "epoch": 60.05,
      "grad_norm": 0.14342746138572693,
      "learning_rate": 8.253330164153118e-06,
      "loss": 0.0015,
      "step": 1445
    },
    {
      "epoch": 60.09,
      "grad_norm": 0.6473680734634399,
      "learning_rate": 8.250895197267227e-06,
      "loss": 0.0046,
      "step": 1446
    },
    {
      "epoch": 60.14,
      "grad_norm": 0.6063244342803955,
      "learning_rate": 8.248458894061117e-06,
      "loss": 0.0045,
      "step": 1447
    },
    {
      "epoch": 60.18,
      "grad_norm": 0.4305933713912964,
      "learning_rate": 8.24602125553626e-06,
      "loss": 0.0025,
      "step": 1448
    },
    {
      "epoch": 60.22,
      "grad_norm": 0.21977804601192474,
      "learning_rate": 8.24358228269468e-06,
      "loss": 0.0013,
      "step": 1449
    },
    {
      "epoch": 60.26,
      "grad_norm": 0.411499559879303,
      "learning_rate": 8.241141976538944e-06,
      "loss": 0.002,
      "step": 1450
    },
    {
      "epoch": 60.3,
      "grad_norm": 0.8475086092948914,
      "learning_rate": 8.238700338072167e-06,
      "loss": 0.006,
      "step": 1451
    },
    {
      "epoch": 60.34,
      "grad_norm": 1.11546790599823,
      "learning_rate": 8.236257368298022e-06,
      "loss": 0.0051,
      "step": 1452
    },
    {
      "epoch": 60.38,
      "grad_norm": 0.37826696038246155,
      "learning_rate": 8.233813068220712e-06,
      "loss": 0.0035,
      "step": 1453
    },
    {
      "epoch": 60.43,
      "grad_norm": 2.902745485305786,
      "learning_rate": 8.231367438845005e-06,
      "loss": 0.01,
      "step": 1454
    },
    {
      "epoch": 60.47,
      "grad_norm": 0.7752913236618042,
      "learning_rate": 8.228920481176202e-06,
      "loss": 0.006,
      "step": 1455
    },
    {
      "epoch": 60.51,
      "grad_norm": 0.7414926290512085,
      "learning_rate": 8.226472196220156e-06,
      "loss": 0.0048,
      "step": 1456
    },
    {
      "epoch": 60.55,
      "grad_norm": 1.664575457572937,
      "learning_rate": 8.224022584983267e-06,
      "loss": 0.0105,
      "step": 1457
    },
    {
      "epoch": 60.59,
      "grad_norm": 0.9054023027420044,
      "learning_rate": 8.221571648472473e-06,
      "loss": 0.006,
      "step": 1458
    },
    {
      "epoch": 60.63,
      "grad_norm": 0.7020925879478455,
      "learning_rate": 8.219119387695263e-06,
      "loss": 0.0033,
      "step": 1459
    },
    {
      "epoch": 60.68,
      "grad_norm": 0.5508925318717957,
      "learning_rate": 8.216665803659671e-06,
      "loss": 0.0028,
      "step": 1460
    },
    {
      "epoch": 60.72,
      "grad_norm": 0.45463043451309204,
      "learning_rate": 8.214210897374271e-06,
      "loss": 0.0037,
      "step": 1461
    },
    {
      "epoch": 60.76,
      "grad_norm": 0.18160812556743622,
      "learning_rate": 8.211754669848182e-06,
      "loss": 0.0017,
      "step": 1462
    },
    {
      "epoch": 60.8,
      "grad_norm": 0.538200318813324,
      "learning_rate": 8.209297122091066e-06,
      "loss": 0.0026,
      "step": 1463
    },
    {
      "epoch": 60.84,
      "grad_norm": 1.0441901683807373,
      "learning_rate": 8.206838255113132e-06,
      "loss": 0.0061,
      "step": 1464
    },
    {
      "epoch": 60.88,
      "grad_norm": 0.3036138117313385,
      "learning_rate": 8.204378069925121e-06,
      "loss": 0.0032,
      "step": 1465
    },
    {
      "epoch": 60.92,
      "grad_norm": 0.9790105819702148,
      "learning_rate": 8.201916567538327e-06,
      "loss": 0.0082,
      "step": 1466
    },
    {
      "epoch": 60.97,
      "grad_norm": 1.0840650796890259,
      "learning_rate": 8.199453748964579e-06,
      "loss": 0.0039,
      "step": 1467
    },
    {
      "epoch": 61.01,
      "grad_norm": 0.49669283628463745,
      "learning_rate": 8.196989615216248e-06,
      "loss": 0.0045,
      "step": 1468
    },
    {
      "epoch": 61.05,
      "grad_norm": 0.21298548579216003,
      "learning_rate": 8.194524167306247e-06,
      "loss": 0.0019,
      "step": 1469
    },
    {
      "epoch": 61.09,
      "grad_norm": 0.30746084451675415,
      "learning_rate": 8.192057406248028e-06,
      "loss": 0.0021,
      "step": 1470
    },
    {
      "epoch": 61.13,
      "grad_norm": 0.2779885232448578,
      "learning_rate": 8.189589333055586e-06,
      "loss": 0.0028,
      "step": 1471
    },
    {
      "epoch": 61.17,
      "grad_norm": 0.9866470098495483,
      "learning_rate": 8.18711994874345e-06,
      "loss": 0.0036,
      "step": 1472
    },
    {
      "epoch": 61.22,
      "grad_norm": 0.7177519798278809,
      "learning_rate": 8.18464925432669e-06,
      "loss": 0.0038,
      "step": 1473
    },
    {
      "epoch": 61.26,
      "grad_norm": 1.0134376287460327,
      "learning_rate": 8.182177250820918e-06,
      "loss": 0.0063,
      "step": 1474
    },
    {
      "epoch": 61.3,
      "grad_norm": 0.2610144317150116,
      "learning_rate": 8.179703939242276e-06,
      "loss": 0.0022,
      "step": 1475
    },
    {
      "epoch": 61.34,
      "grad_norm": 0.7147529125213623,
      "learning_rate": 8.177229320607455e-06,
      "loss": 0.0048,
      "step": 1476
    },
    {
      "epoch": 61.38,
      "grad_norm": 0.24283191561698914,
      "learning_rate": 8.174753395933674e-06,
      "loss": 0.0017,
      "step": 1477
    },
    {
      "epoch": 61.42,
      "grad_norm": 1.2249480485916138,
      "learning_rate": 8.172276166238694e-06,
      "loss": 0.0024,
      "step": 1478
    },
    {
      "epoch": 61.46,
      "grad_norm": 0.4769899249076843,
      "learning_rate": 8.16979763254081e-06,
      "loss": 0.002,
      "step": 1479
    },
    {
      "epoch": 61.51,
      "grad_norm": 0.4969356060028076,
      "learning_rate": 8.16731779585885e-06,
      "loss": 0.0042,
      "step": 1480
    },
    {
      "epoch": 61.55,
      "grad_norm": 0.47008681297302246,
      "learning_rate": 8.164836657212186e-06,
      "loss": 0.0024,
      "step": 1481
    },
    {
      "epoch": 61.59,
      "grad_norm": 0.6453546285629272,
      "learning_rate": 8.162354217620719e-06,
      "loss": 0.0038,
      "step": 1482
    },
    {
      "epoch": 61.63,
      "grad_norm": 0.24786880612373352,
      "learning_rate": 8.159870478104885e-06,
      "loss": 0.0015,
      "step": 1483
    },
    {
      "epoch": 61.67,
      "grad_norm": 0.5769243240356445,
      "learning_rate": 8.157385439685656e-06,
      "loss": 0.003,
      "step": 1484
    },
    {
      "epoch": 61.71,
      "grad_norm": 0.8417344689369202,
      "learning_rate": 8.154899103384536e-06,
      "loss": 0.004,
      "step": 1485
    },
    {
      "epoch": 61.76,
      "grad_norm": 0.916680097579956,
      "learning_rate": 8.15241147022357e-06,
      "loss": 0.0066,
      "step": 1486
    },
    {
      "epoch": 61.8,
      "grad_norm": 0.8809501528739929,
      "learning_rate": 8.149922541225322e-06,
      "loss": 0.0066,
      "step": 1487
    },
    {
      "epoch": 61.84,
      "grad_norm": 1.2462555170059204,
      "learning_rate": 8.147432317412902e-06,
      "loss": 0.0079,
      "step": 1488
    },
    {
      "epoch": 61.88,
      "grad_norm": 0.5798007249832153,
      "learning_rate": 8.144940799809944e-06,
      "loss": 0.0059,
      "step": 1489
    },
    {
      "epoch": 61.92,
      "grad_norm": 0.5398759841918945,
      "learning_rate": 8.142447989440618e-06,
      "loss": 0.0033,
      "step": 1490
    },
    {
      "epoch": 61.96,
      "grad_norm": 0.5190702676773071,
      "learning_rate": 8.139953887329626e-06,
      "loss": 0.0041,
      "step": 1491
    },
    {
      "epoch": 62.01,
      "grad_norm": 0.10448174923658371,
      "learning_rate": 8.137458494502195e-06,
      "loss": 0.0017,
      "step": 1492
    },
    {
      "epoch": 62.05,
      "grad_norm": 0.24114558100700378,
      "learning_rate": 8.13496181198409e-06,
      "loss": 0.002,
      "step": 1493
    },
    {
      "epoch": 62.09,
      "grad_norm": 1.0274789333343506,
      "learning_rate": 8.132463840801603e-06,
      "loss": 0.0042,
      "step": 1494
    },
    {
      "epoch": 62.13,
      "grad_norm": 0.35685446858406067,
      "learning_rate": 8.129964581981554e-06,
      "loss": 0.0031,
      "step": 1495
    },
    {
      "epoch": 62.17,
      "grad_norm": 0.7553402185440063,
      "learning_rate": 8.127464036551294e-06,
      "loss": 0.0026,
      "step": 1496
    },
    {
      "epoch": 62.21,
      "grad_norm": 0.5916438102722168,
      "learning_rate": 8.124962205538703e-06,
      "loss": 0.004,
      "step": 1497
    },
    {
      "epoch": 62.25,
      "grad_norm": 0.3998604118824005,
      "learning_rate": 8.12245908997219e-06,
      "loss": 0.0023,
      "step": 1498
    },
    {
      "epoch": 62.3,
      "grad_norm": 0.7810829877853394,
      "learning_rate": 8.11995469088069e-06,
      "loss": 0.0024,
      "step": 1499
    },
    {
      "epoch": 62.34,
      "grad_norm": 0.4716508984565735,
      "learning_rate": 8.117449009293668e-06,
      "loss": 0.0021,
      "step": 1500
    },
    {
      "epoch": 62.38,
      "grad_norm": 0.8073709607124329,
      "learning_rate": 8.114942046241115e-06,
      "loss": 0.0071,
      "step": 1501
    },
    {
      "epoch": 62.42,
      "grad_norm": 0.46745479106903076,
      "learning_rate": 8.112433802753547e-06,
      "loss": 0.003,
      "step": 1502
    },
    {
      "epoch": 62.46,
      "grad_norm": 0.2671242654323578,
      "learning_rate": 8.10992427986201e-06,
      "loss": 0.002,
      "step": 1503
    },
    {
      "epoch": 62.5,
      "grad_norm": 0.5171488523483276,
      "learning_rate": 8.107413478598076e-06,
      "loss": 0.0036,
      "step": 1504
    },
    {
      "epoch": 62.55,
      "grad_norm": 0.9740835428237915,
      "learning_rate": 8.104901399993837e-06,
      "loss": 0.0035,
      "step": 1505
    },
    {
      "epoch": 62.59,
      "grad_norm": 0.37216195464134216,
      "learning_rate": 8.102388045081915e-06,
      "loss": 0.0026,
      "step": 1506
    },
    {
      "epoch": 62.63,
      "grad_norm": 0.4120551645755768,
      "learning_rate": 8.099873414895453e-06,
      "loss": 0.0024,
      "step": 1507
    },
    {
      "epoch": 62.67,
      "grad_norm": 0.7663317918777466,
      "learning_rate": 8.097357510468125e-06,
      "loss": 0.0077,
      "step": 1508
    },
    {
      "epoch": 62.71,
      "grad_norm": 0.4456370174884796,
      "learning_rate": 8.094840332834123e-06,
      "loss": 0.0017,
      "step": 1509
    },
    {
      "epoch": 62.75,
      "grad_norm": 0.9266709685325623,
      "learning_rate": 8.092321883028157e-06,
      "loss": 0.0061,
      "step": 1510
    },
    {
      "epoch": 62.79,
      "grad_norm": 0.5846025943756104,
      "learning_rate": 8.089802162085478e-06,
      "loss": 0.0038,
      "step": 1511
    },
    {
      "epoch": 62.84,
      "grad_norm": 0.14623402059078217,
      "learning_rate": 8.087281171041838e-06,
      "loss": 0.0017,
      "step": 1512
    },
    {
      "epoch": 62.88,
      "grad_norm": 0.6242228150367737,
      "learning_rate": 8.084758910933527e-06,
      "loss": 0.0038,
      "step": 1513
    },
    {
      "epoch": 62.92,
      "grad_norm": 0.5876092314720154,
      "learning_rate": 8.08223538279735e-06,
      "loss": 0.0037,
      "step": 1514
    },
    {
      "epoch": 62.96,
      "grad_norm": 0.5906428098678589,
      "learning_rate": 8.079710587670633e-06,
      "loss": 0.0028,
      "step": 1515
    },
    {
      "epoch": 63.0,
      "grad_norm": 0.9653196334838867,
      "learning_rate": 8.077184526591224e-06,
      "loss": 0.004,
      "step": 1516
    },
    {
      "epoch": 63.04,
      "grad_norm": 0.18513457477092743,
      "learning_rate": 8.074657200597492e-06,
      "loss": 0.0019,
      "step": 1517
    },
    {
      "epoch": 63.09,
      "grad_norm": 0.8713278770446777,
      "learning_rate": 8.072128610728324e-06,
      "loss": 0.006,
      "step": 1518
    },
    {
      "epoch": 63.13,
      "grad_norm": 0.32825401425361633,
      "learning_rate": 8.06959875802313e-06,
      "loss": 0.0026,
      "step": 1519
    },
    {
      "epoch": 63.17,
      "grad_norm": 0.4202887713909149,
      "learning_rate": 8.067067643521834e-06,
      "loss": 0.003,
      "step": 1520
    },
    {
      "epoch": 63.21,
      "grad_norm": 0.9735097885131836,
      "learning_rate": 8.064535268264883e-06,
      "loss": 0.0032,
      "step": 1521
    },
    {
      "epoch": 63.25,
      "grad_norm": 0.17051297426223755,
      "learning_rate": 8.062001633293242e-06,
      "loss": 0.0023,
      "step": 1522
    },
    {
      "epoch": 63.29,
      "grad_norm": 0.22925813496112823,
      "learning_rate": 8.05946673964839e-06,
      "loss": 0.0018,
      "step": 1523
    },
    {
      "epoch": 63.34,
      "grad_norm": 0.3267020583152771,
      "learning_rate": 8.056930588372329e-06,
      "loss": 0.002,
      "step": 1524
    },
    {
      "epoch": 63.38,
      "grad_norm": 1.0559805631637573,
      "learning_rate": 8.054393180507572e-06,
      "loss": 0.0045,
      "step": 1525
    },
    {
      "epoch": 63.42,
      "grad_norm": 0.22541436553001404,
      "learning_rate": 8.051854517097154e-06,
      "loss": 0.0019,
      "step": 1526
    },
    {
      "epoch": 63.46,
      "grad_norm": 1.065682053565979,
      "learning_rate": 8.049314599184623e-06,
      "loss": 0.0044,
      "step": 1527
    },
    {
      "epoch": 63.5,
      "grad_norm": 0.844536304473877,
      "learning_rate": 8.046773427814043e-06,
      "loss": 0.0016,
      "step": 1528
    },
    {
      "epoch": 63.54,
      "grad_norm": 0.5452859997749329,
      "learning_rate": 8.044231004029993e-06,
      "loss": 0.0045,
      "step": 1529
    },
    {
      "epoch": 63.58,
      "grad_norm": 0.20100344717502594,
      "learning_rate": 8.041687328877566e-06,
      "loss": 0.0018,
      "step": 1530
    },
    {
      "epoch": 63.63,
      "grad_norm": 1.2057126760482788,
      "learning_rate": 8.039142403402374e-06,
      "loss": 0.0034,
      "step": 1531
    },
    {
      "epoch": 63.67,
      "grad_norm": 0.8524832129478455,
      "learning_rate": 8.03659622865054e-06,
      "loss": 0.0036,
      "step": 1532
    },
    {
      "epoch": 63.71,
      "grad_norm": 0.7965214252471924,
      "learning_rate": 8.034048805668697e-06,
      "loss": 0.0038,
      "step": 1533
    },
    {
      "epoch": 63.75,
      "grad_norm": 0.5880947113037109,
      "learning_rate": 8.031500135503995e-06,
      "loss": 0.0048,
      "step": 1534
    },
    {
      "epoch": 63.79,
      "grad_norm": 0.8560013771057129,
      "learning_rate": 8.0289502192041e-06,
      "loss": 0.0047,
      "step": 1535
    },
    {
      "epoch": 63.83,
      "grad_norm": 0.5674010515213013,
      "learning_rate": 8.026399057817182e-06,
      "loss": 0.0023,
      "step": 1536
    },
    {
      "epoch": 63.88,
      "grad_norm": 0.4253985285758972,
      "learning_rate": 8.023846652391928e-06,
      "loss": 0.0018,
      "step": 1537
    },
    {
      "epoch": 63.92,
      "grad_norm": 0.8935482501983643,
      "learning_rate": 8.021293003977538e-06,
      "loss": 0.0081,
      "step": 1538
    },
    {
      "epoch": 63.96,
      "grad_norm": 0.6691514849662781,
      "learning_rate": 8.018738113623714e-06,
      "loss": 0.004,
      "step": 1539
    },
    {
      "epoch": 64.0,
      "grad_norm": 0.5349974036216736,
      "learning_rate": 8.016181982380682e-06,
      "loss": 0.0041,
      "step": 1540
    },
    {
      "epoch": 64.04,
      "grad_norm": 0.5814852714538574,
      "learning_rate": 8.01362461129917e-06,
      "loss": 0.0026,
      "step": 1541
    },
    {
      "epoch": 64.08,
      "grad_norm": 0.4154580533504486,
      "learning_rate": 8.011066001430412e-06,
      "loss": 0.0024,
      "step": 1542
    },
    {
      "epoch": 64.12,
      "grad_norm": 0.7785847187042236,
      "learning_rate": 8.008506153826158e-06,
      "loss": 0.0023,
      "step": 1543
    },
    {
      "epoch": 64.17,
      "grad_norm": 1.9891047477722168,
      "learning_rate": 8.005945069538668e-06,
      "loss": 0.0047,
      "step": 1544
    },
    {
      "epoch": 64.21,
      "grad_norm": 0.326000839471817,
      "learning_rate": 8.003382749620704e-06,
      "loss": 0.0022,
      "step": 1545
    },
    {
      "epoch": 64.25,
      "grad_norm": 0.4360024631023407,
      "learning_rate": 8.000819195125537e-06,
      "loss": 0.0023,
      "step": 1546
    },
    {
      "epoch": 64.29,
      "grad_norm": 0.4036174714565277,
      "learning_rate": 7.998254407106952e-06,
      "loss": 0.0018,
      "step": 1547
    },
    {
      "epoch": 64.33,
      "grad_norm": 0.7124767303466797,
      "learning_rate": 7.995688386619233e-06,
      "loss": 0.0054,
      "step": 1548
    },
    {
      "epoch": 64.37,
      "grad_norm": 1.0086969137191772,
      "learning_rate": 7.993121134717177e-06,
      "loss": 0.0033,
      "step": 1549
    },
    {
      "epoch": 64.42,
      "grad_norm": 0.61448734998703,
      "learning_rate": 7.99055265245608e-06,
      "loss": 0.0027,
      "step": 1550
    },
    {
      "epoch": 64.46,
      "grad_norm": 0.827938437461853,
      "learning_rate": 7.987982940891751e-06,
      "loss": 0.0053,
      "step": 1551
    },
    {
      "epoch": 64.5,
      "grad_norm": 0.4141453206539154,
      "learning_rate": 7.985412001080503e-06,
      "loss": 0.002,
      "step": 1552
    },
    {
      "epoch": 64.54,
      "grad_norm": 0.5200092196464539,
      "learning_rate": 7.98283983407915e-06,
      "loss": 0.003,
      "step": 1553
    },
    {
      "epoch": 64.58,
      "grad_norm": 0.8610073924064636,
      "learning_rate": 7.980266440945013e-06,
      "loss": 0.0044,
      "step": 1554
    },
    {
      "epoch": 64.62,
      "grad_norm": 0.4894711375236511,
      "learning_rate": 7.977691822735914e-06,
      "loss": 0.004,
      "step": 1555
    },
    {
      "epoch": 64.66,
      "grad_norm": 0.5499009490013123,
      "learning_rate": 7.975115980510187e-06,
      "loss": 0.0027,
      "step": 1556
    },
    {
      "epoch": 64.71,
      "grad_norm": 0.9270853400230408,
      "learning_rate": 7.972538915326658e-06,
      "loss": 0.0045,
      "step": 1557
    },
    {
      "epoch": 64.75,
      "grad_norm": 0.5780571103096008,
      "learning_rate": 7.969960628244664e-06,
      "loss": 0.0031,
      "step": 1558
    },
    {
      "epoch": 64.79,
      "grad_norm": 0.6214079260826111,
      "learning_rate": 7.967381120324043e-06,
      "loss": 0.0034,
      "step": 1559
    },
    {
      "epoch": 64.83,
      "grad_norm": 0.8922154307365417,
      "learning_rate": 7.96480039262513e-06,
      "loss": 0.0083,
      "step": 1560
    },
    {
      "epoch": 64.87,
      "grad_norm": 1.1313780546188354,
      "learning_rate": 7.962218446208765e-06,
      "loss": 0.0032,
      "step": 1561
    },
    {
      "epoch": 64.91,
      "grad_norm": 0.8183439373970032,
      "learning_rate": 7.959635282136292e-06,
      "loss": 0.0044,
      "step": 1562
    },
    {
      "epoch": 64.96,
      "grad_norm": 0.579328715801239,
      "learning_rate": 7.957050901469544e-06,
      "loss": 0.0046,
      "step": 1563
    },
    {
      "epoch": 65.0,
      "grad_norm": 0.8301471471786499,
      "learning_rate": 7.954465305270875e-06,
      "loss": 0.008,
      "step": 1564
    },
    {
      "epoch": 65.04,
      "grad_norm": 0.5497509241104126,
      "learning_rate": 7.951878494603116e-06,
      "loss": 0.0033,
      "step": 1565
    },
    {
      "epoch": 65.08,
      "grad_norm": 0.650677502155304,
      "learning_rate": 7.949290470529609e-06,
      "loss": 0.006,
      "step": 1566
    },
    {
      "epoch": 65.12,
      "grad_norm": 0.8210883736610413,
      "learning_rate": 7.946701234114195e-06,
      "loss": 0.005,
      "step": 1567
    },
    {
      "epoch": 65.16,
      "grad_norm": 0.6215842366218567,
      "learning_rate": 7.94411078642121e-06,
      "loss": 0.0042,
      "step": 1568
    },
    {
      "epoch": 65.21,
      "grad_norm": 0.42945441603660583,
      "learning_rate": 7.94151912851549e-06,
      "loss": 0.0049,
      "step": 1569
    },
    {
      "epoch": 65.25,
      "grad_norm": 0.41226324439048767,
      "learning_rate": 7.938926261462366e-06,
      "loss": 0.0022,
      "step": 1570
    },
    {
      "epoch": 65.29,
      "grad_norm": 0.3063392639160156,
      "learning_rate": 7.936332186327672e-06,
      "loss": 0.0022,
      "step": 1571
    },
    {
      "epoch": 65.33,
      "grad_norm": 0.10830792784690857,
      "learning_rate": 7.933736904177727e-06,
      "loss": 0.0014,
      "step": 1572
    },
    {
      "epoch": 65.37,
      "grad_norm": 0.9594218134880066,
      "learning_rate": 7.931140416079359e-06,
      "loss": 0.0057,
      "step": 1573
    },
    {
      "epoch": 65.41,
      "grad_norm": 0.733208417892456,
      "learning_rate": 7.928542723099886e-06,
      "loss": 0.0049,
      "step": 1574
    },
    {
      "epoch": 65.45,
      "grad_norm": 0.3936973810195923,
      "learning_rate": 7.925943826307119e-06,
      "loss": 0.0035,
      "step": 1575
    },
    {
      "epoch": 65.5,
      "grad_norm": 0.41033509373664856,
      "learning_rate": 7.923343726769368e-06,
      "loss": 0.0018,
      "step": 1576
    },
    {
      "epoch": 65.54,
      "grad_norm": 0.5444778203964233,
      "learning_rate": 7.920742425555436e-06,
      "loss": 0.0046,
      "step": 1577
    },
    {
      "epoch": 65.58,
      "grad_norm": 0.13690446317195892,
      "learning_rate": 7.918139923734618e-06,
      "loss": 0.0015,
      "step": 1578
    },
    {
      "epoch": 65.62,
      "grad_norm": 0.5025758147239685,
      "learning_rate": 7.915536222376705e-06,
      "loss": 0.0026,
      "step": 1579
    },
    {
      "epoch": 65.66,
      "grad_norm": 0.4758835434913635,
      "learning_rate": 7.912931322551981e-06,
      "loss": 0.0033,
      "step": 1580
    },
    {
      "epoch": 65.7,
      "grad_norm": 1.3812072277069092,
      "learning_rate": 7.910325225331221e-06,
      "loss": 0.0124,
      "step": 1581
    },
    {
      "epoch": 65.75,
      "grad_norm": 0.4501248002052307,
      "learning_rate": 7.907717931785694e-06,
      "loss": 0.0037,
      "step": 1582
    },
    {
      "epoch": 65.79,
      "grad_norm": 0.48575466871261597,
      "learning_rate": 7.905109442987159e-06,
      "loss": 0.0027,
      "step": 1583
    },
    {
      "epoch": 65.83,
      "grad_norm": 0.6354451775550842,
      "learning_rate": 7.902499760007867e-06,
      "loss": 0.0045,
      "step": 1584
    },
    {
      "epoch": 65.87,
      "grad_norm": 0.47384464740753174,
      "learning_rate": 7.89988888392056e-06,
      "loss": 0.0053,
      "step": 1585
    },
    {
      "epoch": 65.91,
      "grad_norm": 0.8620724678039551,
      "learning_rate": 7.897276815798476e-06,
      "loss": 0.0079,
      "step": 1586
    },
    {
      "epoch": 65.95,
      "grad_norm": 1.1096165180206299,
      "learning_rate": 7.894663556715328e-06,
      "loss": 0.0099,
      "step": 1587
    },
    {
      "epoch": 65.99,
      "grad_norm": 0.24668456614017487,
      "learning_rate": 7.892049107745334e-06,
      "loss": 0.0023,
      "step": 1588
    },
    {
      "epoch": 66.04,
      "grad_norm": 0.3867136538028717,
      "learning_rate": 7.889433469963195e-06,
      "loss": 0.0032,
      "step": 1589
    },
    {
      "epoch": 66.08,
      "grad_norm": 0.5724138021469116,
      "learning_rate": 7.886816644444099e-06,
      "loss": 0.0036,
      "step": 1590
    },
    {
      "epoch": 66.12,
      "grad_norm": 0.36880946159362793,
      "learning_rate": 7.884198632263725e-06,
      "loss": 0.0029,
      "step": 1591
    },
    {
      "epoch": 66.16,
      "grad_norm": 0.2864348888397217,
      "learning_rate": 7.881579434498239e-06,
      "loss": 0.0018,
      "step": 1592
    },
    {
      "epoch": 66.2,
      "grad_norm": 0.37474051117897034,
      "learning_rate": 7.878959052224294e-06,
      "loss": 0.0022,
      "step": 1593
    },
    {
      "epoch": 66.24,
      "grad_norm": 0.28071853518486023,
      "learning_rate": 7.87633748651903e-06,
      "loss": 0.0024,
      "step": 1594
    },
    {
      "epoch": 66.29,
      "grad_norm": 0.3556196093559265,
      "learning_rate": 7.873714738460075e-06,
      "loss": 0.0022,
      "step": 1595
    },
    {
      "epoch": 66.33,
      "grad_norm": 0.41919347643852234,
      "learning_rate": 7.87109080912554e-06,
      "loss": 0.0029,
      "step": 1596
    },
    {
      "epoch": 66.37,
      "grad_norm": 0.5871697664260864,
      "learning_rate": 7.868465699594025e-06,
      "loss": 0.0036,
      "step": 1597
    },
    {
      "epoch": 66.41,
      "grad_norm": 0.5114227533340454,
      "learning_rate": 7.865839410944613e-06,
      "loss": 0.0025,
      "step": 1598
    },
    {
      "epoch": 66.45,
      "grad_norm": 0.23511902987957,
      "learning_rate": 7.86321194425687e-06,
      "loss": 0.0033,
      "step": 1599
    },
    {
      "epoch": 66.49,
      "grad_norm": 0.30488553643226624,
      "learning_rate": 7.860583300610849e-06,
      "loss": 0.0017,
      "step": 1600
    },
    {
      "epoch": 66.54,
      "grad_norm": 0.7676411271095276,
      "learning_rate": 7.857953481087089e-06,
      "loss": 0.0037,
      "step": 1601
    },
    {
      "epoch": 66.58,
      "grad_norm": 0.42277535796165466,
      "learning_rate": 7.855322486766605e-06,
      "loss": 0.0042,
      "step": 1602
    },
    {
      "epoch": 66.62,
      "grad_norm": 0.31590133905410767,
      "learning_rate": 7.852690318730903e-06,
      "loss": 0.0039,
      "step": 1603
    },
    {
      "epoch": 66.66,
      "grad_norm": 0.43612489104270935,
      "learning_rate": 7.850056978061966e-06,
      "loss": 0.0029,
      "step": 1604
    },
    {
      "epoch": 66.7,
      "grad_norm": 0.5321342945098877,
      "learning_rate": 7.84742246584226e-06,
      "loss": 0.0026,
      "step": 1605
    },
    {
      "epoch": 66.74,
      "grad_norm": 0.9989312887191772,
      "learning_rate": 7.844786783154737e-06,
      "loss": 0.0048,
      "step": 1606
    },
    {
      "epoch": 66.78,
      "grad_norm": 0.2604207694530487,
      "learning_rate": 7.842149931082824e-06,
      "loss": 0.0014,
      "step": 1607
    },
    {
      "epoch": 66.83,
      "grad_norm": 0.4442512094974518,
      "learning_rate": 7.839511910710431e-06,
      "loss": 0.0024,
      "step": 1608
    },
    {
      "epoch": 66.87,
      "grad_norm": 0.49382713437080383,
      "learning_rate": 7.83687272312195e-06,
      "loss": 0.003,
      "step": 1609
    },
    {
      "epoch": 66.91,
      "grad_norm": 0.5018647313117981,
      "learning_rate": 7.83423236940225e-06,
      "loss": 0.0056,
      "step": 1610
    },
    {
      "epoch": 66.95,
      "grad_norm": 0.7723482251167297,
      "learning_rate": 7.831590850636682e-06,
      "loss": 0.0057,
      "step": 1611
    },
    {
      "epoch": 66.99,
      "grad_norm": 0.6994364857673645,
      "learning_rate": 7.828948167911073e-06,
      "loss": 0.0037,
      "step": 1612
    },
    {
      "epoch": 67.03,
      "grad_norm": 0.24242645502090454,
      "learning_rate": 7.826304322311731e-06,
      "loss": 0.0024,
      "step": 1613
    },
    {
      "epoch": 67.08,
      "grad_norm": 0.16594833135604858,
      "learning_rate": 7.823659314925443e-06,
      "loss": 0.002,
      "step": 1614
    },
    {
      "epoch": 67.12,
      "grad_norm": 0.5978083610534668,
      "learning_rate": 7.821013146839467e-06,
      "loss": 0.0027,
      "step": 1615
    },
    {
      "epoch": 67.16,
      "grad_norm": 0.3620150685310364,
      "learning_rate": 7.818365819141545e-06,
      "loss": 0.0025,
      "step": 1616
    },
    {
      "epoch": 67.2,
      "grad_norm": 0.7885094881057739,
      "learning_rate": 7.815717332919897e-06,
      "loss": 0.0041,
      "step": 1617
    },
    {
      "epoch": 67.24,
      "grad_norm": 0.20317481458187103,
      "learning_rate": 7.81306768926321e-06,
      "loss": 0.0017,
      "step": 1618
    },
    {
      "epoch": 67.28,
      "grad_norm": 0.7829120755195618,
      "learning_rate": 7.810416889260653e-06,
      "loss": 0.0056,
      "step": 1619
    },
    {
      "epoch": 67.32,
      "grad_norm": 0.1764545887708664,
      "learning_rate": 7.807764934001875e-06,
      "loss": 0.0017,
      "step": 1620
    },
    {
      "epoch": 67.37,
      "grad_norm": 1.3453620672225952,
      "learning_rate": 7.80511182457699e-06,
      "loss": 0.0045,
      "step": 1621
    },
    {
      "epoch": 67.41,
      "grad_norm": 0.5927833914756775,
      "learning_rate": 7.80245756207659e-06,
      "loss": 0.0035,
      "step": 1622
    },
    {
      "epoch": 67.45,
      "grad_norm": 0.2682400345802307,
      "learning_rate": 7.799802147591747e-06,
      "loss": 0.0015,
      "step": 1623
    },
    {
      "epoch": 67.49,
      "grad_norm": 0.32878243923187256,
      "learning_rate": 7.797145582213998e-06,
      "loss": 0.002,
      "step": 1624
    },
    {
      "epoch": 67.53,
      "grad_norm": 1.6366643905639648,
      "learning_rate": 7.794487867035358e-06,
      "loss": 0.0036,
      "step": 1625
    },
    {
      "epoch": 67.57,
      "grad_norm": 0.2899850904941559,
      "learning_rate": 7.791829003148313e-06,
      "loss": 0.0019,
      "step": 1626
    },
    {
      "epoch": 67.62,
      "grad_norm": 0.284036785364151,
      "learning_rate": 7.789168991645821e-06,
      "loss": 0.0016,
      "step": 1627
    },
    {
      "epoch": 67.66,
      "grad_norm": 0.18523618578910828,
      "learning_rate": 7.786507833621314e-06,
      "loss": 0.0018,
      "step": 1628
    },
    {
      "epoch": 67.7,
      "grad_norm": 0.9150221943855286,
      "learning_rate": 7.78384553016869e-06,
      "loss": 0.0056,
      "step": 1629
    },
    {
      "epoch": 67.74,
      "grad_norm": 0.16364827752113342,
      "learning_rate": 7.781182082382325e-06,
      "loss": 0.0018,
      "step": 1630
    },
    {
      "epoch": 67.78,
      "grad_norm": 0.7409460544586182,
      "learning_rate": 7.77851749135706e-06,
      "loss": 0.0018,
      "step": 1631
    },
    {
      "epoch": 67.82,
      "grad_norm": 0.6931899785995483,
      "learning_rate": 7.775851758188209e-06,
      "loss": 0.0039,
      "step": 1632
    },
    {
      "epoch": 67.86,
      "grad_norm": 0.9158471822738647,
      "learning_rate": 7.773184883971553e-06,
      "loss": 0.0044,
      "step": 1633
    },
    {
      "epoch": 67.91,
      "grad_norm": 0.6984047293663025,
      "learning_rate": 7.770516869803341e-06,
      "loss": 0.0034,
      "step": 1634
    },
    {
      "epoch": 67.95,
      "grad_norm": 0.9411307573318481,
      "learning_rate": 7.767847716780297e-06,
      "loss": 0.0029,
      "step": 1635
    },
    {
      "epoch": 67.99,
      "grad_norm": 0.49516692757606506,
      "learning_rate": 7.765177425999609e-06,
      "loss": 0.003,
      "step": 1636
    },
    {
      "epoch": 68.03,
      "grad_norm": 0.4692796766757965,
      "learning_rate": 7.76250599855893e-06,
      "loss": 0.0037,
      "step": 1637
    },
    {
      "epoch": 68.07,
      "grad_norm": 0.22961346805095673,
      "learning_rate": 7.759833435556381e-06,
      "loss": 0.002,
      "step": 1638
    },
    {
      "epoch": 68.11,
      "grad_norm": 0.06277146935462952,
      "learning_rate": 7.757159738090558e-06,
      "loss": 0.001,
      "step": 1639
    },
    {
      "epoch": 68.16,
      "grad_norm": 0.11633338779211044,
      "learning_rate": 7.754484907260513e-06,
      "loss": 0.0014,
      "step": 1640
    },
    {
      "epoch": 68.2,
      "grad_norm": 0.23209156095981598,
      "learning_rate": 7.751808944165769e-06,
      "loss": 0.0015,
      "step": 1641
    },
    {
      "epoch": 68.24,
      "grad_norm": 0.6866369843482971,
      "learning_rate": 7.749131849906313e-06,
      "loss": 0.0033,
      "step": 1642
    },
    {
      "epoch": 68.28,
      "grad_norm": 0.13818220794200897,
      "learning_rate": 7.746453625582596e-06,
      "loss": 0.0014,
      "step": 1643
    },
    {
      "epoch": 68.32,
      "grad_norm": 0.7671343088150024,
      "learning_rate": 7.743774272295538e-06,
      "loss": 0.0033,
      "step": 1644
    },
    {
      "epoch": 68.36,
      "grad_norm": 0.3818751871585846,
      "learning_rate": 7.741093791146517e-06,
      "loss": 0.0016,
      "step": 1645
    },
    {
      "epoch": 68.41,
      "grad_norm": 0.28042668104171753,
      "learning_rate": 7.738412183237379e-06,
      "loss": 0.002,
      "step": 1646
    },
    {
      "epoch": 68.45,
      "grad_norm": 0.7244462370872498,
      "learning_rate": 7.73572944967043e-06,
      "loss": 0.005,
      "step": 1647
    },
    {
      "epoch": 68.49,
      "grad_norm": 0.6319608092308044,
      "learning_rate": 7.733045591548442e-06,
      "loss": 0.0033,
      "step": 1648
    },
    {
      "epoch": 68.53,
      "grad_norm": 0.654513418674469,
      "learning_rate": 7.73036060997465e-06,
      "loss": 0.0057,
      "step": 1649
    },
    {
      "epoch": 68.57,
      "grad_norm": 1.1638180017471313,
      "learning_rate": 7.727674506052744e-06,
      "loss": 0.0079,
      "step": 1650
    },
    {
      "epoch": 68.61,
      "grad_norm": 0.4581557810306549,
      "learning_rate": 7.724987280886882e-06,
      "loss": 0.0032,
      "step": 1651
    },
    {
      "epoch": 68.65,
      "grad_norm": 0.20526942610740662,
      "learning_rate": 7.72229893558168e-06,
      "loss": 0.0016,
      "step": 1652
    },
    {
      "epoch": 68.7,
      "grad_norm": 0.6457083225250244,
      "learning_rate": 7.719609471242217e-06,
      "loss": 0.0018,
      "step": 1653
    },
    {
      "epoch": 68.74,
      "grad_norm": 0.3638507127761841,
      "learning_rate": 7.716918888974029e-06,
      "loss": 0.002,
      "step": 1654
    },
    {
      "epoch": 68.78,
      "grad_norm": 1.3622337579727173,
      "learning_rate": 7.714227189883112e-06,
      "loss": 0.0051,
      "step": 1655
    },
    {
      "epoch": 68.82,
      "grad_norm": 0.510256290435791,
      "learning_rate": 7.711534375075923e-06,
      "loss": 0.0063,
      "step": 1656
    },
    {
      "epoch": 68.86,
      "grad_norm": 1.0312004089355469,
      "learning_rate": 7.708840445659376e-06,
      "loss": 0.0067,
      "step": 1657
    },
    {
      "epoch": 68.9,
      "grad_norm": 0.3893105983734131,
      "learning_rate": 7.706145402740843e-06,
      "loss": 0.0029,
      "step": 1658
    },
    {
      "epoch": 68.95,
      "grad_norm": 0.6967688202857971,
      "learning_rate": 7.703449247428157e-06,
      "loss": 0.003,
      "step": 1659
    },
    {
      "epoch": 68.99,
      "grad_norm": 0.5518283247947693,
      "learning_rate": 7.700751980829601e-06,
      "loss": 0.0042,
      "step": 1660
    },
    {
      "epoch": 69.03,
      "grad_norm": 0.22066040337085724,
      "learning_rate": 7.698053604053923e-06,
      "loss": 0.0022,
      "step": 1661
    },
    {
      "epoch": 69.07,
      "grad_norm": 0.6562129259109497,
      "learning_rate": 7.695354118210322e-06,
      "loss": 0.005,
      "step": 1662
    },
    {
      "epoch": 69.11,
      "grad_norm": 0.7217493653297424,
      "learning_rate": 7.692653524408458e-06,
      "loss": 0.0022,
      "step": 1663
    },
    {
      "epoch": 69.15,
      "grad_norm": 0.4473060667514801,
      "learning_rate": 7.689951823758439e-06,
      "loss": 0.002,
      "step": 1664
    },
    {
      "epoch": 69.19,
      "grad_norm": 1.3578052520751953,
      "learning_rate": 7.687249017370832e-06,
      "loss": 0.0087,
      "step": 1665
    },
    {
      "epoch": 69.24,
      "grad_norm": 0.4670116901397705,
      "learning_rate": 7.684545106356663e-06,
      "loss": 0.0037,
      "step": 1666
    },
    {
      "epoch": 69.28,
      "grad_norm": 0.49620792269706726,
      "learning_rate": 7.681840091827404e-06,
      "loss": 0.0026,
      "step": 1667
    },
    {
      "epoch": 69.32,
      "grad_norm": 0.20733745396137238,
      "learning_rate": 7.679133974894984e-06,
      "loss": 0.0028,
      "step": 1668
    },
    {
      "epoch": 69.36,
      "grad_norm": 0.17395137250423431,
      "learning_rate": 7.676426756671787e-06,
      "loss": 0.0012,
      "step": 1669
    },
    {
      "epoch": 69.4,
      "grad_norm": 0.22459004819393158,
      "learning_rate": 7.673718438270649e-06,
      "loss": 0.0017,
      "step": 1670
    },
    {
      "epoch": 69.44,
      "grad_norm": 0.9145733714103699,
      "learning_rate": 7.671009020804855e-06,
      "loss": 0.0027,
      "step": 1671
    },
    {
      "epoch": 69.49,
      "grad_norm": 0.40222352743148804,
      "learning_rate": 7.668298505388146e-06,
      "loss": 0.0028,
      "step": 1672
    },
    {
      "epoch": 69.53,
      "grad_norm": 0.19248999655246735,
      "learning_rate": 7.665586893134711e-06,
      "loss": 0.0017,
      "step": 1673
    },
    {
      "epoch": 69.57,
      "grad_norm": 0.652370035648346,
      "learning_rate": 7.662874185159193e-06,
      "loss": 0.0028,
      "step": 1674
    },
    {
      "epoch": 69.61,
      "grad_norm": 0.2804279625415802,
      "learning_rate": 7.660160382576683e-06,
      "loss": 0.0017,
      "step": 1675
    },
    {
      "epoch": 69.65,
      "grad_norm": 0.1822793185710907,
      "learning_rate": 7.657445486502723e-06,
      "loss": 0.0017,
      "step": 1676
    },
    {
      "epoch": 69.69,
      "grad_norm": 0.4070099890232086,
      "learning_rate": 7.654729498053305e-06,
      "loss": 0.0022,
      "step": 1677
    },
    {
      "epoch": 69.74,
      "grad_norm": 1.6645029783248901,
      "learning_rate": 7.652012418344867e-06,
      "loss": 0.0113,
      "step": 1678
    },
    {
      "epoch": 69.78,
      "grad_norm": 1.37944757938385,
      "learning_rate": 7.6492942484943e-06,
      "loss": 0.0066,
      "step": 1679
    },
    {
      "epoch": 69.82,
      "grad_norm": 0.7031854391098022,
      "learning_rate": 7.646574989618938e-06,
      "loss": 0.0056,
      "step": 1680
    },
    {
      "epoch": 69.86,
      "grad_norm": 0.4506677985191345,
      "learning_rate": 7.643854642836569e-06,
      "loss": 0.0026,
      "step": 1681
    },
    {
      "epoch": 69.9,
      "grad_norm": 1.361899495124817,
      "learning_rate": 7.641133209265423e-06,
      "loss": 0.0087,
      "step": 1682
    },
    {
      "epoch": 69.94,
      "grad_norm": 0.30813664197921753,
      "learning_rate": 7.638410690024178e-06,
      "loss": 0.0025,
      "step": 1683
    },
    {
      "epoch": 69.98,
      "grad_norm": 1.1085554361343384,
      "learning_rate": 7.635687086231962e-06,
      "loss": 0.0043,
      "step": 1684
    },
    {
      "epoch": 70.03,
      "grad_norm": 0.41730958223342896,
      "learning_rate": 7.632962399008342e-06,
      "loss": 0.0022,
      "step": 1685
    },
    {
      "epoch": 70.07,
      "grad_norm": 0.9411770105361938,
      "learning_rate": 7.630236629473336e-06,
      "loss": 0.0044,
      "step": 1686
    },
    {
      "epoch": 70.11,
      "grad_norm": 0.431829571723938,
      "learning_rate": 7.627509778747404e-06,
      "loss": 0.0019,
      "step": 1687
    },
    {
      "epoch": 70.15,
      "grad_norm": 0.3361039459705353,
      "learning_rate": 7.624781847951453e-06,
      "loss": 0.0022,
      "step": 1688
    },
    {
      "epoch": 70.19,
      "grad_norm": 0.5174759030342102,
      "learning_rate": 7.622052838206829e-06,
      "loss": 0.0029,
      "step": 1689
    },
    {
      "epoch": 70.23,
      "grad_norm": 1.3461785316467285,
      "learning_rate": 7.619322750635327e-06,
      "loss": 0.0062,
      "step": 1690
    },
    {
      "epoch": 70.28,
      "grad_norm": 0.5569358468055725,
      "learning_rate": 7.616591586359186e-06,
      "loss": 0.0028,
      "step": 1691
    },
    {
      "epoch": 70.32,
      "grad_norm": 0.34149786829948425,
      "learning_rate": 7.613859346501078e-06,
      "loss": 0.0025,
      "step": 1692
    },
    {
      "epoch": 70.36,
      "grad_norm": 0.5221444368362427,
      "learning_rate": 7.611126032184127e-06,
      "loss": 0.0019,
      "step": 1693
    },
    {
      "epoch": 70.4,
      "grad_norm": 0.10509325563907623,
      "learning_rate": 7.608391644531894e-06,
      "loss": 0.0014,
      "step": 1694
    },
    {
      "epoch": 70.44,
      "grad_norm": 0.6004111766815186,
      "learning_rate": 7.605656184668385e-06,
      "loss": 0.0067,
      "step": 1695
    },
    {
      "epoch": 70.48,
      "grad_norm": 0.9454648494720459,
      "learning_rate": 7.602919653718044e-06,
      "loss": 0.0036,
      "step": 1696
    },
    {
      "epoch": 70.52,
      "grad_norm": 0.24789069592952728,
      "learning_rate": 7.600182052805752e-06,
      "loss": 0.0032,
      "step": 1697
    },
    {
      "epoch": 70.57,
      "grad_norm": 0.41491544246673584,
      "learning_rate": 7.597443383056837e-06,
      "loss": 0.0044,
      "step": 1698
    },
    {
      "epoch": 70.61,
      "grad_norm": 0.6407087445259094,
      "learning_rate": 7.59470364559706e-06,
      "loss": 0.0029,
      "step": 1699
    },
    {
      "epoch": 70.65,
      "grad_norm": 0.37684333324432373,
      "learning_rate": 7.591962841552627e-06,
      "loss": 0.003,
      "step": 1700
    },
    {
      "epoch": 70.69,
      "grad_norm": 1.0158324241638184,
      "learning_rate": 7.589220972050174e-06,
      "loss": 0.0074,
      "step": 1701
    },
    {
      "epoch": 70.73,
      "grad_norm": 0.3026164770126343,
      "learning_rate": 7.586478038216785e-06,
      "loss": 0.0022,
      "step": 1702
    },
    {
      "epoch": 70.77,
      "grad_norm": 0.5959147810935974,
      "learning_rate": 7.583734041179973e-06,
      "loss": 0.002,
      "step": 1703
    },
    {
      "epoch": 70.82,
      "grad_norm": 0.4971829652786255,
      "learning_rate": 7.580988982067694e-06,
      "loss": 0.0041,
      "step": 1704
    },
    {
      "epoch": 70.86,
      "grad_norm": 0.3440195620059967,
      "learning_rate": 7.578242862008336e-06,
      "loss": 0.0025,
      "step": 1705
    },
    {
      "epoch": 70.9,
      "grad_norm": 0.5820418000221252,
      "learning_rate": 7.575495682130726e-06,
      "loss": 0.0046,
      "step": 1706
    },
    {
      "epoch": 70.94,
      "grad_norm": 1.0528315305709839,
      "learning_rate": 7.572747443564125e-06,
      "loss": 0.002,
      "step": 1707
    },
    {
      "epoch": 70.98,
      "grad_norm": 0.3610979914665222,
      "learning_rate": 7.569998147438233e-06,
      "loss": 0.002,
      "step": 1708
    },
    {
      "epoch": 71.02,
      "grad_norm": 0.7688815593719482,
      "learning_rate": 7.56724779488318e-06,
      "loss": 0.0021,
      "step": 1709
    },
    {
      "epoch": 71.06,
      "grad_norm": 0.31382250785827637,
      "learning_rate": 7.564496387029532e-06,
      "loss": 0.003,
      "step": 1710
    },
    {
      "epoch": 71.11,
      "grad_norm": 0.4974609613418579,
      "learning_rate": 7.561743925008287e-06,
      "loss": 0.0042,
      "step": 1711
    },
    {
      "epoch": 71.15,
      "grad_norm": 0.7070388793945312,
      "learning_rate": 7.558990409950881e-06,
      "loss": 0.0018,
      "step": 1712
    },
    {
      "epoch": 71.19,
      "grad_norm": 0.48022380471229553,
      "learning_rate": 7.55623584298918e-06,
      "loss": 0.004,
      "step": 1713
    },
    {
      "epoch": 71.23,
      "grad_norm": 0.8247776627540588,
      "learning_rate": 7.553480225255481e-06,
      "loss": 0.0027,
      "step": 1714
    },
    {
      "epoch": 71.27,
      "grad_norm": 1.4858455657958984,
      "learning_rate": 7.550723557882514e-06,
      "loss": 0.0105,
      "step": 1715
    },
    {
      "epoch": 71.31,
      "grad_norm": 0.19344697892665863,
      "learning_rate": 7.5479658420034415e-06,
      "loss": 0.0018,
      "step": 1716
    },
    {
      "epoch": 71.36,
      "grad_norm": 0.08043119311332703,
      "learning_rate": 7.545207078751858e-06,
      "loss": 0.0012,
      "step": 1717
    },
    {
      "epoch": 71.4,
      "grad_norm": 0.6935696601867676,
      "learning_rate": 7.542447269261783e-06,
      "loss": 0.0039,
      "step": 1718
    },
    {
      "epoch": 71.44,
      "grad_norm": 0.4201293885707855,
      "learning_rate": 7.539686414667674e-06,
      "loss": 0.0021,
      "step": 1719
    },
    {
      "epoch": 71.48,
      "grad_norm": 1.6019437313079834,
      "learning_rate": 7.536924516104411e-06,
      "loss": 0.0046,
      "step": 1720
    },
    {
      "epoch": 71.52,
      "grad_norm": 0.3689057230949402,
      "learning_rate": 7.5341615747073085e-06,
      "loss": 0.002,
      "step": 1721
    },
    {
      "epoch": 71.56,
      "grad_norm": 0.5671511292457581,
      "learning_rate": 7.5313975916121065e-06,
      "loss": 0.0043,
      "step": 1722
    },
    {
      "epoch": 71.61,
      "grad_norm": 0.2170814722776413,
      "learning_rate": 7.528632567954973e-06,
      "loss": 0.0023,
      "step": 1723
    },
    {
      "epoch": 71.65,
      "grad_norm": 0.06295821070671082,
      "learning_rate": 7.5258665048725065e-06,
      "loss": 0.0011,
      "step": 1724
    },
    {
      "epoch": 71.69,
      "grad_norm": 0.16977395117282867,
      "learning_rate": 7.52309940350173e-06,
      "loss": 0.003,
      "step": 1725
    },
    {
      "epoch": 71.73,
      "grad_norm": 0.5401462912559509,
      "learning_rate": 7.520331264980094e-06,
      "loss": 0.0023,
      "step": 1726
    },
    {
      "epoch": 71.77,
      "grad_norm": 0.2468992918729782,
      "learning_rate": 7.517562090445476e-06,
      "loss": 0.0024,
      "step": 1727
    },
    {
      "epoch": 71.81,
      "grad_norm": 0.8021250367164612,
      "learning_rate": 7.514791881036179e-06,
      "loss": 0.0017,
      "step": 1728
    },
    {
      "epoch": 71.85,
      "grad_norm": 0.06066001206636429,
      "learning_rate": 7.512020637890931e-06,
      "loss": 0.001,
      "step": 1729
    },
    {
      "epoch": 71.9,
      "grad_norm": 0.4856092035770416,
      "learning_rate": 7.509248362148889e-06,
      "loss": 0.0044,
      "step": 1730
    },
    {
      "epoch": 71.94,
      "grad_norm": 1.199653148651123,
      "learning_rate": 7.506475054949626e-06,
      "loss": 0.0056,
      "step": 1731
    },
    {
      "epoch": 71.98,
      "grad_norm": 0.670693576335907,
      "learning_rate": 7.503700717433145e-06,
      "loss": 0.0061,
      "step": 1732
    },
    {
      "epoch": 72.02,
      "grad_norm": 0.5633376240730286,
      "learning_rate": 7.5009253507398734e-06,
      "loss": 0.0025,
      "step": 1733
    },
    {
      "epoch": 72.06,
      "grad_norm": 0.051643479615449905,
      "learning_rate": 7.498148956010658e-06,
      "loss": 0.001,
      "step": 1734
    },
    {
      "epoch": 72.1,
      "grad_norm": 0.5286021828651428,
      "learning_rate": 7.49537153438677e-06,
      "loss": 0.0052,
      "step": 1735
    },
    {
      "epoch": 72.15,
      "grad_norm": 0.3242807686328888,
      "learning_rate": 7.492593087009903e-06,
      "loss": 0.0024,
      "step": 1736
    },
    {
      "epoch": 72.19,
      "grad_norm": 0.28216540813446045,
      "learning_rate": 7.489813615022171e-06,
      "loss": 0.0024,
      "step": 1737
    },
    {
      "epoch": 72.23,
      "grad_norm": 0.2904427647590637,
      "learning_rate": 7.48703311956611e-06,
      "loss": 0.0013,
      "step": 1738
    },
    {
      "epoch": 72.27,
      "grad_norm": 0.30989378690719604,
      "learning_rate": 7.4842516017846775e-06,
      "loss": 0.0028,
      "step": 1739
    },
    {
      "epoch": 72.31,
      "grad_norm": 0.4964061975479126,
      "learning_rate": 7.481469062821252e-06,
      "loss": 0.0029,
      "step": 1740
    },
    {
      "epoch": 72.35,
      "grad_norm": 0.30289360880851746,
      "learning_rate": 7.478685503819625e-06,
      "loss": 0.0014,
      "step": 1741
    },
    {
      "epoch": 72.39,
      "grad_norm": 0.4055021107196808,
      "learning_rate": 7.475900925924015e-06,
      "loss": 0.0038,
      "step": 1742
    },
    {
      "epoch": 72.44,
      "grad_norm": 0.6419386267662048,
      "learning_rate": 7.473115330279059e-06,
      "loss": 0.0069,
      "step": 1743
    },
    {
      "epoch": 72.48,
      "grad_norm": 1.131235957145691,
      "learning_rate": 7.470328718029809e-06,
      "loss": 0.0029,
      "step": 1744
    },
    {
      "epoch": 72.52,
      "grad_norm": 0.5402343273162842,
      "learning_rate": 7.467541090321735e-06,
      "loss": 0.0054,
      "step": 1745
    },
    {
      "epoch": 72.56,
      "grad_norm": 0.16528736054897308,
      "learning_rate": 7.464752448300726e-06,
      "loss": 0.0024,
      "step": 1746
    },
    {
      "epoch": 72.6,
      "grad_norm": 0.4836658239364624,
      "learning_rate": 7.4619627931130864e-06,
      "loss": 0.0015,
      "step": 1747
    },
    {
      "epoch": 72.64,
      "grad_norm": 0.5613723397254944,
      "learning_rate": 7.45917212590554e-06,
      "loss": 0.0033,
      "step": 1748
    },
    {
      "epoch": 72.69,
      "grad_norm": 0.34294986724853516,
      "learning_rate": 7.456380447825223e-06,
      "loss": 0.0026,
      "step": 1749
    },
    {
      "epoch": 72.73,
      "grad_norm": 0.647770881652832,
      "learning_rate": 7.453587760019691e-06,
      "loss": 0.0063,
      "step": 1750
    },
    {
      "epoch": 72.77,
      "grad_norm": 0.8601690530776978,
      "learning_rate": 7.45079406363691e-06,
      "loss": 0.0037,
      "step": 1751
    },
    {
      "epoch": 72.81,
      "grad_norm": 0.3837006092071533,
      "learning_rate": 7.447999359825263e-06,
      "loss": 0.0023,
      "step": 1752
    },
    {
      "epoch": 72.85,
      "grad_norm": 0.3459351658821106,
      "learning_rate": 7.445203649733549e-06,
      "loss": 0.0022,
      "step": 1753
    },
    {
      "epoch": 72.89,
      "grad_norm": 0.5091816186904907,
      "learning_rate": 7.442406934510977e-06,
      "loss": 0.003,
      "step": 1754
    },
    {
      "epoch": 72.94,
      "grad_norm": 0.40145397186279297,
      "learning_rate": 7.439609215307173e-06,
      "loss": 0.0024,
      "step": 1755
    },
    {
      "epoch": 72.98,
      "grad_norm": 1.481360673904419,
      "learning_rate": 7.436810493272174e-06,
      "loss": 0.0114,
      "step": 1756
    },
    {
      "epoch": 73.02,
      "grad_norm": 0.620469868183136,
      "learning_rate": 7.434010769556426e-06,
      "loss": 0.0028,
      "step": 1757
    },
    {
      "epoch": 73.06,
      "grad_norm": 0.1470111459493637,
      "learning_rate": 7.431210045310792e-06,
      "loss": 0.0012,
      "step": 1758
    },
    {
      "epoch": 73.1,
      "grad_norm": 0.33927175402641296,
      "learning_rate": 7.428408321686542e-06,
      "loss": 0.0011,
      "step": 1759
    },
    {
      "epoch": 73.14,
      "grad_norm": 0.621181309223175,
      "learning_rate": 7.42560559983536e-06,
      "loss": 0.0047,
      "step": 1760
    },
    {
      "epoch": 73.18,
      "grad_norm": 0.18785656988620758,
      "learning_rate": 7.42280188090934e-06,
      "loss": 0.0025,
      "step": 1761
    },
    {
      "epoch": 73.23,
      "grad_norm": 0.6157318353652954,
      "learning_rate": 7.419997166060985e-06,
      "loss": 0.0023,
      "step": 1762
    },
    {
      "epoch": 73.27,
      "grad_norm": 0.3806155025959015,
      "learning_rate": 7.417191456443204e-06,
      "loss": 0.0018,
      "step": 1763
    },
    {
      "epoch": 73.31,
      "grad_norm": 0.25178107619285583,
      "learning_rate": 7.414384753209323e-06,
      "loss": 0.0029,
      "step": 1764
    },
    {
      "epoch": 73.35,
      "grad_norm": 0.17623835802078247,
      "learning_rate": 7.411577057513066e-06,
      "loss": 0.0012,
      "step": 1765
    },
    {
      "epoch": 73.39,
      "grad_norm": 0.15436473488807678,
      "learning_rate": 7.408768370508577e-06,
      "loss": 0.0014,
      "step": 1766
    },
    {
      "epoch": 73.43,
      "grad_norm": 0.06683407723903656,
      "learning_rate": 7.405958693350397e-06,
      "loss": 0.0012,
      "step": 1767
    },
    {
      "epoch": 73.48,
      "grad_norm": 0.562257707118988,
      "learning_rate": 7.403148027193479e-06,
      "loss": 0.005,
      "step": 1768
    },
    {
      "epoch": 73.52,
      "grad_norm": 0.3597116470336914,
      "learning_rate": 7.400336373193182e-06,
      "loss": 0.0017,
      "step": 1769
    },
    {
      "epoch": 73.56,
      "grad_norm": 0.5127090215682983,
      "learning_rate": 7.39752373250527e-06,
      "loss": 0.0057,
      "step": 1770
    },
    {
      "epoch": 73.6,
      "grad_norm": 0.5152186155319214,
      "learning_rate": 7.394710106285915e-06,
      "loss": 0.002,
      "step": 1771
    },
    {
      "epoch": 73.64,
      "grad_norm": 0.4230497181415558,
      "learning_rate": 7.39189549569169e-06,
      "loss": 0.0016,
      "step": 1772
    },
    {
      "epoch": 73.68,
      "grad_norm": 0.35114559531211853,
      "learning_rate": 7.389079901879579e-06,
      "loss": 0.0016,
      "step": 1773
    },
    {
      "epoch": 73.72,
      "grad_norm": 0.32470783591270447,
      "learning_rate": 7.386263326006962e-06,
      "loss": 0.0031,
      "step": 1774
    },
    {
      "epoch": 73.77,
      "grad_norm": 0.7910651564598083,
      "learning_rate": 7.383445769231628e-06,
      "loss": 0.0039,
      "step": 1775
    },
    {
      "epoch": 73.81,
      "grad_norm": 0.17929615080356598,
      "learning_rate": 7.380627232711769e-06,
      "loss": 0.0017,
      "step": 1776
    },
    {
      "epoch": 73.85,
      "grad_norm": 0.5521241426467896,
      "learning_rate": 7.377807717605979e-06,
      "loss": 0.0048,
      "step": 1777
    },
    {
      "epoch": 73.89,
      "grad_norm": 0.18010647594928741,
      "learning_rate": 7.374987225073256e-06,
      "loss": 0.0013,
      "step": 1778
    },
    {
      "epoch": 73.93,
      "grad_norm": 0.441026896238327,
      "learning_rate": 7.372165756272994e-06,
      "loss": 0.0029,
      "step": 1779
    },
    {
      "epoch": 73.97,
      "grad_norm": 0.1998104602098465,
      "learning_rate": 7.369343312364994e-06,
      "loss": 0.0027,
      "step": 1780
    },
    {
      "epoch": 74.02,
      "grad_norm": 0.712394118309021,
      "learning_rate": 7.3665198945094565e-06,
      "loss": 0.0022,
      "step": 1781
    },
    {
      "epoch": 74.06,
      "grad_norm": 0.3672946095466614,
      "learning_rate": 7.363695503866981e-06,
      "loss": 0.0015,
      "step": 1782
    },
    {
      "epoch": 74.1,
      "grad_norm": 0.06326020509004593,
      "learning_rate": 7.36087014159857e-06,
      "loss": 0.0012,
      "step": 1783
    },
    {
      "epoch": 74.14,
      "grad_norm": 0.162825807929039,
      "learning_rate": 7.358043808865621e-06,
      "loss": 0.0014,
      "step": 1784
    },
    {
      "epoch": 74.18,
      "grad_norm": 0.23431159555912018,
      "learning_rate": 7.355216506829933e-06,
      "loss": 0.0012,
      "step": 1785
    },
    {
      "epoch": 74.22,
      "grad_norm": 0.6119410395622253,
      "learning_rate": 7.352388236653704e-06,
      "loss": 0.0025,
      "step": 1786
    },
    {
      "epoch": 74.26,
      "grad_norm": 0.17496296763420105,
      "learning_rate": 7.3495589994995274e-06,
      "loss": 0.0017,
      "step": 1787
    },
    {
      "epoch": 74.31,
      "grad_norm": 0.07938899844884872,
      "learning_rate": 7.346728796530398e-06,
      "loss": 0.0009,
      "step": 1788
    },
    {
      "epoch": 74.35,
      "grad_norm": 0.0998486876487732,
      "learning_rate": 7.343897628909703e-06,
      "loss": 0.0013,
      "step": 1789
    },
    {
      "epoch": 74.39,
      "grad_norm": 0.2879382073879242,
      "learning_rate": 7.34106549780123e-06,
      "loss": 0.0023,
      "step": 1790
    },
    {
      "epoch": 74.43,
      "grad_norm": 0.27962127327919006,
      "learning_rate": 7.338232404369161e-06,
      "loss": 0.0014,
      "step": 1791
    },
    {
      "epoch": 74.47,
      "grad_norm": 0.17322617769241333,
      "learning_rate": 7.3353983497780725e-06,
      "loss": 0.0017,
      "step": 1792
    },
    {
      "epoch": 74.51,
      "grad_norm": 0.34575727581977844,
      "learning_rate": 7.332563335192938e-06,
      "loss": 0.0023,
      "step": 1793
    },
    {
      "epoch": 74.56,
      "grad_norm": 0.5777968168258667,
      "learning_rate": 7.3297273617791246e-06,
      "loss": 0.0042,
      "step": 1794
    },
    {
      "epoch": 74.6,
      "grad_norm": 0.5134087800979614,
      "learning_rate": 7.326890430702396e-06,
      "loss": 0.0033,
      "step": 1795
    },
    {
      "epoch": 74.64,
      "grad_norm": 0.1522579789161682,
      "learning_rate": 7.324052543128904e-06,
      "loss": 0.0018,
      "step": 1796
    },
    {
      "epoch": 74.68,
      "grad_norm": 0.3379562199115753,
      "learning_rate": 7.3212137002252e-06,
      "loss": 0.0019,
      "step": 1797
    },
    {
      "epoch": 74.72,
      "grad_norm": 0.1345316767692566,
      "learning_rate": 7.318373903158222e-06,
      "loss": 0.0019,
      "step": 1798
    },
    {
      "epoch": 74.76,
      "grad_norm": 0.3365827202796936,
      "learning_rate": 7.3155331530953064e-06,
      "loss": 0.0043,
      "step": 1799
    },
    {
      "epoch": 74.81,
      "grad_norm": 0.15075987577438354,
      "learning_rate": 7.312691451204178e-06,
      "loss": 0.0016,
      "step": 1800
    },
    {
      "epoch": 74.85,
      "grad_norm": 0.5794716477394104,
      "learning_rate": 7.30984879865295e-06,
      "loss": 0.0026,
      "step": 1801
    },
    {
      "epoch": 74.89,
      "grad_norm": 0.03139515966176987,
      "learning_rate": 7.307005196610132e-06,
      "loss": 0.0009,
      "step": 1802
    },
    {
      "epoch": 74.93,
      "grad_norm": 0.38597139716148376,
      "learning_rate": 7.30416064624462e-06,
      "loss": 0.0034,
      "step": 1803
    },
    {
      "epoch": 74.97,
      "grad_norm": 0.34713852405548096,
      "learning_rate": 7.301315148725704e-06,
      "loss": 0.0032,
      "step": 1804
    },
    {
      "epoch": 75.01,
      "grad_norm": 0.22735148668289185,
      "learning_rate": 7.2984687052230585e-06,
      "loss": 0.0038,
      "step": 1805
    },
    {
      "epoch": 75.05,
      "grad_norm": 0.13338057696819305,
      "learning_rate": 7.295621316906748e-06,
      "loss": 0.0012,
      "step": 1806
    },
    {
      "epoch": 75.1,
      "grad_norm": 0.23300154507160187,
      "learning_rate": 7.292772984947227e-06,
      "loss": 0.0015,
      "step": 1807
    },
    {
      "epoch": 75.14,
      "grad_norm": 0.10861959308385849,
      "learning_rate": 7.289923710515338e-06,
      "loss": 0.0012,
      "step": 1808
    },
    {
      "epoch": 75.18,
      "grad_norm": 0.39976900815963745,
      "learning_rate": 7.287073494782309e-06,
      "loss": 0.0018,
      "step": 1809
    },
    {
      "epoch": 75.22,
      "grad_norm": 0.09605658799409866,
      "learning_rate": 7.284222338919758e-06,
      "loss": 0.0015,
      "step": 1810
    },
    {
      "epoch": 75.26,
      "grad_norm": 0.15452410280704498,
      "learning_rate": 7.281370244099686e-06,
      "loss": 0.0012,
      "step": 1811
    },
    {
      "epoch": 75.3,
      "grad_norm": 0.17615236341953278,
      "learning_rate": 7.278517211494481e-06,
      "loss": 0.0015,
      "step": 1812
    },
    {
      "epoch": 75.35,
      "grad_norm": 0.05258147045969963,
      "learning_rate": 7.275663242276918e-06,
      "loss": 0.0012,
      "step": 1813
    },
    {
      "epoch": 75.39,
      "grad_norm": 0.06633604317903519,
      "learning_rate": 7.2728083376201545e-06,
      "loss": 0.0011,
      "step": 1814
    },
    {
      "epoch": 75.43,
      "grad_norm": 0.045832134783267975,
      "learning_rate": 7.269952498697734e-06,
      "loss": 0.0009,
      "step": 1815
    },
    {
      "epoch": 75.47,
      "grad_norm": 0.9392749071121216,
      "learning_rate": 7.267095726683587e-06,
      "loss": 0.0031,
      "step": 1816
    },
    {
      "epoch": 75.51,
      "grad_norm": 0.12220660597085953,
      "learning_rate": 7.264238022752021e-06,
      "loss": 0.0015,
      "step": 1817
    },
    {
      "epoch": 75.55,
      "grad_norm": 0.03992094099521637,
      "learning_rate": 7.261379388077733e-06,
      "loss": 0.0008,
      "step": 1818
    },
    {
      "epoch": 75.59,
      "grad_norm": 0.05691499635577202,
      "learning_rate": 7.258519823835797e-06,
      "loss": 0.001,
      "step": 1819
    },
    {
      "epoch": 75.64,
      "grad_norm": 0.3714245855808258,
      "learning_rate": 7.255659331201673e-06,
      "loss": 0.0028,
      "step": 1820
    },
    {
      "epoch": 75.68,
      "grad_norm": 0.22299079596996307,
      "learning_rate": 7.252797911351203e-06,
      "loss": 0.002,
      "step": 1821
    },
    {
      "epoch": 75.72,
      "grad_norm": 0.5397130250930786,
      "learning_rate": 7.249935565460606e-06,
      "loss": 0.0022,
      "step": 1822
    },
    {
      "epoch": 75.76,
      "grad_norm": 0.24648647010326385,
      "learning_rate": 7.247072294706487e-06,
      "loss": 0.0016,
      "step": 1823
    },
    {
      "epoch": 75.8,
      "grad_norm": 0.2884291112422943,
      "learning_rate": 7.244208100265826e-06,
      "loss": 0.0015,
      "step": 1824
    },
    {
      "epoch": 75.84,
      "grad_norm": 0.11064162850379944,
      "learning_rate": 7.241342983315985e-06,
      "loss": 0.0013,
      "step": 1825
    },
    {
      "epoch": 75.89,
      "grad_norm": 0.7494289875030518,
      "learning_rate": 7.238476945034708e-06,
      "loss": 0.0032,
      "step": 1826
    },
    {
      "epoch": 75.93,
      "grad_norm": 0.06281763315200806,
      "learning_rate": 7.235609986600114e-06,
      "loss": 0.0011,
      "step": 1827
    },
    {
      "epoch": 75.97,
      "grad_norm": 0.09309329092502594,
      "learning_rate": 7.2327421091907006e-06,
      "loss": 0.0009,
      "step": 1828
    },
    {
      "epoch": 76.01,
      "grad_norm": 0.22225335240364075,
      "learning_rate": 7.2298733139853425e-06,
      "loss": 0.0018,
      "step": 1829
    },
    {
      "epoch": 76.05,
      "grad_norm": 0.04625449702143669,
      "learning_rate": 7.227003602163296e-06,
      "loss": 0.001,
      "step": 1830
    },
    {
      "epoch": 76.09,
      "grad_norm": 0.03460226207971573,
      "learning_rate": 7.22413297490419e-06,
      "loss": 0.0008,
      "step": 1831
    },
    {
      "epoch": 76.14,
      "grad_norm": 0.09412798285484314,
      "learning_rate": 7.221261433388032e-06,
      "loss": 0.001,
      "step": 1832
    },
    {
      "epoch": 76.18,
      "grad_norm": 0.44773000478744507,
      "learning_rate": 7.218388978795201e-06,
      "loss": 0.0021,
      "step": 1833
    },
    {
      "epoch": 76.22,
      "grad_norm": 0.46876612305641174,
      "learning_rate": 7.2155156123064575e-06,
      "loss": 0.0028,
      "step": 1834
    },
    {
      "epoch": 76.26,
      "grad_norm": 0.08081211149692535,
      "learning_rate": 7.212641335102932e-06,
      "loss": 0.0016,
      "step": 1835
    },
    {
      "epoch": 76.3,
      "grad_norm": 0.22276294231414795,
      "learning_rate": 7.2097661483661355e-06,
      "loss": 0.0024,
      "step": 1836
    },
    {
      "epoch": 76.34,
      "grad_norm": 0.12297741323709488,
      "learning_rate": 7.206890053277943e-06,
      "loss": 0.0012,
      "step": 1837
    },
    {
      "epoch": 76.38,
      "grad_norm": 0.05769263580441475,
      "learning_rate": 7.204013051020612e-06,
      "loss": 0.001,
      "step": 1838
    },
    {
      "epoch": 76.43,
      "grad_norm": 0.08191248774528503,
      "learning_rate": 7.201135142776768e-06,
      "loss": 0.001,
      "step": 1839
    },
    {
      "epoch": 76.47,
      "grad_norm": 0.5424173474311829,
      "learning_rate": 7.198256329729412e-06,
      "loss": 0.0032,
      "step": 1840
    },
    {
      "epoch": 76.51,
      "grad_norm": 0.06352818012237549,
      "learning_rate": 7.1953766130619125e-06,
      "loss": 0.001,
      "step": 1841
    },
    {
      "epoch": 76.55,
      "grad_norm": 0.9341597557067871,
      "learning_rate": 7.192495993958015e-06,
      "loss": 0.0021,
      "step": 1842
    },
    {
      "epoch": 76.59,
      "grad_norm": 0.18030758202075958,
      "learning_rate": 7.189614473601832e-06,
      "loss": 0.0017,
      "step": 1843
    },
    {
      "epoch": 76.63,
      "grad_norm": 0.10930409282445908,
      "learning_rate": 7.186732053177848e-06,
      "loss": 0.0013,
      "step": 1844
    },
    {
      "epoch": 76.68,
      "grad_norm": 0.295380562543869,
      "learning_rate": 7.183848733870917e-06,
      "loss": 0.0013,
      "step": 1845
    },
    {
      "epoch": 76.72,
      "grad_norm": 0.07400455325841904,
      "learning_rate": 7.180964516866263e-06,
      "loss": 0.001,
      "step": 1846
    },
    {
      "epoch": 76.76,
      "grad_norm": 0.17553985118865967,
      "learning_rate": 7.178079403349478e-06,
      "loss": 0.0014,
      "step": 1847
    },
    {
      "epoch": 76.8,
      "grad_norm": 0.1680784672498703,
      "learning_rate": 7.175193394506523e-06,
      "loss": 0.0011,
      "step": 1848
    },
    {
      "epoch": 76.84,
      "grad_norm": 0.26690614223480225,
      "learning_rate": 7.17230649152373e-06,
      "loss": 0.0025,
      "step": 1849
    },
    {
      "epoch": 76.88,
      "grad_norm": 0.04640762507915497,
      "learning_rate": 7.169418695587791e-06,
      "loss": 0.0008,
      "step": 1850
    },
    {
      "epoch": 76.92,
      "grad_norm": 0.10711275786161423,
      "learning_rate": 7.166530007885774e-06,
      "loss": 0.0013,
      "step": 1851
    },
    {
      "epoch": 76.97,
      "grad_norm": 0.10826684534549713,
      "learning_rate": 7.163640429605106e-06,
      "loss": 0.0014,
      "step": 1852
    },
    {
      "epoch": 77.01,
      "grad_norm": 0.49818992614746094,
      "learning_rate": 7.160749961933586e-06,
      "loss": 0.0054,
      "step": 1853
    },
    {
      "epoch": 77.05,
      "grad_norm": 0.2572493851184845,
      "learning_rate": 7.157858606059377e-06,
      "loss": 0.0013,
      "step": 1854
    },
    {
      "epoch": 77.09,
      "grad_norm": 0.15264348685741425,
      "learning_rate": 7.154966363171003e-06,
      "loss": 0.0015,
      "step": 1855
    },
    {
      "epoch": 77.13,
      "grad_norm": 0.4127260148525238,
      "learning_rate": 7.152073234457358e-06,
      "loss": 0.0014,
      "step": 1856
    },
    {
      "epoch": 77.17,
      "grad_norm": 0.07243659347295761,
      "learning_rate": 7.149179221107695e-06,
      "loss": 0.0008,
      "step": 1857
    },
    {
      "epoch": 77.22,
      "grad_norm": 0.039610013365745544,
      "learning_rate": 7.146284324311638e-06,
      "loss": 0.0008,
      "step": 1858
    },
    {
      "epoch": 77.26,
      "grad_norm": 0.06526394188404083,
      "learning_rate": 7.143388545259167e-06,
      "loss": 0.0009,
      "step": 1859
    },
    {
      "epoch": 77.3,
      "grad_norm": 0.08256116509437561,
      "learning_rate": 7.140491885140629e-06,
      "loss": 0.001,
      "step": 1860
    },
    {
      "epoch": 77.34,
      "grad_norm": 0.09450514614582062,
      "learning_rate": 7.137594345146729e-06,
      "loss": 0.0013,
      "step": 1861
    },
    {
      "epoch": 77.38,
      "grad_norm": 0.10130112618207932,
      "learning_rate": 7.1346959264685374e-06,
      "loss": 0.0012,
      "step": 1862
    },
    {
      "epoch": 77.42,
      "grad_norm": 0.29502424597740173,
      "learning_rate": 7.131796630297485e-06,
      "loss": 0.0014,
      "step": 1863
    },
    {
      "epoch": 77.46,
      "grad_norm": 0.08664493262767792,
      "learning_rate": 7.128896457825364e-06,
      "loss": 0.0013,
      "step": 1864
    },
    {
      "epoch": 77.51,
      "grad_norm": 0.026256488636136055,
      "learning_rate": 7.125995410244324e-06,
      "loss": 0.0007,
      "step": 1865
    },
    {
      "epoch": 77.55,
      "grad_norm": 0.11140875518321991,
      "learning_rate": 7.123093488746877e-06,
      "loss": 0.0014,
      "step": 1866
    },
    {
      "epoch": 77.59,
      "grad_norm": 0.3949164152145386,
      "learning_rate": 7.120190694525893e-06,
      "loss": 0.0032,
      "step": 1867
    },
    {
      "epoch": 77.63,
      "grad_norm": 0.2278066873550415,
      "learning_rate": 7.117287028774601e-06,
      "loss": 0.0027,
      "step": 1868
    },
    {
      "epoch": 77.67,
      "grad_norm": 0.11631748825311661,
      "learning_rate": 7.114382492686588e-06,
      "loss": 0.002,
      "step": 1869
    },
    {
      "epoch": 77.71,
      "grad_norm": 0.21940678358078003,
      "learning_rate": 7.1114770874558e-06,
      "loss": 0.0015,
      "step": 1870
    },
    {
      "epoch": 77.76,
      "grad_norm": 0.31475919485092163,
      "learning_rate": 7.108570814276539e-06,
      "loss": 0.0013,
      "step": 1871
    },
    {
      "epoch": 77.8,
      "grad_norm": 0.17984098196029663,
      "learning_rate": 7.105663674343462e-06,
      "loss": 0.0008,
      "step": 1872
    },
    {
      "epoch": 77.84,
      "grad_norm": 0.13740530610084534,
      "learning_rate": 7.102755668851589e-06,
      "loss": 0.0007,
      "step": 1873
    },
    {
      "epoch": 77.88,
      "grad_norm": 0.15010812878608704,
      "learning_rate": 7.099846798996287e-06,
      "loss": 0.0009,
      "step": 1874
    },
    {
      "epoch": 77.92,
      "grad_norm": 0.12925215065479279,
      "learning_rate": 7.096937065973285e-06,
      "loss": 0.0013,
      "step": 1875
    },
    {
      "epoch": 77.96,
      "grad_norm": 0.1112767904996872,
      "learning_rate": 7.094026470978663e-06,
      "loss": 0.0009,
      "step": 1876
    },
    {
      "epoch": 78.01,
      "grad_norm": 0.17072218656539917,
      "learning_rate": 7.091115015208856e-06,
      "loss": 0.0017,
      "step": 1877
    },
    {
      "epoch": 78.05,
      "grad_norm": 0.08245264738798141,
      "learning_rate": 7.088202699860656e-06,
      "loss": 0.0012,
      "step": 1878
    },
    {
      "epoch": 78.09,
      "grad_norm": 0.12080097943544388,
      "learning_rate": 7.085289526131204e-06,
      "loss": 0.0014,
      "step": 1879
    },
    {
      "epoch": 78.13,
      "grad_norm": 0.029324615374207497,
      "learning_rate": 7.082375495217996e-06,
      "loss": 0.0007,
      "step": 1880
    },
    {
      "epoch": 78.17,
      "grad_norm": 0.056929703801870346,
      "learning_rate": 7.07946060831888e-06,
      "loss": 0.0011,
      "step": 1881
    },
    {
      "epoch": 78.21,
      "grad_norm": 0.05278630182147026,
      "learning_rate": 7.076544866632058e-06,
      "loss": 0.0009,
      "step": 1882
    },
    {
      "epoch": 78.25,
      "grad_norm": 0.07724704593420029,
      "learning_rate": 7.073628271356077e-06,
      "loss": 0.0012,
      "step": 1883
    },
    {
      "epoch": 78.3,
      "grad_norm": 0.10153442621231079,
      "learning_rate": 7.070710823689841e-06,
      "loss": 0.0011,
      "step": 1884
    },
    {
      "epoch": 78.34,
      "grad_norm": 0.06657479703426361,
      "learning_rate": 7.067792524832604e-06,
      "loss": 0.0011,
      "step": 1885
    },
    {
      "epoch": 78.38,
      "grad_norm": 0.09852097928524017,
      "learning_rate": 7.064873375983967e-06,
      "loss": 0.0012,
      "step": 1886
    },
    {
      "epoch": 78.42,
      "grad_norm": 0.042036645114421844,
      "learning_rate": 7.061953378343883e-06,
      "loss": 0.001,
      "step": 1887
    },
    {
      "epoch": 78.46,
      "grad_norm": 0.06315102428197861,
      "learning_rate": 7.059032533112652e-06,
      "loss": 0.0011,
      "step": 1888
    },
    {
      "epoch": 78.5,
      "grad_norm": 0.06090114265680313,
      "learning_rate": 7.056110841490922e-06,
      "loss": 0.0009,
      "step": 1889
    },
    {
      "epoch": 78.55,
      "grad_norm": 0.04957445710897446,
      "learning_rate": 7.053188304679691e-06,
      "loss": 0.0009,
      "step": 1890
    },
    {
      "epoch": 78.59,
      "grad_norm": 0.08279246836900711,
      "learning_rate": 7.050264923880304e-06,
      "loss": 0.0013,
      "step": 1891
    },
    {
      "epoch": 78.63,
      "grad_norm": 0.019395694136619568,
      "learning_rate": 7.047340700294454e-06,
      "loss": 0.0007,
      "step": 1892
    },
    {
      "epoch": 78.67,
      "grad_norm": 0.019702445715665817,
      "learning_rate": 7.044415635124176e-06,
      "loss": 0.0006,
      "step": 1893
    },
    {
      "epoch": 78.71,
      "grad_norm": 0.08380433171987534,
      "learning_rate": 7.041489729571853e-06,
      "loss": 0.0013,
      "step": 1894
    },
    {
      "epoch": 78.75,
      "grad_norm": 0.04426733776926994,
      "learning_rate": 7.038562984840216e-06,
      "loss": 0.001,
      "step": 1895
    },
    {
      "epoch": 78.79,
      "grad_norm": 0.24717499315738678,
      "learning_rate": 7.03563540213234e-06,
      "loss": 0.0009,
      "step": 1896
    },
    {
      "epoch": 78.84,
      "grad_norm": 0.185271754860878,
      "learning_rate": 7.032706982651645e-06,
      "loss": 0.0012,
      "step": 1897
    },
    {
      "epoch": 78.88,
      "grad_norm": 0.039278045296669006,
      "learning_rate": 7.029777727601888e-06,
      "loss": 0.0007,
      "step": 1898
    },
    {
      "epoch": 78.92,
      "grad_norm": 0.09220942854881287,
      "learning_rate": 7.026847638187181e-06,
      "loss": 0.001,
      "step": 1899
    },
    {
      "epoch": 78.96,
      "grad_norm": 0.36138254404067993,
      "learning_rate": 7.023916715611969e-06,
      "loss": 0.0035,
      "step": 1900
    },
    {
      "epoch": 79.0,
      "grad_norm": 0.04982133209705353,
      "learning_rate": 7.020984961081047e-06,
      "loss": 0.0008,
      "step": 1901
    },
    {
      "epoch": 79.04,
      "grad_norm": 0.041653089225292206,
      "learning_rate": 7.018052375799545e-06,
      "loss": 0.0008,
      "step": 1902
    },
    {
      "epoch": 79.09,
      "grad_norm": 0.0731750950217247,
      "learning_rate": 7.015118960972942e-06,
      "loss": 0.0011,
      "step": 1903
    },
    {
      "epoch": 79.13,
      "grad_norm": 0.05479179322719574,
      "learning_rate": 7.012184717807051e-06,
      "loss": 0.0009,
      "step": 1904
    },
    {
      "epoch": 79.17,
      "grad_norm": 0.05440635606646538,
      "learning_rate": 7.009249647508028e-06,
      "loss": 0.0009,
      "step": 1905
    },
    {
      "epoch": 79.21,
      "grad_norm": 0.07612289488315582,
      "learning_rate": 7.006313751282372e-06,
      "loss": 0.0012,
      "step": 1906
    },
    {
      "epoch": 79.25,
      "grad_norm": 0.04763943329453468,
      "learning_rate": 7.003377030336917e-06,
      "loss": 0.0009,
      "step": 1907
    },
    {
      "epoch": 79.29,
      "grad_norm": 0.0908827930688858,
      "learning_rate": 7.000439485878841e-06,
      "loss": 0.0012,
      "step": 1908
    },
    {
      "epoch": 79.34,
      "grad_norm": 0.04685240983963013,
      "learning_rate": 6.997501119115654e-06,
      "loss": 0.0009,
      "step": 1909
    },
    {
      "epoch": 79.38,
      "grad_norm": 0.01784658059477806,
      "learning_rate": 6.994561931255209e-06,
      "loss": 0.0006,
      "step": 1910
    },
    {
      "epoch": 79.42,
      "grad_norm": 0.07602834701538086,
      "learning_rate": 6.991621923505696e-06,
      "loss": 0.001,
      "step": 1911
    },
    {
      "epoch": 79.46,
      "grad_norm": 0.01310543529689312,
      "learning_rate": 6.98868109707564e-06,
      "loss": 0.0006,
      "step": 1912
    },
    {
      "epoch": 79.5,
      "grad_norm": 0.03956856578588486,
      "learning_rate": 6.985739453173903e-06,
      "loss": 0.0008,
      "step": 1913
    },
    {
      "epoch": 79.54,
      "grad_norm": 0.08111826330423355,
      "learning_rate": 6.982796993009687e-06,
      "loss": 0.0012,
      "step": 1914
    },
    {
      "epoch": 79.58,
      "grad_norm": 0.07257776707410812,
      "learning_rate": 6.9798537177925226e-06,
      "loss": 0.001,
      "step": 1915
    },
    {
      "epoch": 79.63,
      "grad_norm": 0.07601086795330048,
      "learning_rate": 6.97690962873228e-06,
      "loss": 0.001,
      "step": 1916
    },
    {
      "epoch": 79.67,
      "grad_norm": 0.014363144524395466,
      "learning_rate": 6.973964727039164e-06,
      "loss": 0.0006,
      "step": 1917
    },
    {
      "epoch": 79.71,
      "grad_norm": 0.12882500886917114,
      "learning_rate": 6.971019013923712e-06,
      "loss": 0.0016,
      "step": 1918
    },
    {
      "epoch": 79.75,
      "grad_norm": 0.04979771748185158,
      "learning_rate": 6.968072490596796e-06,
      "loss": 0.0008,
      "step": 1919
    },
    {
      "epoch": 79.79,
      "grad_norm": 0.22040300071239471,
      "learning_rate": 6.965125158269619e-06,
      "loss": 0.0021,
      "step": 1920
    },
    {
      "epoch": 79.83,
      "grad_norm": 0.01622648537158966,
      "learning_rate": 6.962177018153718e-06,
      "loss": 0.0007,
      "step": 1921
    },
    {
      "epoch": 79.88,
      "grad_norm": 0.031995583325624466,
      "learning_rate": 6.959228071460964e-06,
      "loss": 0.0008,
      "step": 1922
    },
    {
      "epoch": 79.92,
      "grad_norm": 0.11269786208868027,
      "learning_rate": 6.956278319403556e-06,
      "loss": 0.0016,
      "step": 1923
    },
    {
      "epoch": 79.96,
      "grad_norm": 0.013783263973891735,
      "learning_rate": 6.953327763194026e-06,
      "loss": 0.0006,
      "step": 1924
    },
    {
      "epoch": 80.0,
      "grad_norm": 0.1132311150431633,
      "learning_rate": 6.950376404045235e-06,
      "loss": 0.0014,
      "step": 1925
    },
    {
      "epoch": 80.04,
      "grad_norm": 0.013674194924533367,
      "learning_rate": 6.947424243170378e-06,
      "loss": 0.0006,
      "step": 1926
    },
    {
      "epoch": 80.08,
      "grad_norm": 0.06641272455453873,
      "learning_rate": 6.944471281782975e-06,
      "loss": 0.001,
      "step": 1927
    },
    {
      "epoch": 80.12,
      "grad_norm": 0.05322805792093277,
      "learning_rate": 6.941517521096877e-06,
      "loss": 0.0008,
      "step": 1928
    },
    {
      "epoch": 80.17,
      "grad_norm": 0.05572506785392761,
      "learning_rate": 6.938562962326263e-06,
      "loss": 0.0009,
      "step": 1929
    },
    {
      "epoch": 80.21,
      "grad_norm": 0.08182182163000107,
      "learning_rate": 6.935607606685642e-06,
      "loss": 0.0009,
      "step": 1930
    },
    {
      "epoch": 80.25,
      "grad_norm": 0.015211294405162334,
      "learning_rate": 6.932651455389847e-06,
      "loss": 0.0006,
      "step": 1931
    },
    {
      "epoch": 80.29,
      "grad_norm": 0.294463187456131,
      "learning_rate": 6.929694509654043e-06,
      "loss": 0.002,
      "step": 1932
    },
    {
      "epoch": 80.33,
      "grad_norm": 0.08673450350761414,
      "learning_rate": 6.926736770693715e-06,
      "loss": 0.0011,
      "step": 1933
    },
    {
      "epoch": 80.37,
      "grad_norm": 0.054274722933769226,
      "learning_rate": 6.923778239724681e-06,
      "loss": 0.0007,
      "step": 1934
    },
    {
      "epoch": 80.42,
      "grad_norm": 0.08165448904037476,
      "learning_rate": 6.9208189179630805e-06,
      "loss": 0.0012,
      "step": 1935
    },
    {
      "epoch": 80.46,
      "grad_norm": 0.09802775830030441,
      "learning_rate": 6.917858806625377e-06,
      "loss": 0.0014,
      "step": 1936
    },
    {
      "epoch": 80.5,
      "grad_norm": 0.03902805596590042,
      "learning_rate": 6.914897906928363e-06,
      "loss": 0.0008,
      "step": 1937
    },
    {
      "epoch": 80.54,
      "grad_norm": 0.07767218351364136,
      "learning_rate": 6.9119362200891514e-06,
      "loss": 0.001,
      "step": 1938
    },
    {
      "epoch": 80.58,
      "grad_norm": 0.02672126516699791,
      "learning_rate": 6.908973747325179e-06,
      "loss": 0.0007,
      "step": 1939
    },
    {
      "epoch": 80.62,
      "grad_norm": 0.024668091908097267,
      "learning_rate": 6.906010489854209e-06,
      "loss": 0.0007,
      "step": 1940
    },
    {
      "epoch": 80.66,
      "grad_norm": 0.07238753139972687,
      "learning_rate": 6.903046448894322e-06,
      "loss": 0.0016,
      "step": 1941
    },
    {
      "epoch": 80.71,
      "grad_norm": 0.05841564014554024,
      "learning_rate": 6.900081625663925e-06,
      "loss": 0.0009,
      "step": 1942
    },
    {
      "epoch": 80.75,
      "grad_norm": 0.026296038180589676,
      "learning_rate": 6.8971160213817425e-06,
      "loss": 0.0008,
      "step": 1943
    },
    {
      "epoch": 80.79,
      "grad_norm": 0.05571722239255905,
      "learning_rate": 6.894149637266825e-06,
      "loss": 0.0009,
      "step": 1944
    },
    {
      "epoch": 80.83,
      "grad_norm": 0.04793379828333855,
      "learning_rate": 6.891182474538539e-06,
      "loss": 0.0008,
      "step": 1945
    },
    {
      "epoch": 80.87,
      "grad_norm": 0.043653205037117004,
      "learning_rate": 6.888214534416575e-06,
      "loss": 0.0007,
      "step": 1946
    },
    {
      "epoch": 80.91,
      "grad_norm": 0.10131055861711502,
      "learning_rate": 6.88524581812094e-06,
      "loss": 0.0009,
      "step": 1947
    },
    {
      "epoch": 80.96,
      "grad_norm": 0.10951142758131027,
      "learning_rate": 6.88227632687196e-06,
      "loss": 0.0019,
      "step": 1948
    },
    {
      "epoch": 81.0,
      "grad_norm": 0.057867661118507385,
      "learning_rate": 6.879306061890284e-06,
      "loss": 0.001,
      "step": 1949
    },
    {
      "epoch": 81.04,
      "grad_norm": 0.07662174850702286,
      "learning_rate": 6.876335024396872e-06,
      "loss": 0.0009,
      "step": 1950
    },
    {
      "epoch": 81.08,
      "grad_norm": 0.08478980511426926,
      "learning_rate": 6.873363215613007e-06,
      "loss": 0.001,
      "step": 1951
    },
    {
      "epoch": 81.12,
      "grad_norm": 0.10339052230119705,
      "learning_rate": 6.870390636760286e-06,
      "loss": 0.001,
      "step": 1952
    },
    {
      "epoch": 81.16,
      "grad_norm": 0.011389452964067459,
      "learning_rate": 6.867417289060627e-06,
      "loss": 0.0005,
      "step": 1953
    },
    {
      "epoch": 81.21,
      "grad_norm": 0.03129406273365021,
      "learning_rate": 6.8644431737362585e-06,
      "loss": 0.0007,
      "step": 1954
    },
    {
      "epoch": 81.25,
      "grad_norm": 0.04680662974715233,
      "learning_rate": 6.8614682920097265e-06,
      "loss": 0.0008,
      "step": 1955
    },
    {
      "epoch": 81.29,
      "grad_norm": 0.06498507410287857,
      "learning_rate": 6.858492645103894e-06,
      "loss": 0.0014,
      "step": 1956
    },
    {
      "epoch": 81.33,
      "grad_norm": 0.08997456729412079,
      "learning_rate": 6.855516234241936e-06,
      "loss": 0.0009,
      "step": 1957
    },
    {
      "epoch": 81.37,
      "grad_norm": 0.08795998245477676,
      "learning_rate": 6.852539060647345e-06,
      "loss": 0.0009,
      "step": 1958
    },
    {
      "epoch": 81.41,
      "grad_norm": 0.07639534026384354,
      "learning_rate": 6.849561125543921e-06,
      "loss": 0.0009,
      "step": 1959
    },
    {
      "epoch": 81.45,
      "grad_norm": 0.10400217026472092,
      "learning_rate": 6.846582430155783e-06,
      "loss": 0.0011,
      "step": 1960
    },
    {
      "epoch": 81.5,
      "grad_norm": 0.02086043357849121,
      "learning_rate": 6.843602975707358e-06,
      "loss": 0.0007,
      "step": 1961
    },
    {
      "epoch": 81.54,
      "grad_norm": 0.054403163492679596,
      "learning_rate": 6.840622763423391e-06,
      "loss": 0.0008,
      "step": 1962
    },
    {
      "epoch": 81.58,
      "grad_norm": 0.03157544508576393,
      "learning_rate": 6.837641794528931e-06,
      "loss": 0.0008,
      "step": 1963
    },
    {
      "epoch": 81.62,
      "grad_norm": 0.05513056740164757,
      "learning_rate": 6.834660070249343e-06,
      "loss": 0.001,
      "step": 1964
    },
    {
      "epoch": 81.66,
      "grad_norm": 0.10398314893245697,
      "learning_rate": 6.831677591810302e-06,
      "loss": 0.0013,
      "step": 1965
    },
    {
      "epoch": 81.7,
      "grad_norm": 0.07816759496927261,
      "learning_rate": 6.82869436043779e-06,
      "loss": 0.0012,
      "step": 1966
    },
    {
      "epoch": 81.75,
      "grad_norm": 0.049629028886556625,
      "learning_rate": 6.825710377358105e-06,
      "loss": 0.0007,
      "step": 1967
    },
    {
      "epoch": 81.79,
      "grad_norm": 0.09210015833377838,
      "learning_rate": 6.822725643797844e-06,
      "loss": 0.0016,
      "step": 1968
    },
    {
      "epoch": 81.83,
      "grad_norm": 0.04497317597270012,
      "learning_rate": 6.819740160983923e-06,
      "loss": 0.0007,
      "step": 1969
    },
    {
      "epoch": 81.87,
      "grad_norm": 0.29443565011024475,
      "learning_rate": 6.816753930143558e-06,
      "loss": 0.0031,
      "step": 1970
    },
    {
      "epoch": 81.91,
      "grad_norm": 0.0623219795525074,
      "learning_rate": 6.813766952504278e-06,
      "loss": 0.0009,
      "step": 1971
    },
    {
      "epoch": 81.95,
      "grad_norm": 0.0542408786714077,
      "learning_rate": 6.810779229293917e-06,
      "loss": 0.0007,
      "step": 1972
    },
    {
      "epoch": 81.99,
      "grad_norm": 0.01419354509562254,
      "learning_rate": 6.807790761740613e-06,
      "loss": 0.0006,
      "step": 1973
    },
    {
      "epoch": 82.04,
      "grad_norm": 0.07009407877922058,
      "learning_rate": 6.8048015510728125e-06,
      "loss": 0.0011,
      "step": 1974
    },
    {
      "epoch": 82.08,
      "grad_norm": 0.07553645968437195,
      "learning_rate": 6.801811598519268e-06,
      "loss": 0.001,
      "step": 1975
    },
    {
      "epoch": 82.12,
      "grad_norm": 0.011907661333680153,
      "learning_rate": 6.798820905309036e-06,
      "loss": 0.0005,
      "step": 1976
    },
    {
      "epoch": 82.16,
      "grad_norm": 0.01368577778339386,
      "learning_rate": 6.795829472671476e-06,
      "loss": 0.0006,
      "step": 1977
    },
    {
      "epoch": 82.2,
      "grad_norm": 0.0572475902736187,
      "learning_rate": 6.7928373018362546e-06,
      "loss": 0.0009,
      "step": 1978
    },
    {
      "epoch": 82.24,
      "grad_norm": 0.12695768475532532,
      "learning_rate": 6.789844394033342e-06,
      "loss": 0.0019,
      "step": 1979
    },
    {
      "epoch": 82.29,
      "grad_norm": 0.1021030843257904,
      "learning_rate": 6.786850750493006e-06,
      "loss": 0.0012,
      "step": 1980
    },
    {
      "epoch": 82.33,
      "grad_norm": 0.08605163544416428,
      "learning_rate": 6.783856372445821e-06,
      "loss": 0.0011,
      "step": 1981
    },
    {
      "epoch": 82.37,
      "grad_norm": 0.13693448901176453,
      "learning_rate": 6.780861261122663e-06,
      "loss": 0.0012,
      "step": 1982
    },
    {
      "epoch": 82.41,
      "grad_norm": 0.0319489948451519,
      "learning_rate": 6.77786541775471e-06,
      "loss": 0.0007,
      "step": 1983
    },
    {
      "epoch": 82.45,
      "grad_norm": 0.05992724001407623,
      "learning_rate": 6.774868843573441e-06,
      "loss": 0.0009,
      "step": 1984
    },
    {
      "epoch": 82.49,
      "grad_norm": 0.05278672277927399,
      "learning_rate": 6.771871539810633e-06,
      "loss": 0.0008,
      "step": 1985
    },
    {
      "epoch": 82.54,
      "grad_norm": 0.01161668449640274,
      "learning_rate": 6.768873507698363e-06,
      "loss": 0.0006,
      "step": 1986
    },
    {
      "epoch": 82.58,
      "grad_norm": 0.07053124904632568,
      "learning_rate": 6.76587474846901e-06,
      "loss": 0.0009,
      "step": 1987
    },
    {
      "epoch": 82.62,
      "grad_norm": 0.1017095223069191,
      "learning_rate": 6.7628752633552505e-06,
      "loss": 0.0012,
      "step": 1988
    },
    {
      "epoch": 82.66,
      "grad_norm": 0.0639202818274498,
      "learning_rate": 6.759875053590063e-06,
      "loss": 0.0011,
      "step": 1989
    },
    {
      "epoch": 82.7,
      "grad_norm": 0.01641593873500824,
      "learning_rate": 6.7568741204067145e-06,
      "loss": 0.0006,
      "step": 1990
    },
    {
      "epoch": 82.74,
      "grad_norm": 0.05574401840567589,
      "learning_rate": 6.753872465038777e-06,
      "loss": 0.0007,
      "step": 1991
    },
    {
      "epoch": 82.78,
      "grad_norm": 0.0977034792304039,
      "learning_rate": 6.750870088720122e-06,
      "loss": 0.0012,
      "step": 1992
    },
    {
      "epoch": 82.83,
      "grad_norm": 0.06452930718660355,
      "learning_rate": 6.747866992684907e-06,
      "loss": 0.0008,
      "step": 1993
    },
    {
      "epoch": 82.87,
      "grad_norm": 0.08874072879552841,
      "learning_rate": 6.744863178167595e-06,
      "loss": 0.0011,
      "step": 1994
    },
    {
      "epoch": 82.91,
      "grad_norm": 0.11261215806007385,
      "learning_rate": 6.741858646402941e-06,
      "loss": 0.0012,
      "step": 1995
    },
    {
      "epoch": 82.95,
      "grad_norm": 0.07793289422988892,
      "learning_rate": 6.738853398625993e-06,
      "loss": 0.0008,
      "step": 1996
    },
    {
      "epoch": 82.99,
      "grad_norm": 0.018519015982747078,
      "learning_rate": 6.735847436072094e-06,
      "loss": 0.0006,
      "step": 1997
    },
    {
      "epoch": 83.03,
      "grad_norm": 0.07711391896009445,
      "learning_rate": 6.732840759976882e-06,
      "loss": 0.0011,
      "step": 1998
    },
    {
      "epoch": 83.08,
      "grad_norm": 0.06113439425826073,
      "learning_rate": 6.7298333715762886e-06,
      "loss": 0.0009,
      "step": 1999
    },
    {
      "epoch": 83.12,
      "grad_norm": 0.12061260640621185,
      "learning_rate": 6.726825272106539e-06,
      "loss": 0.0018,
      "step": 2000
    },
    {
      "epoch": 83.16,
      "grad_norm": 0.06440748274326324,
      "learning_rate": 6.723816462804148e-06,
      "loss": 0.001,
      "step": 2001
    },
    {
      "epoch": 83.2,
      "grad_norm": 0.054587166756391525,
      "learning_rate": 6.720806944905922e-06,
      "loss": 0.0008,
      "step": 2002
    },
    {
      "epoch": 83.24,
      "grad_norm": 0.08181127905845642,
      "learning_rate": 6.717796719648963e-06,
      "loss": 0.001,
      "step": 2003
    },
    {
      "epoch": 83.28,
      "grad_norm": 0.02432967908680439,
      "learning_rate": 6.714785788270658e-06,
      "loss": 0.0006,
      "step": 2004
    },
    {
      "epoch": 83.32,
      "grad_norm": 0.0675596371293068,
      "learning_rate": 6.71177415200869e-06,
      "loss": 0.0009,
      "step": 2005
    },
    {
      "epoch": 83.37,
      "grad_norm": 0.01143217459321022,
      "learning_rate": 6.7087618121010265e-06,
      "loss": 0.0005,
      "step": 2006
    },
    {
      "epoch": 83.41,
      "grad_norm": 0.01914401352405548,
      "learning_rate": 6.705748769785928e-06,
      "loss": 0.0006,
      "step": 2007
    },
    {
      "epoch": 83.45,
      "grad_norm": 0.01186502818018198,
      "learning_rate": 6.702735026301942e-06,
      "loss": 0.0006,
      "step": 2008
    },
    {
      "epoch": 83.49,
      "grad_norm": 0.055605340749025345,
      "learning_rate": 6.699720582887904e-06,
      "loss": 0.0007,
      "step": 2009
    },
    {
      "epoch": 83.53,
      "grad_norm": 0.08991790562868118,
      "learning_rate": 6.696705440782939e-06,
      "loss": 0.0013,
      "step": 2010
    },
    {
      "epoch": 83.57,
      "grad_norm": 0.0824342593550682,
      "learning_rate": 6.693689601226458e-06,
      "loss": 0.001,
      "step": 2011
    },
    {
      "epoch": 83.62,
      "grad_norm": 0.05106459558010101,
      "learning_rate": 6.690673065458158e-06,
      "loss": 0.0007,
      "step": 2012
    },
    {
      "epoch": 83.66,
      "grad_norm": 0.05954213812947273,
      "learning_rate": 6.687655834718022e-06,
      "loss": 0.001,
      "step": 2013
    },
    {
      "epoch": 83.7,
      "grad_norm": 0.06963305920362473,
      "learning_rate": 6.684637910246321e-06,
      "loss": 0.0009,
      "step": 2014
    },
    {
      "epoch": 83.74,
      "grad_norm": 0.07790611684322357,
      "learning_rate": 6.68161929328361e-06,
      "loss": 0.0009,
      "step": 2015
    },
    {
      "epoch": 83.78,
      "grad_norm": 0.1428392231464386,
      "learning_rate": 6.678599985070728e-06,
      "loss": 0.0015,
      "step": 2016
    },
    {
      "epoch": 83.82,
      "grad_norm": 0.03128093481063843,
      "learning_rate": 6.675579986848799e-06,
      "loss": 0.0008,
      "step": 2017
    },
    {
      "epoch": 83.86,
      "grad_norm": 0.07920657098293304,
      "learning_rate": 6.672559299859228e-06,
      "loss": 0.0009,
      "step": 2018
    },
    {
      "epoch": 83.91,
      "grad_norm": 0.03607718646526337,
      "learning_rate": 6.669537925343708e-06,
      "loss": 0.0007,
      "step": 2019
    },
    {
      "epoch": 83.95,
      "grad_norm": 0.09578295052051544,
      "learning_rate": 6.66651586454421e-06,
      "loss": 0.0012,
      "step": 2020
    },
    {
      "epoch": 83.99,
      "grad_norm": 0.14419697225093842,
      "learning_rate": 6.66349311870299e-06,
      "loss": 0.0013,
      "step": 2021
    },
    {
      "epoch": 84.03,
      "grad_norm": 0.07517436891794205,
      "learning_rate": 6.660469689062585e-06,
      "loss": 0.001,
      "step": 2022
    },
    {
      "epoch": 84.07,
      "grad_norm": 0.09082841873168945,
      "learning_rate": 6.657445576865813e-06,
      "loss": 0.001,
      "step": 2023
    },
    {
      "epoch": 84.11,
      "grad_norm": 0.0625135749578476,
      "learning_rate": 6.65442078335577e-06,
      "loss": 0.0008,
      "step": 2024
    },
    {
      "epoch": 84.16,
      "grad_norm": 0.07388680428266525,
      "learning_rate": 6.651395309775837e-06,
      "loss": 0.0008,
      "step": 2025
    },
    {
      "epoch": 84.2,
      "grad_norm": 0.037009600549936295,
      "learning_rate": 6.648369157369669e-06,
      "loss": 0.0007,
      "step": 2026
    },
    {
      "epoch": 84.24,
      "grad_norm": 0.010189256630837917,
      "learning_rate": 6.645342327381208e-06,
      "loss": 0.0005,
      "step": 2027
    },
    {
      "epoch": 84.28,
      "grad_norm": 0.07311184704303741,
      "learning_rate": 6.642314821054663e-06,
      "loss": 0.0008,
      "step": 2028
    },
    {
      "epoch": 84.32,
      "grad_norm": 0.03221690654754639,
      "learning_rate": 6.639286639634532e-06,
      "loss": 0.0006,
      "step": 2029
    },
    {
      "epoch": 84.36,
      "grad_norm": 0.07221202552318573,
      "learning_rate": 6.636257784365585e-06,
      "loss": 0.0009,
      "step": 2030
    },
    {
      "epoch": 84.41,
      "grad_norm": 0.0874766856431961,
      "learning_rate": 6.633228256492869e-06,
      "loss": 0.0011,
      "step": 2031
    },
    {
      "epoch": 84.45,
      "grad_norm": 0.03644075244665146,
      "learning_rate": 6.63019805726171e-06,
      "loss": 0.0009,
      "step": 2032
    },
    {
      "epoch": 84.49,
      "grad_norm": 0.00982198491692543,
      "learning_rate": 6.627167187917707e-06,
      "loss": 0.0005,
      "step": 2033
    },
    {
      "epoch": 84.53,
      "grad_norm": 0.014190461486577988,
      "learning_rate": 6.624135649706738e-06,
      "loss": 0.0007,
      "step": 2034
    },
    {
      "epoch": 84.57,
      "grad_norm": 0.05004448816180229,
      "learning_rate": 6.62110344387495e-06,
      "loss": 0.0008,
      "step": 2035
    },
    {
      "epoch": 84.61,
      "grad_norm": 0.010076078586280346,
      "learning_rate": 6.618070571668771e-06,
      "loss": 0.0005,
      "step": 2036
    },
    {
      "epoch": 84.65,
      "grad_norm": 0.010508183389902115,
      "learning_rate": 6.615037034334901e-06,
      "loss": 0.0005,
      "step": 2037
    },
    {
      "epoch": 84.7,
      "grad_norm": 0.06631024181842804,
      "learning_rate": 6.6120028331203125e-06,
      "loss": 0.0008,
      "step": 2038
    },
    {
      "epoch": 84.74,
      "grad_norm": 0.04521825164556503,
      "learning_rate": 6.608967969272249e-06,
      "loss": 0.0007,
      "step": 2039
    },
    {
      "epoch": 84.78,
      "grad_norm": 0.046483125537633896,
      "learning_rate": 6.605932444038229e-06,
      "loss": 0.0008,
      "step": 2040
    },
    {
      "epoch": 84.82,
      "grad_norm": 0.12162487208843231,
      "learning_rate": 6.602896258666043e-06,
      "loss": 0.0011,
      "step": 2041
    },
    {
      "epoch": 84.86,
      "grad_norm": 0.0600607767701149,
      "learning_rate": 6.599859414403751e-06,
      "loss": 0.0009,
      "step": 2042
    },
    {
      "epoch": 84.9,
      "grad_norm": 0.12538330256938934,
      "learning_rate": 6.596821912499685e-06,
      "loss": 0.0019,
      "step": 2043
    },
    {
      "epoch": 84.95,
      "grad_norm": 0.04397351294755936,
      "learning_rate": 6.59378375420245e-06,
      "loss": 0.0007,
      "step": 2044
    },
    {
      "epoch": 84.99,
      "grad_norm": 0.05608765780925751,
      "learning_rate": 6.5907449407609145e-06,
      "loss": 0.0008,
      "step": 2045
    },
    {
      "epoch": 85.03,
      "grad_norm": 0.052633900195360184,
      "learning_rate": 6.587705473424223e-06,
      "loss": 0.0008,
      "step": 2046
    },
    {
      "epoch": 85.07,
      "grad_norm": 0.010561371222138405,
      "learning_rate": 6.5846653534417826e-06,
      "loss": 0.0005,
      "step": 2047
    },
    {
      "epoch": 85.11,
      "grad_norm": 0.0670156180858612,
      "learning_rate": 6.5816245820632745e-06,
      "loss": 0.0011,
      "step": 2048
    },
    {
      "epoch": 85.15,
      "grad_norm": 0.04526439681649208,
      "learning_rate": 6.578583160538644e-06,
      "loss": 0.001,
      "step": 2049
    },
    {
      "epoch": 85.19,
      "grad_norm": 0.08289337158203125,
      "learning_rate": 6.575541090118105e-06,
      "loss": 0.0011,
      "step": 2050
    },
    {
      "epoch": 85.24,
      "grad_norm": 0.08773060142993927,
      "learning_rate": 6.572498372052136e-06,
      "loss": 0.0009,
      "step": 2051
    },
    {
      "epoch": 85.28,
      "grad_norm": 0.07520680874586105,
      "learning_rate": 6.569455007591485e-06,
      "loss": 0.0009,
      "step": 2052
    },
    {
      "epoch": 85.32,
      "grad_norm": 0.02666337601840496,
      "learning_rate": 6.566410997987164e-06,
      "loss": 0.0006,
      "step": 2053
    },
    {
      "epoch": 85.36,
      "grad_norm": 0.04308127611875534,
      "learning_rate": 6.56336634449045e-06,
      "loss": 0.0007,
      "step": 2054
    },
    {
      "epoch": 85.4,
      "grad_norm": 0.06779944151639938,
      "learning_rate": 6.5603210483528864e-06,
      "loss": 0.0008,
      "step": 2055
    },
    {
      "epoch": 85.44,
      "grad_norm": 0.05648795887827873,
      "learning_rate": 6.557275110826277e-06,
      "loss": 0.0008,
      "step": 2056
    },
    {
      "epoch": 85.49,
      "grad_norm": 0.062108032405376434,
      "learning_rate": 6.554228533162693e-06,
      "loss": 0.0008,
      "step": 2057
    },
    {
      "epoch": 85.53,
      "grad_norm": 0.03829463943839073,
      "learning_rate": 6.551181316614468e-06,
      "loss": 0.0007,
      "step": 2058
    },
    {
      "epoch": 85.57,
      "grad_norm": 0.02519930526614189,
      "learning_rate": 6.548133462434196e-06,
      "loss": 0.0006,
      "step": 2059
    },
    {
      "epoch": 85.61,
      "grad_norm": 0.0357961505651474,
      "learning_rate": 6.545084971874738e-06,
      "loss": 0.0006,
      "step": 2060
    },
    {
      "epoch": 85.65,
      "grad_norm": 0.15661099553108215,
      "learning_rate": 6.542035846189209e-06,
      "loss": 0.0015,
      "step": 2061
    },
    {
      "epoch": 85.69,
      "grad_norm": 0.07393592596054077,
      "learning_rate": 6.538986086630991e-06,
      "loss": 0.0009,
      "step": 2062
    },
    {
      "epoch": 85.74,
      "grad_norm": 0.01083318144083023,
      "learning_rate": 6.535935694453728e-06,
      "loss": 0.0005,
      "step": 2063
    },
    {
      "epoch": 85.78,
      "grad_norm": 0.07861236482858658,
      "learning_rate": 6.532884670911317e-06,
      "loss": 0.001,
      "step": 2064
    },
    {
      "epoch": 85.82,
      "grad_norm": 0.0638996884226799,
      "learning_rate": 6.529833017257919e-06,
      "loss": 0.0008,
      "step": 2065
    },
    {
      "epoch": 85.86,
      "grad_norm": 0.024649929255247116,
      "learning_rate": 6.526780734747956e-06,
      "loss": 0.0005,
      "step": 2066
    },
    {
      "epoch": 85.9,
      "grad_norm": 0.1301690936088562,
      "learning_rate": 6.523727824636103e-06,
      "loss": 0.0012,
      "step": 2067
    },
    {
      "epoch": 85.94,
      "grad_norm": 0.009682693518698215,
      "learning_rate": 6.5206742881772975e-06,
      "loss": 0.0005,
      "step": 2068
    },
    {
      "epoch": 85.98,
      "grad_norm": 0.03674961254000664,
      "learning_rate": 6.517620126626734e-06,
      "loss": 0.0007,
      "step": 2069
    },
    {
      "epoch": 86.03,
      "grad_norm": 0.00965814758092165,
      "learning_rate": 6.514565341239861e-06,
      "loss": 0.0005,
      "step": 2070
    },
    {
      "epoch": 86.07,
      "grad_norm": 0.1087980568408966,
      "learning_rate": 6.511509933272389e-06,
      "loss": 0.0011,
      "step": 2071
    },
    {
      "epoch": 86.11,
      "grad_norm": 0.009694397449493408,
      "learning_rate": 6.508453903980275e-06,
      "loss": 0.0005,
      "step": 2072
    },
    {
      "epoch": 86.15,
      "grad_norm": 0.1993793547153473,
      "learning_rate": 6.505397254619741e-06,
      "loss": 0.0021,
      "step": 2073
    },
    {
      "epoch": 86.19,
      "grad_norm": 0.025449924170970917,
      "learning_rate": 6.50233998644726e-06,
      "loss": 0.0006,
      "step": 2074
    },
    {
      "epoch": 86.23,
      "grad_norm": 0.040570322424173355,
      "learning_rate": 6.499282100719558e-06,
      "loss": 0.0007,
      "step": 2075
    },
    {
      "epoch": 86.28,
      "grad_norm": 0.1139669120311737,
      "learning_rate": 6.496223598693619e-06,
      "loss": 0.001,
      "step": 2076
    },
    {
      "epoch": 86.32,
      "grad_norm": 0.009940934367477894,
      "learning_rate": 6.493164481626675e-06,
      "loss": 0.0005,
      "step": 2077
    },
    {
      "epoch": 86.36,
      "grad_norm": 0.06118682399392128,
      "learning_rate": 6.490104750776214e-06,
      "loss": 0.0007,
      "step": 2078
    },
    {
      "epoch": 86.4,
      "grad_norm": 0.09905241429805756,
      "learning_rate": 6.487044407399976e-06,
      "loss": 0.0012,
      "step": 2079
    },
    {
      "epoch": 86.44,
      "grad_norm": 0.027125433087348938,
      "learning_rate": 6.483983452755953e-06,
      "loss": 0.0006,
      "step": 2080
    },
    {
      "epoch": 86.48,
      "grad_norm": 0.03269701451063156,
      "learning_rate": 6.480921888102389e-06,
      "loss": 0.0006,
      "step": 2081
    },
    {
      "epoch": 86.52,
      "grad_norm": 0.03552791103720665,
      "learning_rate": 6.477859714697774e-06,
      "loss": 0.0006,
      "step": 2082
    },
    {
      "epoch": 86.57,
      "grad_norm": 0.018177036195993423,
      "learning_rate": 6.474796933800855e-06,
      "loss": 0.0005,
      "step": 2083
    },
    {
      "epoch": 86.61,
      "grad_norm": 0.058801356703042984,
      "learning_rate": 6.471733546670624e-06,
      "loss": 0.0007,
      "step": 2084
    },
    {
      "epoch": 86.65,
      "grad_norm": 0.08170662820339203,
      "learning_rate": 6.468669554566324e-06,
      "loss": 0.0011,
      "step": 2085
    },
    {
      "epoch": 86.69,
      "grad_norm": 0.07777570933103561,
      "learning_rate": 6.465604958747448e-06,
      "loss": 0.0008,
      "step": 2086
    },
    {
      "epoch": 86.73,
      "grad_norm": 0.059489067643880844,
      "learning_rate": 6.462539760473733e-06,
      "loss": 0.001,
      "step": 2087
    },
    {
      "epoch": 86.77,
      "grad_norm": 0.009426652453839779,
      "learning_rate": 6.459473961005168e-06,
      "loss": 0.0005,
      "step": 2088
    },
    {
      "epoch": 86.82,
      "grad_norm": 0.08605527132749557,
      "learning_rate": 6.456407561601986e-06,
      "loss": 0.0012,
      "step": 2089
    },
    {
      "epoch": 86.86,
      "grad_norm": 0.08107941597700119,
      "learning_rate": 6.4533405635246696e-06,
      "loss": 0.0012,
      "step": 2090
    },
    {
      "epoch": 86.9,
      "grad_norm": 0.03905512019991875,
      "learning_rate": 6.450272968033944e-06,
      "loss": 0.0006,
      "step": 2091
    },
    {
      "epoch": 86.94,
      "grad_norm": 0.06707621365785599,
      "learning_rate": 6.447204776390783e-06,
      "loss": 0.0009,
      "step": 2092
    },
    {
      "epoch": 86.98,
      "grad_norm": 0.009586230851709843,
      "learning_rate": 6.444135989856405e-06,
      "loss": 0.0005,
      "step": 2093
    },
    {
      "epoch": 87.02,
      "grad_norm": 0.09709765017032623,
      "learning_rate": 6.44106660969227e-06,
      "loss": 0.0012,
      "step": 2094
    },
    {
      "epoch": 87.06,
      "grad_norm": 0.07749896496534348,
      "learning_rate": 6.437996637160086e-06,
      "loss": 0.001,
      "step": 2095
    },
    {
      "epoch": 87.11,
      "grad_norm": 0.13794048130512238,
      "learning_rate": 6.434926073521804e-06,
      "loss": 0.0015,
      "step": 2096
    },
    {
      "epoch": 87.15,
      "grad_norm": 0.037984855473041534,
      "learning_rate": 6.431854920039613e-06,
      "loss": 0.0006,
      "step": 2097
    },
    {
      "epoch": 87.19,
      "grad_norm": 0.10413287580013275,
      "learning_rate": 6.428783177975953e-06,
      "loss": 0.0013,
      "step": 2098
    },
    {
      "epoch": 87.23,
      "grad_norm": 0.13225843012332916,
      "learning_rate": 6.425710848593496e-06,
      "loss": 0.0013,
      "step": 2099
    },
    {
      "epoch": 87.27,
      "grad_norm": 0.06607980281114578,
      "learning_rate": 6.4226379331551625e-06,
      "loss": 0.0008,
      "step": 2100
    },
    {
      "epoch": 87.31,
      "grad_norm": 0.14092275500297546,
      "learning_rate": 6.419564432924114e-06,
      "loss": 0.0011,
      "step": 2101
    },
    {
      "epoch": 87.36,
      "grad_norm": 0.10879884660243988,
      "learning_rate": 6.4164903491637475e-06,
      "loss": 0.0009,
      "step": 2102
    },
    {
      "epoch": 87.4,
      "grad_norm": 0.009810675866901875,
      "learning_rate": 6.413415683137704e-06,
      "loss": 0.0005,
      "step": 2103
    },
    {
      "epoch": 87.44,
      "grad_norm": 0.08450314402580261,
      "learning_rate": 6.410340436109864e-06,
      "loss": 0.0007,
      "step": 2104
    },
    {
      "epoch": 87.48,
      "grad_norm": 0.12250608205795288,
      "learning_rate": 6.407264609344344e-06,
      "loss": 0.0015,
      "step": 2105
    },
    {
      "epoch": 87.52,
      "grad_norm": 0.06444055587053299,
      "learning_rate": 6.404188204105498e-06,
      "loss": 0.0007,
      "step": 2106
    },
    {
      "epoch": 87.56,
      "grad_norm": 0.024961870163679123,
      "learning_rate": 6.401111221657923e-06,
      "loss": 0.0006,
      "step": 2107
    },
    {
      "epoch": 87.61,
      "grad_norm": 0.07143855839967728,
      "learning_rate": 6.3980336632664505e-06,
      "loss": 0.0008,
      "step": 2108
    },
    {
      "epoch": 87.65,
      "grad_norm": 0.10508964210748672,
      "learning_rate": 6.3949555301961474e-06,
      "loss": 0.0012,
      "step": 2109
    },
    {
      "epoch": 87.69,
      "grad_norm": 0.11317043751478195,
      "learning_rate": 6.3918768237123175e-06,
      "loss": 0.0015,
      "step": 2110
    },
    {
      "epoch": 87.73,
      "grad_norm": 0.06249840930104256,
      "learning_rate": 6.388797545080501e-06,
      "loss": 0.0009,
      "step": 2111
    },
    {
      "epoch": 87.77,
      "grad_norm": 0.009343271143734455,
      "learning_rate": 6.385717695566472e-06,
      "loss": 0.0005,
      "step": 2112
    },
    {
      "epoch": 87.81,
      "grad_norm": 0.11467494815587997,
      "learning_rate": 6.382637276436242e-06,
      "loss": 0.0015,
      "step": 2113
    },
    {
      "epoch": 87.85,
      "grad_norm": 0.03344671055674553,
      "learning_rate": 6.379556288956056e-06,
      "loss": 0.0007,
      "step": 2114
    },
    {
      "epoch": 87.9,
      "grad_norm": 0.051572687923908234,
      "learning_rate": 6.376474734392388e-06,
      "loss": 0.0007,
      "step": 2115
    },
    {
      "epoch": 87.94,
      "grad_norm": 0.017287032678723335,
      "learning_rate": 6.373392614011952e-06,
      "loss": 0.0005,
      "step": 2116
    },
    {
      "epoch": 87.98,
      "grad_norm": 0.09058097004890442,
      "learning_rate": 6.370309929081689e-06,
      "loss": 0.0008,
      "step": 2117
    },
    {
      "epoch": 88.02,
      "grad_norm": 0.07311467826366425,
      "learning_rate": 6.367226680868776e-06,
      "loss": 0.0009,
      "step": 2118
    },
    {
      "epoch": 88.06,
      "grad_norm": 0.08725616335868835,
      "learning_rate": 6.364142870640619e-06,
      "loss": 0.0011,
      "step": 2119
    },
    {
      "epoch": 88.1,
      "grad_norm": 0.08444768935441971,
      "learning_rate": 6.361058499664856e-06,
      "loss": 0.0009,
      "step": 2120
    },
    {
      "epoch": 88.15,
      "grad_norm": 0.046951763331890106,
      "learning_rate": 6.357973569209356e-06,
      "loss": 0.0008,
      "step": 2121
    },
    {
      "epoch": 88.19,
      "grad_norm": 0.10309963673353195,
      "learning_rate": 6.354888080542216e-06,
      "loss": 0.0008,
      "step": 2122
    },
    {
      "epoch": 88.23,
      "grad_norm": 0.07948576658964157,
      "learning_rate": 6.3518020349317645e-06,
      "loss": 0.0009,
      "step": 2123
    },
    {
      "epoch": 88.27,
      "grad_norm": 0.05399525165557861,
      "learning_rate": 6.348715433646559e-06,
      "loss": 0.0009,
      "step": 2124
    },
    {
      "epoch": 88.31,
      "grad_norm": 0.07308776676654816,
      "learning_rate": 6.345628277955384e-06,
      "loss": 0.0009,
      "step": 2125
    },
    {
      "epoch": 88.35,
      "grad_norm": 0.027904946357011795,
      "learning_rate": 6.342540569127253e-06,
      "loss": 0.0006,
      "step": 2126
    },
    {
      "epoch": 88.39,
      "grad_norm": 0.08820002526044846,
      "learning_rate": 6.339452308431406e-06,
      "loss": 0.0009,
      "step": 2127
    },
    {
      "epoch": 88.44,
      "grad_norm": 0.01634320616722107,
      "learning_rate": 6.336363497137311e-06,
      "loss": 0.0005,
      "step": 2128
    },
    {
      "epoch": 88.48,
      "grad_norm": 0.053408943116664886,
      "learning_rate": 6.333274136514661e-06,
      "loss": 0.0008,
      "step": 2129
    },
    {
      "epoch": 88.52,
      "grad_norm": 0.04939575865864754,
      "learning_rate": 6.330184227833376e-06,
      "loss": 0.0008,
      "step": 2130
    },
    {
      "epoch": 88.56,
      "grad_norm": 0.024208279326558113,
      "learning_rate": 6.327093772363601e-06,
      "loss": 0.0005,
      "step": 2131
    },
    {
      "epoch": 88.6,
      "grad_norm": 0.10011637210845947,
      "learning_rate": 6.324002771375704e-06,
      "loss": 0.0009,
      "step": 2132
    },
    {
      "epoch": 88.64,
      "grad_norm": 0.08094894886016846,
      "learning_rate": 6.320911226140279e-06,
      "loss": 0.0014,
      "step": 2133
    },
    {
      "epoch": 88.69,
      "grad_norm": 0.009231103584170341,
      "learning_rate": 6.317819137928144e-06,
      "loss": 0.0005,
      "step": 2134
    },
    {
      "epoch": 88.73,
      "grad_norm": 0.06351642310619354,
      "learning_rate": 6.3147265080103405e-06,
      "loss": 0.0008,
      "step": 2135
    },
    {
      "epoch": 88.77,
      "grad_norm": 0.00919952429831028,
      "learning_rate": 6.311633337658132e-06,
      "loss": 0.0005,
      "step": 2136
    },
    {
      "epoch": 88.81,
      "grad_norm": 0.08255654573440552,
      "learning_rate": 6.308539628143001e-06,
      "loss": 0.0011,
      "step": 2137
    },
    {
      "epoch": 88.85,
      "grad_norm": 0.07011529058218002,
      "learning_rate": 6.3054453807366574e-06,
      "loss": 0.001,
      "step": 2138
    },
    {
      "epoch": 88.89,
      "grad_norm": 0.021155035123229027,
      "learning_rate": 6.30235059671103e-06,
      "loss": 0.0007,
      "step": 2139
    },
    {
      "epoch": 88.94,
      "grad_norm": 0.0562950074672699,
      "learning_rate": 6.299255277338265e-06,
      "loss": 0.0007,
      "step": 2140
    },
    {
      "epoch": 88.98,
      "grad_norm": 0.03321249037981033,
      "learning_rate": 6.296159423890735e-06,
      "loss": 0.0005,
      "step": 2141
    },
    {
      "epoch": 89.02,
      "grad_norm": 0.03176603466272354,
      "learning_rate": 6.293063037641023e-06,
      "loss": 0.0006,
      "step": 2142
    },
    {
      "epoch": 89.06,
      "grad_norm": 0.047116950154304504,
      "learning_rate": 6.289966119861941e-06,
      "loss": 0.0007,
      "step": 2143
    },
    {
      "epoch": 89.1,
      "grad_norm": 0.02583964355289936,
      "learning_rate": 6.286868671826513e-06,
      "loss": 0.0006,
      "step": 2144
    },
    {
      "epoch": 89.14,
      "grad_norm": 0.05131783336400986,
      "learning_rate": 6.283770694807983e-06,
      "loss": 0.0005,
      "step": 2145
    },
    {
      "epoch": 89.18,
      "grad_norm": 0.12542866170406342,
      "learning_rate": 6.280672190079814e-06,
      "loss": 0.0016,
      "step": 2146
    },
    {
      "epoch": 89.23,
      "grad_norm": 0.01677098125219345,
      "learning_rate": 6.277573158915682e-06,
      "loss": 0.0005,
      "step": 2147
    },
    {
      "epoch": 89.27,
      "grad_norm": 0.032104216516017914,
      "learning_rate": 6.274473602589481e-06,
      "loss": 0.0006,
      "step": 2148
    },
    {
      "epoch": 89.31,
      "grad_norm": 0.09040619432926178,
      "learning_rate": 6.271373522375325e-06,
      "loss": 0.001,
      "step": 2149
    },
    {
      "epoch": 89.35,
      "grad_norm": 0.058258306235075,
      "learning_rate": 6.268272919547537e-06,
      "loss": 0.0007,
      "step": 2150
    },
    {
      "epoch": 89.39,
      "grad_norm": 0.06011338531970978,
      "learning_rate": 6.265171795380659e-06,
      "loss": 0.0008,
      "step": 2151
    },
    {
      "epoch": 89.43,
      "grad_norm": 0.07641111314296722,
      "learning_rate": 6.262070151149447e-06,
      "loss": 0.0009,
      "step": 2152
    },
    {
      "epoch": 89.48,
      "grad_norm": 0.039535973221063614,
      "learning_rate": 6.258967988128867e-06,
      "loss": 0.0006,
      "step": 2153
    },
    {
      "epoch": 89.52,
      "grad_norm": 0.03542015329003334,
      "learning_rate": 6.255865307594102e-06,
      "loss": 0.0007,
      "step": 2154
    },
    {
      "epoch": 89.56,
      "grad_norm": 0.07481774687767029,
      "learning_rate": 6.252762110820548e-06,
      "loss": 0.0007,
      "step": 2155
    },
    {
      "epoch": 89.6,
      "grad_norm": 0.008155002258718014,
      "learning_rate": 6.249658399083811e-06,
      "loss": 0.0004,
      "step": 2156
    },
    {
      "epoch": 89.64,
      "grad_norm": 0.0874190628528595,
      "learning_rate": 6.24655417365971e-06,
      "loss": 0.0009,
      "step": 2157
    },
    {
      "epoch": 89.68,
      "grad_norm": 0.05417846888303757,
      "learning_rate": 6.243449435824276e-06,
      "loss": 0.0008,
      "step": 2158
    },
    {
      "epoch": 89.72,
      "grad_norm": 0.069187693297863,
      "learning_rate": 6.240344186853746e-06,
      "loss": 0.0009,
      "step": 2159
    },
    {
      "epoch": 89.77,
      "grad_norm": 0.03871430829167366,
      "learning_rate": 6.237238428024573e-06,
      "loss": 0.0006,
      "step": 2160
    },
    {
      "epoch": 89.81,
      "grad_norm": 0.11207784712314606,
      "learning_rate": 6.234132160613418e-06,
      "loss": 0.0011,
      "step": 2161
    },
    {
      "epoch": 89.85,
      "grad_norm": 0.08838048577308655,
      "learning_rate": 6.231025385897147e-06,
      "loss": 0.001,
      "step": 2162
    },
    {
      "epoch": 89.89,
      "grad_norm": 0.05789296329021454,
      "learning_rate": 6.227918105152841e-06,
      "loss": 0.0007,
      "step": 2163
    },
    {
      "epoch": 89.93,
      "grad_norm": 0.09394734352827072,
      "learning_rate": 6.2248103196577846e-06,
      "loss": 0.0008,
      "step": 2164
    },
    {
      "epoch": 89.97,
      "grad_norm": 0.14782318472862244,
      "learning_rate": 6.2217020306894705e-06,
      "loss": 0.0012,
      "step": 2165
    },
    {
      "epoch": 90.02,
      "grad_norm": 0.06137658283114433,
      "learning_rate": 6.2185932395256e-06,
      "loss": 0.0008,
      "step": 2166
    },
    {
      "epoch": 90.06,
      "grad_norm": 0.05289250239729881,
      "learning_rate": 6.2154839474440786e-06,
      "loss": 0.0008,
      "step": 2167
    },
    {
      "epoch": 90.1,
      "grad_norm": 0.07135286182165146,
      "learning_rate": 6.21237415572302e-06,
      "loss": 0.0009,
      "step": 2168
    },
    {
      "epoch": 90.14,
      "grad_norm": 0.016741150990128517,
      "learning_rate": 6.20926386564074e-06,
      "loss": 0.0005,
      "step": 2169
    },
    {
      "epoch": 90.18,
      "grad_norm": 0.06395640224218369,
      "learning_rate": 6.2061530784757625e-06,
      "loss": 0.0008,
      "step": 2170
    },
    {
      "epoch": 90.22,
      "grad_norm": 0.008033704943954945,
      "learning_rate": 6.203041795506815e-06,
      "loss": 0.0005,
      "step": 2171
    },
    {
      "epoch": 90.26,
      "grad_norm": 0.036819349974393845,
      "learning_rate": 6.19993001801283e-06,
      "loss": 0.0006,
      "step": 2172
    },
    {
      "epoch": 90.31,
      "grad_norm": 0.008793787099421024,
      "learning_rate": 6.196817747272939e-06,
      "loss": 0.0005,
      "step": 2173
    },
    {
      "epoch": 90.35,
      "grad_norm": 0.008857973851263523,
      "learning_rate": 6.1937049845664795e-06,
      "loss": 0.0005,
      "step": 2174
    },
    {
      "epoch": 90.39,
      "grad_norm": 0.09944244474172592,
      "learning_rate": 6.1905917311729915e-06,
      "loss": 0.0014,
      "step": 2175
    },
    {
      "epoch": 90.43,
      "grad_norm": 0.07588144391775131,
      "learning_rate": 6.187477988372216e-06,
      "loss": 0.001,
      "step": 2176
    },
    {
      "epoch": 90.47,
      "grad_norm": 0.05303724482655525,
      "learning_rate": 6.184363757444093e-06,
      "loss": 0.0007,
      "step": 2177
    },
    {
      "epoch": 90.51,
      "grad_norm": 0.16972880065441132,
      "learning_rate": 6.181249039668766e-06,
      "loss": 0.0016,
      "step": 2178
    },
    {
      "epoch": 90.56,
      "grad_norm": 0.008668974973261356,
      "learning_rate": 6.178133836326581e-06,
      "loss": 0.0004,
      "step": 2179
    },
    {
      "epoch": 90.6,
      "grad_norm": 0.0468183234333992,
      "learning_rate": 6.175018148698077e-06,
      "loss": 0.0008,
      "step": 2180
    },
    {
      "epoch": 90.64,
      "grad_norm": 0.03652231767773628,
      "learning_rate": 6.171901978063995e-06,
      "loss": 0.0006,
      "step": 2181
    },
    {
      "epoch": 90.68,
      "grad_norm": 0.08322672545909882,
      "learning_rate": 6.168785325705278e-06,
      "loss": 0.0009,
      "step": 2182
    },
    {
      "epoch": 90.72,
      "grad_norm": 0.034620679914951324,
      "learning_rate": 6.1656681929030616e-06,
      "loss": 0.0005,
      "step": 2183
    },
    {
      "epoch": 90.76,
      "grad_norm": 0.07763563096523285,
      "learning_rate": 6.162550580938682e-06,
      "loss": 0.0009,
      "step": 2184
    },
    {
      "epoch": 90.81,
      "grad_norm": 0.06776691973209381,
      "learning_rate": 6.1594324910936734e-06,
      "loss": 0.0007,
      "step": 2185
    },
    {
      "epoch": 90.85,
      "grad_norm": 0.07183419167995453,
      "learning_rate": 6.1563139246497615e-06,
      "loss": 0.0007,
      "step": 2186
    },
    {
      "epoch": 90.89,
      "grad_norm": 0.1304885894060135,
      "learning_rate": 6.1531948828888735e-06,
      "loss": 0.0018,
      "step": 2187
    },
    {
      "epoch": 90.93,
      "grad_norm": 0.008529674261808395,
      "learning_rate": 6.150075367093129e-06,
      "loss": 0.0004,
      "step": 2188
    },
    {
      "epoch": 90.97,
      "grad_norm": 0.11017373204231262,
      "learning_rate": 6.146955378544844e-06,
      "loss": 0.001,
      "step": 2189
    },
    {
      "epoch": 91.01,
      "grad_norm": 0.04106241464614868,
      "learning_rate": 6.143834918526528e-06,
      "loss": 0.0008,
      "step": 2190
    },
    {
      "epoch": 91.05,
      "grad_norm": 0.057113442569971085,
      "learning_rate": 6.140713988320881e-06,
      "loss": 0.0007,
      "step": 2191
    },
    {
      "epoch": 91.1,
      "grad_norm": 0.017210830003023148,
      "learning_rate": 6.137592589210803e-06,
      "loss": 0.0005,
      "step": 2192
    },
    {
      "epoch": 91.14,
      "grad_norm": 0.022003445774316788,
      "learning_rate": 6.134470722479383e-06,
      "loss": 0.0005,
      "step": 2193
    },
    {
      "epoch": 91.18,
      "grad_norm": 0.07384880632162094,
      "learning_rate": 6.1313483894099006e-06,
      "loss": 0.0009,
      "step": 2194
    },
    {
      "epoch": 91.22,
      "grad_norm": 0.08034386485815048,
      "learning_rate": 6.1282255912858315e-06,
      "loss": 0.0008,
      "step": 2195
    },
    {
      "epoch": 91.26,
      "grad_norm": 0.00811647530645132,
      "learning_rate": 6.125102329390837e-06,
      "loss": 0.0004,
      "step": 2196
    },
    {
      "epoch": 91.3,
      "grad_norm": 0.011655059643089771,
      "learning_rate": 6.121978605008775e-06,
      "loss": 0.0004,
      "step": 2197
    },
    {
      "epoch": 91.35,
      "grad_norm": 0.07218030095100403,
      "learning_rate": 6.1188544194236875e-06,
      "loss": 0.0008,
      "step": 2198
    },
    {
      "epoch": 91.39,
      "grad_norm": 0.05979995056986809,
      "learning_rate": 6.115729773919814e-06,
      "loss": 0.0008,
      "step": 2199
    },
    {
      "epoch": 91.43,
      "grad_norm": 0.028931643813848495,
      "learning_rate": 6.112604669781572e-06,
      "loss": 0.0005,
      "step": 2200
    },
    {
      "epoch": 91.47,
      "grad_norm": 0.007961724884808064,
      "learning_rate": 6.10947910829358e-06,
      "loss": 0.0004,
      "step": 2201
    },
    {
      "epoch": 91.51,
      "grad_norm": 0.007494618650525808,
      "learning_rate": 6.106353090740633e-06,
      "loss": 0.0004,
      "step": 2202
    },
    {
      "epoch": 91.55,
      "grad_norm": 0.05798394978046417,
      "learning_rate": 6.103226618407723e-06,
      "loss": 0.0006,
      "step": 2203
    },
    {
      "epoch": 91.59,
      "grad_norm": 0.08008216321468353,
      "learning_rate": 6.100099692580021e-06,
      "loss": 0.0011,
      "step": 2204
    },
    {
      "epoch": 91.64,
      "grad_norm": 0.008326553739607334,
      "learning_rate": 6.096972314542889e-06,
      "loss": 0.0005,
      "step": 2205
    },
    {
      "epoch": 91.68,
      "grad_norm": 0.11098307371139526,
      "learning_rate": 6.093844485581876e-06,
      "loss": 0.0013,
      "step": 2206
    },
    {
      "epoch": 91.72,
      "grad_norm": 0.08772725611925125,
      "learning_rate": 6.090716206982714e-06,
      "loss": 0.0009,
      "step": 2207
    },
    {
      "epoch": 91.76,
      "grad_norm": 0.021981149911880493,
      "learning_rate": 6.0875874800313185e-06,
      "loss": 0.0005,
      "step": 2208
    },
    {
      "epoch": 91.8,
      "grad_norm": 0.08046771585941315,
      "learning_rate": 6.084458306013791e-06,
      "loss": 0.0012,
      "step": 2209
    },
    {
      "epoch": 91.84,
      "grad_norm": 0.16760626435279846,
      "learning_rate": 6.0813286862164175e-06,
      "loss": 0.0015,
      "step": 2210
    },
    {
      "epoch": 91.89,
      "grad_norm": 0.09590515494346619,
      "learning_rate": 6.078198621925667e-06,
      "loss": 0.0009,
      "step": 2211
    },
    {
      "epoch": 91.93,
      "grad_norm": 0.05360410362482071,
      "learning_rate": 6.075068114428191e-06,
      "loss": 0.0008,
      "step": 2212
    },
    {
      "epoch": 91.97,
      "grad_norm": 0.041985347867012024,
      "learning_rate": 6.07193716501082e-06,
      "loss": 0.0009,
      "step": 2213
    },
    {
      "epoch": 92.01,
      "grad_norm": 0.05628162622451782,
      "learning_rate": 6.068805774960574e-06,
      "loss": 0.0006,
      "step": 2214
    },
    {
      "epoch": 92.05,
      "grad_norm": 0.038240205496549606,
      "learning_rate": 6.065673945564643e-06,
      "loss": 0.0005,
      "step": 2215
    },
    {
      "epoch": 92.09,
      "grad_norm": 0.11380039155483246,
      "learning_rate": 6.062541678110409e-06,
      "loss": 0.001,
      "step": 2216
    },
    {
      "epoch": 92.14,
      "grad_norm": 0.048112235963344574,
      "learning_rate": 6.059408973885427e-06,
      "loss": 0.0007,
      "step": 2217
    },
    {
      "epoch": 92.18,
      "grad_norm": 0.026210980489850044,
      "learning_rate": 6.0562758341774315e-06,
      "loss": 0.0006,
      "step": 2218
    },
    {
      "epoch": 92.22,
      "grad_norm": 0.0841871127486229,
      "learning_rate": 6.0531422602743405e-06,
      "loss": 0.0007,
      "step": 2219
    },
    {
      "epoch": 92.26,
      "grad_norm": 0.061204057186841965,
      "learning_rate": 6.050008253464247e-06,
      "loss": 0.0007,
      "step": 2220
    },
    {
      "epoch": 92.3,
      "grad_norm": 0.048418283462524414,
      "learning_rate": 6.046873815035422e-06,
      "loss": 0.0006,
      "step": 2221
    },
    {
      "epoch": 92.34,
      "grad_norm": 0.5113500356674194,
      "learning_rate": 6.043738946276317e-06,
      "loss": 0.0029,
      "step": 2222
    },
    {
      "epoch": 92.38,
      "grad_norm": 0.06555795669555664,
      "learning_rate": 6.040603648475558e-06,
      "loss": 0.0009,
      "step": 2223
    },
    {
      "epoch": 92.43,
      "grad_norm": 0.0850609615445137,
      "learning_rate": 6.037467922921943e-06,
      "loss": 0.001,
      "step": 2224
    },
    {
      "epoch": 92.47,
      "grad_norm": 0.00794969405978918,
      "learning_rate": 6.034331770904455e-06,
      "loss": 0.0004,
      "step": 2225
    },
    {
      "epoch": 92.51,
      "grad_norm": 0.026993224397301674,
      "learning_rate": 6.031195193712246e-06,
      "loss": 0.0005,
      "step": 2226
    },
    {
      "epoch": 92.55,
      "grad_norm": 0.03864669054746628,
      "learning_rate": 6.0280581926346436e-06,
      "loss": 0.0006,
      "step": 2227
    },
    {
      "epoch": 92.59,
      "grad_norm": 0.048836492002010345,
      "learning_rate": 6.024920768961153e-06,
      "loss": 0.0007,
      "step": 2228
    },
    {
      "epoch": 92.63,
      "grad_norm": 0.0781761184334755,
      "learning_rate": 6.021782923981447e-06,
      "loss": 0.0007,
      "step": 2229
    },
    {
      "epoch": 92.68,
      "grad_norm": 0.06486348062753677,
      "learning_rate": 6.018644658985378e-06,
      "loss": 0.0007,
      "step": 2230
    },
    {
      "epoch": 92.72,
      "grad_norm": 0.14900419116020203,
      "learning_rate": 6.015505975262968e-06,
      "loss": 0.0022,
      "step": 2231
    },
    {
      "epoch": 92.76,
      "grad_norm": 0.007425999268889427,
      "learning_rate": 6.01236687410441e-06,
      "loss": 0.0004,
      "step": 2232
    },
    {
      "epoch": 92.8,
      "grad_norm": 0.09256958216428757,
      "learning_rate": 6.009227356800071e-06,
      "loss": 0.0011,
      "step": 2233
    },
    {
      "epoch": 92.84,
      "grad_norm": 0.06701640039682388,
      "learning_rate": 6.006087424640486e-06,
      "loss": 0.001,
      "step": 2234
    },
    {
      "epoch": 92.88,
      "grad_norm": 0.007720279041677713,
      "learning_rate": 6.002947078916365e-06,
      "loss": 0.0004,
      "step": 2235
    },
    {
      "epoch": 92.92,
      "grad_norm": 0.008642246946692467,
      "learning_rate": 5.999806320918584e-06,
      "loss": 0.0004,
      "step": 2236
    },
    {
      "epoch": 92.97,
      "grad_norm": 0.1290021538734436,
      "learning_rate": 5.99666515193819e-06,
      "loss": 0.0014,
      "step": 2237
    },
    {
      "epoch": 93.01,
      "grad_norm": 0.087371326982975,
      "learning_rate": 5.9935235732664e-06,
      "loss": 0.0008,
      "step": 2238
    },
    {
      "epoch": 93.05,
      "grad_norm": 0.059654273092746735,
      "learning_rate": 5.990381586194599e-06,
      "loss": 0.0008,
      "step": 2239
    },
    {
      "epoch": 93.09,
      "grad_norm": 0.0913897305727005,
      "learning_rate": 5.987239192014336e-06,
      "loss": 0.001,
      "step": 2240
    },
    {
      "epoch": 93.13,
      "grad_norm": 0.10269646346569061,
      "learning_rate": 5.984096392017333e-06,
      "loss": 0.0013,
      "step": 2241
    },
    {
      "epoch": 93.17,
      "grad_norm": 0.046406183391809464,
      "learning_rate": 5.980953187495476e-06,
      "loss": 0.0006,
      "step": 2242
    },
    {
      "epoch": 93.22,
      "grad_norm": 0.021894745528697968,
      "learning_rate": 5.9778095797408174e-06,
      "loss": 0.0005,
      "step": 2243
    },
    {
      "epoch": 93.26,
      "grad_norm": 0.03700558468699455,
      "learning_rate": 5.974665570045577e-06,
      "loss": 0.0006,
      "step": 2244
    },
    {
      "epoch": 93.3,
      "grad_norm": 0.007475759368389845,
      "learning_rate": 5.971521159702136e-06,
      "loss": 0.0004,
      "step": 2245
    },
    {
      "epoch": 93.34,
      "grad_norm": 0.07495368272066116,
      "learning_rate": 5.968376350003044e-06,
      "loss": 0.0009,
      "step": 2246
    },
    {
      "epoch": 93.38,
      "grad_norm": 0.043665558099746704,
      "learning_rate": 5.965231142241013e-06,
      "loss": 0.0006,
      "step": 2247
    },
    {
      "epoch": 93.42,
      "grad_norm": 0.08046290278434753,
      "learning_rate": 5.962085537708918e-06,
      "loss": 0.0009,
      "step": 2248
    },
    {
      "epoch": 93.46,
      "grad_norm": 0.008142462931573391,
      "learning_rate": 5.9589395376998e-06,
      "loss": 0.0005,
      "step": 2249
    },
    {
      "epoch": 93.51,
      "grad_norm": 0.06110021471977234,
      "learning_rate": 5.955793143506863e-06,
      "loss": 0.0007,
      "step": 2250
    },
    {
      "epoch": 93.55,
      "grad_norm": 0.0352838933467865,
      "learning_rate": 5.952646356423466e-06,
      "loss": 0.0006,
      "step": 2251
    },
    {
      "epoch": 93.59,
      "grad_norm": 0.05027316138148308,
      "learning_rate": 5.949499177743137e-06,
      "loss": 0.0006,
      "step": 2252
    },
    {
      "epoch": 93.63,
      "grad_norm": 0.04366163909435272,
      "learning_rate": 5.946351608759561e-06,
      "loss": 0.0006,
      "step": 2253
    },
    {
      "epoch": 93.67,
      "grad_norm": 0.031658414751291275,
      "learning_rate": 5.943203650766586e-06,
      "loss": 0.0005,
      "step": 2254
    },
    {
      "epoch": 93.71,
      "grad_norm": 0.08286689966917038,
      "learning_rate": 5.940055305058219e-06,
      "loss": 0.0009,
      "step": 2255
    },
    {
      "epoch": 93.76,
      "grad_norm": 0.10646961629390717,
      "learning_rate": 5.936906572928625e-06,
      "loss": 0.0017,
      "step": 2256
    },
    {
      "epoch": 93.8,
      "grad_norm": 0.0310515146702528,
      "learning_rate": 5.933757455672127e-06,
      "loss": 0.0005,
      "step": 2257
    },
    {
      "epoch": 93.84,
      "grad_norm": 0.13075953722000122,
      "learning_rate": 5.93060795458321e-06,
      "loss": 0.0017,
      "step": 2258
    },
    {
      "epoch": 93.88,
      "grad_norm": 0.14145489037036896,
      "learning_rate": 5.927458070956518e-06,
      "loss": 0.0012,
      "step": 2259
    },
    {
      "epoch": 93.92,
      "grad_norm": 0.034914981573820114,
      "learning_rate": 5.9243078060868445e-06,
      "loss": 0.0005,
      "step": 2260
    },
    {
      "epoch": 93.96,
      "grad_norm": 0.03741680830717087,
      "learning_rate": 5.921157161269146e-06,
      "loss": 0.0006,
      "step": 2261
    },
    {
      "epoch": 94.01,
      "grad_norm": 0.09192322939634323,
      "learning_rate": 5.918006137798533e-06,
      "loss": 0.0012,
      "step": 2262
    },
    {
      "epoch": 94.05,
      "grad_norm": 0.09139866381883621,
      "learning_rate": 5.914854736970274e-06,
      "loss": 0.0011,
      "step": 2263
    },
    {
      "epoch": 94.09,
      "grad_norm": 0.04851026087999344,
      "learning_rate": 5.911702960079788e-06,
      "loss": 0.0005,
      "step": 2264
    },
    {
      "epoch": 94.13,
      "grad_norm": 0.006792027503252029,
      "learning_rate": 5.908550808422656e-06,
      "loss": 0.0004,
      "step": 2265
    },
    {
      "epoch": 94.17,
      "grad_norm": 0.05707588419318199,
      "learning_rate": 5.905398283294603e-06,
      "loss": 0.0007,
      "step": 2266
    },
    {
      "epoch": 94.21,
      "grad_norm": 0.07182719558477402,
      "learning_rate": 5.902245385991519e-06,
      "loss": 0.0008,
      "step": 2267
    },
    {
      "epoch": 94.25,
      "grad_norm": 0.1875971257686615,
      "learning_rate": 5.899092117809434e-06,
      "loss": 0.0023,
      "step": 2268
    },
    {
      "epoch": 94.3,
      "grad_norm": 0.03244272619485855,
      "learning_rate": 5.895938480044543e-06,
      "loss": 0.0008,
      "step": 2269
    },
    {
      "epoch": 94.34,
      "grad_norm": 0.016325082629919052,
      "learning_rate": 5.892784473993184e-06,
      "loss": 0.0004,
      "step": 2270
    },
    {
      "epoch": 94.38,
      "grad_norm": 0.0535837784409523,
      "learning_rate": 5.889630100951852e-06,
      "loss": 0.0007,
      "step": 2271
    },
    {
      "epoch": 94.42,
      "grad_norm": 0.05758634954690933,
      "learning_rate": 5.88647536221719e-06,
      "loss": 0.0006,
      "step": 2272
    },
    {
      "epoch": 94.46,
      "grad_norm": 0.07177092134952545,
      "learning_rate": 5.883320259085992e-06,
      "loss": 0.001,
      "step": 2273
    },
    {
      "epoch": 94.5,
      "grad_norm": 0.06107106804847717,
      "learning_rate": 5.880164792855199e-06,
      "loss": 0.0007,
      "step": 2274
    },
    {
      "epoch": 94.55,
      "grad_norm": 0.03707273304462433,
      "learning_rate": 5.877008964821909e-06,
      "loss": 0.0007,
      "step": 2275
    },
    {
      "epoch": 94.59,
      "grad_norm": 0.06700298190116882,
      "learning_rate": 5.87385277628336e-06,
      "loss": 0.0009,
      "step": 2276
    },
    {
      "epoch": 94.63,
      "grad_norm": 0.06425147503614426,
      "learning_rate": 5.870696228536944e-06,
      "loss": 0.0007,
      "step": 2277
    },
    {
      "epoch": 94.67,
      "grad_norm": 0.11420585215091705,
      "learning_rate": 5.867539322880195e-06,
      "loss": 0.001,
      "step": 2278
    },
    {
      "epoch": 94.71,
      "grad_norm": 0.06741192936897278,
      "learning_rate": 5.864382060610803e-06,
      "loss": 0.0008,
      "step": 2279
    },
    {
      "epoch": 94.75,
      "grad_norm": 0.030148301273584366,
      "learning_rate": 5.861224443026595e-06,
      "loss": 0.0005,
      "step": 2280
    },
    {
      "epoch": 94.79,
      "grad_norm": 0.007464309688657522,
      "learning_rate": 5.85806647142555e-06,
      "loss": 0.0004,
      "step": 2281
    },
    {
      "epoch": 94.84,
      "grad_norm": 0.047191884368658066,
      "learning_rate": 5.8549081471057915e-06,
      "loss": 0.0007,
      "step": 2282
    },
    {
      "epoch": 94.88,
      "grad_norm": 0.0421106293797493,
      "learning_rate": 5.851749471365585e-06,
      "loss": 0.0005,
      "step": 2283
    },
    {
      "epoch": 94.92,
      "grad_norm": 0.07521416991949081,
      "learning_rate": 5.848590445503345e-06,
      "loss": 0.0008,
      "step": 2284
    },
    {
      "epoch": 94.96,
      "grad_norm": 0.2029423862695694,
      "learning_rate": 5.845431070817627e-06,
      "loss": 0.0023,
      "step": 2285
    },
    {
      "epoch": 95.0,
      "grad_norm": 0.04914979636669159,
      "learning_rate": 5.84227134860713e-06,
      "loss": 0.0006,
      "step": 2286
    },
    {
      "epoch": 95.04,
      "grad_norm": 0.09218887239694595,
      "learning_rate": 5.839111280170698e-06,
      "loss": 0.0012,
      "step": 2287
    },
    {
      "epoch": 95.09,
      "grad_norm": 0.007073803339153528,
      "learning_rate": 5.835950866807314e-06,
      "loss": 0.0004,
      "step": 2288
    },
    {
      "epoch": 95.13,
      "grad_norm": 0.024940071627497673,
      "learning_rate": 5.832790109816104e-06,
      "loss": 0.0005,
      "step": 2289
    },
    {
      "epoch": 95.17,
      "grad_norm": 0.08752118796110153,
      "learning_rate": 5.82962901049634e-06,
      "loss": 0.0007,
      "step": 2290
    },
    {
      "epoch": 95.21,
      "grad_norm": 0.10931064933538437,
      "learning_rate": 5.826467570147426e-06,
      "loss": 0.0011,
      "step": 2291
    },
    {
      "epoch": 95.25,
      "grad_norm": 0.06744596362113953,
      "learning_rate": 5.823305790068912e-06,
      "loss": 0.0008,
      "step": 2292
    },
    {
      "epoch": 95.29,
      "grad_norm": 0.0071524446830153465,
      "learning_rate": 5.820143671560488e-06,
      "loss": 0.0004,
      "step": 2293
    },
    {
      "epoch": 95.34,
      "grad_norm": 0.013618703931570053,
      "learning_rate": 5.816981215921978e-06,
      "loss": 0.0004,
      "step": 2294
    },
    {
      "epoch": 95.38,
      "grad_norm": 0.03943419083952904,
      "learning_rate": 5.8138184244533516e-06,
      "loss": 0.0006,
      "step": 2295
    },
    {
      "epoch": 95.42,
      "grad_norm": 0.07081418484449387,
      "learning_rate": 5.810655298454711e-06,
      "loss": 0.0009,
      "step": 2296
    },
    {
      "epoch": 95.46,
      "grad_norm": 0.01881931722164154,
      "learning_rate": 5.807491839226298e-06,
      "loss": 0.0004,
      "step": 2297
    },
    {
      "epoch": 95.5,
      "grad_norm": 0.044247571378946304,
      "learning_rate": 5.8043280480684925e-06,
      "loss": 0.0006,
      "step": 2298
    },
    {
      "epoch": 95.54,
      "grad_norm": 0.11834951490163803,
      "learning_rate": 5.801163926281809e-06,
      "loss": 0.0012,
      "step": 2299
    },
    {
      "epoch": 95.58,
      "grad_norm": 0.037310078740119934,
      "learning_rate": 5.797999475166897e-06,
      "loss": 0.0006,
      "step": 2300
    },
    {
      "epoch": 95.63,
      "grad_norm": 0.08291808515787125,
      "learning_rate": 5.794834696024544e-06,
      "loss": 0.0008,
      "step": 2301
    },
    {
      "epoch": 95.67,
      "grad_norm": 0.09549978375434875,
      "learning_rate": 5.791669590155671e-06,
      "loss": 0.0011,
      "step": 2302
    },
    {
      "epoch": 95.71,
      "grad_norm": 0.05868867412209511,
      "learning_rate": 5.788504158861333e-06,
      "loss": 0.0008,
      "step": 2303
    },
    {
      "epoch": 95.75,
      "grad_norm": 0.07878999412059784,
      "learning_rate": 5.78533840344272e-06,
      "loss": 0.0009,
      "step": 2304
    },
    {
      "epoch": 95.79,
      "grad_norm": 0.028326982632279396,
      "learning_rate": 5.782172325201155e-06,
      "loss": 0.0005,
      "step": 2305
    },
    {
      "epoch": 95.83,
      "grad_norm": 0.13132831454277039,
      "learning_rate": 5.779005925438092e-06,
      "loss": 0.0009,
      "step": 2306
    },
    {
      "epoch": 95.88,
      "grad_norm": 0.13136380910873413,
      "learning_rate": 5.775839205455118e-06,
      "loss": 0.001,
      "step": 2307
    },
    {
      "epoch": 95.92,
      "grad_norm": 0.07366736978292465,
      "learning_rate": 5.772672166553952e-06,
      "loss": 0.0008,
      "step": 2308
    },
    {
      "epoch": 95.96,
      "grad_norm": 0.05298500508069992,
      "learning_rate": 5.769504810036447e-06,
      "loss": 0.0007,
      "step": 2309
    },
    {
      "epoch": 96.0,
      "grad_norm": 0.06032159551978111,
      "learning_rate": 5.766337137204579e-06,
      "loss": 0.0007,
      "step": 2310
    },
    {
      "epoch": 96.04,
      "grad_norm": 0.0338747464120388,
      "learning_rate": 5.7631691493604624e-06,
      "loss": 0.0005,
      "step": 2311
    },
    {
      "epoch": 96.08,
      "grad_norm": 0.007429641671478748,
      "learning_rate": 5.760000847806337e-06,
      "loss": 0.0004,
      "step": 2312
    },
    {
      "epoch": 96.12,
      "grad_norm": 0.03596894070506096,
      "learning_rate": 5.756832233844569e-06,
      "loss": 0.0006,
      "step": 2313
    },
    {
      "epoch": 96.17,
      "grad_norm": 0.01089627854526043,
      "learning_rate": 5.7536633087776575e-06,
      "loss": 0.0004,
      "step": 2314
    },
    {
      "epoch": 96.21,
      "grad_norm": 0.09853275865316391,
      "learning_rate": 5.7504940739082305e-06,
      "loss": 0.0011,
      "step": 2315
    },
    {
      "epoch": 96.25,
      "grad_norm": 0.09115467965602875,
      "learning_rate": 5.7473245305390355e-06,
      "loss": 0.001,
      "step": 2316
    },
    {
      "epoch": 96.29,
      "grad_norm": 0.11912868171930313,
      "learning_rate": 5.744154679972957e-06,
      "loss": 0.0012,
      "step": 2317
    },
    {
      "epoch": 96.33,
      "grad_norm": 0.007378555368632078,
      "learning_rate": 5.740984523512997e-06,
      "loss": 0.0004,
      "step": 2318
    },
    {
      "epoch": 96.37,
      "grad_norm": 0.027622763067483902,
      "learning_rate": 5.737814062462289e-06,
      "loss": 0.0005,
      "step": 2319
    },
    {
      "epoch": 96.42,
      "grad_norm": 0.007469466887414455,
      "learning_rate": 5.734643298124091e-06,
      "loss": 0.0004,
      "step": 2320
    },
    {
      "epoch": 96.46,
      "grad_norm": 0.0870329886674881,
      "learning_rate": 5.731472231801783e-06,
      "loss": 0.001,
      "step": 2321
    },
    {
      "epoch": 96.5,
      "grad_norm": 0.06071116030216217,
      "learning_rate": 5.728300864798868e-06,
      "loss": 0.0007,
      "step": 2322
    },
    {
      "epoch": 96.54,
      "grad_norm": 0.07096264511346817,
      "learning_rate": 5.72512919841898e-06,
      "loss": 0.0012,
      "step": 2323
    },
    {
      "epoch": 96.58,
      "grad_norm": 0.06046539172530174,
      "learning_rate": 5.721957233965868e-06,
      "loss": 0.0006,
      "step": 2324
    },
    {
      "epoch": 96.62,
      "grad_norm": 0.03231538087129593,
      "learning_rate": 5.71878497274341e-06,
      "loss": 0.0006,
      "step": 2325
    },
    {
      "epoch": 96.66,
      "grad_norm": 0.01689089648425579,
      "learning_rate": 5.7156124160555985e-06,
      "loss": 0.0004,
      "step": 2326
    },
    {
      "epoch": 96.71,
      "grad_norm": 0.1248597651720047,
      "learning_rate": 5.712439565206555e-06,
      "loss": 0.0014,
      "step": 2327
    },
    {
      "epoch": 96.75,
      "grad_norm": 0.007023530080914497,
      "learning_rate": 5.709266421500516e-06,
      "loss": 0.0004,
      "step": 2328
    },
    {
      "epoch": 96.79,
      "grad_norm": 0.013218659907579422,
      "learning_rate": 5.706092986241842e-06,
      "loss": 0.0004,
      "step": 2329
    },
    {
      "epoch": 96.83,
      "grad_norm": 0.037780385464429855,
      "learning_rate": 5.702919260735015e-06,
      "loss": 0.0005,
      "step": 2330
    },
    {
      "epoch": 96.87,
      "grad_norm": 0.0711185410618782,
      "learning_rate": 5.699745246284632e-06,
      "loss": 0.0008,
      "step": 2331
    },
    {
      "epoch": 96.91,
      "grad_norm": 0.24167944490909576,
      "learning_rate": 5.696570944195407e-06,
      "loss": 0.0014,
      "step": 2332
    },
    {
      "epoch": 96.96,
      "grad_norm": 0.10346154123544693,
      "learning_rate": 5.693396355772181e-06,
      "loss": 0.0017,
      "step": 2333
    },
    {
      "epoch": 97.0,
      "grad_norm": 0.13059109449386597,
      "learning_rate": 5.690221482319903e-06,
      "loss": 0.0013,
      "step": 2334
    },
    {
      "epoch": 97.04,
      "grad_norm": 0.0195772647857666,
      "learning_rate": 5.6870463251436485e-06,
      "loss": 0.0004,
      "step": 2335
    },
    {
      "epoch": 97.08,
      "grad_norm": 0.07150936126708984,
      "learning_rate": 5.683870885548599e-06,
      "loss": 0.0009,
      "step": 2336
    },
    {
      "epoch": 97.12,
      "grad_norm": 0.06634438782930374,
      "learning_rate": 5.680695164840062e-06,
      "loss": 0.0006,
      "step": 2337
    },
    {
      "epoch": 97.16,
      "grad_norm": 0.016364870592951775,
      "learning_rate": 5.677519164323455e-06,
      "loss": 0.0006,
      "step": 2338
    },
    {
      "epoch": 97.21,
      "grad_norm": 0.007148024160414934,
      "learning_rate": 5.674342885304311e-06,
      "loss": 0.0004,
      "step": 2339
    },
    {
      "epoch": 97.25,
      "grad_norm": 0.1355026513338089,
      "learning_rate": 5.671166329088278e-06,
      "loss": 0.0013,
      "step": 2340
    },
    {
      "epoch": 97.29,
      "grad_norm": 0.08922360092401505,
      "learning_rate": 5.66798949698112e-06,
      "loss": 0.0012,
      "step": 2341
    },
    {
      "epoch": 97.33,
      "grad_norm": 0.006669282913208008,
      "learning_rate": 5.6648123902887135e-06,
      "loss": 0.0004,
      "step": 2342
    },
    {
      "epoch": 97.37,
      "grad_norm": 0.14023007452487946,
      "learning_rate": 5.6616350103170435e-06,
      "loss": 0.001,
      "step": 2343
    },
    {
      "epoch": 97.41,
      "grad_norm": 0.029569486156105995,
      "learning_rate": 5.658457358372213e-06,
      "loss": 0.0005,
      "step": 2344
    },
    {
      "epoch": 97.45,
      "grad_norm": 0.01799684762954712,
      "learning_rate": 5.655279435760436e-06,
      "loss": 0.0004,
      "step": 2345
    },
    {
      "epoch": 97.5,
      "grad_norm": 0.07971574366092682,
      "learning_rate": 5.652101243788034e-06,
      "loss": 0.0009,
      "step": 2346
    },
    {
      "epoch": 97.54,
      "grad_norm": 0.012587016448378563,
      "learning_rate": 5.648922783761443e-06,
      "loss": 0.0004,
      "step": 2347
    },
    {
      "epoch": 97.58,
      "grad_norm": 0.05134397745132446,
      "learning_rate": 5.645744056987208e-06,
      "loss": 0.0006,
      "step": 2348
    },
    {
      "epoch": 97.62,
      "grad_norm": 0.04851450026035309,
      "learning_rate": 5.642565064771982e-06,
      "loss": 0.0007,
      "step": 2349
    },
    {
      "epoch": 97.66,
      "grad_norm": 0.007299479562789202,
      "learning_rate": 5.6393858084225305e-06,
      "loss": 0.0004,
      "step": 2350
    },
    {
      "epoch": 97.7,
      "grad_norm": 0.04873373731970787,
      "learning_rate": 5.636206289245725e-06,
      "loss": 0.0007,
      "step": 2351
    },
    {
      "epoch": 97.75,
      "grad_norm": 0.007586788851767778,
      "learning_rate": 5.6330265085485454e-06,
      "loss": 0.0004,
      "step": 2352
    },
    {
      "epoch": 97.79,
      "grad_norm": 0.05472880229353905,
      "learning_rate": 5.629846467638079e-06,
      "loss": 0.0007,
      "step": 2353
    },
    {
      "epoch": 97.83,
      "grad_norm": 0.026331475004553795,
      "learning_rate": 5.626666167821522e-06,
      "loss": 0.0005,
      "step": 2354
    },
    {
      "epoch": 97.87,
      "grad_norm": 0.12036970257759094,
      "learning_rate": 5.623485610406174e-06,
      "loss": 0.0018,
      "step": 2355
    },
    {
      "epoch": 97.91,
      "grad_norm": 0.051307640969753265,
      "learning_rate": 5.620304796699443e-06,
      "loss": 0.0006,
      "step": 2356
    },
    {
      "epoch": 97.95,
      "grad_norm": 0.09776288270950317,
      "learning_rate": 5.61712372800884e-06,
      "loss": 0.0015,
      "step": 2357
    },
    {
      "epoch": 97.99,
      "grad_norm": 0.06468897312879562,
      "learning_rate": 5.613942405641986e-06,
      "loss": 0.0007,
      "step": 2358
    },
    {
      "epoch": 98.04,
      "grad_norm": 0.1309254765510559,
      "learning_rate": 5.610760830906599e-06,
      "loss": 0.0009,
      "step": 2359
    },
    {
      "epoch": 98.08,
      "grad_norm": 0.032568544149398804,
      "learning_rate": 5.6075790051105025e-06,
      "loss": 0.0005,
      "step": 2360
    },
    {
      "epoch": 98.12,
      "grad_norm": 0.053918950259685516,
      "learning_rate": 5.604396929561629e-06,
      "loss": 0.0007,
      "step": 2361
    },
    {
      "epoch": 98.16,
      "grad_norm": 0.03721746802330017,
      "learning_rate": 5.601214605568006e-06,
      "loss": 0.0005,
      "step": 2362
    },
    {
      "epoch": 98.2,
      "grad_norm": 0.04936802759766579,
      "learning_rate": 5.598032034437771e-06,
      "loss": 0.0006,
      "step": 2363
    },
    {
      "epoch": 98.24,
      "grad_norm": 0.054583728313446045,
      "learning_rate": 5.594849217479155e-06,
      "loss": 0.0006,
      "step": 2364
    },
    {
      "epoch": 98.29,
      "grad_norm": 0.035307079553604126,
      "learning_rate": 5.5916661560004945e-06,
      "loss": 0.0006,
      "step": 2365
    },
    {
      "epoch": 98.33,
      "grad_norm": 0.02915666066110134,
      "learning_rate": 5.588482851310227e-06,
      "loss": 0.0005,
      "step": 2366
    },
    {
      "epoch": 98.37,
      "grad_norm": 0.007346175145357847,
      "learning_rate": 5.5852993047168894e-06,
      "loss": 0.0004,
      "step": 2367
    },
    {
      "epoch": 98.41,
      "grad_norm": 0.10896226018667221,
      "learning_rate": 5.582115517529114e-06,
      "loss": 0.001,
      "step": 2368
    },
    {
      "epoch": 98.45,
      "grad_norm": 0.022000158205628395,
      "learning_rate": 5.57893149105564e-06,
      "loss": 0.0005,
      "step": 2369
    },
    {
      "epoch": 98.49,
      "grad_norm": 0.03759472072124481,
      "learning_rate": 5.575747226605298e-06,
      "loss": 0.0005,
      "step": 2370
    },
    {
      "epoch": 98.54,
      "grad_norm": 0.1869850754737854,
      "learning_rate": 5.572562725487018e-06,
      "loss": 0.002,
      "step": 2371
    },
    {
      "epoch": 98.58,
      "grad_norm": 0.035580579191446304,
      "learning_rate": 5.569377989009829e-06,
      "loss": 0.0005,
      "step": 2372
    },
    {
      "epoch": 98.62,
      "grad_norm": 0.04760272428393364,
      "learning_rate": 5.566193018482856e-06,
      "loss": 0.0005,
      "step": 2373
    },
    {
      "epoch": 98.66,
      "grad_norm": 0.06493259966373444,
      "learning_rate": 5.5630078152153226e-06,
      "loss": 0.0008,
      "step": 2374
    },
    {
      "epoch": 98.7,
      "grad_norm": 0.10785691440105438,
      "learning_rate": 5.559822380516539e-06,
      "loss": 0.0011,
      "step": 2375
    },
    {
      "epoch": 98.74,
      "grad_norm": 0.07101790606975555,
      "learning_rate": 5.5566367156959246e-06,
      "loss": 0.0012,
      "step": 2376
    },
    {
      "epoch": 98.78,
      "grad_norm": 0.07247482985258102,
      "learning_rate": 5.55345082206298e-06,
      "loss": 0.0007,
      "step": 2377
    },
    {
      "epoch": 98.83,
      "grad_norm": 0.04662759229540825,
      "learning_rate": 5.550264700927309e-06,
      "loss": 0.0006,
      "step": 2378
    },
    {
      "epoch": 98.87,
      "grad_norm": 0.09778568893671036,
      "learning_rate": 5.547078353598605e-06,
      "loss": 0.0011,
      "step": 2379
    },
    {
      "epoch": 98.91,
      "grad_norm": 0.05291959270834923,
      "learning_rate": 5.543891781386655e-06,
      "loss": 0.0008,
      "step": 2380
    },
    {
      "epoch": 98.95,
      "grad_norm": 0.0411352701485157,
      "learning_rate": 5.540704985601338e-06,
      "loss": 0.0005,
      "step": 2381
    },
    {
      "epoch": 98.99,
      "grad_norm": 0.13341651856899261,
      "learning_rate": 5.537517967552626e-06,
      "loss": 0.0016,
      "step": 2382
    },
    {
      "epoch": 99.03,
      "grad_norm": 0.08837319910526276,
      "learning_rate": 5.53433072855058e-06,
      "loss": 0.0008,
      "step": 2383
    },
    {
      "epoch": 99.08,
      "grad_norm": 0.00678120506927371,
      "learning_rate": 5.531143269905356e-06,
      "loss": 0.0004,
      "step": 2384
    },
    {
      "epoch": 99.12,
      "grad_norm": 0.028322551399469376,
      "learning_rate": 5.527955592927198e-06,
      "loss": 0.0005,
      "step": 2385
    },
    {
      "epoch": 99.16,
      "grad_norm": 0.11283976584672928,
      "learning_rate": 5.524767698926437e-06,
      "loss": 0.001,
      "step": 2386
    },
    {
      "epoch": 99.2,
      "grad_norm": 0.007277297787368298,
      "learning_rate": 5.521579589213496e-06,
      "loss": 0.0004,
      "step": 2387
    },
    {
      "epoch": 99.24,
      "grad_norm": 0.04061352089047432,
      "learning_rate": 5.518391265098888e-06,
      "loss": 0.0005,
      "step": 2388
    },
    {
      "epoch": 99.28,
      "grad_norm": 0.06310468167066574,
      "learning_rate": 5.515202727893213e-06,
      "loss": 0.0008,
      "step": 2389
    },
    {
      "epoch": 99.32,
      "grad_norm": 0.04061620309948921,
      "learning_rate": 5.512013978907157e-06,
      "loss": 0.0005,
      "step": 2390
    },
    {
      "epoch": 99.37,
      "grad_norm": 0.07363440841436386,
      "learning_rate": 5.508825019451497e-06,
      "loss": 0.0008,
      "step": 2391
    },
    {
      "epoch": 99.41,
      "grad_norm": 0.06682130694389343,
      "learning_rate": 5.5056358508370884e-06,
      "loss": 0.0006,
      "step": 2392
    },
    {
      "epoch": 99.45,
      "grad_norm": 0.048409152776002884,
      "learning_rate": 5.502446474374883e-06,
      "loss": 0.0006,
      "step": 2393
    },
    {
      "epoch": 99.49,
      "grad_norm": 0.06600248068571091,
      "learning_rate": 5.49925689137591e-06,
      "loss": 0.0008,
      "step": 2394
    },
    {
      "epoch": 99.53,
      "grad_norm": 0.00655342498794198,
      "learning_rate": 5.496067103151288e-06,
      "loss": 0.0003,
      "step": 2395
    },
    {
      "epoch": 99.57,
      "grad_norm": 0.03977515920996666,
      "learning_rate": 5.4928771110122185e-06,
      "loss": 0.0007,
      "step": 2396
    },
    {
      "epoch": 99.62,
      "grad_norm": 0.10953886061906815,
      "learning_rate": 5.489686916269985e-06,
      "loss": 0.0013,
      "step": 2397
    },
    {
      "epoch": 99.66,
      "grad_norm": 0.17190136015415192,
      "learning_rate": 5.486496520235959e-06,
      "loss": 0.0016,
      "step": 2398
    },
    {
      "epoch": 99.7,
      "grad_norm": 0.047785572707653046,
      "learning_rate": 5.483305924221588e-06,
      "loss": 0.0006,
      "step": 2399
    },
    {
      "epoch": 99.74,
      "grad_norm": 0.0710911750793457,
      "learning_rate": 5.480115129538409e-06,
      "loss": 0.0007,
      "step": 2400
    },
    {
      "epoch": 99.78,
      "grad_norm": 0.029330797493457794,
      "learning_rate": 5.4769241374980365e-06,
      "loss": 0.0004,
      "step": 2401
    },
    {
      "epoch": 99.82,
      "grad_norm": 0.08115801960229874,
      "learning_rate": 5.473732949412165e-06,
      "loss": 0.0008,
      "step": 2402
    },
    {
      "epoch": 99.86,
      "grad_norm": 0.023611625656485558,
      "learning_rate": 5.470541566592573e-06,
      "loss": 0.0004,
      "step": 2403
    },
    {
      "epoch": 99.91,
      "grad_norm": 0.08960434049367905,
      "learning_rate": 5.467349990351116e-06,
      "loss": 0.0009,
      "step": 2404
    },
    {
      "epoch": 99.95,
      "grad_norm": 0.17273403704166412,
      "learning_rate": 5.464158221999731e-06,
      "loss": 0.002,
      "step": 2405
    },
    {
      "epoch": 99.99,
      "grad_norm": 0.15453748404979706,
      "learning_rate": 5.460966262850434e-06,
      "loss": 0.0017,
      "step": 2406
    },
    {
      "epoch": 100.03,
      "grad_norm": 0.04115394875407219,
      "learning_rate": 5.45777411421532e-06,
      "loss": 0.0006,
      "step": 2407
    },
    {
      "epoch": 100.07,
      "grad_norm": 0.0922175720334053,
      "learning_rate": 5.454581777406559e-06,
      "loss": 0.0009,
      "step": 2408
    },
    {
      "epoch": 100.11,
      "grad_norm": 0.0666644498705864,
      "learning_rate": 5.4513892537364e-06,
      "loss": 0.0007,
      "step": 2409
    },
    {
      "epoch": 100.16,
      "grad_norm": 0.0630660131573677,
      "learning_rate": 5.448196544517168e-06,
      "loss": 0.0009,
      "step": 2410
    },
    {
      "epoch": 100.2,
      "grad_norm": 0.036161720752716064,
      "learning_rate": 5.445003651061267e-06,
      "loss": 0.0005,
      "step": 2411
    },
    {
      "epoch": 100.24,
      "grad_norm": 0.03563602268695831,
      "learning_rate": 5.441810574681175e-06,
      "loss": 0.0005,
      "step": 2412
    },
    {
      "epoch": 100.28,
      "grad_norm": 0.02432238683104515,
      "learning_rate": 5.4386173166894455e-06,
      "loss": 0.0005,
      "step": 2413
    },
    {
      "epoch": 100.32,
      "grad_norm": 0.04266718775033951,
      "learning_rate": 5.435423878398703e-06,
      "loss": 0.0005,
      "step": 2414
    },
    {
      "epoch": 100.36,
      "grad_norm": 0.02591860108077526,
      "learning_rate": 5.4322302611216515e-06,
      "loss": 0.0005,
      "step": 2415
    },
    {
      "epoch": 100.41,
      "grad_norm": 0.156783789396286,
      "learning_rate": 5.429036466171067e-06,
      "loss": 0.0015,
      "step": 2416
    },
    {
      "epoch": 100.45,
      "grad_norm": 0.04596616327762604,
      "learning_rate": 5.425842494859798e-06,
      "loss": 0.0006,
      "step": 2417
    },
    {
      "epoch": 100.49,
      "grad_norm": 0.1190098226070404,
      "learning_rate": 5.422648348500764e-06,
      "loss": 0.001,
      "step": 2418
    },
    {
      "epoch": 100.53,
      "grad_norm": 0.04969616234302521,
      "learning_rate": 5.4194540284069596e-06,
      "loss": 0.0006,
      "step": 2419
    },
    {
      "epoch": 100.57,
      "grad_norm": 0.006862211041152477,
      "learning_rate": 5.4162595358914475e-06,
      "loss": 0.0004,
      "step": 2420
    },
    {
      "epoch": 100.61,
      "grad_norm": 0.006312835495918989,
      "learning_rate": 5.413064872267364e-06,
      "loss": 0.0004,
      "step": 2421
    },
    {
      "epoch": 100.65,
      "grad_norm": 0.10098616033792496,
      "learning_rate": 5.4098700388479135e-06,
      "loss": 0.0012,
      "step": 2422
    },
    {
      "epoch": 100.7,
      "grad_norm": 0.11338721215724945,
      "learning_rate": 5.406675036946374e-06,
      "loss": 0.0011,
      "step": 2423
    },
    {
      "epoch": 100.74,
      "grad_norm": 0.007187958806753159,
      "learning_rate": 5.403479867876087e-06,
      "loss": 0.0004,
      "step": 2424
    },
    {
      "epoch": 100.78,
      "grad_norm": 0.07057364284992218,
      "learning_rate": 5.4002845329504675e-06,
      "loss": 0.0008,
      "step": 2425
    },
    {
      "epoch": 100.82,
      "grad_norm": 0.06163496524095535,
      "learning_rate": 5.3970890334829976e-06,
      "loss": 0.0008,
      "step": 2426
    },
    {
      "epoch": 100.86,
      "grad_norm": 0.012974144890904427,
      "learning_rate": 5.3938933707872236e-06,
      "loss": 0.0004,
      "step": 2427
    },
    {
      "epoch": 100.9,
      "grad_norm": 0.052575163543224335,
      "learning_rate": 5.390697546176763e-06,
      "loss": 0.0006,
      "step": 2428
    },
    {
      "epoch": 100.95,
      "grad_norm": 0.10557461529970169,
      "learning_rate": 5.387501560965301e-06,
      "loss": 0.0013,
      "step": 2429
    },
    {
      "epoch": 100.99,
      "grad_norm": 0.03142361715435982,
      "learning_rate": 5.384305416466584e-06,
      "loss": 0.0005,
      "step": 2430
    },
    {
      "epoch": 101.03,
      "grad_norm": 0.06375977396965027,
      "learning_rate": 5.381109113994426e-06,
      "loss": 0.0008,
      "step": 2431
    },
    {
      "epoch": 101.07,
      "grad_norm": 0.006657157558947802,
      "learning_rate": 5.377912654862708e-06,
      "loss": 0.0003,
      "step": 2432
    },
    {
      "epoch": 101.11,
      "grad_norm": 0.05172253027558327,
      "learning_rate": 5.374716040385371e-06,
      "loss": 0.0006,
      "step": 2433
    },
    {
      "epoch": 101.15,
      "grad_norm": 0.08930712938308716,
      "learning_rate": 5.371519271876426e-06,
      "loss": 0.0009,
      "step": 2434
    },
    {
      "epoch": 101.19,
      "grad_norm": 0.06334531307220459,
      "learning_rate": 5.368322350649942e-06,
      "loss": 0.0006,
      "step": 2435
    },
    {
      "epoch": 101.24,
      "grad_norm": 0.06010973080992699,
      "learning_rate": 5.36512527802005e-06,
      "loss": 0.0008,
      "step": 2436
    },
    {
      "epoch": 101.28,
      "grad_norm": 0.044452909380197525,
      "learning_rate": 5.36192805530095e-06,
      "loss": 0.0007,
      "step": 2437
    },
    {
      "epoch": 101.32,
      "grad_norm": 0.013560798950493336,
      "learning_rate": 5.358730683806897e-06,
      "loss": 0.0004,
      "step": 2438
    },
    {
      "epoch": 101.36,
      "grad_norm": 0.044771429151296616,
      "learning_rate": 5.355533164852211e-06,
      "loss": 0.0006,
      "step": 2439
    },
    {
      "epoch": 101.4,
      "grad_norm": 0.1001865491271019,
      "learning_rate": 5.35233549975127e-06,
      "loss": 0.0009,
      "step": 2440
    },
    {
      "epoch": 101.44,
      "grad_norm": 0.06078821420669556,
      "learning_rate": 5.349137689818514e-06,
      "loss": 0.0008,
      "step": 2441
    },
    {
      "epoch": 101.49,
      "grad_norm": 0.006679890211671591,
      "learning_rate": 5.345939736368442e-06,
      "loss": 0.0004,
      "step": 2442
    },
    {
      "epoch": 101.53,
      "grad_norm": 0.055286869406700134,
      "learning_rate": 5.342741640715611e-06,
      "loss": 0.0008,
      "step": 2443
    },
    {
      "epoch": 101.57,
      "grad_norm": 0.019849229604005814,
      "learning_rate": 5.339543404174639e-06,
      "loss": 0.0004,
      "step": 2444
    },
    {
      "epoch": 101.61,
      "grad_norm": 0.01906355656683445,
      "learning_rate": 5.336345028060199e-06,
      "loss": 0.0004,
      "step": 2445
    },
    {
      "epoch": 101.65,
      "grad_norm": 0.01580219529569149,
      "learning_rate": 5.333146513687022e-06,
      "loss": 0.0004,
      "step": 2446
    },
    {
      "epoch": 101.69,
      "grad_norm": 0.14343762397766113,
      "learning_rate": 5.329947862369897e-06,
      "loss": 0.0018,
      "step": 2447
    },
    {
      "epoch": 101.74,
      "grad_norm": 0.03625111281871796,
      "learning_rate": 5.326749075423672e-06,
      "loss": 0.0005,
      "step": 2448
    },
    {
      "epoch": 101.78,
      "grad_norm": 0.023388028144836426,
      "learning_rate": 5.3235501541632414e-06,
      "loss": 0.0004,
      "step": 2449
    },
    {
      "epoch": 101.82,
      "grad_norm": 0.03478454425930977,
      "learning_rate": 5.320351099903565e-06,
      "loss": 0.0005,
      "step": 2450
    },
    {
      "epoch": 101.86,
      "grad_norm": 0.02243076264858246,
      "learning_rate": 5.317151913959653e-06,
      "loss": 0.0006,
      "step": 2451
    },
    {
      "epoch": 101.9,
      "grad_norm": 0.006781103555113077,
      "learning_rate": 5.3139525976465675e-06,
      "loss": 0.0004,
      "step": 2452
    },
    {
      "epoch": 101.94,
      "grad_norm": 0.07056421041488647,
      "learning_rate": 5.310753152279429e-06,
      "loss": 0.001,
      "step": 2453
    },
    {
      "epoch": 101.98,
      "grad_norm": 0.034043047577142715,
      "learning_rate": 5.307553579173408e-06,
      "loss": 0.0005,
      "step": 2454
    },
    {
      "epoch": 102.03,
      "grad_norm": 0.006823445670306683,
      "learning_rate": 5.304353879643727e-06,
      "loss": 0.0004,
      "step": 2455
    },
    {
      "epoch": 102.07,
      "grad_norm": 0.08361528813838959,
      "learning_rate": 5.301154055005664e-06,
      "loss": 0.0008,
      "step": 2456
    },
    {
      "epoch": 102.11,
      "grad_norm": 0.05054937303066254,
      "learning_rate": 5.297954106574543e-06,
      "loss": 0.0007,
      "step": 2457
    },
    {
      "epoch": 102.15,
      "grad_norm": 0.027152929455041885,
      "learning_rate": 5.294754035665743e-06,
      "loss": 0.0004,
      "step": 2458
    },
    {
      "epoch": 102.19,
      "grad_norm": 0.030014418065547943,
      "learning_rate": 5.291553843594695e-06,
      "loss": 0.0005,
      "step": 2459
    },
    {
      "epoch": 102.23,
      "grad_norm": 0.006366490852087736,
      "learning_rate": 5.288353531676873e-06,
      "loss": 0.0003,
      "step": 2460
    },
    {
      "epoch": 102.28,
      "grad_norm": 0.053286049515008926,
      "learning_rate": 5.285153101227808e-06,
      "loss": 0.0006,
      "step": 2461
    },
    {
      "epoch": 102.32,
      "grad_norm": 0.04536386951804161,
      "learning_rate": 5.2819525535630725e-06,
      "loss": 0.0007,
      "step": 2462
    },
    {
      "epoch": 102.36,
      "grad_norm": 0.09264278411865234,
      "learning_rate": 5.278751889998292e-06,
      "loss": 0.0007,
      "step": 2463
    },
    {
      "epoch": 102.4,
      "grad_norm": 0.06302645802497864,
      "learning_rate": 5.2755511118491405e-06,
      "loss": 0.0007,
      "step": 2464
    },
    {
      "epoch": 102.44,
      "grad_norm": 0.06383947283029556,
      "learning_rate": 5.2723502204313346e-06,
      "loss": 0.0008,
      "step": 2465
    },
    {
      "epoch": 102.48,
      "grad_norm": 0.036103446036577225,
      "learning_rate": 5.269149217060642e-06,
      "loss": 0.0005,
      "step": 2466
    },
    {
      "epoch": 102.52,
      "grad_norm": 0.13022206723690033,
      "learning_rate": 5.265948103052871e-06,
      "loss": 0.0015,
      "step": 2467
    },
    {
      "epoch": 102.57,
      "grad_norm": 0.006501273717731237,
      "learning_rate": 5.262746879723882e-06,
      "loss": 0.0004,
      "step": 2468
    },
    {
      "epoch": 102.61,
      "grad_norm": 0.09923107177019119,
      "learning_rate": 5.259545548389575e-06,
      "loss": 0.0011,
      "step": 2469
    },
    {
      "epoch": 102.65,
      "grad_norm": 0.006595449522137642,
      "learning_rate": 5.256344110365896e-06,
      "loss": 0.0003,
      "step": 2470
    },
    {
      "epoch": 102.69,
      "grad_norm": 0.12231157720088959,
      "learning_rate": 5.253142566968838e-06,
      "loss": 0.0012,
      "step": 2471
    },
    {
      "epoch": 102.73,
      "grad_norm": 0.0659080371260643,
      "learning_rate": 5.249940919514434e-06,
      "loss": 0.0008,
      "step": 2472
    },
    {
      "epoch": 102.77,
      "grad_norm": 0.006205762270838022,
      "learning_rate": 5.2467391693187565e-06,
      "loss": 0.0003,
      "step": 2473
    },
    {
      "epoch": 102.82,
      "grad_norm": 0.03242097422480583,
      "learning_rate": 5.243537317697927e-06,
      "loss": 0.0006,
      "step": 2474
    },
    {
      "epoch": 102.86,
      "grad_norm": 0.03527124226093292,
      "learning_rate": 5.240335365968104e-06,
      "loss": 0.0005,
      "step": 2475
    },
    {
      "epoch": 102.9,
      "grad_norm": 0.06016567349433899,
      "learning_rate": 5.237133315445493e-06,
      "loss": 0.0006,
      "step": 2476
    },
    {
      "epoch": 102.94,
      "grad_norm": 0.05818776413798332,
      "learning_rate": 5.233931167446331e-06,
      "loss": 0.0006,
      "step": 2477
    },
    {
      "epoch": 102.98,
      "grad_norm": 0.11250666528940201,
      "learning_rate": 5.230728923286905e-06,
      "loss": 0.0011,
      "step": 2478
    },
    {
      "epoch": 103.02,
      "grad_norm": 0.02944801189005375,
      "learning_rate": 5.227526584283532e-06,
      "loss": 0.0006,
      "step": 2479
    },
    {
      "epoch": 103.06,
      "grad_norm": 0.060859378427267075,
      "learning_rate": 5.224324151752575e-06,
      "loss": 0.0006,
      "step": 2480
    },
    {
      "epoch": 103.11,
      "grad_norm": 0.0465037040412426,
      "learning_rate": 5.2211216270104326e-06,
      "loss": 0.0005,
      "step": 2481
    },
    {
      "epoch": 103.15,
      "grad_norm": 0.062456294894218445,
      "learning_rate": 5.2179190113735425e-06,
      "loss": 0.0008,
      "step": 2482
    },
    {
      "epoch": 103.19,
      "grad_norm": 0.0575128048658371,
      "learning_rate": 5.214716306158378e-06,
      "loss": 0.0006,
      "step": 2483
    },
    {
      "epoch": 103.23,
      "grad_norm": 0.07254152745008469,
      "learning_rate": 5.211513512681451e-06,
      "loss": 0.0007,
      "step": 2484
    },
    {
      "epoch": 103.27,
      "grad_norm": 0.03131202235817909,
      "learning_rate": 5.208310632259308e-06,
      "loss": 0.0005,
      "step": 2485
    },
    {
      "epoch": 103.31,
      "grad_norm": 0.047063786536455154,
      "learning_rate": 5.205107666208533e-06,
      "loss": 0.0006,
      "step": 2486
    },
    {
      "epoch": 103.36,
      "grad_norm": 0.08041809499263763,
      "learning_rate": 5.201904615845743e-06,
      "loss": 0.001,
      "step": 2487
    },
    {
      "epoch": 103.4,
      "grad_norm": 0.05611753091216087,
      "learning_rate": 5.198701482487594e-06,
      "loss": 0.0006,
      "step": 2488
    },
    {
      "epoch": 103.44,
      "grad_norm": 0.07832735031843185,
      "learning_rate": 5.195498267450769e-06,
      "loss": 0.0008,
      "step": 2489
    },
    {
      "epoch": 103.48,
      "grad_norm": 0.006438216660171747,
      "learning_rate": 5.192294972051992e-06,
      "loss": 0.0003,
      "step": 2490
    },
    {
      "epoch": 103.52,
      "grad_norm": 0.05057075619697571,
      "learning_rate": 5.189091597608016e-06,
      "loss": 0.0006,
      "step": 2491
    },
    {
      "epoch": 103.56,
      "grad_norm": 0.04413389042019844,
      "learning_rate": 5.185888145435626e-06,
      "loss": 0.0005,
      "step": 2492
    },
    {
      "epoch": 103.61,
      "grad_norm": 0.14328354597091675,
      "learning_rate": 5.182684616851642e-06,
      "loss": 0.0012,
      "step": 2493
    },
    {
      "epoch": 103.65,
      "grad_norm": 0.07333394140005112,
      "learning_rate": 5.179481013172912e-06,
      "loss": 0.0007,
      "step": 2494
    },
    {
      "epoch": 103.69,
      "grad_norm": 0.04661286249756813,
      "learning_rate": 5.1762773357163175e-06,
      "loss": 0.0006,
      "step": 2495
    },
    {
      "epoch": 103.73,
      "grad_norm": 0.04610106348991394,
      "learning_rate": 5.173073585798768e-06,
      "loss": 0.0005,
      "step": 2496
    },
    {
      "epoch": 103.77,
      "grad_norm": 0.04396683722734451,
      "learning_rate": 5.169869764737205e-06,
      "loss": 0.0007,
      "step": 2497
    },
    {
      "epoch": 103.81,
      "grad_norm": 0.04193466529250145,
      "learning_rate": 5.1666658738485985e-06,
      "loss": 0.0006,
      "step": 2498
    },
    {
      "epoch": 103.85,
      "grad_norm": 0.02958308719098568,
      "learning_rate": 5.163461914449948e-06,
      "loss": 0.0004,
      "step": 2499
    },
    {
      "epoch": 103.9,
      "grad_norm": 0.06733661144971848,
      "learning_rate": 5.160257887858278e-06,
      "loss": 0.0008,
      "step": 2500
    },
    {
      "epoch": 103.94,
      "grad_norm": 0.00732116075232625,
      "learning_rate": 5.157053795390642e-06,
      "loss": 0.0004,
      "step": 2501
    },
    {
      "epoch": 103.98,
      "grad_norm": 0.07538177073001862,
      "learning_rate": 5.153849638364125e-06,
      "loss": 0.0009,
      "step": 2502
    },
    {
      "epoch": 104.02,
      "grad_norm": 0.15863019227981567,
      "learning_rate": 5.150645418095832e-06,
      "loss": 0.0015,
      "step": 2503
    },
    {
      "epoch": 104.06,
      "grad_norm": 0.0464165173470974,
      "learning_rate": 5.1474411359029e-06,
      "loss": 0.0005,
      "step": 2504
    },
    {
      "epoch": 104.1,
      "grad_norm": 0.06736421585083008,
      "learning_rate": 5.144236793102485e-06,
      "loss": 0.0008,
      "step": 2505
    },
    {
      "epoch": 104.15,
      "grad_norm": 0.00626250309869647,
      "learning_rate": 5.141032391011774e-06,
      "loss": 0.0004,
      "step": 2506
    },
    {
      "epoch": 104.19,
      "grad_norm": 0.031127620488405228,
      "learning_rate": 5.137827930947972e-06,
      "loss": 0.0005,
      "step": 2507
    },
    {
      "epoch": 104.23,
      "grad_norm": 0.005998896900564432,
      "learning_rate": 5.134623414228315e-06,
      "loss": 0.0003,
      "step": 2508
    },
    {
      "epoch": 104.27,
      "grad_norm": 0.1445949524641037,
      "learning_rate": 5.131418842170056e-06,
      "loss": 0.0012,
      "step": 2509
    },
    {
      "epoch": 104.31,
      "grad_norm": 0.006245523225516081,
      "learning_rate": 5.128214216090478e-06,
      "loss": 0.0004,
      "step": 2510
    },
    {
      "epoch": 104.35,
      "grad_norm": 0.06090964749455452,
      "learning_rate": 5.125009537306878e-06,
      "loss": 0.0008,
      "step": 2511
    },
    {
      "epoch": 104.39,
      "grad_norm": 0.007188516203314066,
      "learning_rate": 5.121804807136578e-06,
      "loss": 0.0004,
      "step": 2512
    },
    {
      "epoch": 104.44,
      "grad_norm": 0.010260453447699547,
      "learning_rate": 5.118600026896923e-06,
      "loss": 0.0004,
      "step": 2513
    },
    {
      "epoch": 104.48,
      "grad_norm": 0.09042909741401672,
      "learning_rate": 5.115395197905277e-06,
      "loss": 0.0008,
      "step": 2514
    },
    {
      "epoch": 104.52,
      "grad_norm": 0.07304490357637405,
      "learning_rate": 5.112190321479026e-06,
      "loss": 0.0008,
      "step": 2515
    },
    {
      "epoch": 104.56,
      "grad_norm": 0.04458026960492134,
      "learning_rate": 5.108985398935569e-06,
      "loss": 0.0004,
      "step": 2516
    },
    {
      "epoch": 104.6,
      "grad_norm": 0.03471490368247032,
      "learning_rate": 5.105780431592333e-06,
      "loss": 0.0005,
      "step": 2517
    },
    {
      "epoch": 104.64,
      "grad_norm": 0.006386172957718372,
      "learning_rate": 5.1025754207667565e-06,
      "loss": 0.0004,
      "step": 2518
    },
    {
      "epoch": 104.69,
      "grad_norm": 0.0538405105471611,
      "learning_rate": 5.0993703677762985e-06,
      "loss": 0.0006,
      "step": 2519
    },
    {
      "epoch": 104.73,
      "grad_norm": 0.10927899926900864,
      "learning_rate": 5.0961652739384356e-06,
      "loss": 0.0015,
      "step": 2520
    },
    {
      "epoch": 104.77,
      "grad_norm": 0.017148587852716446,
      "learning_rate": 5.092960140570664e-06,
      "loss": 0.0004,
      "step": 2521
    },
    {
      "epoch": 104.81,
      "grad_norm": 0.08336766064167023,
      "learning_rate": 5.0897549689904865e-06,
      "loss": 0.0008,
      "step": 2522
    },
    {
      "epoch": 104.85,
      "grad_norm": 0.11830395460128784,
      "learning_rate": 5.0865497605154335e-06,
      "loss": 0.001,
      "step": 2523
    },
    {
      "epoch": 104.89,
      "grad_norm": 0.08311416208744049,
      "learning_rate": 5.083344516463043e-06,
      "loss": 0.0009,
      "step": 2524
    },
    {
      "epoch": 104.94,
      "grad_norm": 0.006251211743801832,
      "learning_rate": 5.080139238150869e-06,
      "loss": 0.0003,
      "step": 2525
    },
    {
      "epoch": 104.98,
      "grad_norm": 0.11952332407236099,
      "learning_rate": 5.076933926896484e-06,
      "loss": 0.0018,
      "step": 2526
    },
    {
      "epoch": 105.02,
      "grad_norm": 0.08414675295352936,
      "learning_rate": 5.073728584017465e-06,
      "loss": 0.0011,
      "step": 2527
    },
    {
      "epoch": 105.06,
      "grad_norm": 0.006416326854377985,
      "learning_rate": 5.07052321083141e-06,
      "loss": 0.0004,
      "step": 2528
    },
    {
      "epoch": 105.1,
      "grad_norm": 0.1021459698677063,
      "learning_rate": 5.067317808655927e-06,
      "loss": 0.0011,
      "step": 2529
    },
    {
      "epoch": 105.14,
      "grad_norm": 0.02066420391201973,
      "learning_rate": 5.064112378808636e-06,
      "loss": 0.0004,
      "step": 2530
    },
    {
      "epoch": 105.18,
      "grad_norm": 0.0316598005592823,
      "learning_rate": 5.06090692260717e-06,
      "loss": 0.0004,
      "step": 2531
    },
    {
      "epoch": 105.23,
      "grad_norm": 0.023718250915408134,
      "learning_rate": 5.057701441369167e-06,
      "loss": 0.0004,
      "step": 2532
    },
    {
      "epoch": 105.27,
      "grad_norm": 0.024763140827417374,
      "learning_rate": 5.054495936412281e-06,
      "loss": 0.0004,
      "step": 2533
    },
    {
      "epoch": 105.31,
      "grad_norm": 0.07945156842470169,
      "learning_rate": 5.051290409054173e-06,
      "loss": 0.0007,
      "step": 2534
    },
    {
      "epoch": 105.35,
      "grad_norm": 0.0531713031232357,
      "learning_rate": 5.048084860612516e-06,
      "loss": 0.0006,
      "step": 2535
    },
    {
      "epoch": 105.39,
      "grad_norm": 0.06726127117872238,
      "learning_rate": 5.04487929240499e-06,
      "loss": 0.0007,
      "step": 2536
    },
    {
      "epoch": 105.43,
      "grad_norm": 0.006106032989919186,
      "learning_rate": 5.041673705749281e-06,
      "loss": 0.0004,
      "step": 2537
    },
    {
      "epoch": 105.48,
      "grad_norm": 0.005853430833667517,
      "learning_rate": 5.038468101963087e-06,
      "loss": 0.0003,
      "step": 2538
    },
    {
      "epoch": 105.52,
      "grad_norm": 0.03677138313651085,
      "learning_rate": 5.03526248236411e-06,
      "loss": 0.0005,
      "step": 2539
    },
    {
      "epoch": 105.56,
      "grad_norm": 0.09780915826559067,
      "learning_rate": 5.032056848270056e-06,
      "loss": 0.001,
      "step": 2540
    },
    {
      "epoch": 105.6,
      "grad_norm": 0.08390149474143982,
      "learning_rate": 5.028851200998644e-06,
      "loss": 0.0011,
      "step": 2541
    },
    {
      "epoch": 105.64,
      "grad_norm": 0.08049336075782776,
      "learning_rate": 5.025645541867592e-06,
      "loss": 0.0008,
      "step": 2542
    },
    {
      "epoch": 105.68,
      "grad_norm": 0.04280194640159607,
      "learning_rate": 5.022439872194629e-06,
      "loss": 0.0005,
      "step": 2543
    },
    {
      "epoch": 105.72,
      "grad_norm": 0.15109598636627197,
      "learning_rate": 5.01923419329748e-06,
      "loss": 0.0009,
      "step": 2544
    },
    {
      "epoch": 105.77,
      "grad_norm": 0.05711919441819191,
      "learning_rate": 5.016028506493881e-06,
      "loss": 0.0007,
      "step": 2545
    },
    {
      "epoch": 105.81,
      "grad_norm": 0.07601993530988693,
      "learning_rate": 5.012822813101568e-06,
      "loss": 0.0009,
      "step": 2546
    },
    {
      "epoch": 105.85,
      "grad_norm": 0.06707476079463959,
      "learning_rate": 5.0096171144382814e-06,
      "loss": 0.0009,
      "step": 2547
    },
    {
      "epoch": 105.89,
      "grad_norm": 0.08923736214637756,
      "learning_rate": 5.006411411821762e-06,
      "loss": 0.0012,
      "step": 2548
    },
    {
      "epoch": 105.93,
      "grad_norm": 0.011808677576482296,
      "learning_rate": 5.003205706569753e-06,
      "loss": 0.0004,
      "step": 2549
    },
    {
      "epoch": 105.97,
      "grad_norm": 0.11606138944625854,
      "learning_rate": 5e-06,
      "loss": 0.001,
      "step": 2550
    },
    {
      "epoch": 106.02,
      "grad_norm": 0.006330085452646017,
      "learning_rate": 4.9967942934302475e-06,
      "loss": 0.0003,
      "step": 2551
    },
    {
      "epoch": 106.06,
      "grad_norm": 0.05558967590332031,
      "learning_rate": 4.993588588178239e-06,
      "loss": 0.0006,
      "step": 2552
    },
    {
      "epoch": 106.1,
      "grad_norm": 0.14593498408794403,
      "learning_rate": 4.99038288556172e-06,
      "loss": 0.001,
      "step": 2553
    },
    {
      "epoch": 106.14,
      "grad_norm": 0.04392213374376297,
      "learning_rate": 4.987177186898434e-06,
      "loss": 0.0005,
      "step": 2554
    },
    {
      "epoch": 106.18,
      "grad_norm": 0.04146160930395126,
      "learning_rate": 4.9839714935061215e-06,
      "loss": 0.0005,
      "step": 2555
    },
    {
      "epoch": 106.22,
      "grad_norm": 0.028160633519291878,
      "learning_rate": 4.980765806702522e-06,
      "loss": 0.0005,
      "step": 2556
    },
    {
      "epoch": 106.26,
      "grad_norm": 0.10342870652675629,
      "learning_rate": 4.977560127805373e-06,
      "loss": 0.0008,
      "step": 2557
    },
    {
      "epoch": 106.31,
      "grad_norm": 0.08384987711906433,
      "learning_rate": 4.974354458132409e-06,
      "loss": 0.0011,
      "step": 2558
    },
    {
      "epoch": 106.35,
      "grad_norm": 0.12213827669620514,
      "learning_rate": 4.971148799001357e-06,
      "loss": 0.0011,
      "step": 2559
    },
    {
      "epoch": 106.39,
      "grad_norm": 0.048722077161073685,
      "learning_rate": 4.967943151729945e-06,
      "loss": 0.0006,
      "step": 2560
    },
    {
      "epoch": 106.43,
      "grad_norm": 0.0066931103356182575,
      "learning_rate": 4.964737517635892e-06,
      "loss": 0.0003,
      "step": 2561
    },
    {
      "epoch": 106.47,
      "grad_norm": 0.10227407515048981,
      "learning_rate": 4.961531898036913e-06,
      "loss": 0.0009,
      "step": 2562
    },
    {
      "epoch": 106.51,
      "grad_norm": 0.05931786820292473,
      "learning_rate": 4.9583262942507195e-06,
      "loss": 0.0006,
      "step": 2563
    },
    {
      "epoch": 106.56,
      "grad_norm": 0.05263860896229744,
      "learning_rate": 4.955120707595011e-06,
      "loss": 0.0007,
      "step": 2564
    },
    {
      "epoch": 106.6,
      "grad_norm": 0.1276608258485794,
      "learning_rate": 4.951915139387484e-06,
      "loss": 0.0012,
      "step": 2565
    },
    {
      "epoch": 106.64,
      "grad_norm": 0.05225388705730438,
      "learning_rate": 4.9487095909458275e-06,
      "loss": 0.0006,
      "step": 2566
    },
    {
      "epoch": 106.68,
      "grad_norm": 0.024702904745936394,
      "learning_rate": 4.94550406358772e-06,
      "loss": 0.0005,
      "step": 2567
    },
    {
      "epoch": 106.72,
      "grad_norm": 0.03809407725930214,
      "learning_rate": 4.942298558630834e-06,
      "loss": 0.0005,
      "step": 2568
    },
    {
      "epoch": 106.76,
      "grad_norm": 0.10536542534828186,
      "learning_rate": 4.939093077392831e-06,
      "loss": 0.0012,
      "step": 2569
    },
    {
      "epoch": 106.81,
      "grad_norm": 0.14340108633041382,
      "learning_rate": 4.935887621191364e-06,
      "loss": 0.0011,
      "step": 2570
    },
    {
      "epoch": 106.85,
      "grad_norm": 0.1227017492055893,
      "learning_rate": 4.932682191344073e-06,
      "loss": 0.0009,
      "step": 2571
    },
    {
      "epoch": 106.89,
      "grad_norm": 0.0754198431968689,
      "learning_rate": 4.9294767891685904e-06,
      "loss": 0.0008,
      "step": 2572
    },
    {
      "epoch": 106.93,
      "grad_norm": 0.04562905430793762,
      "learning_rate": 4.926271415982538e-06,
      "loss": 0.0006,
      "step": 2573
    },
    {
      "epoch": 106.97,
      "grad_norm": 0.08829937875270844,
      "learning_rate": 4.92306607310352e-06,
      "loss": 0.001,
      "step": 2574
    },
    {
      "epoch": 107.01,
      "grad_norm": 0.058631666004657745,
      "learning_rate": 4.919860761849132e-06,
      "loss": 0.0008,
      "step": 2575
    },
    {
      "epoch": 107.05,
      "grad_norm": 0.06353025883436203,
      "learning_rate": 4.91665548353696e-06,
      "loss": 0.0007,
      "step": 2576
    },
    {
      "epoch": 107.1,
      "grad_norm": 0.0412483848631382,
      "learning_rate": 4.913450239484569e-06,
      "loss": 0.0006,
      "step": 2577
    },
    {
      "epoch": 107.14,
      "grad_norm": 0.027958232909440994,
      "learning_rate": 4.910245031009515e-06,
      "loss": 0.0004,
      "step": 2578
    },
    {
      "epoch": 107.18,
      "grad_norm": 0.04942731559276581,
      "learning_rate": 4.907039859429339e-06,
      "loss": 0.0006,
      "step": 2579
    },
    {
      "epoch": 107.22,
      "grad_norm": 0.005785330198705196,
      "learning_rate": 4.903834726061565e-06,
      "loss": 0.0003,
      "step": 2580
    },
    {
      "epoch": 107.26,
      "grad_norm": 0.026255419477820396,
      "learning_rate": 4.900629632223704e-06,
      "loss": 0.0005,
      "step": 2581
    },
    {
      "epoch": 107.3,
      "grad_norm": 0.16418768465518951,
      "learning_rate": 4.897424579233246e-06,
      "loss": 0.0011,
      "step": 2582
    },
    {
      "epoch": 107.35,
      "grad_norm": 0.07680105417966843,
      "learning_rate": 4.89421956840767e-06,
      "loss": 0.0008,
      "step": 2583
    },
    {
      "epoch": 107.39,
      "grad_norm": 0.10780075937509537,
      "learning_rate": 4.891014601064432e-06,
      "loss": 0.0011,
      "step": 2584
    },
    {
      "epoch": 107.43,
      "grad_norm": 0.034925442188978195,
      "learning_rate": 4.887809678520976e-06,
      "loss": 0.0005,
      "step": 2585
    },
    {
      "epoch": 107.47,
      "grad_norm": 0.08982451260089874,
      "learning_rate": 4.884604802094724e-06,
      "loss": 0.0008,
      "step": 2586
    },
    {
      "epoch": 107.51,
      "grad_norm": 0.09021312743425369,
      "learning_rate": 4.881399973103078e-06,
      "loss": 0.0009,
      "step": 2587
    },
    {
      "epoch": 107.55,
      "grad_norm": 0.08155574649572372,
      "learning_rate": 4.8781951928634235e-06,
      "loss": 0.0008,
      "step": 2588
    },
    {
      "epoch": 107.59,
      "grad_norm": 0.006864198949187994,
      "learning_rate": 4.874990462693124e-06,
      "loss": 0.0004,
      "step": 2589
    },
    {
      "epoch": 107.64,
      "grad_norm": 0.0725095272064209,
      "learning_rate": 4.871785783909523e-06,
      "loss": 0.0007,
      "step": 2590
    },
    {
      "epoch": 107.68,
      "grad_norm": 0.02804291434586048,
      "learning_rate": 4.8685811578299445e-06,
      "loss": 0.0004,
      "step": 2591
    },
    {
      "epoch": 107.72,
      "grad_norm": 0.0285332053899765,
      "learning_rate": 4.865376585771687e-06,
      "loss": 0.0004,
      "step": 2592
    },
    {
      "epoch": 107.76,
      "grad_norm": 0.06387929618358612,
      "learning_rate": 4.86217206905203e-06,
      "loss": 0.0007,
      "step": 2593
    },
    {
      "epoch": 107.8,
      "grad_norm": 0.055428244173526764,
      "learning_rate": 4.858967608988229e-06,
      "loss": 0.0006,
      "step": 2594
    },
    {
      "epoch": 107.84,
      "grad_norm": 0.10637956857681274,
      "learning_rate": 4.855763206897516e-06,
      "loss": 0.0012,
      "step": 2595
    },
    {
      "epoch": 107.89,
      "grad_norm": 0.005964271724224091,
      "learning_rate": 4.852558864097101e-06,
      "loss": 0.0003,
      "step": 2596
    },
    {
      "epoch": 107.93,
      "grad_norm": 0.07015674561262131,
      "learning_rate": 4.849354581904169e-06,
      "loss": 0.0007,
      "step": 2597
    },
    {
      "epoch": 107.97,
      "grad_norm": 0.07907231152057648,
      "learning_rate": 4.846150361635876e-06,
      "loss": 0.0008,
      "step": 2598
    },
    {
      "epoch": 108.01,
      "grad_norm": 0.08224621415138245,
      "learning_rate": 4.842946204609359e-06,
      "loss": 0.0009,
      "step": 2599
    },
    {
      "epoch": 108.05,
      "grad_norm": 0.015371984802186489,
      "learning_rate": 4.839742112141725e-06,
      "loss": 0.0006,
      "step": 2600
    },
    {
      "epoch": 108.09,
      "grad_norm": 0.08624088764190674,
      "learning_rate": 4.836538085550054e-06,
      "loss": 0.001,
      "step": 2601
    },
    {
      "epoch": 108.14,
      "grad_norm": 0.04148736968636513,
      "learning_rate": 4.833334126151403e-06,
      "loss": 0.0005,
      "step": 2602
    },
    {
      "epoch": 108.18,
      "grad_norm": 0.06546816974878311,
      "learning_rate": 4.8301302352627964e-06,
      "loss": 0.0007,
      "step": 2603
    },
    {
      "epoch": 108.22,
      "grad_norm": 0.0058595119044184685,
      "learning_rate": 4.826926414201234e-06,
      "loss": 0.0004,
      "step": 2604
    },
    {
      "epoch": 108.26,
      "grad_norm": 0.03909333795309067,
      "learning_rate": 4.823722664283684e-06,
      "loss": 0.0005,
      "step": 2605
    },
    {
      "epoch": 108.3,
      "grad_norm": 0.09008201211690903,
      "learning_rate": 4.8205189868270894e-06,
      "loss": 0.0007,
      "step": 2606
    },
    {
      "epoch": 108.34,
      "grad_norm": 0.054642342031002045,
      "learning_rate": 4.8173153831483595e-06,
      "loss": 0.0005,
      "step": 2607
    },
    {
      "epoch": 108.38,
      "grad_norm": 0.005915675312280655,
      "learning_rate": 4.814111854564375e-06,
      "loss": 0.0003,
      "step": 2608
    },
    {
      "epoch": 108.43,
      "grad_norm": 0.02765023708343506,
      "learning_rate": 4.8109084023919846e-06,
      "loss": 0.0004,
      "step": 2609
    },
    {
      "epoch": 108.47,
      "grad_norm": 0.058642253279685974,
      "learning_rate": 4.807705027948008e-06,
      "loss": 0.0007,
      "step": 2610
    },
    {
      "epoch": 108.51,
      "grad_norm": 0.07597031444311142,
      "learning_rate": 4.804501732549231e-06,
      "loss": 0.0008,
      "step": 2611
    },
    {
      "epoch": 108.55,
      "grad_norm": 0.032206542789936066,
      "learning_rate": 4.801298517512408e-06,
      "loss": 0.0004,
      "step": 2612
    },
    {
      "epoch": 108.59,
      "grad_norm": 0.026901062577962875,
      "learning_rate": 4.7980953841542575e-06,
      "loss": 0.0005,
      "step": 2613
    },
    {
      "epoch": 108.63,
      "grad_norm": 0.005902049131691456,
      "learning_rate": 4.794892333791468e-06,
      "loss": 0.0003,
      "step": 2614
    },
    {
      "epoch": 108.68,
      "grad_norm": 0.029339369386434555,
      "learning_rate": 4.7916893677406925e-06,
      "loss": 0.0004,
      "step": 2615
    },
    {
      "epoch": 108.72,
      "grad_norm": 0.03829445689916611,
      "learning_rate": 4.7884864873185485e-06,
      "loss": 0.0006,
      "step": 2616
    },
    {
      "epoch": 108.76,
      "grad_norm": 0.07999061048030853,
      "learning_rate": 4.785283693841623e-06,
      "loss": 0.0009,
      "step": 2617
    },
    {
      "epoch": 108.8,
      "grad_norm": 0.060956455767154694,
      "learning_rate": 4.782080988626458e-06,
      "loss": 0.0008,
      "step": 2618
    },
    {
      "epoch": 108.84,
      "grad_norm": 0.1771969497203827,
      "learning_rate": 4.778878372989569e-06,
      "loss": 0.001,
      "step": 2619
    },
    {
      "epoch": 108.88,
      "grad_norm": 0.0056956070475280285,
      "learning_rate": 4.775675848247427e-06,
      "loss": 0.0003,
      "step": 2620
    },
    {
      "epoch": 108.92,
      "grad_norm": 0.10010135173797607,
      "learning_rate": 4.772473415716471e-06,
      "loss": 0.0008,
      "step": 2621
    },
    {
      "epoch": 108.97,
      "grad_norm": 0.13037516176700592,
      "learning_rate": 4.7692710767130975e-06,
      "loss": 0.0018,
      "step": 2622
    },
    {
      "epoch": 109.01,
      "grad_norm": 0.07721994817256927,
      "learning_rate": 4.76606883255367e-06,
      "loss": 0.0009,
      "step": 2623
    },
    {
      "epoch": 109.05,
      "grad_norm": 0.03458229824900627,
      "learning_rate": 4.762866684554509e-06,
      "loss": 0.0005,
      "step": 2624
    },
    {
      "epoch": 109.09,
      "grad_norm": 0.005636407528072596,
      "learning_rate": 4.759664634031897e-06,
      "loss": 0.0003,
      "step": 2625
    },
    {
      "epoch": 109.13,
      "grad_norm": 0.04013273864984512,
      "learning_rate": 4.756462682302076e-06,
      "loss": 0.0005,
      "step": 2626
    },
    {
      "epoch": 109.17,
      "grad_norm": 0.03994550183415413,
      "learning_rate": 4.753260830681247e-06,
      "loss": 0.0005,
      "step": 2627
    },
    {
      "epoch": 109.22,
      "grad_norm": 0.01992262899875641,
      "learning_rate": 4.7500590804855695e-06,
      "loss": 0.0004,
      "step": 2628
    },
    {
      "epoch": 109.26,
      "grad_norm": 0.03483074530959129,
      "learning_rate": 4.746857433031163e-06,
      "loss": 0.0005,
      "step": 2629
    },
    {
      "epoch": 109.3,
      "grad_norm": 0.048838529735803604,
      "learning_rate": 4.743655889634105e-06,
      "loss": 0.0008,
      "step": 2630
    },
    {
      "epoch": 109.34,
      "grad_norm": 0.005660901311784983,
      "learning_rate": 4.740454451610427e-06,
      "loss": 0.0003,
      "step": 2631
    },
    {
      "epoch": 109.38,
      "grad_norm": 0.04883021116256714,
      "learning_rate": 4.73725312027612e-06,
      "loss": 0.0008,
      "step": 2632
    },
    {
      "epoch": 109.42,
      "grad_norm": 0.13110749423503876,
      "learning_rate": 4.73405189694713e-06,
      "loss": 0.0013,
      "step": 2633
    },
    {
      "epoch": 109.46,
      "grad_norm": 0.05895597115159035,
      "learning_rate": 4.73085078293936e-06,
      "loss": 0.0008,
      "step": 2634
    },
    {
      "epoch": 109.51,
      "grad_norm": 0.11415724456310272,
      "learning_rate": 4.727649779568666e-06,
      "loss": 0.001,
      "step": 2635
    },
    {
      "epoch": 109.55,
      "grad_norm": 0.057120420038700104,
      "learning_rate": 4.724448888150861e-06,
      "loss": 0.0006,
      "step": 2636
    },
    {
      "epoch": 109.59,
      "grad_norm": 0.07936999201774597,
      "learning_rate": 4.7212481100017085e-06,
      "loss": 0.0009,
      "step": 2637
    },
    {
      "epoch": 109.63,
      "grad_norm": 0.03531508520245552,
      "learning_rate": 4.718047446436929e-06,
      "loss": 0.0007,
      "step": 2638
    },
    {
      "epoch": 109.67,
      "grad_norm": 0.06946062296628952,
      "learning_rate": 4.714846898772194e-06,
      "loss": 0.0007,
      "step": 2639
    },
    {
      "epoch": 109.71,
      "grad_norm": 0.07707934081554413,
      "learning_rate": 4.711646468323129e-06,
      "loss": 0.0009,
      "step": 2640
    },
    {
      "epoch": 109.76,
      "grad_norm": 0.0059598335064947605,
      "learning_rate": 4.708446156405308e-06,
      "loss": 0.0003,
      "step": 2641
    },
    {
      "epoch": 109.8,
      "grad_norm": 0.19706828892230988,
      "learning_rate": 4.7052459643342576e-06,
      "loss": 0.0017,
      "step": 2642
    },
    {
      "epoch": 109.84,
      "grad_norm": 0.1531810611486435,
      "learning_rate": 4.702045893425459e-06,
      "loss": 0.0013,
      "step": 2643
    },
    {
      "epoch": 109.88,
      "grad_norm": 0.005763858091086149,
      "learning_rate": 4.698845944994338e-06,
      "loss": 0.0003,
      "step": 2644
    },
    {
      "epoch": 109.92,
      "grad_norm": 0.006434364710003138,
      "learning_rate": 4.695646120356275e-06,
      "loss": 0.0003,
      "step": 2645
    },
    {
      "epoch": 109.96,
      "grad_norm": 0.06239241361618042,
      "learning_rate": 4.6924464208265945e-06,
      "loss": 0.0007,
      "step": 2646
    },
    {
      "epoch": 110.01,
      "grad_norm": 0.029712842777371407,
      "learning_rate": 4.689246847720572e-06,
      "loss": 0.0005,
      "step": 2647
    },
    {
      "epoch": 110.05,
      "grad_norm": 0.034819167107343674,
      "learning_rate": 4.686047402353433e-06,
      "loss": 0.0004,
      "step": 2648
    },
    {
      "epoch": 110.09,
      "grad_norm": 0.028954923152923584,
      "learning_rate": 4.682848086040348e-06,
      "loss": 0.0005,
      "step": 2649
    },
    {
      "epoch": 110.13,
      "grad_norm": 0.031258657574653625,
      "learning_rate": 4.679648900096436e-06,
      "loss": 0.0008,
      "step": 2650
    },
    {
      "epoch": 110.17,
      "grad_norm": 0.0724731832742691,
      "learning_rate": 4.676449845836759e-06,
      "loss": 0.0007,
      "step": 2651
    },
    {
      "epoch": 110.21,
      "grad_norm": 0.05435954034328461,
      "learning_rate": 4.67325092457633e-06,
      "loss": 0.0006,
      "step": 2652
    },
    {
      "epoch": 110.25,
      "grad_norm": 0.09809920191764832,
      "learning_rate": 4.670052137630103e-06,
      "loss": 0.0009,
      "step": 2653
    },
    {
      "epoch": 110.3,
      "grad_norm": 0.05246347188949585,
      "learning_rate": 4.666853486312978e-06,
      "loss": 0.0006,
      "step": 2654
    },
    {
      "epoch": 110.34,
      "grad_norm": 0.08028236776590347,
      "learning_rate": 4.663654971939802e-06,
      "loss": 0.0007,
      "step": 2655
    },
    {
      "epoch": 110.38,
      "grad_norm": 0.031101783737540245,
      "learning_rate": 4.660456595825362e-06,
      "loss": 0.0005,
      "step": 2656
    },
    {
      "epoch": 110.42,
      "grad_norm": 0.09823598712682724,
      "learning_rate": 4.657258359284389e-06,
      "loss": 0.0009,
      "step": 2657
    },
    {
      "epoch": 110.46,
      "grad_norm": 0.06629036366939545,
      "learning_rate": 4.654060263631558e-06,
      "loss": 0.0008,
      "step": 2658
    },
    {
      "epoch": 110.5,
      "grad_norm": 0.054827671498060226,
      "learning_rate": 4.650862310181487e-06,
      "loss": 0.0006,
      "step": 2659
    },
    {
      "epoch": 110.55,
      "grad_norm": 0.03536731377243996,
      "learning_rate": 4.64766450024873e-06,
      "loss": 0.0006,
      "step": 2660
    },
    {
      "epoch": 110.59,
      "grad_norm": 0.061279408633708954,
      "learning_rate": 4.64446683514779e-06,
      "loss": 0.0007,
      "step": 2661
    },
    {
      "epoch": 110.63,
      "grad_norm": 0.04592806100845337,
      "learning_rate": 4.641269316193104e-06,
      "loss": 0.0007,
      "step": 2662
    },
    {
      "epoch": 110.67,
      "grad_norm": 0.06785792857408524,
      "learning_rate": 4.638071944699051e-06,
      "loss": 0.0007,
      "step": 2663
    },
    {
      "epoch": 110.71,
      "grad_norm": 0.005606199149042368,
      "learning_rate": 4.634874721979952e-06,
      "loss": 0.0003,
      "step": 2664
    },
    {
      "epoch": 110.75,
      "grad_norm": 0.04203706234693527,
      "learning_rate": 4.6316776493500615e-06,
      "loss": 0.0006,
      "step": 2665
    },
    {
      "epoch": 110.79,
      "grad_norm": 0.005706154275685549,
      "learning_rate": 4.628480728123576e-06,
      "loss": 0.0003,
      "step": 2666
    },
    {
      "epoch": 110.84,
      "grad_norm": 0.12528249621391296,
      "learning_rate": 4.6252839596146295e-06,
      "loss": 0.001,
      "step": 2667
    },
    {
      "epoch": 110.88,
      "grad_norm": 0.09612969309091568,
      "learning_rate": 4.622087345137295e-06,
      "loss": 0.0011,
      "step": 2668
    },
    {
      "epoch": 110.92,
      "grad_norm": 0.005637096706777811,
      "learning_rate": 4.618890886005576e-06,
      "loss": 0.0003,
      "step": 2669
    },
    {
      "epoch": 110.96,
      "grad_norm": 0.022893216460943222,
      "learning_rate": 4.615694583533418e-06,
      "loss": 0.0005,
      "step": 2670
    },
    {
      "epoch": 111.0,
      "grad_norm": 0.0854220762848854,
      "learning_rate": 4.612498439034701e-06,
      "loss": 0.0006,
      "step": 2671
    },
    {
      "epoch": 111.04,
      "grad_norm": 0.05375312641263008,
      "learning_rate": 4.609302453823238e-06,
      "loss": 0.0006,
      "step": 2672
    },
    {
      "epoch": 111.09,
      "grad_norm": 0.0051818485371768475,
      "learning_rate": 4.606106629212779e-06,
      "loss": 0.0003,
      "step": 2673
    },
    {
      "epoch": 111.13,
      "grad_norm": 0.07705170661211014,
      "learning_rate": 4.602910966517006e-06,
      "loss": 0.0009,
      "step": 2674
    },
    {
      "epoch": 111.17,
      "grad_norm": 0.05008320510387421,
      "learning_rate": 4.599715467049534e-06,
      "loss": 0.0006,
      "step": 2675
    },
    {
      "epoch": 111.21,
      "grad_norm": 0.006002333480864763,
      "learning_rate": 4.596520132123915e-06,
      "loss": 0.0005,
      "step": 2676
    },
    {
      "epoch": 111.25,
      "grad_norm": 0.06905192881822586,
      "learning_rate": 4.593324963053628e-06,
      "loss": 0.0008,
      "step": 2677
    },
    {
      "epoch": 111.29,
      "grad_norm": 0.0795455202460289,
      "learning_rate": 4.590129961152087e-06,
      "loss": 0.0007,
      "step": 2678
    },
    {
      "epoch": 111.34,
      "grad_norm": 0.09134624153375626,
      "learning_rate": 4.586935127732638e-06,
      "loss": 0.0009,
      "step": 2679
    },
    {
      "epoch": 111.38,
      "grad_norm": 0.018785687163472176,
      "learning_rate": 4.583740464108554e-06,
      "loss": 0.0004,
      "step": 2680
    },
    {
      "epoch": 111.42,
      "grad_norm": 0.02157568745315075,
      "learning_rate": 4.580545971593042e-06,
      "loss": 0.0004,
      "step": 2681
    },
    {
      "epoch": 111.46,
      "grad_norm": 0.09243394434452057,
      "learning_rate": 4.577351651499237e-06,
      "loss": 0.0009,
      "step": 2682
    },
    {
      "epoch": 111.5,
      "grad_norm": 0.07893971353769302,
      "learning_rate": 4.574157505140204e-06,
      "loss": 0.001,
      "step": 2683
    },
    {
      "epoch": 111.54,
      "grad_norm": 0.1063607707619667,
      "learning_rate": 4.570963533828934e-06,
      "loss": 0.0008,
      "step": 2684
    },
    {
      "epoch": 111.58,
      "grad_norm": 0.06050211563706398,
      "learning_rate": 4.56776973887835e-06,
      "loss": 0.0006,
      "step": 2685
    },
    {
      "epoch": 111.63,
      "grad_norm": 0.005930610466748476,
      "learning_rate": 4.564576121601298e-06,
      "loss": 0.0003,
      "step": 2686
    },
    {
      "epoch": 111.67,
      "grad_norm": 0.10080747306346893,
      "learning_rate": 4.561382683310557e-06,
      "loss": 0.001,
      "step": 2687
    },
    {
      "epoch": 111.71,
      "grad_norm": 0.04272371903061867,
      "learning_rate": 4.558189425318826e-06,
      "loss": 0.0005,
      "step": 2688
    },
    {
      "epoch": 111.75,
      "grad_norm": 0.0416577085852623,
      "learning_rate": 4.554996348938734e-06,
      "loss": 0.0005,
      "step": 2689
    },
    {
      "epoch": 111.79,
      "grad_norm": 0.07060468196868896,
      "learning_rate": 4.551803455482833e-06,
      "loss": 0.0009,
      "step": 2690
    },
    {
      "epoch": 111.83,
      "grad_norm": 0.14180278778076172,
      "learning_rate": 4.548610746263602e-06,
      "loss": 0.0009,
      "step": 2691
    },
    {
      "epoch": 111.88,
      "grad_norm": 0.03390995040535927,
      "learning_rate": 4.545418222593442e-06,
      "loss": 0.0005,
      "step": 2692
    },
    {
      "epoch": 111.92,
      "grad_norm": 0.04901859909296036,
      "learning_rate": 4.54222588578468e-06,
      "loss": 0.0006,
      "step": 2693
    },
    {
      "epoch": 111.96,
      "grad_norm": 0.01068879570811987,
      "learning_rate": 4.5390337371495665e-06,
      "loss": 0.0003,
      "step": 2694
    },
    {
      "epoch": 112.0,
      "grad_norm": 0.06427063792943954,
      "learning_rate": 4.53584177800027e-06,
      "loss": 0.0007,
      "step": 2695
    },
    {
      "epoch": 112.04,
      "grad_norm": 0.005857174750417471,
      "learning_rate": 4.532650009648885e-06,
      "loss": 0.0003,
      "step": 2696
    },
    {
      "epoch": 112.08,
      "grad_norm": 0.07701626420021057,
      "learning_rate": 4.529458433407429e-06,
      "loss": 0.0007,
      "step": 2697
    },
    {
      "epoch": 112.12,
      "grad_norm": 0.10042920708656311,
      "learning_rate": 4.526267050587836e-06,
      "loss": 0.0009,
      "step": 2698
    },
    {
      "epoch": 112.17,
      "grad_norm": 0.10687617212533951,
      "learning_rate": 4.523075862501965e-06,
      "loss": 0.001,
      "step": 2699
    },
    {
      "epoch": 112.21,
      "grad_norm": 0.025653371587395668,
      "learning_rate": 4.5198848704615915e-06,
      "loss": 0.0004,
      "step": 2700
    },
    {
      "epoch": 112.25,
      "grad_norm": 0.018517693504691124,
      "learning_rate": 4.516694075778412e-06,
      "loss": 0.0004,
      "step": 2701
    },
    {
      "epoch": 112.29,
      "grad_norm": 0.05674133822321892,
      "learning_rate": 4.513503479764042e-06,
      "loss": 0.0007,
      "step": 2702
    },
    {
      "epoch": 112.33,
      "grad_norm": 0.3496610224246979,
      "learning_rate": 4.5103130837300145e-06,
      "loss": 0.0021,
      "step": 2703
    },
    {
      "epoch": 112.37,
      "grad_norm": 0.03344876691699028,
      "learning_rate": 4.507122888987782e-06,
      "loss": 0.0006,
      "step": 2704
    },
    {
      "epoch": 112.42,
      "grad_norm": 0.005301880184561014,
      "learning_rate": 4.503932896848713e-06,
      "loss": 0.0003,
      "step": 2705
    },
    {
      "epoch": 112.46,
      "grad_norm": 0.05281827971339226,
      "learning_rate": 4.5007431086240905e-06,
      "loss": 0.0006,
      "step": 2706
    },
    {
      "epoch": 112.5,
      "grad_norm": 0.062294524163007736,
      "learning_rate": 4.497553525625118e-06,
      "loss": 0.0007,
      "step": 2707
    },
    {
      "epoch": 112.54,
      "grad_norm": 0.04196162149310112,
      "learning_rate": 4.4943641491629115e-06,
      "loss": 0.0005,
      "step": 2708
    },
    {
      "epoch": 112.58,
      "grad_norm": 0.023395376279950142,
      "learning_rate": 4.491174980548506e-06,
      "loss": 0.0004,
      "step": 2709
    },
    {
      "epoch": 112.62,
      "grad_norm": 0.01435163989663124,
      "learning_rate": 4.487986021092844e-06,
      "loss": 0.0003,
      "step": 2710
    },
    {
      "epoch": 112.66,
      "grad_norm": 0.07412312924861908,
      "learning_rate": 4.48479727210679e-06,
      "loss": 0.0007,
      "step": 2711
    },
    {
      "epoch": 112.71,
      "grad_norm": 0.0289787445217371,
      "learning_rate": 4.481608734901114e-06,
      "loss": 0.0004,
      "step": 2712
    },
    {
      "epoch": 112.75,
      "grad_norm": 0.05516990274190903,
      "learning_rate": 4.478420410786507e-06,
      "loss": 0.0008,
      "step": 2713
    },
    {
      "epoch": 112.79,
      "grad_norm": 0.09876454621553421,
      "learning_rate": 4.475232301073567e-06,
      "loss": 0.0011,
      "step": 2714
    },
    {
      "epoch": 112.83,
      "grad_norm": 0.08220571279525757,
      "learning_rate": 4.472044407072805e-06,
      "loss": 0.001,
      "step": 2715
    },
    {
      "epoch": 112.87,
      "grad_norm": 0.09602547436952591,
      "learning_rate": 4.468856730094646e-06,
      "loss": 0.0012,
      "step": 2716
    },
    {
      "epoch": 112.91,
      "grad_norm": 0.0342269130051136,
      "learning_rate": 4.4656692714494224e-06,
      "loss": 0.0005,
      "step": 2717
    },
    {
      "epoch": 112.96,
      "grad_norm": 0.07120099663734436,
      "learning_rate": 4.462482032447377e-06,
      "loss": 0.0007,
      "step": 2718
    },
    {
      "epoch": 113.0,
      "grad_norm": 0.011738021858036518,
      "learning_rate": 4.459295014398664e-06,
      "loss": 0.0003,
      "step": 2719
    },
    {
      "epoch": 113.04,
      "grad_norm": 0.06776927411556244,
      "learning_rate": 4.456108218613346e-06,
      "loss": 0.0008,
      "step": 2720
    },
    {
      "epoch": 113.08,
      "grad_norm": 0.04950382560491562,
      "learning_rate": 4.4529216464013955e-06,
      "loss": 0.0006,
      "step": 2721
    },
    {
      "epoch": 113.12,
      "grad_norm": 0.01715923845767975,
      "learning_rate": 4.4497352990726926e-06,
      "loss": 0.0004,
      "step": 2722
    },
    {
      "epoch": 113.16,
      "grad_norm": 0.005918539594858885,
      "learning_rate": 4.446549177937021e-06,
      "loss": 0.0003,
      "step": 2723
    },
    {
      "epoch": 113.21,
      "grad_norm": 0.04181697592139244,
      "learning_rate": 4.443363284304077e-06,
      "loss": 0.0005,
      "step": 2724
    },
    {
      "epoch": 113.25,
      "grad_norm": 0.048122890293598175,
      "learning_rate": 4.4401776194834615e-06,
      "loss": 0.0006,
      "step": 2725
    },
    {
      "epoch": 113.29,
      "grad_norm": 0.02544490620493889,
      "learning_rate": 4.43699218478468e-06,
      "loss": 0.0004,
      "step": 2726
    },
    {
      "epoch": 113.33,
      "grad_norm": 0.10536961257457733,
      "learning_rate": 4.4338069815171446e-06,
      "loss": 0.0009,
      "step": 2727
    },
    {
      "epoch": 113.37,
      "grad_norm": 0.03325781971216202,
      "learning_rate": 4.430622010990172e-06,
      "loss": 0.0004,
      "step": 2728
    },
    {
      "epoch": 113.41,
      "grad_norm": 0.02353895828127861,
      "learning_rate": 4.427437274512983e-06,
      "loss": 0.0004,
      "step": 2729
    },
    {
      "epoch": 113.45,
      "grad_norm": 0.07686319202184677,
      "learning_rate": 4.424252773394704e-06,
      "loss": 0.0008,
      "step": 2730
    },
    {
      "epoch": 113.5,
      "grad_norm": 0.07622690498828888,
      "learning_rate": 4.421068508944361e-06,
      "loss": 0.0006,
      "step": 2731
    },
    {
      "epoch": 113.54,
      "grad_norm": 0.05682264640927315,
      "learning_rate": 4.417884482470887e-06,
      "loss": 0.0006,
      "step": 2732
    },
    {
      "epoch": 113.58,
      "grad_norm": 0.07900875806808472,
      "learning_rate": 4.414700695283113e-06,
      "loss": 0.0008,
      "step": 2733
    },
    {
      "epoch": 113.62,
      "grad_norm": 0.06994276493787766,
      "learning_rate": 4.411517148689774e-06,
      "loss": 0.0007,
      "step": 2734
    },
    {
      "epoch": 113.66,
      "grad_norm": 0.08376303315162659,
      "learning_rate": 4.408333843999506e-06,
      "loss": 0.0009,
      "step": 2735
    },
    {
      "epoch": 113.7,
      "grad_norm": 0.005403253249824047,
      "learning_rate": 4.405150782520846e-06,
      "loss": 0.0003,
      "step": 2736
    },
    {
      "epoch": 113.75,
      "grad_norm": 0.005461003631353378,
      "learning_rate": 4.401967965562232e-06,
      "loss": 0.0003,
      "step": 2737
    },
    {
      "epoch": 113.79,
      "grad_norm": 0.11883273720741272,
      "learning_rate": 4.398785394431995e-06,
      "loss": 0.0016,
      "step": 2738
    },
    {
      "epoch": 113.83,
      "grad_norm": 0.031171001493930817,
      "learning_rate": 4.395603070438373e-06,
      "loss": 0.0004,
      "step": 2739
    },
    {
      "epoch": 113.87,
      "grad_norm": 0.026992766186594963,
      "learning_rate": 4.392420994889498e-06,
      "loss": 0.0005,
      "step": 2740
    },
    {
      "epoch": 113.91,
      "grad_norm": 0.005640668794512749,
      "learning_rate": 4.3892391690934035e-06,
      "loss": 0.0003,
      "step": 2741
    },
    {
      "epoch": 113.95,
      "grad_norm": 0.039304040372371674,
      "learning_rate": 4.386057594358017e-06,
      "loss": 0.0006,
      "step": 2742
    },
    {
      "epoch": 113.99,
      "grad_norm": 0.1429915577173233,
      "learning_rate": 4.38287627199116e-06,
      "loss": 0.0014,
      "step": 2743
    },
    {
      "epoch": 114.04,
      "grad_norm": 0.0359007902443409,
      "learning_rate": 4.379695203300558e-06,
      "loss": 0.0004,
      "step": 2744
    },
    {
      "epoch": 114.08,
      "grad_norm": 0.024847976863384247,
      "learning_rate": 4.376514389593827e-06,
      "loss": 0.0004,
      "step": 2745
    },
    {
      "epoch": 114.12,
      "grad_norm": 0.011473449878394604,
      "learning_rate": 4.373333832178478e-06,
      "loss": 0.0003,
      "step": 2746
    },
    {
      "epoch": 114.16,
      "grad_norm": 0.11860762536525726,
      "learning_rate": 4.370153532361921e-06,
      "loss": 0.0012,
      "step": 2747
    },
    {
      "epoch": 114.2,
      "grad_norm": 0.038268543779850006,
      "learning_rate": 4.366973491451456e-06,
      "loss": 0.0005,
      "step": 2748
    },
    {
      "epoch": 114.24,
      "grad_norm": 0.03715718165040016,
      "learning_rate": 4.363793710754276e-06,
      "loss": 0.0005,
      "step": 2749
    },
    {
      "epoch": 114.29,
      "grad_norm": 0.09668929874897003,
      "learning_rate": 4.3606141915774695e-06,
      "loss": 0.0009,
      "step": 2750
    },
    {
      "epoch": 114.33,
      "grad_norm": 0.04259355366230011,
      "learning_rate": 4.357434935228017e-06,
      "loss": 0.0006,
      "step": 2751
    },
    {
      "epoch": 114.37,
      "grad_norm": 0.028727343305945396,
      "learning_rate": 4.354255943012793e-06,
      "loss": 0.0004,
      "step": 2752
    },
    {
      "epoch": 114.41,
      "grad_norm": 0.03332978114485741,
      "learning_rate": 4.351077216238558e-06,
      "loss": 0.0005,
      "step": 2753
    },
    {
      "epoch": 114.45,
      "grad_norm": 0.09513678401708603,
      "learning_rate": 4.347898756211966e-06,
      "loss": 0.0008,
      "step": 2754
    },
    {
      "epoch": 114.49,
      "grad_norm": 0.11237692832946777,
      "learning_rate": 4.344720564239567e-06,
      "loss": 0.0012,
      "step": 2755
    },
    {
      "epoch": 114.54,
      "grad_norm": 0.08311570435762405,
      "learning_rate": 4.3415426416277885e-06,
      "loss": 0.0007,
      "step": 2756
    },
    {
      "epoch": 114.58,
      "grad_norm": 0.005071586929261684,
      "learning_rate": 4.338364989682959e-06,
      "loss": 0.0003,
      "step": 2757
    },
    {
      "epoch": 114.62,
      "grad_norm": 0.06932207942008972,
      "learning_rate": 4.335187609711289e-06,
      "loss": 0.0009,
      "step": 2758
    },
    {
      "epoch": 114.66,
      "grad_norm": 0.029819978401064873,
      "learning_rate": 4.332010503018881e-06,
      "loss": 0.0004,
      "step": 2759
    },
    {
      "epoch": 114.7,
      "grad_norm": 0.09312459081411362,
      "learning_rate": 4.3288336709117246e-06,
      "loss": 0.0008,
      "step": 2760
    },
    {
      "epoch": 114.74,
      "grad_norm": 0.08357355743646622,
      "learning_rate": 4.325657114695692e-06,
      "loss": 0.0006,
      "step": 2761
    },
    {
      "epoch": 114.78,
      "grad_norm": 0.03879746049642563,
      "learning_rate": 4.322480835676548e-06,
      "loss": 0.0005,
      "step": 2762
    },
    {
      "epoch": 114.83,
      "grad_norm": 0.03660295158624649,
      "learning_rate": 4.3193048351599395e-06,
      "loss": 0.0004,
      "step": 2763
    },
    {
      "epoch": 114.87,
      "grad_norm": 0.004956481512635946,
      "learning_rate": 4.316129114451402e-06,
      "loss": 0.0003,
      "step": 2764
    },
    {
      "epoch": 114.91,
      "grad_norm": 0.10226940363645554,
      "learning_rate": 4.312953674856355e-06,
      "loss": 0.0008,
      "step": 2765
    },
    {
      "epoch": 114.95,
      "grad_norm": 0.005729637108743191,
      "learning_rate": 4.3097785176800985e-06,
      "loss": 0.0003,
      "step": 2766
    },
    {
      "epoch": 114.99,
      "grad_norm": 0.09090711921453476,
      "learning_rate": 4.306603644227821e-06,
      "loss": 0.001,
      "step": 2767
    },
    {
      "epoch": 115.03,
      "grad_norm": 0.049682047218084335,
      "learning_rate": 4.303429055804594e-06,
      "loss": 0.0006,
      "step": 2768
    },
    {
      "epoch": 115.08,
      "grad_norm": 0.07487627118825912,
      "learning_rate": 4.300254753715371e-06,
      "loss": 0.0007,
      "step": 2769
    },
    {
      "epoch": 115.12,
      "grad_norm": 0.11360049992799759,
      "learning_rate": 4.297080739264987e-06,
      "loss": 0.0011,
      "step": 2770
    },
    {
      "epoch": 115.16,
      "grad_norm": 0.023697279393672943,
      "learning_rate": 4.2939070137581584e-06,
      "loss": 0.0004,
      "step": 2771
    },
    {
      "epoch": 115.2,
      "grad_norm": 0.03572924807667732,
      "learning_rate": 4.290733578499486e-06,
      "loss": 0.0006,
      "step": 2772
    },
    {
      "epoch": 115.24,
      "grad_norm": 0.08607444167137146,
      "learning_rate": 4.287560434793448e-06,
      "loss": 0.0007,
      "step": 2773
    },
    {
      "epoch": 115.28,
      "grad_norm": 0.21992285549640656,
      "learning_rate": 4.284387583944403e-06,
      "loss": 0.0012,
      "step": 2774
    },
    {
      "epoch": 115.32,
      "grad_norm": 0.06165506690740585,
      "learning_rate": 4.281215027256592e-06,
      "loss": 0.0006,
      "step": 2775
    },
    {
      "epoch": 115.37,
      "grad_norm": 0.03714524954557419,
      "learning_rate": 4.278042766034134e-06,
      "loss": 0.0006,
      "step": 2776
    },
    {
      "epoch": 115.41,
      "grad_norm": 0.061571747064590454,
      "learning_rate": 4.274870801581021e-06,
      "loss": 0.0008,
      "step": 2777
    },
    {
      "epoch": 115.45,
      "grad_norm": 0.0551631785929203,
      "learning_rate": 4.271699135201133e-06,
      "loss": 0.0007,
      "step": 2778
    },
    {
      "epoch": 115.49,
      "grad_norm": 0.13067194819450378,
      "learning_rate": 4.26852776819822e-06,
      "loss": 0.0009,
      "step": 2779
    },
    {
      "epoch": 115.53,
      "grad_norm": 0.08843909204006195,
      "learning_rate": 4.265356701875911e-06,
      "loss": 0.0009,
      "step": 2780
    },
    {
      "epoch": 115.57,
      "grad_norm": 0.10460763424634933,
      "learning_rate": 4.262185937537713e-06,
      "loss": 0.001,
      "step": 2781
    },
    {
      "epoch": 115.62,
      "grad_norm": 0.05931548401713371,
      "learning_rate": 4.259015476487005e-06,
      "loss": 0.0006,
      "step": 2782
    },
    {
      "epoch": 115.66,
      "grad_norm": 0.005400096997618675,
      "learning_rate": 4.255845320027046e-06,
      "loss": 0.0003,
      "step": 2783
    },
    {
      "epoch": 115.7,
      "grad_norm": 0.042119599878787994,
      "learning_rate": 4.252675469460965e-06,
      "loss": 0.0005,
      "step": 2784
    },
    {
      "epoch": 115.74,
      "grad_norm": 0.02332010120153427,
      "learning_rate": 4.249505926091771e-06,
      "loss": 0.0004,
      "step": 2785
    },
    {
      "epoch": 115.78,
      "grad_norm": 0.005366088822484016,
      "learning_rate": 4.246336691222343e-06,
      "loss": 0.0003,
      "step": 2786
    },
    {
      "epoch": 115.82,
      "grad_norm": 0.027355022728443146,
      "learning_rate": 4.243167766155433e-06,
      "loss": 0.0004,
      "step": 2787
    },
    {
      "epoch": 115.86,
      "grad_norm": 0.054028041660785675,
      "learning_rate": 4.239999152193664e-06,
      "loss": 0.0007,
      "step": 2788
    },
    {
      "epoch": 115.91,
      "grad_norm": 0.08434445410966873,
      "learning_rate": 4.2368308506395375e-06,
      "loss": 0.0009,
      "step": 2789
    },
    {
      "epoch": 115.95,
      "grad_norm": 0.005479054059833288,
      "learning_rate": 4.23366286279542e-06,
      "loss": 0.0003,
      "step": 2790
    },
    {
      "epoch": 115.99,
      "grad_norm": 0.07944341748952866,
      "learning_rate": 4.230495189963554e-06,
      "loss": 0.0009,
      "step": 2791
    },
    {
      "epoch": 116.03,
      "grad_norm": 0.049704477190971375,
      "learning_rate": 4.227327833446047e-06,
      "loss": 0.0006,
      "step": 2792
    },
    {
      "epoch": 116.07,
      "grad_norm": 0.04837307706475258,
      "learning_rate": 4.224160794544883e-06,
      "loss": 0.0006,
      "step": 2793
    },
    {
      "epoch": 116.11,
      "grad_norm": 0.057745762169361115,
      "learning_rate": 4.220994074561909e-06,
      "loss": 0.0006,
      "step": 2794
    },
    {
      "epoch": 116.16,
      "grad_norm": 0.005281738005578518,
      "learning_rate": 4.217827674798845e-06,
      "loss": 0.0003,
      "step": 2795
    },
    {
      "epoch": 116.2,
      "grad_norm": 0.03058624640107155,
      "learning_rate": 4.2146615965572804e-06,
      "loss": 0.0004,
      "step": 2796
    },
    {
      "epoch": 116.24,
      "grad_norm": 0.09742767363786697,
      "learning_rate": 4.211495841138668e-06,
      "loss": 0.0009,
      "step": 2797
    },
    {
      "epoch": 116.28,
      "grad_norm": 0.026425184682011604,
      "learning_rate": 4.20833040984433e-06,
      "loss": 0.0004,
      "step": 2798
    },
    {
      "epoch": 116.32,
      "grad_norm": 0.046333953738212585,
      "learning_rate": 4.205165303975457e-06,
      "loss": 0.0007,
      "step": 2799
    },
    {
      "epoch": 116.36,
      "grad_norm": 0.026543252170085907,
      "learning_rate": 4.2020005248331056e-06,
      "loss": 0.0004,
      "step": 2800
    },
    {
      "epoch": 116.41,
      "grad_norm": 0.040821801871061325,
      "learning_rate": 4.1988360737181935e-06,
      "loss": 0.0005,
      "step": 2801
    },
    {
      "epoch": 116.45,
      "grad_norm": 0.026900140568614006,
      "learning_rate": 4.195671951931509e-06,
      "loss": 0.0004,
      "step": 2802
    },
    {
      "epoch": 116.49,
      "grad_norm": 0.05692440643906593,
      "learning_rate": 4.192508160773703e-06,
      "loss": 0.0005,
      "step": 2803
    },
    {
      "epoch": 116.53,
      "grad_norm": 0.08877189457416534,
      "learning_rate": 4.189344701545291e-06,
      "loss": 0.0008,
      "step": 2804
    },
    {
      "epoch": 116.57,
      "grad_norm": 0.05937286466360092,
      "learning_rate": 4.186181575546651e-06,
      "loss": 0.0005,
      "step": 2805
    },
    {
      "epoch": 116.61,
      "grad_norm": 0.03210151940584183,
      "learning_rate": 4.1830187840780236e-06,
      "loss": 0.0004,
      "step": 2806
    },
    {
      "epoch": 116.65,
      "grad_norm": 0.0964224711060524,
      "learning_rate": 4.179856328439515e-06,
      "loss": 0.0008,
      "step": 2807
    },
    {
      "epoch": 116.7,
      "grad_norm": 0.20259132981300354,
      "learning_rate": 4.176694209931089e-06,
      "loss": 0.0018,
      "step": 2808
    },
    {
      "epoch": 116.74,
      "grad_norm": 0.004921429790556431,
      "learning_rate": 4.1735324298525765e-06,
      "loss": 0.0003,
      "step": 2809
    },
    {
      "epoch": 116.78,
      "grad_norm": 0.06843005865812302,
      "learning_rate": 4.170370989503662e-06,
      "loss": 0.0006,
      "step": 2810
    },
    {
      "epoch": 116.82,
      "grad_norm": 0.03667975962162018,
      "learning_rate": 4.1672098901838965e-06,
      "loss": 0.0005,
      "step": 2811
    },
    {
      "epoch": 116.86,
      "grad_norm": 0.07678966969251633,
      "learning_rate": 4.164049133192688e-06,
      "loss": 0.0008,
      "step": 2812
    },
    {
      "epoch": 116.9,
      "grad_norm": 0.07717493921518326,
      "learning_rate": 4.160888719829303e-06,
      "loss": 0.0006,
      "step": 2813
    },
    {
      "epoch": 116.95,
      "grad_norm": 0.04285542294383049,
      "learning_rate": 4.1577286513928715e-06,
      "loss": 0.0005,
      "step": 2814
    },
    {
      "epoch": 116.99,
      "grad_norm": 0.06323760747909546,
      "learning_rate": 4.154568929182374e-06,
      "loss": 0.0008,
      "step": 2815
    },
    {
      "epoch": 117.03,
      "grad_norm": 0.02385510876774788,
      "learning_rate": 4.1514095544966556e-06,
      "loss": 0.0004,
      "step": 2816
    },
    {
      "epoch": 117.07,
      "grad_norm": 0.0744471326470375,
      "learning_rate": 4.148250528634416e-06,
      "loss": 0.0008,
      "step": 2817
    },
    {
      "epoch": 117.11,
      "grad_norm": 0.062268082052469254,
      "learning_rate": 4.145091852894209e-06,
      "loss": 0.0006,
      "step": 2818
    },
    {
      "epoch": 117.15,
      "grad_norm": 0.08023480325937271,
      "learning_rate": 4.141933528574451e-06,
      "loss": 0.0008,
      "step": 2819
    },
    {
      "epoch": 117.19,
      "grad_norm": 0.09062670171260834,
      "learning_rate": 4.138775556973406e-06,
      "loss": 0.0008,
      "step": 2820
    },
    {
      "epoch": 117.24,
      "grad_norm": 0.005394638981670141,
      "learning_rate": 4.135617939389198e-06,
      "loss": 0.0003,
      "step": 2821
    },
    {
      "epoch": 117.28,
      "grad_norm": 0.06447003036737442,
      "learning_rate": 4.132460677119805e-06,
      "loss": 0.0005,
      "step": 2822
    },
    {
      "epoch": 117.32,
      "grad_norm": 0.07611198723316193,
      "learning_rate": 4.1293037714630575e-06,
      "loss": 0.0006,
      "step": 2823
    },
    {
      "epoch": 117.36,
      "grad_norm": 0.10666460543870926,
      "learning_rate": 4.126147223716642e-06,
      "loss": 0.0008,
      "step": 2824
    },
    {
      "epoch": 117.4,
      "grad_norm": 0.07607629150152206,
      "learning_rate": 4.122991035178093e-06,
      "loss": 0.0006,
      "step": 2825
    },
    {
      "epoch": 117.44,
      "grad_norm": 0.03196030855178833,
      "learning_rate": 4.119835207144801e-06,
      "loss": 0.0004,
      "step": 2826
    },
    {
      "epoch": 117.49,
      "grad_norm": 0.03218645602464676,
      "learning_rate": 4.11667974091401e-06,
      "loss": 0.0004,
      "step": 2827
    },
    {
      "epoch": 117.53,
      "grad_norm": 0.02682926319539547,
      "learning_rate": 4.113524637782812e-06,
      "loss": 0.0003,
      "step": 2828
    },
    {
      "epoch": 117.57,
      "grad_norm": 0.05048508942127228,
      "learning_rate": 4.11036989904815e-06,
      "loss": 0.0006,
      "step": 2829
    },
    {
      "epoch": 117.61,
      "grad_norm": 0.07734538614749908,
      "learning_rate": 4.107215526006818e-06,
      "loss": 0.0007,
      "step": 2830
    },
    {
      "epoch": 117.65,
      "grad_norm": 0.133989155292511,
      "learning_rate": 4.104061519955459e-06,
      "loss": 0.0011,
      "step": 2831
    },
    {
      "epoch": 117.69,
      "grad_norm": 0.12735635042190552,
      "learning_rate": 4.100907882190567e-06,
      "loss": 0.0008,
      "step": 2832
    },
    {
      "epoch": 117.74,
      "grad_norm": 0.10341820865869522,
      "learning_rate": 4.097754614008484e-06,
      "loss": 0.0009,
      "step": 2833
    },
    {
      "epoch": 117.78,
      "grad_norm": 0.10413839668035507,
      "learning_rate": 4.094601716705398e-06,
      "loss": 0.0009,
      "step": 2834
    },
    {
      "epoch": 117.82,
      "grad_norm": 0.10215087980031967,
      "learning_rate": 4.091449191577346e-06,
      "loss": 0.001,
      "step": 2835
    },
    {
      "epoch": 117.86,
      "grad_norm": 0.005205129738897085,
      "learning_rate": 4.088297039920212e-06,
      "loss": 0.0003,
      "step": 2836
    },
    {
      "epoch": 117.9,
      "grad_norm": 0.08859607577323914,
      "learning_rate": 4.085145263029727e-06,
      "loss": 0.0011,
      "step": 2837
    },
    {
      "epoch": 117.94,
      "grad_norm": 0.004934762138873339,
      "learning_rate": 4.081993862201467e-06,
      "loss": 0.0003,
      "step": 2838
    },
    {
      "epoch": 117.98,
      "grad_norm": 0.12778811156749725,
      "learning_rate": 4.078842838730854e-06,
      "loss": 0.0014,
      "step": 2839
    },
    {
      "epoch": 118.03,
      "grad_norm": 0.025819947943091393,
      "learning_rate": 4.075692193913156e-06,
      "loss": 0.0004,
      "step": 2840
    },
    {
      "epoch": 118.07,
      "grad_norm": 0.005198732018470764,
      "learning_rate": 4.072541929043483e-06,
      "loss": 0.0003,
      "step": 2841
    },
    {
      "epoch": 118.11,
      "grad_norm": 0.07544024288654327,
      "learning_rate": 4.069392045416789e-06,
      "loss": 0.0008,
      "step": 2842
    },
    {
      "epoch": 118.15,
      "grad_norm": 0.06063232198357582,
      "learning_rate": 4.066242544327873e-06,
      "loss": 0.0006,
      "step": 2843
    },
    {
      "epoch": 118.19,
      "grad_norm": 0.03969169408082962,
      "learning_rate": 4.063093427071376e-06,
      "loss": 0.0005,
      "step": 2844
    },
    {
      "epoch": 118.23,
      "grad_norm": 0.08428238332271576,
      "learning_rate": 4.059944694941783e-06,
      "loss": 0.0006,
      "step": 2845
    },
    {
      "epoch": 118.28,
      "grad_norm": 0.04605025798082352,
      "learning_rate": 4.056796349233415e-06,
      "loss": 0.0004,
      "step": 2846
    },
    {
      "epoch": 118.32,
      "grad_norm": 0.06114673241972923,
      "learning_rate": 4.053648391240441e-06,
      "loss": 0.0006,
      "step": 2847
    },
    {
      "epoch": 118.36,
      "grad_norm": 0.06650592386722565,
      "learning_rate": 4.0505008222568655e-06,
      "loss": 0.0006,
      "step": 2848
    },
    {
      "epoch": 118.4,
      "grad_norm": 0.017433591187000275,
      "learning_rate": 4.0473536435765364e-06,
      "loss": 0.0004,
      "step": 2849
    },
    {
      "epoch": 118.44,
      "grad_norm": 0.0704968199133873,
      "learning_rate": 4.04420685649314e-06,
      "loss": 0.0005,
      "step": 2850
    },
    {
      "epoch": 118.48,
      "grad_norm": 0.033878594636917114,
      "learning_rate": 4.0410604623002004e-06,
      "loss": 0.0005,
      "step": 2851
    },
    {
      "epoch": 118.52,
      "grad_norm": 0.07883017510175705,
      "learning_rate": 4.037914462291085e-06,
      "loss": 0.0007,
      "step": 2852
    },
    {
      "epoch": 118.57,
      "grad_norm": 0.17304302752017975,
      "learning_rate": 4.0347688577589905e-06,
      "loss": 0.0011,
      "step": 2853
    },
    {
      "epoch": 118.61,
      "grad_norm": 0.10406019538640976,
      "learning_rate": 4.031623649996959e-06,
      "loss": 0.0012,
      "step": 2854
    },
    {
      "epoch": 118.65,
      "grad_norm": 0.059493761509656906,
      "learning_rate": 4.028478840297867e-06,
      "loss": 0.0006,
      "step": 2855
    },
    {
      "epoch": 118.69,
      "grad_norm": 0.08139576762914658,
      "learning_rate": 4.025334429954425e-06,
      "loss": 0.0007,
      "step": 2856
    },
    {
      "epoch": 118.73,
      "grad_norm": 0.03820178657770157,
      "learning_rate": 4.022190420259184e-06,
      "loss": 0.0004,
      "step": 2857
    },
    {
      "epoch": 118.77,
      "grad_norm": 0.005247462075203657,
      "learning_rate": 4.019046812504526e-06,
      "loss": 0.0003,
      "step": 2858
    },
    {
      "epoch": 118.82,
      "grad_norm": 0.08062087744474411,
      "learning_rate": 4.015903607982668e-06,
      "loss": 0.0008,
      "step": 2859
    },
    {
      "epoch": 118.86,
      "grad_norm": 0.07202687859535217,
      "learning_rate": 4.012760807985665e-06,
      "loss": 0.0006,
      "step": 2860
    },
    {
      "epoch": 118.9,
      "grad_norm": 0.07180852442979813,
      "learning_rate": 4.0096184138054024e-06,
      "loss": 0.0008,
      "step": 2861
    },
    {
      "epoch": 118.94,
      "grad_norm": 0.15132559835910797,
      "learning_rate": 4.006476426733601e-06,
      "loss": 0.0015,
      "step": 2862
    },
    {
      "epoch": 118.98,
      "grad_norm": 0.07541915774345398,
      "learning_rate": 4.003334848061811e-06,
      "loss": 0.0008,
      "step": 2863
    },
    {
      "epoch": 119.02,
      "grad_norm": 0.026462499052286148,
      "learning_rate": 4.0001936790814175e-06,
      "loss": 0.0004,
      "step": 2864
    },
    {
      "epoch": 119.06,
      "grad_norm": 0.04609457775950432,
      "learning_rate": 3.997052921083637e-06,
      "loss": 0.0004,
      "step": 2865
    },
    {
      "epoch": 119.11,
      "grad_norm": 0.03423246741294861,
      "learning_rate": 3.993912575359515e-06,
      "loss": 0.0005,
      "step": 2866
    },
    {
      "epoch": 119.15,
      "grad_norm": 0.059179503470659256,
      "learning_rate": 3.990772643199931e-06,
      "loss": 0.0008,
      "step": 2867
    },
    {
      "epoch": 119.19,
      "grad_norm": 0.0568399615585804,
      "learning_rate": 3.987633125895593e-06,
      "loss": 0.0006,
      "step": 2868
    },
    {
      "epoch": 119.23,
      "grad_norm": 0.13957437872886658,
      "learning_rate": 3.984494024737034e-06,
      "loss": 0.001,
      "step": 2869
    },
    {
      "epoch": 119.27,
      "grad_norm": 0.053651969879865646,
      "learning_rate": 3.9813553410146225e-06,
      "loss": 0.0006,
      "step": 2870
    },
    {
      "epoch": 119.31,
      "grad_norm": 0.08412037044763565,
      "learning_rate": 3.978217076018553e-06,
      "loss": 0.0009,
      "step": 2871
    },
    {
      "epoch": 119.36,
      "grad_norm": 0.038583673536777496,
      "learning_rate": 3.975079231038848e-06,
      "loss": 0.0004,
      "step": 2872
    },
    {
      "epoch": 119.4,
      "grad_norm": 0.057662975043058395,
      "learning_rate": 3.971941807365357e-06,
      "loss": 0.0007,
      "step": 2873
    },
    {
      "epoch": 119.44,
      "grad_norm": 0.06976889073848724,
      "learning_rate": 3.968804806287756e-06,
      "loss": 0.0006,
      "step": 2874
    },
    {
      "epoch": 119.48,
      "grad_norm": 0.05711056664586067,
      "learning_rate": 3.965668229095546e-06,
      "loss": 0.0006,
      "step": 2875
    },
    {
      "epoch": 119.52,
      "grad_norm": 0.034255143254995346,
      "learning_rate": 3.962532077078058e-06,
      "loss": 0.0004,
      "step": 2876
    },
    {
      "epoch": 119.56,
      "grad_norm": 0.03408696502447128,
      "learning_rate": 3.9593963515244436e-06,
      "loss": 0.0004,
      "step": 2877
    },
    {
      "epoch": 119.61,
      "grad_norm": 0.03709930181503296,
      "learning_rate": 3.9562610537236836e-06,
      "loss": 0.0005,
      "step": 2878
    },
    {
      "epoch": 119.65,
      "grad_norm": 0.048397134989500046,
      "learning_rate": 3.953126184964578e-06,
      "loss": 0.0005,
      "step": 2879
    },
    {
      "epoch": 119.69,
      "grad_norm": 0.11672603338956833,
      "learning_rate": 3.949991746535753e-06,
      "loss": 0.0009,
      "step": 2880
    },
    {
      "epoch": 119.73,
      "grad_norm": 0.07524619996547699,
      "learning_rate": 3.9468577397256594e-06,
      "loss": 0.0006,
      "step": 2881
    },
    {
      "epoch": 119.77,
      "grad_norm": 0.04534028843045235,
      "learning_rate": 3.943724165822569e-06,
      "loss": 0.0006,
      "step": 2882
    },
    {
      "epoch": 119.81,
      "grad_norm": 0.05820233374834061,
      "learning_rate": 3.940591026114575e-06,
      "loss": 0.0006,
      "step": 2883
    },
    {
      "epoch": 119.85,
      "grad_norm": 0.05793023481965065,
      "learning_rate": 3.937458321889592e-06,
      "loss": 0.0005,
      "step": 2884
    },
    {
      "epoch": 119.9,
      "grad_norm": 0.05237910524010658,
      "learning_rate": 3.934326054435358e-06,
      "loss": 0.0005,
      "step": 2885
    },
    {
      "epoch": 119.94,
      "grad_norm": 0.015492725186049938,
      "learning_rate": 3.931194225039427e-06,
      "loss": 0.0003,
      "step": 2886
    },
    {
      "epoch": 119.98,
      "grad_norm": 0.11720443516969681,
      "learning_rate": 3.928062834989179e-06,
      "loss": 0.001,
      "step": 2887
    },
    {
      "epoch": 120.02,
      "grad_norm": 0.004955093376338482,
      "learning_rate": 3.924931885571811e-06,
      "loss": 0.0003,
      "step": 2888
    },
    {
      "epoch": 120.06,
      "grad_norm": 0.21380971372127533,
      "learning_rate": 3.921801378074334e-06,
      "loss": 0.0016,
      "step": 2889
    },
    {
      "epoch": 120.1,
      "grad_norm": 0.03963266685605049,
      "learning_rate": 3.918671313783583e-06,
      "loss": 0.0004,
      "step": 2890
    },
    {
      "epoch": 120.15,
      "grad_norm": 0.07136070728302002,
      "learning_rate": 3.915541693986212e-06,
      "loss": 0.0007,
      "step": 2891
    },
    {
      "epoch": 120.19,
      "grad_norm": 0.005184154491871595,
      "learning_rate": 3.912412519968685e-06,
      "loss": 0.0003,
      "step": 2892
    },
    {
      "epoch": 120.23,
      "grad_norm": 0.026462923735380173,
      "learning_rate": 3.909283793017289e-06,
      "loss": 0.0004,
      "step": 2893
    },
    {
      "epoch": 120.27,
      "grad_norm": 0.09777019172906876,
      "learning_rate": 3.906155514418125e-06,
      "loss": 0.0007,
      "step": 2894
    },
    {
      "epoch": 120.31,
      "grad_norm": 0.06270228326320648,
      "learning_rate": 3.903027685457112e-06,
      "loss": 0.0008,
      "step": 2895
    },
    {
      "epoch": 120.35,
      "grad_norm": 0.050650790333747864,
      "learning_rate": 3.899900307419982e-06,
      "loss": 0.0006,
      "step": 2896
    },
    {
      "epoch": 120.39,
      "grad_norm": 0.004916185047477484,
      "learning_rate": 3.896773381592281e-06,
      "loss": 0.0003,
      "step": 2897
    },
    {
      "epoch": 120.44,
      "grad_norm": 0.2034836709499359,
      "learning_rate": 3.893646909259368e-06,
      "loss": 0.0014,
      "step": 2898
    },
    {
      "epoch": 120.48,
      "grad_norm": 0.060186997056007385,
      "learning_rate": 3.890520891706422e-06,
      "loss": 0.0005,
      "step": 2899
    },
    {
      "epoch": 120.52,
      "grad_norm": 0.04184335097670555,
      "learning_rate": 3.887395330218429e-06,
      "loss": 0.0004,
      "step": 2900
    },
    {
      "epoch": 120.56,
      "grad_norm": 0.07093382626771927,
      "learning_rate": 3.884270226080189e-06,
      "loss": 0.0008,
      "step": 2901
    },
    {
      "epoch": 120.6,
      "grad_norm": 0.0048599448055028915,
      "learning_rate": 3.881145580576313e-06,
      "loss": 0.0003,
      "step": 2902
    },
    {
      "epoch": 120.64,
      "grad_norm": 0.1285371631383896,
      "learning_rate": 3.878021394991227e-06,
      "loss": 0.0011,
      "step": 2903
    },
    {
      "epoch": 120.69,
      "grad_norm": 0.004863612353801727,
      "learning_rate": 3.874897670609164e-06,
      "loss": 0.0003,
      "step": 2904
    },
    {
      "epoch": 120.73,
      "grad_norm": 0.04223642870783806,
      "learning_rate": 3.87177440871417e-06,
      "loss": 0.0005,
      "step": 2905
    },
    {
      "epoch": 120.77,
      "grad_norm": 0.0577540323138237,
      "learning_rate": 3.8686516105901e-06,
      "loss": 0.0007,
      "step": 2906
    },
    {
      "epoch": 120.81,
      "grad_norm": 0.10839136689901352,
      "learning_rate": 3.8655292775206185e-06,
      "loss": 0.0011,
      "step": 2907
    },
    {
      "epoch": 120.85,
      "grad_norm": 0.05959848687052727,
      "learning_rate": 3.862407410789198e-06,
      "loss": 0.0008,
      "step": 2908
    },
    {
      "epoch": 120.89,
      "grad_norm": 0.0434337817132473,
      "learning_rate": 3.8592860116791195e-06,
      "loss": 0.0005,
      "step": 2909
    },
    {
      "epoch": 120.94,
      "grad_norm": 0.08314830809831619,
      "learning_rate": 3.856165081473474e-06,
      "loss": 0.0006,
      "step": 2910
    },
    {
      "epoch": 120.98,
      "grad_norm": 0.06868000328540802,
      "learning_rate": 3.8530446214551576e-06,
      "loss": 0.0007,
      "step": 2911
    },
    {
      "epoch": 121.02,
      "grad_norm": 0.030954310670495033,
      "learning_rate": 3.849924632906872e-06,
      "loss": 0.0004,
      "step": 2912
    },
    {
      "epoch": 121.06,
      "grad_norm": 0.07434345781803131,
      "learning_rate": 3.846805117111127e-06,
      "loss": 0.0007,
      "step": 2913
    },
    {
      "epoch": 121.1,
      "grad_norm": 0.04790213331580162,
      "learning_rate": 3.843686075350239e-06,
      "loss": 0.0004,
      "step": 2914
    },
    {
      "epoch": 121.14,
      "grad_norm": 0.04776860773563385,
      "learning_rate": 3.840567508906328e-06,
      "loss": 0.0005,
      "step": 2915
    },
    {
      "epoch": 121.18,
      "grad_norm": 0.1675722748041153,
      "learning_rate": 3.83744941906132e-06,
      "loss": 0.0013,
      "step": 2916
    },
    {
      "epoch": 121.23,
      "grad_norm": 0.02645585685968399,
      "learning_rate": 3.83433180709694e-06,
      "loss": 0.0004,
      "step": 2917
    },
    {
      "epoch": 121.27,
      "grad_norm": 0.004486992489546537,
      "learning_rate": 3.831214674294724e-06,
      "loss": 0.0003,
      "step": 2918
    },
    {
      "epoch": 121.31,
      "grad_norm": 0.06280037015676498,
      "learning_rate": 3.828098021936006e-06,
      "loss": 0.0005,
      "step": 2919
    },
    {
      "epoch": 121.35,
      "grad_norm": 0.09687341004610062,
      "learning_rate": 3.824981851301924e-06,
      "loss": 0.0007,
      "step": 2920
    },
    {
      "epoch": 121.39,
      "grad_norm": 0.004693188238888979,
      "learning_rate": 3.821866163673421e-06,
      "loss": 0.0003,
      "step": 2921
    },
    {
      "epoch": 121.43,
      "grad_norm": 0.04070892557501793,
      "learning_rate": 3.8187509603312345e-06,
      "loss": 0.0006,
      "step": 2922
    },
    {
      "epoch": 121.48,
      "grad_norm": 0.06326913088560104,
      "learning_rate": 3.815636242555908e-06,
      "loss": 0.0008,
      "step": 2923
    },
    {
      "epoch": 121.52,
      "grad_norm": 0.07197622209787369,
      "learning_rate": 3.8125220116277855e-06,
      "loss": 0.0007,
      "step": 2924
    },
    {
      "epoch": 121.56,
      "grad_norm": 0.005274778697639704,
      "learning_rate": 3.809408268827009e-06,
      "loss": 0.0003,
      "step": 2925
    },
    {
      "epoch": 121.6,
      "grad_norm": 0.0623631589114666,
      "learning_rate": 3.8062950154335205e-06,
      "loss": 0.0006,
      "step": 2926
    },
    {
      "epoch": 121.64,
      "grad_norm": 0.08125881850719452,
      "learning_rate": 3.8031822527270624e-06,
      "loss": 0.001,
      "step": 2927
    },
    {
      "epoch": 121.68,
      "grad_norm": 0.07129324227571487,
      "learning_rate": 3.8000699819871704e-06,
      "loss": 0.0005,
      "step": 2928
    },
    {
      "epoch": 121.72,
      "grad_norm": 0.0582394152879715,
      "learning_rate": 3.796958204493184e-06,
      "loss": 0.0005,
      "step": 2929
    },
    {
      "epoch": 121.77,
      "grad_norm": 0.04449513554573059,
      "learning_rate": 3.7938469215242374e-06,
      "loss": 0.0006,
      "step": 2930
    },
    {
      "epoch": 121.81,
      "grad_norm": 0.07252772897481918,
      "learning_rate": 3.79073613435926e-06,
      "loss": 0.0011,
      "step": 2931
    },
    {
      "epoch": 121.85,
      "grad_norm": 0.21442240476608276,
      "learning_rate": 3.787625844276982e-06,
      "loss": 0.0014,
      "step": 2932
    },
    {
      "epoch": 121.89,
      "grad_norm": 0.0903295949101448,
      "learning_rate": 3.7845160525559223e-06,
      "loss": 0.0009,
      "step": 2933
    },
    {
      "epoch": 121.93,
      "grad_norm": 0.0952150821685791,
      "learning_rate": 3.781406760474401e-06,
      "loss": 0.0009,
      "step": 2934
    },
    {
      "epoch": 121.97,
      "grad_norm": 0.05990001931786537,
      "learning_rate": 3.778297969310529e-06,
      "loss": 0.0006,
      "step": 2935
    },
    {
      "epoch": 122.02,
      "grad_norm": 0.005355275701731443,
      "learning_rate": 3.775189680342217e-06,
      "loss": 0.0003,
      "step": 2936
    },
    {
      "epoch": 122.06,
      "grad_norm": 0.10113558173179626,
      "learning_rate": 3.7720818948471605e-06,
      "loss": 0.0009,
      "step": 2937
    },
    {
      "epoch": 122.1,
      "grad_norm": 0.036718785762786865,
      "learning_rate": 3.7689746141028542e-06,
      "loss": 0.0004,
      "step": 2938
    },
    {
      "epoch": 122.14,
      "grad_norm": 0.0053943609818816185,
      "learning_rate": 3.7658678393865856e-06,
      "loss": 0.0003,
      "step": 2939
    },
    {
      "epoch": 122.18,
      "grad_norm": 0.03594527393579483,
      "learning_rate": 3.7627615719754294e-06,
      "loss": 0.0005,
      "step": 2940
    },
    {
      "epoch": 122.22,
      "grad_norm": 0.039475370198488235,
      "learning_rate": 3.7596558131462566e-06,
      "loss": 0.0005,
      "step": 2941
    },
    {
      "epoch": 122.26,
      "grad_norm": 0.035226546227931976,
      "learning_rate": 3.756550564175727e-06,
      "loss": 0.0004,
      "step": 2942
    },
    {
      "epoch": 122.31,
      "grad_norm": 0.04048993065953255,
      "learning_rate": 3.7534458263402916e-06,
      "loss": 0.0007,
      "step": 2943
    },
    {
      "epoch": 122.35,
      "grad_norm": 0.046722859144210815,
      "learning_rate": 3.7503416009161915e-06,
      "loss": 0.0004,
      "step": 2944
    },
    {
      "epoch": 122.39,
      "grad_norm": 0.11885137110948563,
      "learning_rate": 3.7472378891794537e-06,
      "loss": 0.0012,
      "step": 2945
    },
    {
      "epoch": 122.43,
      "grad_norm": 0.04002329334616661,
      "learning_rate": 3.7441346924059e-06,
      "loss": 0.0004,
      "step": 2946
    },
    {
      "epoch": 122.47,
      "grad_norm": 0.02393336594104767,
      "learning_rate": 3.7410320118711353e-06,
      "loss": 0.0004,
      "step": 2947
    },
    {
      "epoch": 122.51,
      "grad_norm": 0.06872542947530746,
      "learning_rate": 3.737929848850555e-06,
      "loss": 0.0007,
      "step": 2948
    },
    {
      "epoch": 122.56,
      "grad_norm": 0.053173862397670746,
      "learning_rate": 3.7348282046193425e-06,
      "loss": 0.0005,
      "step": 2949
    },
    {
      "epoch": 122.6,
      "grad_norm": 0.12831217050552368,
      "learning_rate": 3.731727080452464e-06,
      "loss": 0.0012,
      "step": 2950
    },
    {
      "epoch": 122.64,
      "grad_norm": 0.006021043751388788,
      "learning_rate": 3.7286264776246766e-06,
      "loss": 0.0003,
      "step": 2951
    },
    {
      "epoch": 122.68,
      "grad_norm": 0.11188796907663345,
      "learning_rate": 3.72552639741052e-06,
      "loss": 0.0009,
      "step": 2952
    },
    {
      "epoch": 122.72,
      "grad_norm": 0.1827477514743805,
      "learning_rate": 3.72242684108432e-06,
      "loss": 0.0019,
      "step": 2953
    },
    {
      "epoch": 122.76,
      "grad_norm": 0.005457854829728603,
      "learning_rate": 3.719327809920188e-06,
      "loss": 0.0003,
      "step": 2954
    },
    {
      "epoch": 122.81,
      "grad_norm": 0.0666826143860817,
      "learning_rate": 3.7162293051920185e-06,
      "loss": 0.0006,
      "step": 2955
    },
    {
      "epoch": 122.85,
      "grad_norm": 0.11400134861469269,
      "learning_rate": 3.7131313281734895e-06,
      "loss": 0.001,
      "step": 2956
    },
    {
      "epoch": 122.89,
      "grad_norm": 0.10228990018367767,
      "learning_rate": 3.710033880138061e-06,
      "loss": 0.0011,
      "step": 2957
    },
    {
      "epoch": 122.93,
      "grad_norm": 0.061333637684583664,
      "learning_rate": 3.7069369623589786e-06,
      "loss": 0.0008,
      "step": 2958
    },
    {
      "epoch": 122.97,
      "grad_norm": 0.04274114593863487,
      "learning_rate": 3.703840576109268e-06,
      "loss": 0.0004,
      "step": 2959
    },
    {
      "epoch": 123.01,
      "grad_norm": 0.005526043474674225,
      "learning_rate": 3.7007447226617367e-06,
      "loss": 0.0003,
      "step": 2960
    },
    {
      "epoch": 123.05,
      "grad_norm": 0.07493332028388977,
      "learning_rate": 3.697649403288972e-06,
      "loss": 0.0008,
      "step": 2961
    },
    {
      "epoch": 123.1,
      "grad_norm": 0.1621503233909607,
      "learning_rate": 3.694554619263343e-06,
      "loss": 0.0009,
      "step": 2962
    },
    {
      "epoch": 123.14,
      "grad_norm": 0.04617936536669731,
      "learning_rate": 3.6914603718569996e-06,
      "loss": 0.0005,
      "step": 2963
    },
    {
      "epoch": 123.18,
      "grad_norm": 0.05189530923962593,
      "learning_rate": 3.6883666623418702e-06,
      "loss": 0.0005,
      "step": 2964
    },
    {
      "epoch": 123.22,
      "grad_norm": 0.034421924501657486,
      "learning_rate": 3.685273491989661e-06,
      "loss": 0.0004,
      "step": 2965
    },
    {
      "epoch": 123.26,
      "grad_norm": 0.051103588193655014,
      "learning_rate": 3.6821808620718567e-06,
      "loss": 0.0005,
      "step": 2966
    },
    {
      "epoch": 123.3,
      "grad_norm": 0.0492914542555809,
      "learning_rate": 3.6790887738597227e-06,
      "loss": 0.0005,
      "step": 2967
    },
    {
      "epoch": 123.35,
      "grad_norm": 0.028214432299137115,
      "learning_rate": 3.6759972286242977e-06,
      "loss": 0.0005,
      "step": 2968
    },
    {
      "epoch": 123.39,
      "grad_norm": 0.13498619198799133,
      "learning_rate": 3.6729062276364003e-06,
      "loss": 0.001,
      "step": 2969
    },
    {
      "epoch": 123.43,
      "grad_norm": 0.0523417666554451,
      "learning_rate": 3.669815772166625e-06,
      "loss": 0.0005,
      "step": 2970
    },
    {
      "epoch": 123.47,
      "grad_norm": 0.03991726413369179,
      "learning_rate": 3.6667258634853396e-06,
      "loss": 0.0005,
      "step": 2971
    },
    {
      "epoch": 123.51,
      "grad_norm": 0.03730788454413414,
      "learning_rate": 3.663636502862689e-06,
      "loss": 0.0006,
      "step": 2972
    },
    {
      "epoch": 123.55,
      "grad_norm": 0.09117083251476288,
      "learning_rate": 3.6605476915685936e-06,
      "loss": 0.0007,
      "step": 2973
    },
    {
      "epoch": 123.59,
      "grad_norm": 0.004536819644272327,
      "learning_rate": 3.657459430872746e-06,
      "loss": 0.0002,
      "step": 2974
    },
    {
      "epoch": 123.64,
      "grad_norm": 0.07485629618167877,
      "learning_rate": 3.654371722044616e-06,
      "loss": 0.0008,
      "step": 2975
    },
    {
      "epoch": 123.68,
      "grad_norm": 0.004980776458978653,
      "learning_rate": 3.6512845663534412e-06,
      "loss": 0.0003,
      "step": 2976
    },
    {
      "epoch": 123.72,
      "grad_norm": 0.05711688846349716,
      "learning_rate": 3.6481979650682355e-06,
      "loss": 0.0006,
      "step": 2977
    },
    {
      "epoch": 123.76,
      "grad_norm": 0.08947788923978806,
      "learning_rate": 3.6451119194577842e-06,
      "loss": 0.0007,
      "step": 2978
    },
    {
      "epoch": 123.8,
      "grad_norm": 0.058993276208639145,
      "learning_rate": 3.6420264307906442e-06,
      "loss": 0.0008,
      "step": 2979
    },
    {
      "epoch": 123.84,
      "grad_norm": 0.01653999090194702,
      "learning_rate": 3.638941500335145e-06,
      "loss": 0.0003,
      "step": 2980
    },
    {
      "epoch": 123.89,
      "grad_norm": 0.05920787528157234,
      "learning_rate": 3.6358571293593813e-06,
      "loss": 0.0007,
      "step": 2981
    },
    {
      "epoch": 123.93,
      "grad_norm": 0.1383940428495407,
      "learning_rate": 3.6327733191312254e-06,
      "loss": 0.0011,
      "step": 2982
    },
    {
      "epoch": 123.97,
      "grad_norm": 0.04301772639155388,
      "learning_rate": 3.6296900709183132e-06,
      "loss": 0.0005,
      "step": 2983
    },
    {
      "epoch": 124.01,
      "grad_norm": 0.06699764728546143,
      "learning_rate": 3.62660738598805e-06,
      "loss": 0.0006,
      "step": 2984
    },
    {
      "epoch": 124.05,
      "grad_norm": 0.022686734795570374,
      "learning_rate": 3.6235252656076138e-06,
      "loss": 0.0004,
      "step": 2985
    },
    {
      "epoch": 124.09,
      "grad_norm": 0.004876102786511183,
      "learning_rate": 3.620443711043946e-06,
      "loss": 0.0003,
      "step": 2986
    },
    {
      "epoch": 124.14,
      "grad_norm": 0.05457837134599686,
      "learning_rate": 3.6173627235637587e-06,
      "loss": 0.0007,
      "step": 2987
    },
    {
      "epoch": 124.18,
      "grad_norm": 0.020043734461069107,
      "learning_rate": 3.6142823044335306e-06,
      "loss": 0.0003,
      "step": 2988
    },
    {
      "epoch": 124.22,
      "grad_norm": 0.03025227226316929,
      "learning_rate": 3.6112024549195026e-06,
      "loss": 0.0004,
      "step": 2989
    },
    {
      "epoch": 124.26,
      "grad_norm": 0.0050064679235219955,
      "learning_rate": 3.608123176287685e-06,
      "loss": 0.0003,
      "step": 2990
    },
    {
      "epoch": 124.3,
      "grad_norm": 0.15957042574882507,
      "learning_rate": 3.6050444698038547e-06,
      "loss": 0.001,
      "step": 2991
    },
    {
      "epoch": 124.34,
      "grad_norm": 0.02310776151716709,
      "learning_rate": 3.6019663367335507e-06,
      "loss": 0.0003,
      "step": 2992
    },
    {
      "epoch": 124.38,
      "grad_norm": 0.05924435332417488,
      "learning_rate": 3.5988887783420778e-06,
      "loss": 0.0006,
      "step": 2993
    },
    {
      "epoch": 124.43,
      "grad_norm": 0.09365136176347733,
      "learning_rate": 3.5958117958945033e-06,
      "loss": 0.0008,
      "step": 2994
    },
    {
      "epoch": 124.47,
      "grad_norm": 0.049619436264038086,
      "learning_rate": 3.5927353906556583e-06,
      "loss": 0.0005,
      "step": 2995
    },
    {
      "epoch": 124.51,
      "grad_norm": 0.00466350931674242,
      "learning_rate": 3.5896595638901373e-06,
      "loss": 0.0003,
      "step": 2996
    },
    {
      "epoch": 124.55,
      "grad_norm": 0.021117301657795906,
      "learning_rate": 3.586584316862296e-06,
      "loss": 0.0003,
      "step": 2997
    },
    {
      "epoch": 124.59,
      "grad_norm": 0.059447187930345535,
      "learning_rate": 3.583509650836254e-06,
      "loss": 0.0005,
      "step": 2998
    },
    {
      "epoch": 124.63,
      "grad_norm": 0.019422562792897224,
      "learning_rate": 3.580435567075888e-06,
      "loss": 0.0003,
      "step": 2999
    },
    {
      "epoch": 124.68,
      "grad_norm": 0.035631295293569565,
      "learning_rate": 3.5773620668448384e-06,
      "loss": 0.0004,
      "step": 3000
    },
    {
      "epoch": 124.72,
      "grad_norm": 0.004609037656337023,
      "learning_rate": 3.574289151406506e-06,
      "loss": 0.0003,
      "step": 3001
    },
    {
      "epoch": 124.76,
      "grad_norm": 0.10323313623666763,
      "learning_rate": 3.5712168220240494e-06,
      "loss": 0.001,
      "step": 3002
    },
    {
      "epoch": 124.8,
      "grad_norm": 0.06420930474996567,
      "learning_rate": 3.568145079960388e-06,
      "loss": 0.0009,
      "step": 3003
    },
    {
      "epoch": 124.84,
      "grad_norm": 0.13656671345233917,
      "learning_rate": 3.5650739264781976e-06,
      "loss": 0.0015,
      "step": 3004
    },
    {
      "epoch": 124.88,
      "grad_norm": 0.07109864056110382,
      "learning_rate": 3.562003362839914e-06,
      "loss": 0.0006,
      "step": 3005
    },
    {
      "epoch": 124.92,
      "grad_norm": 0.12425973266363144,
      "learning_rate": 3.5589333903077306e-06,
      "loss": 0.0017,
      "step": 3006
    },
    {
      "epoch": 124.97,
      "grad_norm": 0.048163991421461105,
      "learning_rate": 3.555864010143596e-06,
      "loss": 0.0007,
      "step": 3007
    },
    {
      "epoch": 125.01,
      "grad_norm": 0.004982956685125828,
      "learning_rate": 3.552795223609219e-06,
      "loss": 0.0003,
      "step": 3008
    },
    {
      "epoch": 125.05,
      "grad_norm": 0.06646794825792313,
      "learning_rate": 3.5497270319660576e-06,
      "loss": 0.0008,
      "step": 3009
    },
    {
      "epoch": 125.09,
      "grad_norm": 0.0735635906457901,
      "learning_rate": 3.5466594364753325e-06,
      "loss": 0.001,
      "step": 3010
    },
    {
      "epoch": 125.13,
      "grad_norm": 0.2186814546585083,
      "learning_rate": 3.5435924383980154e-06,
      "loss": 0.0016,
      "step": 3011
    },
    {
      "epoch": 125.17,
      "grad_norm": 0.09764020144939423,
      "learning_rate": 3.540526038994834e-06,
      "loss": 0.0008,
      "step": 3012
    },
    {
      "epoch": 125.22,
      "grad_norm": 0.1283956617116928,
      "learning_rate": 3.537460239526269e-06,
      "loss": 0.0011,
      "step": 3013
    },
    {
      "epoch": 125.26,
      "grad_norm": 0.004635307937860489,
      "learning_rate": 3.534395041252554e-06,
      "loss": 0.0003,
      "step": 3014
    },
    {
      "epoch": 125.3,
      "grad_norm": 0.004668720532208681,
      "learning_rate": 3.531330445433677e-06,
      "loss": 0.0003,
      "step": 3015
    },
    {
      "epoch": 125.34,
      "grad_norm": 0.06303364038467407,
      "learning_rate": 3.5282664533293763e-06,
      "loss": 0.0006,
      "step": 3016
    },
    {
      "epoch": 125.38,
      "grad_norm": 0.020348485559225082,
      "learning_rate": 3.5252030661991455e-06,
      "loss": 0.0004,
      "step": 3017
    },
    {
      "epoch": 125.42,
      "grad_norm": 0.004402942024171352,
      "learning_rate": 3.5221402853022256e-06,
      "loss": 0.0002,
      "step": 3018
    },
    {
      "epoch": 125.46,
      "grad_norm": 0.040160831063985825,
      "learning_rate": 3.5190781118976125e-06,
      "loss": 0.0005,
      "step": 3019
    },
    {
      "epoch": 125.51,
      "grad_norm": 0.039838772267103195,
      "learning_rate": 3.516016547244047e-06,
      "loss": 0.0004,
      "step": 3020
    },
    {
      "epoch": 125.55,
      "grad_norm": 0.07342343032360077,
      "learning_rate": 3.5129555926000237e-06,
      "loss": 0.0007,
      "step": 3021
    },
    {
      "epoch": 125.59,
      "grad_norm": 0.059850748628377914,
      "learning_rate": 3.5098952492237858e-06,
      "loss": 0.0007,
      "step": 3022
    },
    {
      "epoch": 125.63,
      "grad_norm": 0.053212448954582214,
      "learning_rate": 3.506835518373325e-06,
      "loss": 0.0005,
      "step": 3023
    },
    {
      "epoch": 125.67,
      "grad_norm": 0.037735603749752045,
      "learning_rate": 3.5037764013063825e-06,
      "loss": 0.0005,
      "step": 3024
    },
    {
      "epoch": 125.71,
      "grad_norm": 0.004575362429022789,
      "learning_rate": 3.500717899280442e-06,
      "loss": 0.0003,
      "step": 3025
    },
    {
      "epoch": 125.76,
      "grad_norm": 0.059322159737348557,
      "learning_rate": 3.4976600135527403e-06,
      "loss": 0.0006,
      "step": 3026
    },
    {
      "epoch": 125.8,
      "grad_norm": 0.004881738219410181,
      "learning_rate": 3.494602745380261e-06,
      "loss": 0.0003,
      "step": 3027
    },
    {
      "epoch": 125.84,
      "grad_norm": 0.039979513734579086,
      "learning_rate": 3.4915460960197277e-06,
      "loss": 0.0004,
      "step": 3028
    },
    {
      "epoch": 125.88,
      "grad_norm": 0.08275604993104935,
      "learning_rate": 3.4884900667276143e-06,
      "loss": 0.0006,
      "step": 3029
    },
    {
      "epoch": 125.92,
      "grad_norm": 0.11975008994340897,
      "learning_rate": 3.48543465876014e-06,
      "loss": 0.0011,
      "step": 3030
    },
    {
      "epoch": 125.96,
      "grad_norm": 0.004917668644338846,
      "learning_rate": 3.4823798733732684e-06,
      "loss": 0.0003,
      "step": 3031
    },
    {
      "epoch": 126.01,
      "grad_norm": 0.04553087800741196,
      "learning_rate": 3.479325711822704e-06,
      "loss": 0.0006,
      "step": 3032
    },
    {
      "epoch": 126.05,
      "grad_norm": 0.01515450794249773,
      "learning_rate": 3.4762721753638994e-06,
      "loss": 0.0003,
      "step": 3033
    },
    {
      "epoch": 126.09,
      "grad_norm": 0.02985970303416252,
      "learning_rate": 3.473219265252047e-06,
      "loss": 0.0004,
      "step": 3034
    },
    {
      "epoch": 126.13,
      "grad_norm": 0.06920108199119568,
      "learning_rate": 3.4701669827420827e-06,
      "loss": 0.0008,
      "step": 3035
    },
    {
      "epoch": 126.17,
      "grad_norm": 0.04145834967494011,
      "learning_rate": 3.4671153290886863e-06,
      "loss": 0.0005,
      "step": 3036
    },
    {
      "epoch": 126.21,
      "grad_norm": 0.004784056916832924,
      "learning_rate": 3.464064305546274e-06,
      "loss": 0.0003,
      "step": 3037
    },
    {
      "epoch": 126.25,
      "grad_norm": 0.0556521937251091,
      "learning_rate": 3.461013913369009e-06,
      "loss": 0.0004,
      "step": 3038
    },
    {
      "epoch": 126.3,
      "grad_norm": 0.004684589337557554,
      "learning_rate": 3.4579641538107923e-06,
      "loss": 0.0002,
      "step": 3039
    },
    {
      "epoch": 126.34,
      "grad_norm": 0.06039968505501747,
      "learning_rate": 3.4549150281252635e-06,
      "loss": 0.0006,
      "step": 3040
    },
    {
      "epoch": 126.38,
      "grad_norm": 0.0402941070497036,
      "learning_rate": 3.4518665375658044e-06,
      "loss": 0.0005,
      "step": 3041
    },
    {
      "epoch": 126.42,
      "grad_norm": 0.07341217249631882,
      "learning_rate": 3.4488186833855334e-06,
      "loss": 0.0005,
      "step": 3042
    },
    {
      "epoch": 126.46,
      "grad_norm": 0.12018449604511261,
      "learning_rate": 3.4457714668373075e-06,
      "loss": 0.0009,
      "step": 3043
    },
    {
      "epoch": 126.5,
      "grad_norm": 0.05516103655099869,
      "learning_rate": 3.442724889173724e-06,
      "loss": 0.0006,
      "step": 3044
    },
    {
      "epoch": 126.55,
      "grad_norm": 0.02044241689145565,
      "learning_rate": 3.4396789516471152e-06,
      "loss": 0.0004,
      "step": 3045
    },
    {
      "epoch": 126.59,
      "grad_norm": 0.004644742235541344,
      "learning_rate": 3.43663365550955e-06,
      "loss": 0.0003,
      "step": 3046
    },
    {
      "epoch": 126.63,
      "grad_norm": 0.09207665920257568,
      "learning_rate": 3.4335890020128382e-06,
      "loss": 0.0009,
      "step": 3047
    },
    {
      "epoch": 126.67,
      "grad_norm": 0.12816086411476135,
      "learning_rate": 3.4305449924085165e-06,
      "loss": 0.001,
      "step": 3048
    },
    {
      "epoch": 126.71,
      "grad_norm": 0.030951345339417458,
      "learning_rate": 3.4275016279478657e-06,
      "loss": 0.0004,
      "step": 3049
    },
    {
      "epoch": 126.75,
      "grad_norm": 0.06970758736133575,
      "learning_rate": 3.424458909881897e-06,
      "loss": 0.0006,
      "step": 3050
    },
    {
      "epoch": 126.79,
      "grad_norm": 0.08290372043848038,
      "learning_rate": 3.4214168394613566e-06,
      "loss": 0.0008,
      "step": 3051
    },
    {
      "epoch": 126.84,
      "grad_norm": 0.05893033742904663,
      "learning_rate": 3.4183754179367268e-06,
      "loss": 0.0007,
      "step": 3052
    },
    {
      "epoch": 126.88,
      "grad_norm": 0.11234556883573532,
      "learning_rate": 3.4153346465582183e-06,
      "loss": 0.0012,
      "step": 3053
    },
    {
      "epoch": 126.92,
      "grad_norm": 0.13353045284748077,
      "learning_rate": 3.412294526575779e-06,
      "loss": 0.0013,
      "step": 3054
    },
    {
      "epoch": 126.96,
      "grad_norm": 0.04366351291537285,
      "learning_rate": 3.409255059239086e-06,
      "loss": 0.0005,
      "step": 3055
    },
    {
      "epoch": 127.0,
      "grad_norm": 0.04870256036520004,
      "learning_rate": 3.406216245797551e-06,
      "loss": 0.0004,
      "step": 3056
    },
    {
      "epoch": 127.04,
      "grad_norm": 0.05214297026395798,
      "learning_rate": 3.4031780875003162e-06,
      "loss": 0.0006,
      "step": 3057
    },
    {
      "epoch": 127.09,
      "grad_norm": 0.059736378490924835,
      "learning_rate": 3.400140585596251e-06,
      "loss": 0.0007,
      "step": 3058
    },
    {
      "epoch": 127.13,
      "grad_norm": 0.004694732371717691,
      "learning_rate": 3.3971037413339592e-06,
      "loss": 0.0002,
      "step": 3059
    },
    {
      "epoch": 127.17,
      "grad_norm": 0.059289366006851196,
      "learning_rate": 3.3940675559617724e-06,
      "loss": 0.0005,
      "step": 3060
    },
    {
      "epoch": 127.21,
      "grad_norm": 0.0554262176156044,
      "learning_rate": 3.3910320307277522e-06,
      "loss": 0.0005,
      "step": 3061
    },
    {
      "epoch": 127.25,
      "grad_norm": 0.02606009691953659,
      "learning_rate": 3.3879971668796896e-06,
      "loss": 0.0004,
      "step": 3062
    },
    {
      "epoch": 127.29,
      "grad_norm": 0.045888498425483704,
      "learning_rate": 3.3849629656650996e-06,
      "loss": 0.0005,
      "step": 3063
    },
    {
      "epoch": 127.34,
      "grad_norm": 0.024117758497595787,
      "learning_rate": 3.3819294283312286e-06,
      "loss": 0.0003,
      "step": 3064
    },
    {
      "epoch": 127.38,
      "grad_norm": 0.03538215905427933,
      "learning_rate": 3.37889655612505e-06,
      "loss": 0.0004,
      "step": 3065
    },
    {
      "epoch": 127.42,
      "grad_norm": 0.04100770503282547,
      "learning_rate": 3.375864350293263e-06,
      "loss": 0.0005,
      "step": 3066
    },
    {
      "epoch": 127.46,
      "grad_norm": 0.041124340146780014,
      "learning_rate": 3.372832812082294e-06,
      "loss": 0.0004,
      "step": 3067
    },
    {
      "epoch": 127.5,
      "grad_norm": 0.11475212872028351,
      "learning_rate": 3.3698019427382912e-06,
      "loss": 0.0009,
      "step": 3068
    },
    {
      "epoch": 127.54,
      "grad_norm": 0.08282539248466492,
      "learning_rate": 3.366771743507131e-06,
      "loss": 0.0008,
      "step": 3069
    },
    {
      "epoch": 127.58,
      "grad_norm": 0.05742770433425903,
      "learning_rate": 3.363742215634416e-06,
      "loss": 0.0007,
      "step": 3070
    },
    {
      "epoch": 127.63,
      "grad_norm": 0.055029962211847305,
      "learning_rate": 3.3607133603654685e-06,
      "loss": 0.0006,
      "step": 3071
    },
    {
      "epoch": 127.67,
      "grad_norm": 0.05213721841573715,
      "learning_rate": 3.357685178945339e-06,
      "loss": 0.0005,
      "step": 3072
    },
    {
      "epoch": 127.71,
      "grad_norm": 0.004710099659860134,
      "learning_rate": 3.3546576726187953e-06,
      "loss": 0.0003,
      "step": 3073
    },
    {
      "epoch": 127.75,
      "grad_norm": 0.04577811434864998,
      "learning_rate": 3.3516308426303324e-06,
      "loss": 0.0006,
      "step": 3074
    },
    {
      "epoch": 127.79,
      "grad_norm": 0.10277540236711502,
      "learning_rate": 3.3486046902241663e-06,
      "loss": 0.001,
      "step": 3075
    },
    {
      "epoch": 127.83,
      "grad_norm": 0.02857290580868721,
      "learning_rate": 3.3455792166442323e-06,
      "loss": 0.0003,
      "step": 3076
    },
    {
      "epoch": 127.88,
      "grad_norm": 0.034501805901527405,
      "learning_rate": 3.34255442313419e-06,
      "loss": 0.0004,
      "step": 3077
    },
    {
      "epoch": 127.92,
      "grad_norm": 0.044898707419633865,
      "learning_rate": 3.3395303109374165e-06,
      "loss": 0.0005,
      "step": 3078
    },
    {
      "epoch": 127.96,
      "grad_norm": 0.08196061849594116,
      "learning_rate": 3.3365068812970113e-06,
      "loss": 0.0012,
      "step": 3079
    },
    {
      "epoch": 128.0,
      "grad_norm": 0.23424150049686432,
      "learning_rate": 3.3334841354557923e-06,
      "loss": 0.0014,
      "step": 3080
    },
    {
      "epoch": 128.04,
      "grad_norm": 0.01516584400087595,
      "learning_rate": 3.330462074656295e-06,
      "loss": 0.0003,
      "step": 3081
    },
    {
      "epoch": 128.08,
      "grad_norm": 0.08319821208715439,
      "learning_rate": 3.327440700140774e-06,
      "loss": 0.0007,
      "step": 3082
    },
    {
      "epoch": 128.12,
      "grad_norm": 0.05750340223312378,
      "learning_rate": 3.324420013151204e-06,
      "loss": 0.0007,
      "step": 3083
    },
    {
      "epoch": 128.17,
      "grad_norm": 0.02609158679842949,
      "learning_rate": 3.3214000149292734e-06,
      "loss": 0.0003,
      "step": 3084
    },
    {
      "epoch": 128.21,
      "grad_norm": 0.027969347313046455,
      "learning_rate": 3.318380706716392e-06,
      "loss": 0.0004,
      "step": 3085
    },
    {
      "epoch": 128.25,
      "grad_norm": 0.05067812651395798,
      "learning_rate": 3.315362089753681e-06,
      "loss": 0.0007,
      "step": 3086
    },
    {
      "epoch": 128.29,
      "grad_norm": 0.03632965683937073,
      "learning_rate": 3.31234416528198e-06,
      "loss": 0.0004,
      "step": 3087
    },
    {
      "epoch": 128.33,
      "grad_norm": 0.031232675537467003,
      "learning_rate": 3.3093269345418443e-06,
      "loss": 0.0004,
      "step": 3088
    },
    {
      "epoch": 128.37,
      "grad_norm": 0.05747547000646591,
      "learning_rate": 3.3063103987735433e-06,
      "loss": 0.0006,
      "step": 3089
    },
    {
      "epoch": 128.42,
      "grad_norm": 0.0910961925983429,
      "learning_rate": 3.303294559217063e-06,
      "loss": 0.0009,
      "step": 3090
    },
    {
      "epoch": 128.46,
      "grad_norm": 0.15745316445827484,
      "learning_rate": 3.3002794171120978e-06,
      "loss": 0.0011,
      "step": 3091
    },
    {
      "epoch": 128.5,
      "grad_norm": 0.036380913108587265,
      "learning_rate": 3.29726497369806e-06,
      "loss": 0.0004,
      "step": 3092
    },
    {
      "epoch": 128.54,
      "grad_norm": 0.016647491604089737,
      "learning_rate": 3.2942512302140735e-06,
      "loss": 0.0005,
      "step": 3093
    },
    {
      "epoch": 128.58,
      "grad_norm": 0.029782459139823914,
      "learning_rate": 3.2912381878989748e-06,
      "loss": 0.0004,
      "step": 3094
    },
    {
      "epoch": 128.62,
      "grad_norm": 0.005020336247980595,
      "learning_rate": 3.288225847991312e-06,
      "loss": 0.0003,
      "step": 3095
    },
    {
      "epoch": 128.66,
      "grad_norm": 0.10440520942211151,
      "learning_rate": 3.2852142117293435e-06,
      "loss": 0.001,
      "step": 3096
    },
    {
      "epoch": 128.71,
      "grad_norm": 0.02785625495016575,
      "learning_rate": 3.2822032803510385e-06,
      "loss": 0.0003,
      "step": 3097
    },
    {
      "epoch": 128.75,
      "grad_norm": 0.09410091489553452,
      "learning_rate": 3.279193055094079e-06,
      "loss": 0.0009,
      "step": 3098
    },
    {
      "epoch": 128.79,
      "grad_norm": 0.08323345333337784,
      "learning_rate": 3.276183537195854e-06,
      "loss": 0.0007,
      "step": 3099
    },
    {
      "epoch": 128.83,
      "grad_norm": 0.1135738417506218,
      "learning_rate": 3.273174727893463e-06,
      "loss": 0.001,
      "step": 3100
    },
    {
      "epoch": 128.87,
      "grad_norm": 0.062313441187143326,
      "learning_rate": 3.2701666284237123e-06,
      "loss": 0.0005,
      "step": 3101
    },
    {
      "epoch": 128.91,
      "grad_norm": 0.035243190824985504,
      "learning_rate": 3.26715924002312e-06,
      "loss": 0.0004,
      "step": 3102
    },
    {
      "epoch": 128.96,
      "grad_norm": 0.004357821773737669,
      "learning_rate": 3.264152563927908e-06,
      "loss": 0.0002,
      "step": 3103
    },
    {
      "epoch": 129.0,
      "grad_norm": 0.10386583209037781,
      "learning_rate": 3.261146601374009e-06,
      "loss": 0.001,
      "step": 3104
    },
    {
      "epoch": 129.04,
      "grad_norm": 0.04281669110059738,
      "learning_rate": 3.2581413535970597e-06,
      "loss": 0.0005,
      "step": 3105
    },
    {
      "epoch": 129.08,
      "grad_norm": 0.04644116759300232,
      "learning_rate": 3.255136821832405e-06,
      "loss": 0.0005,
      "step": 3106
    },
    {
      "epoch": 129.12,
      "grad_norm": 0.055212538689374924,
      "learning_rate": 3.252133007315093e-06,
      "loss": 0.0006,
      "step": 3107
    },
    {
      "epoch": 129.16,
      "grad_norm": 0.033816494047641754,
      "learning_rate": 3.2491299112798793e-06,
      "loss": 0.0005,
      "step": 3108
    },
    {
      "epoch": 129.21,
      "grad_norm": 0.04922351613640785,
      "learning_rate": 3.246127534961222e-06,
      "loss": 0.0006,
      "step": 3109
    },
    {
      "epoch": 129.25,
      "grad_norm": 0.13770447671413422,
      "learning_rate": 3.2431258795932863e-06,
      "loss": 0.0011,
      "step": 3110
    },
    {
      "epoch": 129.29,
      "grad_norm": 0.0876680389046669,
      "learning_rate": 3.240124946409939e-06,
      "loss": 0.0009,
      "step": 3111
    },
    {
      "epoch": 129.33,
      "grad_norm": 0.08147421479225159,
      "learning_rate": 3.237124736644749e-06,
      "loss": 0.0008,
      "step": 3112
    },
    {
      "epoch": 129.37,
      "grad_norm": 0.03533970192074776,
      "learning_rate": 3.234125251530991e-06,
      "loss": 0.0004,
      "step": 3113
    },
    {
      "epoch": 129.41,
      "grad_norm": 0.03672782704234123,
      "learning_rate": 3.2311264923016384e-06,
      "loss": 0.0003,
      "step": 3114
    },
    {
      "epoch": 129.45,
      "grad_norm": 0.004525594878941774,
      "learning_rate": 3.228128460189368e-06,
      "loss": 0.0003,
      "step": 3115
    },
    {
      "epoch": 129.5,
      "grad_norm": 0.023780127987265587,
      "learning_rate": 3.22513115642656e-06,
      "loss": 0.0004,
      "step": 3116
    },
    {
      "epoch": 129.54,
      "grad_norm": 0.12752851843833923,
      "learning_rate": 3.22213458224529e-06,
      "loss": 0.0009,
      "step": 3117
    },
    {
      "epoch": 129.58,
      "grad_norm": 0.09597989916801453,
      "learning_rate": 3.2191387388773393e-06,
      "loss": 0.0009,
      "step": 3118
    },
    {
      "epoch": 129.62,
      "grad_norm": 0.10109695792198181,
      "learning_rate": 3.216143627554182e-06,
      "loss": 0.0007,
      "step": 3119
    },
    {
      "epoch": 129.66,
      "grad_norm": 0.052397146821022034,
      "learning_rate": 3.213149249506997e-06,
      "loss": 0.0005,
      "step": 3120
    },
    {
      "epoch": 129.7,
      "grad_norm": 0.18779677152633667,
      "learning_rate": 3.2101556059666607e-06,
      "loss": 0.0014,
      "step": 3121
    },
    {
      "epoch": 129.75,
      "grad_norm": 0.03755107522010803,
      "learning_rate": 3.207162698163746e-06,
      "loss": 0.0004,
      "step": 3122
    },
    {
      "epoch": 129.79,
      "grad_norm": 0.09394155442714691,
      "learning_rate": 3.204170527328526e-06,
      "loss": 0.0008,
      "step": 3123
    },
    {
      "epoch": 129.83,
      "grad_norm": 0.13460825383663177,
      "learning_rate": 3.2011790946909673e-06,
      "loss": 0.0009,
      "step": 3124
    },
    {
      "epoch": 129.87,
      "grad_norm": 0.06712382286787033,
      "learning_rate": 3.198188401480734e-06,
      "loss": 0.0006,
      "step": 3125
    },
    {
      "epoch": 129.91,
      "grad_norm": 0.03361675143241882,
      "learning_rate": 3.195198448927189e-06,
      "loss": 0.0004,
      "step": 3126
    },
    {
      "epoch": 129.95,
      "grad_norm": 0.03635156899690628,
      "learning_rate": 3.1922092382593894e-06,
      "loss": 0.0004,
      "step": 3127
    },
    {
      "epoch": 129.99,
      "grad_norm": 0.046827636659145355,
      "learning_rate": 3.189220770706086e-06,
      "loss": 0.0005,
      "step": 3128
    },
    {
      "epoch": 130.04,
      "grad_norm": 0.08221746236085892,
      "learning_rate": 3.1862330474957237e-06,
      "loss": 0.0007,
      "step": 3129
    },
    {
      "epoch": 130.08,
      "grad_norm": 0.08542460203170776,
      "learning_rate": 3.183246069856443e-06,
      "loss": 0.0007,
      "step": 3130
    },
    {
      "epoch": 130.12,
      "grad_norm": 0.0330653190612793,
      "learning_rate": 3.1802598390160788e-06,
      "loss": 0.0004,
      "step": 3131
    },
    {
      "epoch": 130.16,
      "grad_norm": 0.06458517163991928,
      "learning_rate": 3.177274356202157e-06,
      "loss": 0.0005,
      "step": 3132
    },
    {
      "epoch": 130.2,
      "grad_norm": 0.033573366701602936,
      "learning_rate": 3.1742896226418973e-06,
      "loss": 0.0004,
      "step": 3133
    },
    {
      "epoch": 130.24,
      "grad_norm": 0.07606738060712814,
      "learning_rate": 3.171305639562211e-06,
      "loss": 0.0007,
      "step": 3134
    },
    {
      "epoch": 130.29,
      "grad_norm": 0.004449964035302401,
      "learning_rate": 3.1683224081897e-06,
      "loss": 0.0003,
      "step": 3135
    },
    {
      "epoch": 130.33,
      "grad_norm": 0.04761862754821777,
      "learning_rate": 3.1653399297506583e-06,
      "loss": 0.0005,
      "step": 3136
    },
    {
      "epoch": 130.37,
      "grad_norm": 0.004467240069061518,
      "learning_rate": 3.1623582054710705e-06,
      "loss": 0.0003,
      "step": 3137
    },
    {
      "epoch": 130.41,
      "grad_norm": 0.09504026919603348,
      "learning_rate": 3.1593772365766107e-06,
      "loss": 0.001,
      "step": 3138
    },
    {
      "epoch": 130.45,
      "grad_norm": 0.004575696773827076,
      "learning_rate": 3.1563970242926433e-06,
      "loss": 0.0003,
      "step": 3139
    },
    {
      "epoch": 130.49,
      "grad_norm": 0.025195714086294174,
      "learning_rate": 3.1534175698442194e-06,
      "loss": 0.0003,
      "step": 3140
    },
    {
      "epoch": 130.54,
      "grad_norm": 0.02956581674516201,
      "learning_rate": 3.1504388744560804e-06,
      "loss": 0.0004,
      "step": 3141
    },
    {
      "epoch": 130.58,
      "grad_norm": 0.004435520153492689,
      "learning_rate": 3.147460939352657e-06,
      "loss": 0.0003,
      "step": 3142
    },
    {
      "epoch": 130.62,
      "grad_norm": 0.004495280794799328,
      "learning_rate": 3.144483765758064e-06,
      "loss": 0.0003,
      "step": 3143
    },
    {
      "epoch": 130.66,
      "grad_norm": 0.10431517660617828,
      "learning_rate": 3.141507354896107e-06,
      "loss": 0.0014,
      "step": 3144
    },
    {
      "epoch": 130.7,
      "grad_norm": 0.08291564136743546,
      "learning_rate": 3.1385317079902743e-06,
      "loss": 0.0008,
      "step": 3145
    },
    {
      "epoch": 130.74,
      "grad_norm": 0.02310137450695038,
      "learning_rate": 3.135556826263743e-06,
      "loss": 0.0003,
      "step": 3146
    },
    {
      "epoch": 130.78,
      "grad_norm": 0.07514066994190216,
      "learning_rate": 3.132582710939374e-06,
      "loss": 0.0009,
      "step": 3147
    },
    {
      "epoch": 130.83,
      "grad_norm": 0.0615231953561306,
      "learning_rate": 3.129609363239714e-06,
      "loss": 0.0004,
      "step": 3148
    },
    {
      "epoch": 130.87,
      "grad_norm": 0.09244820475578308,
      "learning_rate": 3.126636784386995e-06,
      "loss": 0.001,
      "step": 3149
    },
    {
      "epoch": 130.91,
      "grad_norm": 0.004560655448585749,
      "learning_rate": 3.12366497560313e-06,
      "loss": 0.0003,
      "step": 3150
    },
    {
      "epoch": 130.95,
      "grad_norm": 0.08213287591934204,
      "learning_rate": 3.1206939381097177e-06,
      "loss": 0.0007,
      "step": 3151
    },
    {
      "epoch": 130.99,
      "grad_norm": 0.0510263666510582,
      "learning_rate": 3.11772367312804e-06,
      "loss": 0.0005,
      "step": 3152
    },
    {
      "epoch": 131.03,
      "grad_norm": 0.06321433931589127,
      "learning_rate": 3.1147541818790605e-06,
      "loss": 0.0005,
      "step": 3153
    },
    {
      "epoch": 131.08,
      "grad_norm": 0.06235311180353165,
      "learning_rate": 3.111785465583426e-06,
      "loss": 0.0005,
      "step": 3154
    },
    {
      "epoch": 131.12,
      "grad_norm": 0.04305832087993622,
      "learning_rate": 3.1088175254614616e-06,
      "loss": 0.0004,
      "step": 3155
    },
    {
      "epoch": 131.16,
      "grad_norm": 0.017077207565307617,
      "learning_rate": 3.105850362733176e-06,
      "loss": 0.0003,
      "step": 3156
    },
    {
      "epoch": 131.2,
      "grad_norm": 0.09156230837106705,
      "learning_rate": 3.102883978618258e-06,
      "loss": 0.0008,
      "step": 3157
    },
    {
      "epoch": 131.24,
      "grad_norm": 0.04407745227217674,
      "learning_rate": 3.099918374336076e-06,
      "loss": 0.0006,
      "step": 3158
    },
    {
      "epoch": 131.28,
      "grad_norm": 0.07177804410457611,
      "learning_rate": 3.096953551105679e-06,
      "loss": 0.0006,
      "step": 3159
    },
    {
      "epoch": 131.32,
      "grad_norm": 0.10169395804405212,
      "learning_rate": 3.093989510145792e-06,
      "loss": 0.0007,
      "step": 3160
    },
    {
      "epoch": 131.37,
      "grad_norm": 0.05882416293025017,
      "learning_rate": 3.0910262526748206e-06,
      "loss": 0.0005,
      "step": 3161
    },
    {
      "epoch": 131.41,
      "grad_norm": 0.03418014198541641,
      "learning_rate": 3.0880637799108494e-06,
      "loss": 0.0004,
      "step": 3162
    },
    {
      "epoch": 131.45,
      "grad_norm": 0.061132799834012985,
      "learning_rate": 3.0851020930716393e-06,
      "loss": 0.0007,
      "step": 3163
    },
    {
      "epoch": 131.49,
      "grad_norm": 0.21309274435043335,
      "learning_rate": 3.082141193374625e-06,
      "loss": 0.0013,
      "step": 3164
    },
    {
      "epoch": 131.53,
      "grad_norm": 0.004381564445793629,
      "learning_rate": 3.079181082036922e-06,
      "loss": 0.0003,
      "step": 3165
    },
    {
      "epoch": 131.57,
      "grad_norm": 0.021336965262889862,
      "learning_rate": 3.076221760275321e-06,
      "loss": 0.0003,
      "step": 3166
    },
    {
      "epoch": 131.62,
      "grad_norm": 0.004524301737546921,
      "learning_rate": 3.0732632293062873e-06,
      "loss": 0.0002,
      "step": 3167
    },
    {
      "epoch": 131.66,
      "grad_norm": 0.07230471074581146,
      "learning_rate": 3.0703054903459607e-06,
      "loss": 0.0007,
      "step": 3168
    },
    {
      "epoch": 131.7,
      "grad_norm": 0.03426972031593323,
      "learning_rate": 3.067348544610155e-06,
      "loss": 0.0004,
      "step": 3169
    },
    {
      "epoch": 131.74,
      "grad_norm": 0.05954650416970253,
      "learning_rate": 3.0643923933143603e-06,
      "loss": 0.0006,
      "step": 3170
    },
    {
      "epoch": 131.78,
      "grad_norm": 0.06467452645301819,
      "learning_rate": 3.061437037673739e-06,
      "loss": 0.0009,
      "step": 3171
    },
    {
      "epoch": 131.82,
      "grad_norm": 0.07481571286916733,
      "learning_rate": 3.0584824789031266e-06,
      "loss": 0.0007,
      "step": 3172
    },
    {
      "epoch": 131.86,
      "grad_norm": 0.121638223528862,
      "learning_rate": 3.055528718217028e-06,
      "loss": 0.001,
      "step": 3173
    },
    {
      "epoch": 131.91,
      "grad_norm": 0.09974876791238785,
      "learning_rate": 3.0525757568296242e-06,
      "loss": 0.0009,
      "step": 3174
    },
    {
      "epoch": 131.95,
      "grad_norm": 0.03457776457071304,
      "learning_rate": 3.049623595954766e-06,
      "loss": 0.0003,
      "step": 3175
    },
    {
      "epoch": 131.99,
      "grad_norm": 0.004566344898194075,
      "learning_rate": 3.046672236805976e-06,
      "loss": 0.0003,
      "step": 3176
    },
    {
      "epoch": 132.03,
      "grad_norm": 0.022071292623877525,
      "learning_rate": 3.0437216805964463e-06,
      "loss": 0.0003,
      "step": 3177
    },
    {
      "epoch": 132.07,
      "grad_norm": 0.13355736434459686,
      "learning_rate": 3.040771928539038e-06,
      "loss": 0.0013,
      "step": 3178
    },
    {
      "epoch": 132.11,
      "grad_norm": 0.0643358826637268,
      "learning_rate": 3.0378229818462835e-06,
      "loss": 0.0007,
      "step": 3179
    },
    {
      "epoch": 132.16,
      "grad_norm": 0.1347804069519043,
      "learning_rate": 3.0348748417303826e-06,
      "loss": 0.0014,
      "step": 3180
    },
    {
      "epoch": 132.2,
      "grad_norm": 0.0043385266326367855,
      "learning_rate": 3.0319275094032053e-06,
      "loss": 0.0003,
      "step": 3181
    },
    {
      "epoch": 132.24,
      "grad_norm": 0.06602799892425537,
      "learning_rate": 3.0289809860762893e-06,
      "loss": 0.0006,
      "step": 3182
    },
    {
      "epoch": 132.28,
      "grad_norm": 0.06378619372844696,
      "learning_rate": 3.026035272960837e-06,
      "loss": 0.0006,
      "step": 3183
    },
    {
      "epoch": 132.32,
      "grad_norm": 0.13732652366161346,
      "learning_rate": 3.0230903712677207e-06,
      "loss": 0.0009,
      "step": 3184
    },
    {
      "epoch": 132.36,
      "grad_norm": 0.03181292489171028,
      "learning_rate": 3.020146282207479e-06,
      "loss": 0.0004,
      "step": 3185
    },
    {
      "epoch": 132.41,
      "grad_norm": 0.02028530277311802,
      "learning_rate": 3.0172030069903147e-06,
      "loss": 0.0003,
      "step": 3186
    },
    {
      "epoch": 132.45,
      "grad_norm": 0.037806130945682526,
      "learning_rate": 3.0142605468260976e-06,
      "loss": 0.0004,
      "step": 3187
    },
    {
      "epoch": 132.49,
      "grad_norm": 0.06495369970798492,
      "learning_rate": 3.0113189029243616e-06,
      "loss": 0.0005,
      "step": 3188
    },
    {
      "epoch": 132.53,
      "grad_norm": 0.059274088591337204,
      "learning_rate": 3.008378076494306e-06,
      "loss": 0.0006,
      "step": 3189
    },
    {
      "epoch": 132.57,
      "grad_norm": 0.05604527145624161,
      "learning_rate": 3.005438068744792e-06,
      "loss": 0.0005,
      "step": 3190
    },
    {
      "epoch": 132.61,
      "grad_norm": 0.11496201157569885,
      "learning_rate": 3.0024988808843475e-06,
      "loss": 0.0013,
      "step": 3191
    },
    {
      "epoch": 132.65,
      "grad_norm": 0.004815190099179745,
      "learning_rate": 2.9995605141211615e-06,
      "loss": 0.0003,
      "step": 3192
    },
    {
      "epoch": 132.7,
      "grad_norm": 0.033507317304611206,
      "learning_rate": 2.9966229696630837e-06,
      "loss": 0.0004,
      "step": 3193
    },
    {
      "epoch": 132.74,
      "grad_norm": 0.004564433358609676,
      "learning_rate": 2.9936862487176295e-06,
      "loss": 0.0002,
      "step": 3194
    },
    {
      "epoch": 132.78,
      "grad_norm": 0.038127947598695755,
      "learning_rate": 2.9907503524919734e-06,
      "loss": 0.0004,
      "step": 3195
    },
    {
      "epoch": 132.82,
      "grad_norm": 0.015444963239133358,
      "learning_rate": 2.987815282192951e-06,
      "loss": 0.0003,
      "step": 3196
    },
    {
      "epoch": 132.86,
      "grad_norm": 0.004084729123860598,
      "learning_rate": 2.984881039027059e-06,
      "loss": 0.0002,
      "step": 3197
    },
    {
      "epoch": 132.9,
      "grad_norm": 0.024527041241526604,
      "learning_rate": 2.981947624200455e-06,
      "loss": 0.0003,
      "step": 3198
    },
    {
      "epoch": 132.95,
      "grad_norm": 0.15192781388759613,
      "learning_rate": 2.9790150389189544e-06,
      "loss": 0.0013,
      "step": 3199
    },
    {
      "epoch": 132.99,
      "grad_norm": 0.055927958339452744,
      "learning_rate": 2.976083284388031e-06,
      "loss": 0.0005,
      "step": 3200
    },
    {
      "epoch": 133.03,
      "grad_norm": 0.07706937938928604,
      "learning_rate": 2.97315236181282e-06,
      "loss": 0.0007,
      "step": 3201
    },
    {
      "epoch": 133.07,
      "grad_norm": 0.05089866742491722,
      "learning_rate": 2.9702222723981113e-06,
      "loss": 0.0005,
      "step": 3202
    },
    {
      "epoch": 133.11,
      "grad_norm": 0.08647633343935013,
      "learning_rate": 2.967293017348357e-06,
      "loss": 0.0007,
      "step": 3203
    },
    {
      "epoch": 133.15,
      "grad_norm": 0.06131656467914581,
      "learning_rate": 2.964364597867659e-06,
      "loss": 0.0007,
      "step": 3204
    },
    {
      "epoch": 133.19,
      "grad_norm": 0.04108545184135437,
      "learning_rate": 2.9614370151597837e-06,
      "loss": 0.0005,
      "step": 3205
    },
    {
      "epoch": 133.24,
      "grad_norm": 0.08355040848255157,
      "learning_rate": 2.9585102704281475e-06,
      "loss": 0.0009,
      "step": 3206
    },
    {
      "epoch": 133.28,
      "grad_norm": 0.0044241915456950665,
      "learning_rate": 2.9555843648758255e-06,
      "loss": 0.0003,
      "step": 3207
    },
    {
      "epoch": 133.32,
      "grad_norm": 0.004314904101192951,
      "learning_rate": 2.9526592997055488e-06,
      "loss": 0.0002,
      "step": 3208
    },
    {
      "epoch": 133.36,
      "grad_norm": 0.10616973042488098,
      "learning_rate": 2.949735076119697e-06,
      "loss": 0.0008,
      "step": 3209
    },
    {
      "epoch": 133.4,
      "grad_norm": 0.056690312922000885,
      "learning_rate": 2.9468116953203107e-06,
      "loss": 0.0007,
      "step": 3210
    },
    {
      "epoch": 133.44,
      "grad_norm": 0.004416616167873144,
      "learning_rate": 2.94388915850908e-06,
      "loss": 0.0003,
      "step": 3211
    },
    {
      "epoch": 133.49,
      "grad_norm": 0.027638277038931847,
      "learning_rate": 2.940967466887351e-06,
      "loss": 0.0004,
      "step": 3212
    },
    {
      "epoch": 133.53,
      "grad_norm": 0.11067907512187958,
      "learning_rate": 2.9380466216561186e-06,
      "loss": 0.001,
      "step": 3213
    },
    {
      "epoch": 133.57,
      "grad_norm": 0.058527469635009766,
      "learning_rate": 2.9351266240160344e-06,
      "loss": 0.0005,
      "step": 3214
    },
    {
      "epoch": 133.61,
      "grad_norm": 0.04417358711361885,
      "learning_rate": 2.932207475167398e-06,
      "loss": 0.0005,
      "step": 3215
    },
    {
      "epoch": 133.65,
      "grad_norm": 0.061700861901044846,
      "learning_rate": 2.929289176310161e-06,
      "loss": 0.0006,
      "step": 3216
    },
    {
      "epoch": 133.69,
      "grad_norm": 0.004249413963407278,
      "learning_rate": 2.9263717286439254e-06,
      "loss": 0.0002,
      "step": 3217
    },
    {
      "epoch": 133.74,
      "grad_norm": 0.08424663543701172,
      "learning_rate": 2.923455133367945e-06,
      "loss": 0.0007,
      "step": 3218
    },
    {
      "epoch": 133.78,
      "grad_norm": 0.06704867631196976,
      "learning_rate": 2.920539391681121e-06,
      "loss": 0.0006,
      "step": 3219
    },
    {
      "epoch": 133.82,
      "grad_norm": 0.019726525992155075,
      "learning_rate": 2.9176245047820064e-06,
      "loss": 0.0003,
      "step": 3220
    },
    {
      "epoch": 133.86,
      "grad_norm": 0.04712662473320961,
      "learning_rate": 2.9147104738687975e-06,
      "loss": 0.0005,
      "step": 3221
    },
    {
      "epoch": 133.9,
      "grad_norm": 0.03382495418190956,
      "learning_rate": 2.911797300139345e-06,
      "loss": 0.0004,
      "step": 3222
    },
    {
      "epoch": 133.94,
      "grad_norm": 0.08854461461305618,
      "learning_rate": 2.908884984791145e-06,
      "loss": 0.0007,
      "step": 3223
    },
    {
      "epoch": 133.98,
      "grad_norm": 0.04970989003777504,
      "learning_rate": 2.9059735290213387e-06,
      "loss": 0.0005,
      "step": 3224
    },
    {
      "epoch": 134.03,
      "grad_norm": 0.126395121216774,
      "learning_rate": 2.9030629340267165e-06,
      "loss": 0.0009,
      "step": 3225
    },
    {
      "epoch": 134.07,
      "grad_norm": 0.039694856852293015,
      "learning_rate": 2.9001532010037136e-06,
      "loss": 0.0005,
      "step": 3226
    },
    {
      "epoch": 134.11,
      "grad_norm": 0.046802472323179245,
      "learning_rate": 2.8972443311484116e-06,
      "loss": 0.0008,
      "step": 3227
    },
    {
      "epoch": 134.15,
      "grad_norm": 0.06965404748916626,
      "learning_rate": 2.8943363256565394e-06,
      "loss": 0.0007,
      "step": 3228
    },
    {
      "epoch": 134.19,
      "grad_norm": 0.07372491806745529,
      "learning_rate": 2.891429185723464e-06,
      "loss": 0.0006,
      "step": 3229
    },
    {
      "epoch": 134.23,
      "grad_norm": 0.04345780611038208,
      "learning_rate": 2.8885229125442022e-06,
      "loss": 0.0004,
      "step": 3230
    },
    {
      "epoch": 134.28,
      "grad_norm": 0.0875023603439331,
      "learning_rate": 2.885617507313414e-06,
      "loss": 0.001,
      "step": 3231
    },
    {
      "epoch": 134.32,
      "grad_norm": 0.0207917932420969,
      "learning_rate": 2.882712971225401e-06,
      "loss": 0.0003,
      "step": 3232
    },
    {
      "epoch": 134.36,
      "grad_norm": 0.04312150552868843,
      "learning_rate": 2.879809305474108e-06,
      "loss": 0.0005,
      "step": 3233
    },
    {
      "epoch": 134.4,
      "grad_norm": 0.05106602981686592,
      "learning_rate": 2.8769065112531243e-06,
      "loss": 0.0005,
      "step": 3234
    },
    {
      "epoch": 134.44,
      "grad_norm": 0.0761248990893364,
      "learning_rate": 2.8740045897556766e-06,
      "loss": 0.0009,
      "step": 3235
    },
    {
      "epoch": 134.48,
      "grad_norm": 0.043824270367622375,
      "learning_rate": 2.871103542174637e-06,
      "loss": 0.0006,
      "step": 3236
    },
    {
      "epoch": 134.52,
      "grad_norm": 0.10504481196403503,
      "learning_rate": 2.8682033697025145e-06,
      "loss": 0.0012,
      "step": 3237
    },
    {
      "epoch": 134.57,
      "grad_norm": 0.1269329935312271,
      "learning_rate": 2.865304073531463e-06,
      "loss": 0.001,
      "step": 3238
    },
    {
      "epoch": 134.61,
      "grad_norm": 0.02259850688278675,
      "learning_rate": 2.8624056548532738e-06,
      "loss": 0.0003,
      "step": 3239
    },
    {
      "epoch": 134.65,
      "grad_norm": 0.06399255245923996,
      "learning_rate": 2.859508114859374e-06,
      "loss": 0.0006,
      "step": 3240
    },
    {
      "epoch": 134.69,
      "grad_norm": 0.03040248155593872,
      "learning_rate": 2.8566114547408342e-06,
      "loss": 0.0004,
      "step": 3241
    },
    {
      "epoch": 134.73,
      "grad_norm": 0.00429829815402627,
      "learning_rate": 2.8537156756883634e-06,
      "loss": 0.0003,
      "step": 3242
    },
    {
      "epoch": 134.77,
      "grad_norm": 0.024389691650867462,
      "learning_rate": 2.850820778892305e-06,
      "loss": 0.0003,
      "step": 3243
    },
    {
      "epoch": 134.82,
      "grad_norm": 0.004514005035161972,
      "learning_rate": 2.8479267655426435e-06,
      "loss": 0.0002,
      "step": 3244
    },
    {
      "epoch": 134.86,
      "grad_norm": 0.004120887257158756,
      "learning_rate": 2.845033636828998e-06,
      "loss": 0.0002,
      "step": 3245
    },
    {
      "epoch": 134.9,
      "grad_norm": 0.03572593629360199,
      "learning_rate": 2.842141393940624e-06,
      "loss": 0.0004,
      "step": 3246
    },
    {
      "epoch": 134.94,
      "grad_norm": 0.004545103292912245,
      "learning_rate": 2.8392500380664128e-06,
      "loss": 0.0002,
      "step": 3247
    },
    {
      "epoch": 134.98,
      "grad_norm": 0.04128793254494667,
      "learning_rate": 2.8363595703948933e-06,
      "loss": 0.0005,
      "step": 3248
    },
    {
      "epoch": 135.02,
      "grad_norm": 0.0678093433380127,
      "learning_rate": 2.833469992114228e-06,
      "loss": 0.0007,
      "step": 3249
    },
    {
      "epoch": 135.06,
      "grad_norm": 0.08041400462388992,
      "learning_rate": 2.83058130441221e-06,
      "loss": 0.0009,
      "step": 3250
    },
    {
      "epoch": 135.11,
      "grad_norm": 0.03887240216135979,
      "learning_rate": 2.827693508476271e-06,
      "loss": 0.0004,
      "step": 3251
    },
    {
      "epoch": 135.15,
      "grad_norm": 0.08554932475090027,
      "learning_rate": 2.824806605493477e-06,
      "loss": 0.0005,
      "step": 3252
    },
    {
      "epoch": 135.19,
      "grad_norm": 0.041884828358888626,
      "learning_rate": 2.8219205966505224e-06,
      "loss": 0.0004,
      "step": 3253
    },
    {
      "epoch": 135.23,
      "grad_norm": 0.04526301100850105,
      "learning_rate": 2.81903548313374e-06,
      "loss": 0.0004,
      "step": 3254
    },
    {
      "epoch": 135.27,
      "grad_norm": 0.12263626605272293,
      "learning_rate": 2.8161512661290847e-06,
      "loss": 0.001,
      "step": 3255
    },
    {
      "epoch": 135.31,
      "grad_norm": 0.004169909283518791,
      "learning_rate": 2.8132679468221537e-06,
      "loss": 0.0003,
      "step": 3256
    },
    {
      "epoch": 135.36,
      "grad_norm": 0.0680975690484047,
      "learning_rate": 2.8103855263981695e-06,
      "loss": 0.0006,
      "step": 3257
    },
    {
      "epoch": 135.4,
      "grad_norm": 0.09502742439508438,
      "learning_rate": 2.8075040060419867e-06,
      "loss": 0.0009,
      "step": 3258
    },
    {
      "epoch": 135.44,
      "grad_norm": 0.03801250085234642,
      "learning_rate": 2.804623386938089e-06,
      "loss": 0.0004,
      "step": 3259
    },
    {
      "epoch": 135.48,
      "grad_norm": 0.0683569461107254,
      "learning_rate": 2.80174367027059e-06,
      "loss": 0.0005,
      "step": 3260
    },
    {
      "epoch": 135.52,
      "grad_norm": 0.03983624652028084,
      "learning_rate": 2.798864857223235e-06,
      "loss": 0.0005,
      "step": 3261
    },
    {
      "epoch": 135.56,
      "grad_norm": 0.05509570240974426,
      "learning_rate": 2.7959869489793912e-06,
      "loss": 0.0005,
      "step": 3262
    },
    {
      "epoch": 135.61,
      "grad_norm": 0.03269055858254433,
      "learning_rate": 2.7931099467220594e-06,
      "loss": 0.0004,
      "step": 3263
    },
    {
      "epoch": 135.65,
      "grad_norm": 0.022090308368206024,
      "learning_rate": 2.790233851633868e-06,
      "loss": 0.0003,
      "step": 3264
    },
    {
      "epoch": 135.69,
      "grad_norm": 0.027138523757457733,
      "learning_rate": 2.7873586648970686e-06,
      "loss": 0.0003,
      "step": 3265
    },
    {
      "epoch": 135.73,
      "grad_norm": 0.08940760046243668,
      "learning_rate": 2.7844843876935446e-06,
      "loss": 0.0007,
      "step": 3266
    },
    {
      "epoch": 135.77,
      "grad_norm": 0.04254162684082985,
      "learning_rate": 2.781611021204801e-06,
      "loss": 0.0005,
      "step": 3267
    },
    {
      "epoch": 135.81,
      "grad_norm": 0.0393201969563961,
      "learning_rate": 2.7787385666119704e-06,
      "loss": 0.0005,
      "step": 3268
    },
    {
      "epoch": 135.85,
      "grad_norm": 0.10025846213102341,
      "learning_rate": 2.775867025095811e-06,
      "loss": 0.0007,
      "step": 3269
    },
    {
      "epoch": 135.9,
      "grad_norm": 0.0371379517018795,
      "learning_rate": 2.772996397836704e-06,
      "loss": 0.0007,
      "step": 3270
    },
    {
      "epoch": 135.94,
      "grad_norm": 0.04817836359143257,
      "learning_rate": 2.7701266860146575e-06,
      "loss": 0.0004,
      "step": 3271
    },
    {
      "epoch": 135.98,
      "grad_norm": 0.03529445081949234,
      "learning_rate": 2.7672578908093024e-06,
      "loss": 0.0005,
      "step": 3272
    },
    {
      "epoch": 136.02,
      "grad_norm": 0.06256917864084244,
      "learning_rate": 2.764390013399888e-06,
      "loss": 0.0006,
      "step": 3273
    },
    {
      "epoch": 136.06,
      "grad_norm": 0.028341049328446388,
      "learning_rate": 2.7615230549652933e-06,
      "loss": 0.0004,
      "step": 3274
    },
    {
      "epoch": 136.1,
      "grad_norm": 0.004155456088483334,
      "learning_rate": 2.7586570166840154e-06,
      "loss": 0.0002,
      "step": 3275
    },
    {
      "epoch": 136.15,
      "grad_norm": 0.004136839881539345,
      "learning_rate": 2.755791899734176e-06,
      "loss": 0.0002,
      "step": 3276
    },
    {
      "epoch": 136.19,
      "grad_norm": 0.02705666795372963,
      "learning_rate": 2.7529277052935142e-06,
      "loss": 0.0003,
      "step": 3277
    },
    {
      "epoch": 136.23,
      "grad_norm": 0.0043060858733952045,
      "learning_rate": 2.7500644345393945e-06,
      "loss": 0.0003,
      "step": 3278
    },
    {
      "epoch": 136.27,
      "grad_norm": 0.09693845361471176,
      "learning_rate": 2.7472020886487977e-06,
      "loss": 0.001,
      "step": 3279
    },
    {
      "epoch": 136.31,
      "grad_norm": 0.07330675423145294,
      "learning_rate": 2.7443406687983267e-06,
      "loss": 0.0007,
      "step": 3280
    },
    {
      "epoch": 136.35,
      "grad_norm": 0.023577695712447166,
      "learning_rate": 2.741480176164203e-06,
      "loss": 0.0003,
      "step": 3281
    },
    {
      "epoch": 136.39,
      "grad_norm": 0.06923097372055054,
      "learning_rate": 2.738620611922269e-06,
      "loss": 0.0007,
      "step": 3282
    },
    {
      "epoch": 136.44,
      "grad_norm": 0.02151472680270672,
      "learning_rate": 2.7357619772479805e-06,
      "loss": 0.0003,
      "step": 3283
    },
    {
      "epoch": 136.48,
      "grad_norm": 0.04530884698033333,
      "learning_rate": 2.7329042733164145e-06,
      "loss": 0.0005,
      "step": 3284
    },
    {
      "epoch": 136.52,
      "grad_norm": 0.01137604657560587,
      "learning_rate": 2.7300475013022666e-06,
      "loss": 0.0003,
      "step": 3285
    },
    {
      "epoch": 136.56,
      "grad_norm": 0.06353501975536346,
      "learning_rate": 2.727191662379847e-06,
      "loss": 0.0007,
      "step": 3286
    },
    {
      "epoch": 136.6,
      "grad_norm": 0.06384061276912689,
      "learning_rate": 2.724336757723084e-06,
      "loss": 0.0006,
      "step": 3287
    },
    {
      "epoch": 136.64,
      "grad_norm": 0.004255049396306276,
      "learning_rate": 2.7214827885055194e-06,
      "loss": 0.0002,
      "step": 3288
    },
    {
      "epoch": 136.69,
      "grad_norm": 0.04352188855409622,
      "learning_rate": 2.718629755900315e-06,
      "loss": 0.0004,
      "step": 3289
    },
    {
      "epoch": 136.73,
      "grad_norm": 0.05544060468673706,
      "learning_rate": 2.7157776610802416e-06,
      "loss": 0.0006,
      "step": 3290
    },
    {
      "epoch": 136.77,
      "grad_norm": 0.07600650191307068,
      "learning_rate": 2.7129265052176902e-06,
      "loss": 0.0005,
      "step": 3291
    },
    {
      "epoch": 136.81,
      "grad_norm": 0.026172418147325516,
      "learning_rate": 2.7100762894846633e-06,
      "loss": 0.0004,
      "step": 3292
    },
    {
      "epoch": 136.85,
      "grad_norm": 0.06411270797252655,
      "learning_rate": 2.707227015052774e-06,
      "loss": 0.0009,
      "step": 3293
    },
    {
      "epoch": 136.89,
      "grad_norm": 0.08692473918199539,
      "learning_rate": 2.704378683093254e-06,
      "loss": 0.0008,
      "step": 3294
    },
    {
      "epoch": 136.94,
      "grad_norm": 0.12423156201839447,
      "learning_rate": 2.7015312947769436e-06,
      "loss": 0.0011,
      "step": 3295
    },
    {
      "epoch": 136.98,
      "grad_norm": 0.029198236763477325,
      "learning_rate": 2.698684851274297e-06,
      "loss": 0.0004,
      "step": 3296
    },
    {
      "epoch": 137.02,
      "grad_norm": 0.061397675424814224,
      "learning_rate": 2.6958393537553793e-06,
      "loss": 0.0005,
      "step": 3297
    },
    {
      "epoch": 137.06,
      "grad_norm": 0.059899136424064636,
      "learning_rate": 2.6929948033898683e-06,
      "loss": 0.0004,
      "step": 3298
    },
    {
      "epoch": 137.1,
      "grad_norm": 0.057085081934928894,
      "learning_rate": 2.6901512013470523e-06,
      "loss": 0.0006,
      "step": 3299
    },
    {
      "epoch": 137.14,
      "grad_norm": 0.07350123673677444,
      "learning_rate": 2.687308548795825e-06,
      "loss": 0.0007,
      "step": 3300
    },
    {
      "epoch": 137.18,
      "grad_norm": 0.026950031518936157,
      "learning_rate": 2.684466846904695e-06,
      "loss": 0.0004,
      "step": 3301
    },
    {
      "epoch": 137.23,
      "grad_norm": 0.004478003364056349,
      "learning_rate": 2.6816260968417785e-06,
      "loss": 0.0002,
      "step": 3302
    },
    {
      "epoch": 137.27,
      "grad_norm": 0.13780570030212402,
      "learning_rate": 2.6787862997748014e-06,
      "loss": 0.0012,
      "step": 3303
    },
    {
      "epoch": 137.31,
      "grad_norm": 0.06072232127189636,
      "learning_rate": 2.675947456871096e-06,
      "loss": 0.0005,
      "step": 3304
    },
    {
      "epoch": 137.35,
      "grad_norm": 0.08184506744146347,
      "learning_rate": 2.6731095692976073e-06,
      "loss": 0.0008,
      "step": 3305
    },
    {
      "epoch": 137.39,
      "grad_norm": 0.021724428981542587,
      "learning_rate": 2.6702726382208775e-06,
      "loss": 0.0003,
      "step": 3306
    },
    {
      "epoch": 137.43,
      "grad_norm": 0.07043638825416565,
      "learning_rate": 2.667436664807065e-06,
      "loss": 0.0006,
      "step": 3307
    },
    {
      "epoch": 137.48,
      "grad_norm": 0.027371278032660484,
      "learning_rate": 2.6646016502219304e-06,
      "loss": 0.0003,
      "step": 3308
    },
    {
      "epoch": 137.52,
      "grad_norm": 0.04484659060835838,
      "learning_rate": 2.661767595630842e-06,
      "loss": 0.0004,
      "step": 3309
    },
    {
      "epoch": 137.56,
      "grad_norm": 0.08727559447288513,
      "learning_rate": 2.6589345021987725e-06,
      "loss": 0.0006,
      "step": 3310
    },
    {
      "epoch": 137.6,
      "grad_norm": 0.0548698827624321,
      "learning_rate": 2.6561023710902985e-06,
      "loss": 0.0006,
      "step": 3311
    },
    {
      "epoch": 137.64,
      "grad_norm": 0.043092332780361176,
      "learning_rate": 2.6532712034696034e-06,
      "loss": 0.0005,
      "step": 3312
    },
    {
      "epoch": 137.68,
      "grad_norm": 0.06639162451028824,
      "learning_rate": 2.6504410005004734e-06,
      "loss": 0.0006,
      "step": 3313
    },
    {
      "epoch": 137.72,
      "grad_norm": 0.029088493436574936,
      "learning_rate": 2.6476117633462966e-06,
      "loss": 0.0004,
      "step": 3314
    },
    {
      "epoch": 137.77,
      "grad_norm": 0.004130925517529249,
      "learning_rate": 2.6447834931700688e-06,
      "loss": 0.0002,
      "step": 3315
    },
    {
      "epoch": 137.81,
      "grad_norm": 0.06446783244609833,
      "learning_rate": 2.6419561911343812e-06,
      "loss": 0.0004,
      "step": 3316
    },
    {
      "epoch": 137.85,
      "grad_norm": 0.05876903235912323,
      "learning_rate": 2.6391298584014313e-06,
      "loss": 0.0005,
      "step": 3317
    },
    {
      "epoch": 137.89,
      "grad_norm": 0.022502973675727844,
      "learning_rate": 2.6363044961330196e-06,
      "loss": 0.0003,
      "step": 3318
    },
    {
      "epoch": 137.93,
      "grad_norm": 0.054527007043361664,
      "learning_rate": 2.6334801054905447e-06,
      "loss": 0.0005,
      "step": 3319
    },
    {
      "epoch": 137.97,
      "grad_norm": 0.06945934891700745,
      "learning_rate": 2.6306566876350072e-06,
      "loss": 0.0007,
      "step": 3320
    },
    {
      "epoch": 138.02,
      "grad_norm": 0.12612903118133545,
      "learning_rate": 2.627834243727008e-06,
      "loss": 0.0008,
      "step": 3321
    },
    {
      "epoch": 138.06,
      "grad_norm": 0.06631918996572495,
      "learning_rate": 2.6250127749267458e-06,
      "loss": 0.0006,
      "step": 3322
    },
    {
      "epoch": 138.1,
      "grad_norm": 0.056575629860162735,
      "learning_rate": 2.6221922823940205e-06,
      "loss": 0.0004,
      "step": 3323
    },
    {
      "epoch": 138.14,
      "grad_norm": 0.043061066418886185,
      "learning_rate": 2.6193727672882308e-06,
      "loss": 0.0005,
      "step": 3324
    },
    {
      "epoch": 138.18,
      "grad_norm": 0.05061020329594612,
      "learning_rate": 2.6165542307683744e-06,
      "loss": 0.0004,
      "step": 3325
    },
    {
      "epoch": 138.22,
      "grad_norm": 0.12192101776599884,
      "learning_rate": 2.6137366739930415e-06,
      "loss": 0.001,
      "step": 3326
    },
    {
      "epoch": 138.26,
      "grad_norm": 0.15500551462173462,
      "learning_rate": 2.610920098120424e-06,
      "loss": 0.0012,
      "step": 3327
    },
    {
      "epoch": 138.31,
      "grad_norm": 0.0437592975795269,
      "learning_rate": 2.608104504308311e-06,
      "loss": 0.0005,
      "step": 3328
    },
    {
      "epoch": 138.35,
      "grad_norm": 0.08992220461368561,
      "learning_rate": 2.605289893714087e-06,
      "loss": 0.0009,
      "step": 3329
    },
    {
      "epoch": 138.39,
      "grad_norm": 0.027684077620506287,
      "learning_rate": 2.6024762674947313e-06,
      "loss": 0.0003,
      "step": 3330
    },
    {
      "epoch": 138.43,
      "grad_norm": 0.00442394707351923,
      "learning_rate": 2.5996636268068186e-06,
      "loss": 0.0002,
      "step": 3331
    },
    {
      "epoch": 138.47,
      "grad_norm": 0.06384424865245819,
      "learning_rate": 2.596851972806522e-06,
      "loss": 0.0006,
      "step": 3332
    },
    {
      "epoch": 138.51,
      "grad_norm": 0.028455132618546486,
      "learning_rate": 2.5940413066496033e-06,
      "loss": 0.0003,
      "step": 3333
    },
    {
      "epoch": 138.56,
      "grad_norm": 0.08113575726747513,
      "learning_rate": 2.5912316294914232e-06,
      "loss": 0.0006,
      "step": 3334
    },
    {
      "epoch": 138.6,
      "grad_norm": 0.04915155470371246,
      "learning_rate": 2.588422942486932e-06,
      "loss": 0.0004,
      "step": 3335
    },
    {
      "epoch": 138.64,
      "grad_norm": 0.040823787450790405,
      "learning_rate": 2.5856152467906793e-06,
      "loss": 0.0004,
      "step": 3336
    },
    {
      "epoch": 138.68,
      "grad_norm": 0.017566382884979248,
      "learning_rate": 2.582808543556796e-06,
      "loss": 0.0003,
      "step": 3337
    },
    {
      "epoch": 138.72,
      "grad_norm": 0.11981866508722305,
      "learning_rate": 2.5800028339390164e-06,
      "loss": 0.002,
      "step": 3338
    },
    {
      "epoch": 138.76,
      "grad_norm": 0.07924176752567291,
      "learning_rate": 2.57719811909066e-06,
      "loss": 0.0006,
      "step": 3339
    },
    {
      "epoch": 138.81,
      "grad_norm": 0.05739292502403259,
      "learning_rate": 2.5743944001646394e-06,
      "loss": 0.0007,
      "step": 3340
    },
    {
      "epoch": 138.85,
      "grad_norm": 0.00413434999063611,
      "learning_rate": 2.5715916783134583e-06,
      "loss": 0.0002,
      "step": 3341
    },
    {
      "epoch": 138.89,
      "grad_norm": 0.05422649160027504,
      "learning_rate": 2.5687899546892087e-06,
      "loss": 0.0004,
      "step": 3342
    },
    {
      "epoch": 138.93,
      "grad_norm": 0.05186155065894127,
      "learning_rate": 2.565989230443574e-06,
      "loss": 0.0006,
      "step": 3343
    },
    {
      "epoch": 138.97,
      "grad_norm": 0.030236244201660156,
      "learning_rate": 2.563189506727828e-06,
      "loss": 0.0003,
      "step": 3344
    },
    {
      "epoch": 139.01,
      "grad_norm": 0.1472548395395279,
      "learning_rate": 2.5603907846928277e-06,
      "loss": 0.0013,
      "step": 3345
    },
    {
      "epoch": 139.05,
      "grad_norm": 0.034638818353414536,
      "learning_rate": 2.5575930654890235e-06,
      "loss": 0.0004,
      "step": 3346
    },
    {
      "epoch": 139.1,
      "grad_norm": 0.04346667230129242,
      "learning_rate": 2.5547963502664518e-06,
      "loss": 0.0004,
      "step": 3347
    },
    {
      "epoch": 139.14,
      "grad_norm": 0.05084613338112831,
      "learning_rate": 2.55200064017474e-06,
      "loss": 0.0004,
      "step": 3348
    },
    {
      "epoch": 139.18,
      "grad_norm": 0.07258988916873932,
      "learning_rate": 2.549205936363094e-06,
      "loss": 0.0008,
      "step": 3349
    },
    {
      "epoch": 139.22,
      "grad_norm": 0.04824227839708328,
      "learning_rate": 2.5464122399803126e-06,
      "loss": 0.0004,
      "step": 3350
    },
    {
      "epoch": 139.26,
      "grad_norm": 0.020994462072849274,
      "learning_rate": 2.5436195521747788e-06,
      "loss": 0.0003,
      "step": 3351
    },
    {
      "epoch": 139.3,
      "grad_norm": 0.06692570447921753,
      "learning_rate": 2.540827874094462e-06,
      "loss": 0.0007,
      "step": 3352
    },
    {
      "epoch": 139.35,
      "grad_norm": 0.030320018529891968,
      "learning_rate": 2.5380372068869152e-06,
      "loss": 0.0004,
      "step": 3353
    },
    {
      "epoch": 139.39,
      "grad_norm": 0.028114695101976395,
      "learning_rate": 2.5352475516992758e-06,
      "loss": 0.0004,
      "step": 3354
    },
    {
      "epoch": 139.43,
      "grad_norm": 0.040825486183166504,
      "learning_rate": 2.532458909678266e-06,
      "loss": 0.0004,
      "step": 3355
    },
    {
      "epoch": 139.47,
      "grad_norm": 0.13140447437763214,
      "learning_rate": 2.529671281970192e-06,
      "loss": 0.0009,
      "step": 3356
    },
    {
      "epoch": 139.51,
      "grad_norm": 0.05346349626779556,
      "learning_rate": 2.5268846697209407e-06,
      "loss": 0.0004,
      "step": 3357
    },
    {
      "epoch": 139.55,
      "grad_norm": 0.03608167544007301,
      "learning_rate": 2.5240990740759843e-06,
      "loss": 0.0004,
      "step": 3358
    },
    {
      "epoch": 139.59,
      "grad_norm": 0.004113290458917618,
      "learning_rate": 2.5213144961803778e-06,
      "loss": 0.0002,
      "step": 3359
    },
    {
      "epoch": 139.64,
      "grad_norm": 0.02253629080951214,
      "learning_rate": 2.5185309371787515e-06,
      "loss": 0.0003,
      "step": 3360
    },
    {
      "epoch": 139.68,
      "grad_norm": 0.06936071813106537,
      "learning_rate": 2.5157483982153237e-06,
      "loss": 0.0007,
      "step": 3361
    },
    {
      "epoch": 139.72,
      "grad_norm": 0.08538495004177094,
      "learning_rate": 2.512966880433891e-06,
      "loss": 0.0007,
      "step": 3362
    },
    {
      "epoch": 139.76,
      "grad_norm": 0.08301747590303421,
      "learning_rate": 2.5101863849778304e-06,
      "loss": 0.0008,
      "step": 3363
    },
    {
      "epoch": 139.8,
      "grad_norm": 0.0391354076564312,
      "learning_rate": 2.507406912990098e-06,
      "loss": 0.0005,
      "step": 3364
    },
    {
      "epoch": 139.84,
      "grad_norm": 0.11625775694847107,
      "learning_rate": 2.50462846561323e-06,
      "loss": 0.0013,
      "step": 3365
    },
    {
      "epoch": 139.89,
      "grad_norm": 0.10240693390369415,
      "learning_rate": 2.501851043989343e-06,
      "loss": 0.0012,
      "step": 3366
    },
    {
      "epoch": 139.93,
      "grad_norm": 0.003996486309915781,
      "learning_rate": 2.499074649260127e-06,
      "loss": 0.0002,
      "step": 3367
    },
    {
      "epoch": 139.97,
      "grad_norm": 0.08877085894346237,
      "learning_rate": 2.4962992825668546e-06,
      "loss": 0.0006,
      "step": 3368
    },
    {
      "epoch": 140.01,
      "grad_norm": 0.004399593453854322,
      "learning_rate": 2.493524945050376e-06,
      "loss": 0.0003,
      "step": 3369
    },
    {
      "epoch": 140.05,
      "grad_norm": 0.01879606395959854,
      "learning_rate": 2.4907516378511137e-06,
      "loss": 0.0003,
      "step": 3370
    },
    {
      "epoch": 140.09,
      "grad_norm": 0.05006158724427223,
      "learning_rate": 2.48797936210907e-06,
      "loss": 0.0004,
      "step": 3371
    },
    {
      "epoch": 140.14,
      "grad_norm": 0.00399907398968935,
      "learning_rate": 2.4852081189638227e-06,
      "loss": 0.0002,
      "step": 3372
    },
    {
      "epoch": 140.18,
      "grad_norm": 0.003836708376184106,
      "learning_rate": 2.4824379095545257e-06,
      "loss": 0.0002,
      "step": 3373
    },
    {
      "epoch": 140.22,
      "grad_norm": 0.10648214817047119,
      "learning_rate": 2.4796687350199077e-06,
      "loss": 0.0011,
      "step": 3374
    },
    {
      "epoch": 140.26,
      "grad_norm": 0.07482362538576126,
      "learning_rate": 2.4769005964982718e-06,
      "loss": 0.0005,
      "step": 3375
    },
    {
      "epoch": 140.3,
      "grad_norm": 0.023356696590781212,
      "learning_rate": 2.4741334951274948e-06,
      "loss": 0.0003,
      "step": 3376
    },
    {
      "epoch": 140.34,
      "grad_norm": 0.02481035143136978,
      "learning_rate": 2.4713674320450276e-06,
      "loss": 0.0003,
      "step": 3377
    },
    {
      "epoch": 140.38,
      "grad_norm": 0.07532865554094315,
      "learning_rate": 2.468602408387894e-06,
      "loss": 0.0006,
      "step": 3378
    },
    {
      "epoch": 140.43,
      "grad_norm": 0.09934905171394348,
      "learning_rate": 2.465838425292693e-06,
      "loss": 0.001,
      "step": 3379
    },
    {
      "epoch": 140.47,
      "grad_norm": 0.004242331255227327,
      "learning_rate": 2.46307548389559e-06,
      "loss": 0.0002,
      "step": 3380
    },
    {
      "epoch": 140.51,
      "grad_norm": 0.055977314710617065,
      "learning_rate": 2.4603135853323275e-06,
      "loss": 0.0005,
      "step": 3381
    },
    {
      "epoch": 140.55,
      "grad_norm": 0.056444089859724045,
      "learning_rate": 2.4575527307382174e-06,
      "loss": 0.0005,
      "step": 3382
    },
    {
      "epoch": 140.59,
      "grad_norm": 0.10100051760673523,
      "learning_rate": 2.4547929212481436e-06,
      "loss": 0.0009,
      "step": 3383
    },
    {
      "epoch": 140.63,
      "grad_norm": 0.038601528853178024,
      "learning_rate": 2.452034157996559e-06,
      "loss": 0.0004,
      "step": 3384
    },
    {
      "epoch": 140.68,
      "grad_norm": 0.004118523094803095,
      "learning_rate": 2.4492764421174863e-06,
      "loss": 0.0002,
      "step": 3385
    },
    {
      "epoch": 140.72,
      "grad_norm": 0.033555880188941956,
      "learning_rate": 2.4465197747445197e-06,
      "loss": 0.0005,
      "step": 3386
    },
    {
      "epoch": 140.76,
      "grad_norm": 0.07804335653781891,
      "learning_rate": 2.44376415701082e-06,
      "loss": 0.0008,
      "step": 3387
    },
    {
      "epoch": 140.8,
      "grad_norm": 0.03596970811486244,
      "learning_rate": 2.441009590049118e-06,
      "loss": 0.0004,
      "step": 3388
    },
    {
      "epoch": 140.84,
      "grad_norm": 0.07121952623128891,
      "learning_rate": 2.438256074991712e-06,
      "loss": 0.0007,
      "step": 3389
    },
    {
      "epoch": 140.88,
      "grad_norm": 0.46421679854393005,
      "learning_rate": 2.43550361297047e-06,
      "loss": 0.0023,
      "step": 3390
    },
    {
      "epoch": 140.92,
      "grad_norm": 0.12865011394023895,
      "learning_rate": 2.432752205116821e-06,
      "loss": 0.0006,
      "step": 3391
    },
    {
      "epoch": 140.97,
      "grad_norm": 0.11641496419906616,
      "learning_rate": 2.430001852561769e-06,
      "loss": 0.0009,
      "step": 3392
    },
    {
      "epoch": 141.01,
      "grad_norm": 0.17317095398902893,
      "learning_rate": 2.4272525564358767e-06,
      "loss": 0.0016,
      "step": 3393
    },
    {
      "epoch": 141.05,
      "grad_norm": 0.06783287227153778,
      "learning_rate": 2.4245043178692766e-06,
      "loss": 0.0006,
      "step": 3394
    },
    {
      "epoch": 141.09,
      "grad_norm": 0.07360635697841644,
      "learning_rate": 2.4217571379916673e-06,
      "loss": 0.0006,
      "step": 3395
    },
    {
      "epoch": 141.13,
      "grad_norm": 0.13029791414737701,
      "learning_rate": 2.419011017932309e-06,
      "loss": 0.0012,
      "step": 3396
    },
    {
      "epoch": 141.17,
      "grad_norm": 0.04626245051622391,
      "learning_rate": 2.4162659588200287e-06,
      "loss": 0.0004,
      "step": 3397
    },
    {
      "epoch": 141.22,
      "grad_norm": 0.029645441100001335,
      "learning_rate": 2.4135219617832173e-06,
      "loss": 0.0003,
      "step": 3398
    },
    {
      "epoch": 141.26,
      "grad_norm": 0.03127402067184448,
      "learning_rate": 2.4107790279498273e-06,
      "loss": 0.0003,
      "step": 3399
    },
    {
      "epoch": 141.3,
      "grad_norm": 0.028262659907341003,
      "learning_rate": 2.408037158447375e-06,
      "loss": 0.0003,
      "step": 3400
    },
    {
      "epoch": 141.34,
      "grad_norm": 0.019584590569138527,
      "learning_rate": 2.4052963544029405e-06,
      "loss": 0.0004,
      "step": 3401
    },
    {
      "epoch": 141.38,
      "grad_norm": 0.021472888067364693,
      "learning_rate": 2.402556616943166e-06,
      "loss": 0.0003,
      "step": 3402
    },
    {
      "epoch": 141.42,
      "grad_norm": 0.03212708234786987,
      "learning_rate": 2.39981794719425e-06,
      "loss": 0.0003,
      "step": 3403
    },
    {
      "epoch": 141.46,
      "grad_norm": 0.09671904146671295,
      "learning_rate": 2.3970803462819586e-06,
      "loss": 0.0008,
      "step": 3404
    },
    {
      "epoch": 141.51,
      "grad_norm": 0.0041498723439872265,
      "learning_rate": 2.394343815331616e-06,
      "loss": 0.0002,
      "step": 3405
    },
    {
      "epoch": 141.55,
      "grad_norm": 0.08255563676357269,
      "learning_rate": 2.3916083554681064e-06,
      "loss": 0.0009,
      "step": 3406
    },
    {
      "epoch": 141.59,
      "grad_norm": 0.022348221391439438,
      "learning_rate": 2.3888739678158746e-06,
      "loss": 0.0003,
      "step": 3407
    },
    {
      "epoch": 141.63,
      "grad_norm": 0.056932780891656876,
      "learning_rate": 2.386140653498924e-06,
      "loss": 0.0006,
      "step": 3408
    },
    {
      "epoch": 141.67,
      "grad_norm": 0.046764928847551346,
      "learning_rate": 2.3834084136408163e-06,
      "loss": 0.0006,
      "step": 3409
    },
    {
      "epoch": 141.71,
      "grad_norm": 0.07191451638936996,
      "learning_rate": 2.3806772493646725e-06,
      "loss": 0.0008,
      "step": 3410
    },
    {
      "epoch": 141.76,
      "grad_norm": 0.028610235080122948,
      "learning_rate": 2.377947161793171e-06,
      "loss": 0.0003,
      "step": 3411
    },
    {
      "epoch": 141.8,
      "grad_norm": 0.09257063269615173,
      "learning_rate": 2.3752181520485497e-06,
      "loss": 0.001,
      "step": 3412
    },
    {
      "epoch": 141.84,
      "grad_norm": 0.055850666016340256,
      "learning_rate": 2.3724902212525972e-06,
      "loss": 0.0005,
      "step": 3413
    },
    {
      "epoch": 141.88,
      "grad_norm": 0.05978710576891899,
      "learning_rate": 2.3697633705266652e-06,
      "loss": 0.0004,
      "step": 3414
    },
    {
      "epoch": 141.92,
      "grad_norm": 0.07205533981323242,
      "learning_rate": 2.3670376009916596e-06,
      "loss": 0.0007,
      "step": 3415
    },
    {
      "epoch": 141.96,
      "grad_norm": 0.16896137595176697,
      "learning_rate": 2.36431291376804e-06,
      "loss": 0.0017,
      "step": 3416
    },
    {
      "epoch": 142.01,
      "grad_norm": 0.035416409373283386,
      "learning_rate": 2.3615893099758227e-06,
      "loss": 0.0003,
      "step": 3417
    },
    {
      "epoch": 142.05,
      "grad_norm": 0.11496912688016891,
      "learning_rate": 2.3588667907345787e-06,
      "loss": 0.001,
      "step": 3418
    },
    {
      "epoch": 142.09,
      "grad_norm": 0.02328640967607498,
      "learning_rate": 2.3561453571634323e-06,
      "loss": 0.0003,
      "step": 3419
    },
    {
      "epoch": 142.13,
      "grad_norm": 0.08147231489419937,
      "learning_rate": 2.353425010381063e-06,
      "loss": 0.0007,
      "step": 3420
    },
    {
      "epoch": 142.17,
      "grad_norm": 0.0318191833794117,
      "learning_rate": 2.350705751505702e-06,
      "loss": 0.0005,
      "step": 3421
    },
    {
      "epoch": 142.21,
      "grad_norm": 0.05128950998187065,
      "learning_rate": 2.3479875816551338e-06,
      "loss": 0.0004,
      "step": 3422
    },
    {
      "epoch": 142.25,
      "grad_norm": 0.01904863491654396,
      "learning_rate": 2.3452705019466977e-06,
      "loss": 0.0003,
      "step": 3423
    },
    {
      "epoch": 142.3,
      "grad_norm": 0.024430107325315475,
      "learning_rate": 2.342554513497278e-06,
      "loss": 0.0003,
      "step": 3424
    },
    {
      "epoch": 142.34,
      "grad_norm": 0.032639406621456146,
      "learning_rate": 2.339839617423318e-06,
      "loss": 0.0004,
      "step": 3425
    },
    {
      "epoch": 142.38,
      "grad_norm": 0.004053943324834108,
      "learning_rate": 2.3371258148408075e-06,
      "loss": 0.0002,
      "step": 3426
    },
    {
      "epoch": 142.42,
      "grad_norm": 0.05305226147174835,
      "learning_rate": 2.3344131068652894e-06,
      "loss": 0.0006,
      "step": 3427
    },
    {
      "epoch": 142.46,
      "grad_norm": 0.06684400886297226,
      "learning_rate": 2.331701494611855e-06,
      "loss": 0.0008,
      "step": 3428
    },
    {
      "epoch": 142.5,
      "grad_norm": 0.004127563908696175,
      "learning_rate": 2.3289909791951454e-06,
      "loss": 0.0002,
      "step": 3429
    },
    {
      "epoch": 142.55,
      "grad_norm": 0.00408734567463398,
      "learning_rate": 2.3262815617293517e-06,
      "loss": 0.0002,
      "step": 3430
    },
    {
      "epoch": 142.59,
      "grad_norm": 0.12412000447511673,
      "learning_rate": 2.3235732433282127e-06,
      "loss": 0.0009,
      "step": 3431
    },
    {
      "epoch": 142.63,
      "grad_norm": 0.10655483603477478,
      "learning_rate": 2.320866025105016e-06,
      "loss": 0.001,
      "step": 3432
    },
    {
      "epoch": 142.67,
      "grad_norm": 0.13139736652374268,
      "learning_rate": 2.3181599081725986e-06,
      "loss": 0.001,
      "step": 3433
    },
    {
      "epoch": 142.71,
      "grad_norm": 0.080230213701725,
      "learning_rate": 2.3154548936433387e-06,
      "loss": 0.0005,
      "step": 3434
    },
    {
      "epoch": 142.75,
      "grad_norm": 0.004235441796481609,
      "learning_rate": 2.31275098262917e-06,
      "loss": 0.0003,
      "step": 3435
    },
    {
      "epoch": 142.79,
      "grad_norm": 0.07539902627468109,
      "learning_rate": 2.3100481762415642e-06,
      "loss": 0.0008,
      "step": 3436
    },
    {
      "epoch": 142.84,
      "grad_norm": 0.11875637620687485,
      "learning_rate": 2.307346475591545e-06,
      "loss": 0.0008,
      "step": 3437
    },
    {
      "epoch": 142.88,
      "grad_norm": 0.03103514574468136,
      "learning_rate": 2.304645881789679e-06,
      "loss": 0.0003,
      "step": 3438
    },
    {
      "epoch": 142.92,
      "grad_norm": 0.003928851801902056,
      "learning_rate": 2.3019463959460784e-06,
      "loss": 0.0002,
      "step": 3439
    },
    {
      "epoch": 142.96,
      "grad_norm": 0.051543451845645905,
      "learning_rate": 2.2992480191704003e-06,
      "loss": 0.0005,
      "step": 3440
    },
    {
      "epoch": 143.0,
      "grad_norm": 0.08278694748878479,
      "learning_rate": 2.2965507525718457e-06,
      "loss": 0.0009,
      "step": 3441
    },
    {
      "epoch": 143.04,
      "grad_norm": 0.017808301374316216,
      "learning_rate": 2.2938545972591575e-06,
      "loss": 0.0003,
      "step": 3442
    },
    {
      "epoch": 143.09,
      "grad_norm": 0.11574368178844452,
      "learning_rate": 2.291159554340625e-06,
      "loss": 0.0012,
      "step": 3443
    },
    {
      "epoch": 143.13,
      "grad_norm": 0.04750983044505119,
      "learning_rate": 2.288465624924078e-06,
      "loss": 0.0005,
      "step": 3444
    },
    {
      "epoch": 143.17,
      "grad_norm": 0.04074690863490105,
      "learning_rate": 2.28577281011689e-06,
      "loss": 0.0004,
      "step": 3445
    },
    {
      "epoch": 143.21,
      "grad_norm": 0.004014249891042709,
      "learning_rate": 2.283081111025973e-06,
      "loss": 0.0002,
      "step": 3446
    },
    {
      "epoch": 143.25,
      "grad_norm": 0.08378709852695465,
      "learning_rate": 2.280390528757785e-06,
      "loss": 0.0007,
      "step": 3447
    },
    {
      "epoch": 143.29,
      "grad_norm": 0.09938038140535355,
      "learning_rate": 2.277701064418321e-06,
      "loss": 0.0009,
      "step": 3448
    },
    {
      "epoch": 143.34,
      "grad_norm": 0.04351846128702164,
      "learning_rate": 2.2750127191131195e-06,
      "loss": 0.0005,
      "step": 3449
    },
    {
      "epoch": 143.38,
      "grad_norm": 0.03532731160521507,
      "learning_rate": 2.272325493947257e-06,
      "loss": 0.0004,
      "step": 3450
    },
    {
      "epoch": 143.42,
      "grad_norm": 0.03311336040496826,
      "learning_rate": 2.2696393900253515e-06,
      "loss": 0.0003,
      "step": 3451
    },
    {
      "epoch": 143.46,
      "grad_norm": 0.055587608367204666,
      "learning_rate": 2.2669544084515578e-06,
      "loss": 0.0005,
      "step": 3452
    },
    {
      "epoch": 143.5,
      "grad_norm": 0.07385794818401337,
      "learning_rate": 2.2642705503295705e-06,
      "loss": 0.0007,
      "step": 3453
    },
    {
      "epoch": 143.54,
      "grad_norm": 0.019569406285881996,
      "learning_rate": 2.2615878167626222e-06,
      "loss": 0.0003,
      "step": 3454
    },
    {
      "epoch": 143.58,
      "grad_norm": 0.14579729735851288,
      "learning_rate": 2.2589062088534837e-06,
      "loss": 0.0011,
      "step": 3455
    },
    {
      "epoch": 143.63,
      "grad_norm": 0.05189293995499611,
      "learning_rate": 2.2562257277044645e-06,
      "loss": 0.0006,
      "step": 3456
    },
    {
      "epoch": 143.67,
      "grad_norm": 0.00423282478004694,
      "learning_rate": 2.253546374417405e-06,
      "loss": 0.0002,
      "step": 3457
    },
    {
      "epoch": 143.71,
      "grad_norm": 0.02353525720536709,
      "learning_rate": 2.2508681500936893e-06,
      "loss": 0.0003,
      "step": 3458
    },
    {
      "epoch": 143.75,
      "grad_norm": 0.05804656445980072,
      "learning_rate": 2.2481910558342324e-06,
      "loss": 0.0005,
      "step": 3459
    },
    {
      "epoch": 143.79,
      "grad_norm": 0.07250672578811646,
      "learning_rate": 2.245515092739488e-06,
      "loss": 0.0007,
      "step": 3460
    },
    {
      "epoch": 143.83,
      "grad_norm": 0.03813892602920532,
      "learning_rate": 2.2428402619094425e-06,
      "loss": 0.0004,
      "step": 3461
    },
    {
      "epoch": 143.88,
      "grad_norm": 0.06172050163149834,
      "learning_rate": 2.2401665644436187e-06,
      "loss": 0.0004,
      "step": 3462
    },
    {
      "epoch": 143.92,
      "grad_norm": 0.08719638735055923,
      "learning_rate": 2.2374940014410724e-06,
      "loss": 0.0008,
      "step": 3463
    },
    {
      "epoch": 143.96,
      "grad_norm": 0.06046179309487343,
      "learning_rate": 2.2348225740003927e-06,
      "loss": 0.0006,
      "step": 3464
    },
    {
      "epoch": 144.0,
      "grad_norm": 0.004165220074355602,
      "learning_rate": 2.2321522832197036e-06,
      "loss": 0.0002,
      "step": 3465
    },
    {
      "epoch": 144.04,
      "grad_norm": 0.03609137982130051,
      "learning_rate": 2.229483130196661e-06,
      "loss": 0.0004,
      "step": 3466
    },
    {
      "epoch": 144.08,
      "grad_norm": 0.026236169040203094,
      "learning_rate": 2.2268151160284508e-06,
      "loss": 0.0003,
      "step": 3467
    },
    {
      "epoch": 144.12,
      "grad_norm": 0.06572145968675613,
      "learning_rate": 2.224148241811794e-06,
      "loss": 0.0006,
      "step": 3468
    },
    {
      "epoch": 144.17,
      "grad_norm": 0.12403634190559387,
      "learning_rate": 2.2214825086429415e-06,
      "loss": 0.001,
      "step": 3469
    },
    {
      "epoch": 144.21,
      "grad_norm": 0.05287155508995056,
      "learning_rate": 2.2188179176176767e-06,
      "loss": 0.0006,
      "step": 3470
    },
    {
      "epoch": 144.25,
      "grad_norm": 0.004066606052219868,
      "learning_rate": 2.2161544698313107e-06,
      "loss": 0.0002,
      "step": 3471
    },
    {
      "epoch": 144.29,
      "grad_norm": 0.02293788269162178,
      "learning_rate": 2.2134921663786875e-06,
      "loss": 0.0005,
      "step": 3472
    },
    {
      "epoch": 144.33,
      "grad_norm": 0.003945484291762114,
      "learning_rate": 2.210831008354179e-06,
      "loss": 0.0002,
      "step": 3473
    },
    {
      "epoch": 144.37,
      "grad_norm": 0.02508413977921009,
      "learning_rate": 2.2081709968516867e-06,
      "loss": 0.0003,
      "step": 3474
    },
    {
      "epoch": 144.42,
      "grad_norm": 0.04420150816440582,
      "learning_rate": 2.2055121329646416e-06,
      "loss": 0.0005,
      "step": 3475
    },
    {
      "epoch": 144.46,
      "grad_norm": 0.07173676788806915,
      "learning_rate": 2.2028544177860028e-06,
      "loss": 0.0006,
      "step": 3476
    },
    {
      "epoch": 144.5,
      "grad_norm": 0.0659252479672432,
      "learning_rate": 2.200197852408254e-06,
      "loss": 0.0006,
      "step": 3477
    },
    {
      "epoch": 144.54,
      "grad_norm": 0.05752578750252724,
      "learning_rate": 2.19754243792341e-06,
      "loss": 0.0005,
      "step": 3478
    },
    {
      "epoch": 144.58,
      "grad_norm": 0.09065961837768555,
      "learning_rate": 2.1948881754230116e-06,
      "loss": 0.0007,
      "step": 3479
    },
    {
      "epoch": 144.62,
      "grad_norm": 0.0692565068602562,
      "learning_rate": 2.1922350659981262e-06,
      "loss": 0.0007,
      "step": 3480
    },
    {
      "epoch": 144.66,
      "grad_norm": 0.06586243212223053,
      "learning_rate": 2.1895831107393485e-06,
      "loss": 0.001,
      "step": 3481
    },
    {
      "epoch": 144.71,
      "grad_norm": 0.050079621374607086,
      "learning_rate": 2.1869323107367936e-06,
      "loss": 0.0005,
      "step": 3482
    },
    {
      "epoch": 144.75,
      "grad_norm": 0.07159477472305298,
      "learning_rate": 2.1842826670801063e-06,
      "loss": 0.0006,
      "step": 3483
    },
    {
      "epoch": 144.79,
      "grad_norm": 0.042519763112068176,
      "learning_rate": 2.1816341808584564e-06,
      "loss": 0.0004,
      "step": 3484
    },
    {
      "epoch": 144.83,
      "grad_norm": 0.113706573843956,
      "learning_rate": 2.178986853160535e-06,
      "loss": 0.0007,
      "step": 3485
    },
    {
      "epoch": 144.87,
      "grad_norm": 0.062126703560352325,
      "learning_rate": 2.1763406850745596e-06,
      "loss": 0.0004,
      "step": 3486
    },
    {
      "epoch": 144.91,
      "grad_norm": 0.06117359548807144,
      "learning_rate": 2.1736956776882697e-06,
      "loss": 0.0005,
      "step": 3487
    },
    {
      "epoch": 144.96,
      "grad_norm": 0.04363492503762245,
      "learning_rate": 2.171051832088928e-06,
      "loss": 0.0004,
      "step": 3488
    },
    {
      "epoch": 145.0,
      "grad_norm": 0.04270840808749199,
      "learning_rate": 2.1684091493633207e-06,
      "loss": 0.0003,
      "step": 3489
    },
    {
      "epoch": 145.04,
      "grad_norm": 0.004180146846920252,
      "learning_rate": 2.165767630597752e-06,
      "loss": 0.0002,
      "step": 3490
    },
    {
      "epoch": 145.08,
      "grad_norm": 0.05920414254069328,
      "learning_rate": 2.163127276878052e-06,
      "loss": 0.0007,
      "step": 3491
    },
    {
      "epoch": 145.12,
      "grad_norm": 0.04638396203517914,
      "learning_rate": 2.1604880892895707e-06,
      "loss": 0.0005,
      "step": 3492
    },
    {
      "epoch": 145.16,
      "grad_norm": 0.05728175491094589,
      "learning_rate": 2.1578500689171777e-06,
      "loss": 0.0004,
      "step": 3493
    },
    {
      "epoch": 145.21,
      "grad_norm": 0.0618407167494297,
      "learning_rate": 2.1552132168452646e-06,
      "loss": 0.0006,
      "step": 3494
    },
    {
      "epoch": 145.25,
      "grad_norm": 0.05119246244430542,
      "learning_rate": 2.1525775341577404e-06,
      "loss": 0.0005,
      "step": 3495
    },
    {
      "epoch": 145.29,
      "grad_norm": 0.004134580958634615,
      "learning_rate": 2.1499430219380357e-06,
      "loss": 0.0002,
      "step": 3496
    },
    {
      "epoch": 145.33,
      "grad_norm": 0.06620270013809204,
      "learning_rate": 2.1473096812690986e-06,
      "loss": 0.0007,
      "step": 3497
    },
    {
      "epoch": 145.37,
      "grad_norm": 0.018021922558546066,
      "learning_rate": 2.1446775132333957e-06,
      "loss": 0.0003,
      "step": 3498
    },
    {
      "epoch": 145.41,
      "grad_norm": 0.026384711265563965,
      "learning_rate": 2.1420465189129147e-06,
      "loss": 0.0003,
      "step": 3499
    },
    {
      "epoch": 145.45,
      "grad_norm": 0.04420630261301994,
      "learning_rate": 2.139416699389153e-06,
      "loss": 0.0004,
      "step": 3500
    },
    {
      "epoch": 145.5,
      "grad_norm": 0.08577638119459152,
      "learning_rate": 2.1367880557431325e-06,
      "loss": 0.0007,
      "step": 3501
    },
    {
      "epoch": 145.54,
      "grad_norm": 0.12571129202842712,
      "learning_rate": 2.1341605890553895e-06,
      "loss": 0.0011,
      "step": 3502
    },
    {
      "epoch": 145.58,
      "grad_norm": 0.02505049668252468,
      "learning_rate": 2.1315343004059763e-06,
      "loss": 0.0003,
      "step": 3503
    },
    {
      "epoch": 145.62,
      "grad_norm": 0.02615157887339592,
      "learning_rate": 2.128909190874461e-06,
      "loss": 0.0003,
      "step": 3504
    },
    {
      "epoch": 145.66,
      "grad_norm": 0.033425744622945786,
      "learning_rate": 2.126285261539926e-06,
      "loss": 0.0003,
      "step": 3505
    },
    {
      "epoch": 145.7,
      "grad_norm": 0.04519832506775856,
      "learning_rate": 2.1236625134809707e-06,
      "loss": 0.0004,
      "step": 3506
    },
    {
      "epoch": 145.75,
      "grad_norm": 0.08415572345256805,
      "learning_rate": 2.121040947775707e-06,
      "loss": 0.0008,
      "step": 3507
    },
    {
      "epoch": 145.79,
      "grad_norm": 0.030464161187410355,
      "learning_rate": 2.118420565501762e-06,
      "loss": 0.0004,
      "step": 3508
    },
    {
      "epoch": 145.83,
      "grad_norm": 0.05577141419053078,
      "learning_rate": 2.115801367736276e-06,
      "loss": 0.0004,
      "step": 3509
    },
    {
      "epoch": 145.87,
      "grad_norm": 0.0640149936079979,
      "learning_rate": 2.1131833555559037e-06,
      "loss": 0.0006,
      "step": 3510
    },
    {
      "epoch": 145.91,
      "grad_norm": 0.03840620070695877,
      "learning_rate": 2.110566530036808e-06,
      "loss": 0.0004,
      "step": 3511
    },
    {
      "epoch": 145.95,
      "grad_norm": 0.05428961664438248,
      "learning_rate": 2.107950892254668e-06,
      "loss": 0.0008,
      "step": 3512
    },
    {
      "epoch": 145.99,
      "grad_norm": 0.07031341642141342,
      "learning_rate": 2.1053364432846735e-06,
      "loss": 0.0007,
      "step": 3513
    },
    {
      "epoch": 146.04,
      "grad_norm": 0.05756258964538574,
      "learning_rate": 2.102723184201526e-06,
      "loss": 0.0006,
      "step": 3514
    },
    {
      "epoch": 146.08,
      "grad_norm": 0.07233980298042297,
      "learning_rate": 2.1001111160794387e-06,
      "loss": 0.0006,
      "step": 3515
    },
    {
      "epoch": 146.12,
      "grad_norm": 0.03444645553827286,
      "learning_rate": 2.097500239992132e-06,
      "loss": 0.0004,
      "step": 3516
    },
    {
      "epoch": 146.16,
      "grad_norm": 0.0942864716053009,
      "learning_rate": 2.094890557012841e-06,
      "loss": 0.0009,
      "step": 3517
    },
    {
      "epoch": 146.2,
      "grad_norm": 0.05764260143041611,
      "learning_rate": 2.0922820682143057e-06,
      "loss": 0.0004,
      "step": 3518
    },
    {
      "epoch": 146.24,
      "grad_norm": 0.004632290918380022,
      "learning_rate": 2.0896747746687785e-06,
      "loss": 0.0002,
      "step": 3519
    },
    {
      "epoch": 146.29,
      "grad_norm": 0.07669435441493988,
      "learning_rate": 2.08706867744802e-06,
      "loss": 0.0007,
      "step": 3520
    },
    {
      "epoch": 146.33,
      "grad_norm": 0.03908946365118027,
      "learning_rate": 2.0844637776232954e-06,
      "loss": 0.0004,
      "step": 3521
    },
    {
      "epoch": 146.37,
      "grad_norm": 0.054891008883714676,
      "learning_rate": 2.081860076265383e-06,
      "loss": 0.0004,
      "step": 3522
    },
    {
      "epoch": 146.41,
      "grad_norm": 0.038854364305734634,
      "learning_rate": 2.0792575744445654e-06,
      "loss": 0.0004,
      "step": 3523
    },
    {
      "epoch": 146.45,
      "grad_norm": 0.047172851860523224,
      "learning_rate": 2.0766562732306323e-06,
      "loss": 0.0005,
      "step": 3524
    },
    {
      "epoch": 146.49,
      "grad_norm": 0.004041418898850679,
      "learning_rate": 2.074056173692881e-06,
      "loss": 0.0002,
      "step": 3525
    },
    {
      "epoch": 146.54,
      "grad_norm": 0.0725874975323677,
      "learning_rate": 2.071457276900116e-06,
      "loss": 0.0007,
      "step": 3526
    },
    {
      "epoch": 146.58,
      "grad_norm": 0.07870595157146454,
      "learning_rate": 2.0688595839206425e-06,
      "loss": 0.0007,
      "step": 3527
    },
    {
      "epoch": 146.62,
      "grad_norm": 0.13364025950431824,
      "learning_rate": 2.0662630958222747e-06,
      "loss": 0.001,
      "step": 3528
    },
    {
      "epoch": 146.66,
      "grad_norm": 0.04209975525736809,
      "learning_rate": 2.0636678136723314e-06,
      "loss": 0.0004,
      "step": 3529
    },
    {
      "epoch": 146.7,
      "grad_norm": 0.07597710192203522,
      "learning_rate": 2.061073738537635e-06,
      "loss": 0.0008,
      "step": 3530
    },
    {
      "epoch": 146.74,
      "grad_norm": 0.0946231484413147,
      "learning_rate": 2.0584808714845118e-06,
      "loss": 0.0007,
      "step": 3531
    },
    {
      "epoch": 146.78,
      "grad_norm": 0.03570140525698662,
      "learning_rate": 2.0558892135787927e-06,
      "loss": 0.0004,
      "step": 3532
    },
    {
      "epoch": 146.83,
      "grad_norm": 0.09046505391597748,
      "learning_rate": 2.053298765885808e-06,
      "loss": 0.0007,
      "step": 3533
    },
    {
      "epoch": 146.87,
      "grad_norm": 0.004159686155617237,
      "learning_rate": 2.0507095294703932e-06,
      "loss": 0.0003,
      "step": 3534
    },
    {
      "epoch": 146.91,
      "grad_norm": 0.059591855853796005,
      "learning_rate": 2.0481215053968874e-06,
      "loss": 0.0005,
      "step": 3535
    },
    {
      "epoch": 146.95,
      "grad_norm": 0.08235780894756317,
      "learning_rate": 2.0455346947291277e-06,
      "loss": 0.0007,
      "step": 3536
    },
    {
      "epoch": 146.99,
      "grad_norm": 0.03757240250706673,
      "learning_rate": 2.0429490985304556e-06,
      "loss": 0.0005,
      "step": 3537
    },
    {
      "epoch": 147.03,
      "grad_norm": 0.044497597962617874,
      "learning_rate": 2.040364717863711e-06,
      "loss": 0.0007,
      "step": 3538
    },
    {
      "epoch": 147.08,
      "grad_norm": 0.004030370153486729,
      "learning_rate": 2.037781553791236e-06,
      "loss": 0.0002,
      "step": 3539
    },
    {
      "epoch": 147.12,
      "grad_norm": 0.03279215842485428,
      "learning_rate": 2.0351996073748713e-06,
      "loss": 0.0003,
      "step": 3540
    },
    {
      "epoch": 147.16,
      "grad_norm": 0.06662284582853317,
      "learning_rate": 2.0326188796759583e-06,
      "loss": 0.0006,
      "step": 3541
    },
    {
      "epoch": 147.2,
      "grad_norm": 0.03478441387414932,
      "learning_rate": 2.0300393717553355e-06,
      "loss": 0.0004,
      "step": 3542
    },
    {
      "epoch": 147.24,
      "grad_norm": 0.05435878783464432,
      "learning_rate": 2.027461084673344e-06,
      "loss": 0.0006,
      "step": 3543
    },
    {
      "epoch": 147.28,
      "grad_norm": 0.04748936742544174,
      "learning_rate": 2.0248840194898155e-06,
      "loss": 0.0004,
      "step": 3544
    },
    {
      "epoch": 147.32,
      "grad_norm": 0.0430954173207283,
      "learning_rate": 2.0223081772640867e-06,
      "loss": 0.0004,
      "step": 3545
    },
    {
      "epoch": 147.37,
      "grad_norm": 0.01709415204823017,
      "learning_rate": 2.019733559054989e-06,
      "loss": 0.0003,
      "step": 3546
    },
    {
      "epoch": 147.41,
      "grad_norm": 0.17956441640853882,
      "learning_rate": 2.0171601659208516e-06,
      "loss": 0.001,
      "step": 3547
    },
    {
      "epoch": 147.45,
      "grad_norm": 0.030588829889893532,
      "learning_rate": 2.014587998919498e-06,
      "loss": 0.0004,
      "step": 3548
    },
    {
      "epoch": 147.49,
      "grad_norm": 0.030997570604085922,
      "learning_rate": 2.0120170591082484e-06,
      "loss": 0.0004,
      "step": 3549
    },
    {
      "epoch": 147.53,
      "grad_norm": 0.018635133281350136,
      "learning_rate": 2.00944734754392e-06,
      "loss": 0.0003,
      "step": 3550
    },
    {
      "epoch": 147.57,
      "grad_norm": 0.014492151327431202,
      "learning_rate": 2.006878865282824e-06,
      "loss": 0.0003,
      "step": 3551
    },
    {
      "epoch": 147.62,
      "grad_norm": 0.04985259845852852,
      "learning_rate": 2.0043116133807673e-06,
      "loss": 0.0005,
      "step": 3552
    },
    {
      "epoch": 147.66,
      "grad_norm": 0.08184085041284561,
      "learning_rate": 2.00174559289305e-06,
      "loss": 0.0007,
      "step": 3553
    },
    {
      "epoch": 147.7,
      "grad_norm": 0.110539510846138,
      "learning_rate": 1.999180804874464e-06,
      "loss": 0.0008,
      "step": 3554
    },
    {
      "epoch": 147.74,
      "grad_norm": 0.1227993592619896,
      "learning_rate": 1.9966172503792986e-06,
      "loss": 0.0009,
      "step": 3555
    },
    {
      "epoch": 147.78,
      "grad_norm": 0.004146205261349678,
      "learning_rate": 1.9940549304613334e-06,
      "loss": 0.0002,
      "step": 3556
    },
    {
      "epoch": 147.82,
      "grad_norm": 0.050792016088962555,
      "learning_rate": 1.991493846173842e-06,
      "loss": 0.0005,
      "step": 3557
    },
    {
      "epoch": 147.86,
      "grad_norm": 0.07850982248783112,
      "learning_rate": 1.9889339985695894e-06,
      "loss": 0.0007,
      "step": 3558
    },
    {
      "epoch": 147.91,
      "grad_norm": 0.07780114561319351,
      "learning_rate": 1.986375388700832e-06,
      "loss": 0.0005,
      "step": 3559
    },
    {
      "epoch": 147.95,
      "grad_norm": 0.08301138132810593,
      "learning_rate": 1.983818017619318e-06,
      "loss": 0.0008,
      "step": 3560
    },
    {
      "epoch": 147.99,
      "grad_norm": 0.21202287077903748,
      "learning_rate": 1.981261886376285e-06,
      "loss": 0.0015,
      "step": 3561
    },
    {
      "epoch": 148.03,
      "grad_norm": 0.14344535768032074,
      "learning_rate": 1.9787069960224635e-06,
      "loss": 0.0009,
      "step": 3562
    },
    {
      "epoch": 148.07,
      "grad_norm": 0.0276117455214262,
      "learning_rate": 1.9761533476080736e-06,
      "loss": 0.0003,
      "step": 3563
    },
    {
      "epoch": 148.11,
      "grad_norm": 0.0774366483092308,
      "learning_rate": 1.9736009421828196e-06,
      "loss": 0.0007,
      "step": 3564
    },
    {
      "epoch": 148.16,
      "grad_norm": 0.042541343718767166,
      "learning_rate": 1.971049780795901e-06,
      "loss": 0.0004,
      "step": 3565
    },
    {
      "epoch": 148.2,
      "grad_norm": 0.059143297374248505,
      "learning_rate": 1.9684998644960045e-06,
      "loss": 0.0009,
      "step": 3566
    },
    {
      "epoch": 148.24,
      "grad_norm": 0.06078936159610748,
      "learning_rate": 1.9659511943313043e-06,
      "loss": 0.0004,
      "step": 3567
    },
    {
      "epoch": 148.28,
      "grad_norm": 0.02970276027917862,
      "learning_rate": 1.963403771349461e-06,
      "loss": 0.0003,
      "step": 3568
    },
    {
      "epoch": 148.32,
      "grad_norm": 0.12193700671195984,
      "learning_rate": 1.960857596597626e-06,
      "loss": 0.001,
      "step": 3569
    },
    {
      "epoch": 148.36,
      "grad_norm": 0.037172626703977585,
      "learning_rate": 1.9583126711224342e-06,
      "loss": 0.0004,
      "step": 3570
    },
    {
      "epoch": 148.41,
      "grad_norm": 0.02300785295665264,
      "learning_rate": 1.9557689959700105e-06,
      "loss": 0.0005,
      "step": 3571
    },
    {
      "epoch": 148.45,
      "grad_norm": 0.029354384168982506,
      "learning_rate": 1.95322657218596e-06,
      "loss": 0.0003,
      "step": 3572
    },
    {
      "epoch": 148.49,
      "grad_norm": 0.04846266657114029,
      "learning_rate": 1.950685400815379e-06,
      "loss": 0.0005,
      "step": 3573
    },
    {
      "epoch": 148.53,
      "grad_norm": 0.004055141005665064,
      "learning_rate": 1.9481454829028474e-06,
      "loss": 0.0002,
      "step": 3574
    },
    {
      "epoch": 148.57,
      "grad_norm": 0.046227604150772095,
      "learning_rate": 1.945606819492429e-06,
      "loss": 0.0004,
      "step": 3575
    },
    {
      "epoch": 148.61,
      "grad_norm": 0.16134709119796753,
      "learning_rate": 1.9430694116276745e-06,
      "loss": 0.0016,
      "step": 3576
    },
    {
      "epoch": 148.65,
      "grad_norm": 0.03740723803639412,
      "learning_rate": 1.940533260351612e-06,
      "loss": 0.0004,
      "step": 3577
    },
    {
      "epoch": 148.7,
      "grad_norm": 0.09426421672105789,
      "learning_rate": 1.937998366706761e-06,
      "loss": 0.0007,
      "step": 3578
    },
    {
      "epoch": 148.74,
      "grad_norm": 0.054796382784843445,
      "learning_rate": 1.9354647317351187e-06,
      "loss": 0.0008,
      "step": 3579
    },
    {
      "epoch": 148.78,
      "grad_norm": 0.07051417231559753,
      "learning_rate": 1.932932356478168e-06,
      "loss": 0.0006,
      "step": 3580
    },
    {
      "epoch": 148.82,
      "grad_norm": 0.03553549945354462,
      "learning_rate": 1.930401241976872e-06,
      "loss": 0.0003,
      "step": 3581
    },
    {
      "epoch": 148.86,
      "grad_norm": 0.06824151426553726,
      "learning_rate": 1.927871389271677e-06,
      "loss": 0.0006,
      "step": 3582
    },
    {
      "epoch": 148.9,
      "grad_norm": 0.11757000535726547,
      "learning_rate": 1.925342799402509e-06,
      "loss": 0.0008,
      "step": 3583
    },
    {
      "epoch": 148.95,
      "grad_norm": 0.06209684908390045,
      "learning_rate": 1.9228154734087766e-06,
      "loss": 0.0005,
      "step": 3584
    },
    {
      "epoch": 148.99,
      "grad_norm": 0.07382147014141083,
      "learning_rate": 1.9202894123293677e-06,
      "loss": 0.0009,
      "step": 3585
    },
    {
      "epoch": 149.03,
      "grad_norm": 0.0640237033367157,
      "learning_rate": 1.9177646172026513e-06,
      "loss": 0.0005,
      "step": 3586
    },
    {
      "epoch": 149.07,
      "grad_norm": 0.004078201949596405,
      "learning_rate": 1.915241089066474e-06,
      "loss": 0.0002,
      "step": 3587
    },
    {
      "epoch": 149.11,
      "grad_norm": 0.05660976096987724,
      "learning_rate": 1.912718828958163e-06,
      "loss": 0.0004,
      "step": 3588
    },
    {
      "epoch": 149.15,
      "grad_norm": 0.0837329626083374,
      "learning_rate": 1.9101978379145246e-06,
      "loss": 0.0006,
      "step": 3589
    },
    {
      "epoch": 149.19,
      "grad_norm": 0.02715553529560566,
      "learning_rate": 1.9076781169718426e-06,
      "loss": 0.0003,
      "step": 3590
    },
    {
      "epoch": 149.24,
      "grad_norm": 0.052684176713228226,
      "learning_rate": 1.90515966716588e-06,
      "loss": 0.0006,
      "step": 3591
    },
    {
      "epoch": 149.28,
      "grad_norm": 0.09698187559843063,
      "learning_rate": 1.9026424895318762e-06,
      "loss": 0.0006,
      "step": 3592
    },
    {
      "epoch": 149.32,
      "grad_norm": 0.0629897266626358,
      "learning_rate": 1.900126585104547e-06,
      "loss": 0.0005,
      "step": 3593
    },
    {
      "epoch": 149.36,
      "grad_norm": 0.03126228228211403,
      "learning_rate": 1.8976119549180865e-06,
      "loss": 0.0004,
      "step": 3594
    },
    {
      "epoch": 149.4,
      "grad_norm": 0.034248366951942444,
      "learning_rate": 1.895098600006164e-06,
      "loss": 0.0003,
      "step": 3595
    },
    {
      "epoch": 149.44,
      "grad_norm": 0.03262711316347122,
      "learning_rate": 1.892586521401924e-06,
      "loss": 0.0004,
      "step": 3596
    },
    {
      "epoch": 149.49,
      "grad_norm": 0.03684350475668907,
      "learning_rate": 1.8900757201379899e-06,
      "loss": 0.0004,
      "step": 3597
    },
    {
      "epoch": 149.53,
      "grad_norm": 0.06413760781288147,
      "learning_rate": 1.8875661972464532e-06,
      "loss": 0.0005,
      "step": 3598
    },
    {
      "epoch": 149.57,
      "grad_norm": 0.024723222479224205,
      "learning_rate": 1.8850579537588865e-06,
      "loss": 0.0003,
      "step": 3599
    },
    {
      "epoch": 149.61,
      "grad_norm": 0.08434083312749863,
      "learning_rate": 1.8825509907063328e-06,
      "loss": 0.0008,
      "step": 3600
    },
    {
      "epoch": 149.65,
      "grad_norm": 0.08006653934717178,
      "learning_rate": 1.8800453091193104e-06,
      "loss": 0.0006,
      "step": 3601
    },
    {
      "epoch": 149.69,
      "grad_norm": 0.11139167100191116,
      "learning_rate": 1.8775409100278108e-06,
      "loss": 0.0013,
      "step": 3602
    },
    {
      "epoch": 149.74,
      "grad_norm": 0.03477456420660019,
      "learning_rate": 1.8750377944612975e-06,
      "loss": 0.0004,
      "step": 3603
    },
    {
      "epoch": 149.78,
      "grad_norm": 0.06220816820859909,
      "learning_rate": 1.8725359634487068e-06,
      "loss": 0.0005,
      "step": 3604
    },
    {
      "epoch": 149.82,
      "grad_norm": 0.0039520589634776115,
      "learning_rate": 1.8700354180184465e-06,
      "loss": 0.0002,
      "step": 3605
    },
    {
      "epoch": 149.86,
      "grad_norm": 0.11280671507120132,
      "learning_rate": 1.867536159198397e-06,
      "loss": 0.0011,
      "step": 3606
    },
    {
      "epoch": 149.9,
      "grad_norm": 0.024365931749343872,
      "learning_rate": 1.8650381880159108e-06,
      "loss": 0.0003,
      "step": 3607
    },
    {
      "epoch": 149.94,
      "grad_norm": 0.03796010836958885,
      "learning_rate": 1.8625415054978058e-06,
      "loss": 0.0004,
      "step": 3608
    },
    {
      "epoch": 149.98,
      "grad_norm": 0.07655614614486694,
      "learning_rate": 1.8600461126703755e-06,
      "loss": 0.0006,
      "step": 3609
    },
    {
      "epoch": 150.03,
      "grad_norm": 0.003898398019373417,
      "learning_rate": 1.857552010559382e-06,
      "loss": 0.0002,
      "step": 3610
    },
    {
      "epoch": 150.07,
      "grad_norm": 0.003573726862668991,
      "learning_rate": 1.8550592001900565e-06,
      "loss": 0.0002,
      "step": 3611
    },
    {
      "epoch": 150.11,
      "grad_norm": 0.025813251733779907,
      "learning_rate": 1.8525676825870986e-06,
      "loss": 0.0003,
      "step": 3612
    },
    {
      "epoch": 150.15,
      "grad_norm": 0.08945200592279434,
      "learning_rate": 1.8500774587746777e-06,
      "loss": 0.0009,
      "step": 3613
    },
    {
      "epoch": 150.19,
      "grad_norm": 0.09987615048885345,
      "learning_rate": 1.8475885297764307e-06,
      "loss": 0.0011,
      "step": 3614
    },
    {
      "epoch": 150.23,
      "grad_norm": 0.0036515251267701387,
      "learning_rate": 1.8451008966154622e-06,
      "loss": 0.0002,
      "step": 3615
    },
    {
      "epoch": 150.28,
      "grad_norm": 0.01487702690064907,
      "learning_rate": 1.8426145603143441e-06,
      "loss": 0.0003,
      "step": 3616
    },
    {
      "epoch": 150.32,
      "grad_norm": 0.003997305408120155,
      "learning_rate": 1.8401295218951165e-06,
      "loss": 0.0002,
      "step": 3617
    },
    {
      "epoch": 150.36,
      "grad_norm": 0.17838191986083984,
      "learning_rate": 1.8376457823792826e-06,
      "loss": 0.0013,
      "step": 3618
    },
    {
      "epoch": 150.4,
      "grad_norm": 0.06954044103622437,
      "learning_rate": 1.8351633427878163e-06,
      "loss": 0.0005,
      "step": 3619
    },
    {
      "epoch": 150.44,
      "grad_norm": 0.00410020537674427,
      "learning_rate": 1.8326822041411524e-06,
      "loss": 0.0002,
      "step": 3620
    },
    {
      "epoch": 150.48,
      "grad_norm": 0.04473785310983658,
      "learning_rate": 1.8302023674591934e-06,
      "loss": 0.0005,
      "step": 3621
    },
    {
      "epoch": 150.52,
      "grad_norm": 0.03296433016657829,
      "learning_rate": 1.827723833761308e-06,
      "loss": 0.0005,
      "step": 3622
    },
    {
      "epoch": 150.57,
      "grad_norm": 0.09287688136100769,
      "learning_rate": 1.8252466040663275e-06,
      "loss": 0.0008,
      "step": 3623
    },
    {
      "epoch": 150.61,
      "grad_norm": 0.04703334718942642,
      "learning_rate": 1.8227706793925464e-06,
      "loss": 0.0004,
      "step": 3624
    },
    {
      "epoch": 150.65,
      "grad_norm": 0.06785210967063904,
      "learning_rate": 1.8202960607577246e-06,
      "loss": 0.0005,
      "step": 3625
    },
    {
      "epoch": 150.69,
      "grad_norm": 0.055783092975616455,
      "learning_rate": 1.817822749179085e-06,
      "loss": 0.0006,
      "step": 3626
    },
    {
      "epoch": 150.73,
      "grad_norm": 0.042908988893032074,
      "learning_rate": 1.8153507456733115e-06,
      "loss": 0.0004,
      "step": 3627
    },
    {
      "epoch": 150.77,
      "grad_norm": 0.14537639915943146,
      "learning_rate": 1.8128800512565514e-06,
      "loss": 0.001,
      "step": 3628
    },
    {
      "epoch": 150.82,
      "grad_norm": 0.08729911595582962,
      "learning_rate": 1.8104106669444148e-06,
      "loss": 0.0006,
      "step": 3629
    },
    {
      "epoch": 150.86,
      "grad_norm": 0.0038904156535863876,
      "learning_rate": 1.8079425937519729e-06,
      "loss": 0.0002,
      "step": 3630
    },
    {
      "epoch": 150.9,
      "grad_norm": 0.055748820304870605,
      "learning_rate": 1.8054758326937548e-06,
      "loss": 0.0004,
      "step": 3631
    },
    {
      "epoch": 150.94,
      "grad_norm": 0.06597987562417984,
      "learning_rate": 1.8030103847837538e-06,
      "loss": 0.0006,
      "step": 3632
    },
    {
      "epoch": 150.98,
      "grad_norm": 0.050731513649225235,
      "learning_rate": 1.8005462510354233e-06,
      "loss": 0.0005,
      "step": 3633
    },
    {
      "epoch": 151.02,
      "grad_norm": 0.003597328206524253,
      "learning_rate": 1.798083432461674e-06,
      "loss": 0.0002,
      "step": 3634
    },
    {
      "epoch": 151.06,
      "grad_norm": 0.020580442622303963,
      "learning_rate": 1.7956219300748796e-06,
      "loss": 0.0003,
      "step": 3635
    },
    {
      "epoch": 151.11,
      "grad_norm": 0.04303005710244179,
      "learning_rate": 1.79316174488687e-06,
      "loss": 0.0004,
      "step": 3636
    },
    {
      "epoch": 151.15,
      "grad_norm": 0.062751904129982,
      "learning_rate": 1.7907028779089335e-06,
      "loss": 0.0005,
      "step": 3637
    },
    {
      "epoch": 151.19,
      "grad_norm": 0.051050979644060135,
      "learning_rate": 1.7882453301518182e-06,
      "loss": 0.0006,
      "step": 3638
    },
    {
      "epoch": 151.23,
      "grad_norm": 0.08278048783540726,
      "learning_rate": 1.7857891026257295e-06,
      "loss": 0.0009,
      "step": 3639
    },
    {
      "epoch": 151.27,
      "grad_norm": 0.003758285427466035,
      "learning_rate": 1.7833341963403312e-06,
      "loss": 0.0002,
      "step": 3640
    },
    {
      "epoch": 151.31,
      "grad_norm": 0.12801234424114227,
      "learning_rate": 1.7808806123047385e-06,
      "loss": 0.0009,
      "step": 3641
    },
    {
      "epoch": 151.36,
      "grad_norm": 0.11863569170236588,
      "learning_rate": 1.7784283515275292e-06,
      "loss": 0.0007,
      "step": 3642
    },
    {
      "epoch": 151.4,
      "grad_norm": 0.019626038148999214,
      "learning_rate": 1.7759774150167352e-06,
      "loss": 0.0003,
      "step": 3643
    },
    {
      "epoch": 151.44,
      "grad_norm": 0.09491275250911713,
      "learning_rate": 1.7735278037798442e-06,
      "loss": 0.0006,
      "step": 3644
    },
    {
      "epoch": 151.48,
      "grad_norm": 0.028532715514302254,
      "learning_rate": 1.771079518823799e-06,
      "loss": 0.0004,
      "step": 3645
    },
    {
      "epoch": 151.52,
      "grad_norm": 0.04118109121918678,
      "learning_rate": 1.768632561154996e-06,
      "loss": 0.0004,
      "step": 3646
    },
    {
      "epoch": 151.56,
      "grad_norm": 0.08856287598609924,
      "learning_rate": 1.766186931779288e-06,
      "loss": 0.0011,
      "step": 3647
    },
    {
      "epoch": 151.61,
      "grad_norm": 0.08771727979183197,
      "learning_rate": 1.7637426317019801e-06,
      "loss": 0.0008,
      "step": 3648
    },
    {
      "epoch": 151.65,
      "grad_norm": 0.04283731058239937,
      "learning_rate": 1.7612996619278322e-06,
      "loss": 0.0004,
      "step": 3649
    },
    {
      "epoch": 151.69,
      "grad_norm": 0.04992372542619705,
      "learning_rate": 1.7588580234610592e-06,
      "loss": 0.0005,
      "step": 3650
    },
    {
      "epoch": 151.73,
      "grad_norm": 0.06502145528793335,
      "learning_rate": 1.7564177173053215e-06,
      "loss": 0.0005,
      "step": 3651
    },
    {
      "epoch": 151.77,
      "grad_norm": 0.022627102211117744,
      "learning_rate": 1.7539787444637402e-06,
      "loss": 0.0003,
      "step": 3652
    },
    {
      "epoch": 151.81,
      "grad_norm": 0.05742071941494942,
      "learning_rate": 1.7515411059388837e-06,
      "loss": 0.0006,
      "step": 3653
    },
    {
      "epoch": 151.85,
      "grad_norm": 0.04105087369680405,
      "learning_rate": 1.7491048027327739e-06,
      "loss": 0.0004,
      "step": 3654
    },
    {
      "epoch": 151.9,
      "grad_norm": 0.03677185997366905,
      "learning_rate": 1.7466698358468825e-06,
      "loss": 0.0003,
      "step": 3655
    },
    {
      "epoch": 151.94,
      "grad_norm": 0.05749713256955147,
      "learning_rate": 1.7442362062821323e-06,
      "loss": 0.0005,
      "step": 3656
    },
    {
      "epoch": 151.98,
      "grad_norm": 0.07928849011659622,
      "learning_rate": 1.7418039150388971e-06,
      "loss": 0.0005,
      "step": 3657
    },
    {
      "epoch": 152.02,
      "grad_norm": 0.2867606580257416,
      "learning_rate": 1.7393729631169993e-06,
      "loss": 0.0019,
      "step": 3658
    },
    {
      "epoch": 152.06,
      "grad_norm": 0.10055616497993469,
      "learning_rate": 1.7369433515157118e-06,
      "loss": 0.0008,
      "step": 3659
    },
    {
      "epoch": 152.1,
      "grad_norm": 0.0382179394364357,
      "learning_rate": 1.7345150812337564e-06,
      "loss": 0.0004,
      "step": 3660
    },
    {
      "epoch": 152.15,
      "grad_norm": 0.054065268486738205,
      "learning_rate": 1.732088153269304e-06,
      "loss": 0.0005,
      "step": 3661
    },
    {
      "epoch": 152.19,
      "grad_norm": 0.10062818974256516,
      "learning_rate": 1.7296625686199708e-06,
      "loss": 0.0009,
      "step": 3662
    },
    {
      "epoch": 152.23,
      "grad_norm": 0.04058695584535599,
      "learning_rate": 1.7272383282828254e-06,
      "loss": 0.0004,
      "step": 3663
    },
    {
      "epoch": 152.27,
      "grad_norm": 0.05698320269584656,
      "learning_rate": 1.7248154332543788e-06,
      "loss": 0.0005,
      "step": 3664
    },
    {
      "epoch": 152.31,
      "grad_norm": 0.019900623708963394,
      "learning_rate": 1.7223938845305932e-06,
      "loss": 0.0003,
      "step": 3665
    },
    {
      "epoch": 152.35,
      "grad_norm": 0.021649574860930443,
      "learning_rate": 1.7199736831068758e-06,
      "loss": 0.0004,
      "step": 3666
    },
    {
      "epoch": 152.39,
      "grad_norm": 0.023417357355356216,
      "learning_rate": 1.7175548299780791e-06,
      "loss": 0.0005,
      "step": 3667
    },
    {
      "epoch": 152.44,
      "grad_norm": 0.024004077538847923,
      "learning_rate": 1.715137326138504e-06,
      "loss": 0.0003,
      "step": 3668
    },
    {
      "epoch": 152.48,
      "grad_norm": 0.2218615710735321,
      "learning_rate": 1.7127211725818933e-06,
      "loss": 0.0012,
      "step": 3669
    },
    {
      "epoch": 152.52,
      "grad_norm": 0.02216489240527153,
      "learning_rate": 1.7103063703014372e-06,
      "loss": 0.0003,
      "step": 3670
    },
    {
      "epoch": 152.56,
      "grad_norm": 0.023492498323321342,
      "learning_rate": 1.70789292028977e-06,
      "loss": 0.0003,
      "step": 3671
    },
    {
      "epoch": 152.6,
      "grad_norm": 0.047358352690935135,
      "learning_rate": 1.7054808235389696e-06,
      "loss": 0.0004,
      "step": 3672
    },
    {
      "epoch": 152.64,
      "grad_norm": 0.02578980103135109,
      "learning_rate": 1.7030700810405592e-06,
      "loss": 0.0004,
      "step": 3673
    },
    {
      "epoch": 152.69,
      "grad_norm": 0.003856072435155511,
      "learning_rate": 1.7006606937855008e-06,
      "loss": 0.0002,
      "step": 3674
    },
    {
      "epoch": 152.73,
      "grad_norm": 0.049854401499032974,
      "learning_rate": 1.6982526627642043e-06,
      "loss": 0.0005,
      "step": 3675
    },
    {
      "epoch": 152.77,
      "grad_norm": 0.05062352120876312,
      "learning_rate": 1.6958459889665202e-06,
      "loss": 0.0004,
      "step": 3676
    },
    {
      "epoch": 152.81,
      "grad_norm": 0.047328852117061615,
      "learning_rate": 1.6934406733817417e-06,
      "loss": 0.0004,
      "step": 3677
    },
    {
      "epoch": 152.85,
      "grad_norm": 0.06812852621078491,
      "learning_rate": 1.6910367169986018e-06,
      "loss": 0.0006,
      "step": 3678
    },
    {
      "epoch": 152.89,
      "grad_norm": 0.07650624960660934,
      "learning_rate": 1.6886341208052775e-06,
      "loss": 0.0008,
      "step": 3679
    },
    {
      "epoch": 152.94,
      "grad_norm": 0.03378622606396675,
      "learning_rate": 1.6862328857893856e-06,
      "loss": 0.0003,
      "step": 3680
    },
    {
      "epoch": 152.98,
      "grad_norm": 0.05848175659775734,
      "learning_rate": 1.6838330129379816e-06,
      "loss": 0.0005,
      "step": 3681
    },
    {
      "epoch": 153.02,
      "grad_norm": 0.06132277473807335,
      "learning_rate": 1.6814345032375633e-06,
      "loss": 0.0006,
      "step": 3682
    },
    {
      "epoch": 153.06,
      "grad_norm": 0.0038529669400304556,
      "learning_rate": 1.67903735767407e-06,
      "loss": 0.0002,
      "step": 3683
    },
    {
      "epoch": 153.1,
      "grad_norm": 0.06334852427244186,
      "learning_rate": 1.6766415772328732e-06,
      "loss": 0.0005,
      "step": 3684
    },
    {
      "epoch": 153.14,
      "grad_norm": 0.06010974198579788,
      "learning_rate": 1.6742471628987894e-06,
      "loss": 0.0007,
      "step": 3685
    },
    {
      "epoch": 153.18,
      "grad_norm": 0.003769176546484232,
      "learning_rate": 1.6718541156560725e-06,
      "loss": 0.0002,
      "step": 3686
    },
    {
      "epoch": 153.23,
      "grad_norm": 0.022388966754078865,
      "learning_rate": 1.6694624364884138e-06,
      "loss": 0.0005,
      "step": 3687
    },
    {
      "epoch": 153.27,
      "grad_norm": 0.07421771436929703,
      "learning_rate": 1.667072126378942e-06,
      "loss": 0.0004,
      "step": 3688
    },
    {
      "epoch": 153.31,
      "grad_norm": 0.026820242404937744,
      "learning_rate": 1.664683186310223e-06,
      "loss": 0.0003,
      "step": 3689
    },
    {
      "epoch": 153.35,
      "grad_norm": 0.0362420417368412,
      "learning_rate": 1.6622956172642601e-06,
      "loss": 0.0004,
      "step": 3690
    },
    {
      "epoch": 153.39,
      "grad_norm": 0.004081633407622576,
      "learning_rate": 1.6599094202224936e-06,
      "loss": 0.0002,
      "step": 3691
    },
    {
      "epoch": 153.43,
      "grad_norm": 0.07780227810144424,
      "learning_rate": 1.6575245961657977e-06,
      "loss": 0.0007,
      "step": 3692
    },
    {
      "epoch": 153.48,
      "grad_norm": 0.09511324763298035,
      "learning_rate": 1.6551411460744836e-06,
      "loss": 0.0011,
      "step": 3693
    },
    {
      "epoch": 153.52,
      "grad_norm": 0.04907139763236046,
      "learning_rate": 1.6527590709283003e-06,
      "loss": 0.0006,
      "step": 3694
    },
    {
      "epoch": 153.56,
      "grad_norm": 0.021735914051532745,
      "learning_rate": 1.6503783717064247e-06,
      "loss": 0.0003,
      "step": 3695
    },
    {
      "epoch": 153.6,
      "grad_norm": 0.11295562982559204,
      "learning_rate": 1.6479990493874741e-06,
      "loss": 0.0014,
      "step": 3696
    },
    {
      "epoch": 153.64,
      "grad_norm": 0.017315400764346123,
      "learning_rate": 1.6456211049494986e-06,
      "loss": 0.0003,
      "step": 3697
    },
    {
      "epoch": 153.68,
      "grad_norm": 0.03102954290807247,
      "learning_rate": 1.6432445393699803e-06,
      "loss": 0.0003,
      "step": 3698
    },
    {
      "epoch": 153.72,
      "grad_norm": 0.10811598598957062,
      "learning_rate": 1.6408693536258364e-06,
      "loss": 0.0009,
      "step": 3699
    },
    {
      "epoch": 153.77,
      "grad_norm": 0.07921776175498962,
      "learning_rate": 1.6384955486934157e-06,
      "loss": 0.0006,
      "step": 3700
    },
    {
      "epoch": 153.81,
      "grad_norm": 0.02476072683930397,
      "learning_rate": 1.6361231255485e-06,
      "loss": 0.0003,
      "step": 3701
    },
    {
      "epoch": 153.85,
      "grad_norm": 0.34308433532714844,
      "learning_rate": 1.6337520851663025e-06,
      "loss": 0.0018,
      "step": 3702
    },
    {
      "epoch": 153.89,
      "grad_norm": 0.07147490978240967,
      "learning_rate": 1.6313824285214686e-06,
      "loss": 0.0006,
      "step": 3703
    },
    {
      "epoch": 153.93,
      "grad_norm": 0.028441645205020905,
      "learning_rate": 1.6290141565880758e-06,
      "loss": 0.0003,
      "step": 3704
    },
    {
      "epoch": 153.97,
      "grad_norm": 0.0545719638466835,
      "learning_rate": 1.6266472703396286e-06,
      "loss": 0.0004,
      "step": 3705
    },
    {
      "epoch": 154.02,
      "grad_norm": 0.0038792141713202,
      "learning_rate": 1.624281770749066e-06,
      "loss": 0.0002,
      "step": 3706
    },
    {
      "epoch": 154.06,
      "grad_norm": 0.08527028560638428,
      "learning_rate": 1.6219176587887569e-06,
      "loss": 0.0007,
      "step": 3707
    },
    {
      "epoch": 154.1,
      "grad_norm": 0.003867596620693803,
      "learning_rate": 1.6195549354304952e-06,
      "loss": 0.0002,
      "step": 3708
    },
    {
      "epoch": 154.14,
      "grad_norm": 0.027126967906951904,
      "learning_rate": 1.6171936016455091e-06,
      "loss": 0.0003,
      "step": 3709
    },
    {
      "epoch": 154.18,
      "grad_norm": 0.0038482702802866697,
      "learning_rate": 1.6148336584044539e-06,
      "loss": 0.0002,
      "step": 3710
    },
    {
      "epoch": 154.22,
      "grad_norm": 0.07098773121833801,
      "learning_rate": 1.6124751066774124e-06,
      "loss": 0.0008,
      "step": 3711
    },
    {
      "epoch": 154.26,
      "grad_norm": 0.050972405821084976,
      "learning_rate": 1.610117947433897e-06,
      "loss": 0.0006,
      "step": 3712
    },
    {
      "epoch": 154.31,
      "grad_norm": 0.11256445199251175,
      "learning_rate": 1.6077621816428457e-06,
      "loss": 0.0008,
      "step": 3713
    },
    {
      "epoch": 154.35,
      "grad_norm": 0.06333758682012558,
      "learning_rate": 1.6054078102726257e-06,
      "loss": 0.0004,
      "step": 3714
    },
    {
      "epoch": 154.39,
      "grad_norm": 0.06176307424902916,
      "learning_rate": 1.6030548342910302e-06,
      "loss": 0.0006,
      "step": 3715
    },
    {
      "epoch": 154.43,
      "grad_norm": 0.0038782821502536535,
      "learning_rate": 1.6007032546652784e-06,
      "loss": 0.0002,
      "step": 3716
    },
    {
      "epoch": 154.47,
      "grad_norm": 0.0037681651301681995,
      "learning_rate": 1.5983530723620173e-06,
      "loss": 0.0002,
      "step": 3717
    },
    {
      "epoch": 154.51,
      "grad_norm": 0.050047654658555984,
      "learning_rate": 1.5960042883473154e-06,
      "loss": 0.0004,
      "step": 3718
    },
    {
      "epoch": 154.56,
      "grad_norm": 0.05992366001009941,
      "learning_rate": 1.59365690358667e-06,
      "loss": 0.0005,
      "step": 3719
    },
    {
      "epoch": 154.6,
      "grad_norm": 0.06651749461889267,
      "learning_rate": 1.5913109190450033e-06,
      "loss": 0.0006,
      "step": 3720
    },
    {
      "epoch": 154.64,
      "grad_norm": 0.05956123396754265,
      "learning_rate": 1.5889663356866597e-06,
      "loss": 0.0006,
      "step": 3721
    },
    {
      "epoch": 154.68,
      "grad_norm": 0.025659935548901558,
      "learning_rate": 1.58662315447541e-06,
      "loss": 0.0003,
      "step": 3722
    },
    {
      "epoch": 154.72,
      "grad_norm": 0.06494472920894623,
      "learning_rate": 1.5842813763744468e-06,
      "loss": 0.0007,
      "step": 3723
    },
    {
      "epoch": 154.76,
      "grad_norm": 0.06633814424276352,
      "learning_rate": 1.581941002346387e-06,
      "loss": 0.0007,
      "step": 3724
    },
    {
      "epoch": 154.81,
      "grad_norm": 0.06599374860525131,
      "learning_rate": 1.5796020333532696e-06,
      "loss": 0.0007,
      "step": 3725
    },
    {
      "epoch": 154.85,
      "grad_norm": 0.04283246770501137,
      "learning_rate": 1.5772644703565564e-06,
      "loss": 0.0005,
      "step": 3726
    },
    {
      "epoch": 154.89,
      "grad_norm": 0.0038454404566437006,
      "learning_rate": 1.5749283143171335e-06,
      "loss": 0.0002,
      "step": 3727
    },
    {
      "epoch": 154.93,
      "grad_norm": 0.03703029826283455,
      "learning_rate": 1.5725935661953024e-06,
      "loss": 0.0004,
      "step": 3728
    },
    {
      "epoch": 154.97,
      "grad_norm": 0.04692709073424339,
      "learning_rate": 1.5702602269507917e-06,
      "loss": 0.0006,
      "step": 3729
    },
    {
      "epoch": 155.01,
      "grad_norm": 0.04422452300786972,
      "learning_rate": 1.567928297542749e-06,
      "loss": 0.0004,
      "step": 3730
    },
    {
      "epoch": 155.05,
      "grad_norm": 0.0658685639500618,
      "learning_rate": 1.5655977789297428e-06,
      "loss": 0.0007,
      "step": 3731
    },
    {
      "epoch": 155.1,
      "grad_norm": 0.030105050653219223,
      "learning_rate": 1.5632686720697604e-06,
      "loss": 0.0004,
      "step": 3732
    },
    {
      "epoch": 155.14,
      "grad_norm": 0.015989014878869057,
      "learning_rate": 1.5609409779202105e-06,
      "loss": 0.0003,
      "step": 3733
    },
    {
      "epoch": 155.18,
      "grad_norm": 0.08257845044136047,
      "learning_rate": 1.5586146974379201e-06,
      "loss": 0.0006,
      "step": 3734
    },
    {
      "epoch": 155.22,
      "grad_norm": 0.02604689635336399,
      "learning_rate": 1.5562898315791354e-06,
      "loss": 0.0003,
      "step": 3735
    },
    {
      "epoch": 155.26,
      "grad_norm": 0.023773377761244774,
      "learning_rate": 1.5539663812995204e-06,
      "loss": 0.0003,
      "step": 3736
    },
    {
      "epoch": 155.3,
      "grad_norm": 0.003865996841341257,
      "learning_rate": 1.5516443475541592e-06,
      "loss": 0.0002,
      "step": 3737
    },
    {
      "epoch": 155.35,
      "grad_norm": 0.047770339995622635,
      "learning_rate": 1.5493237312975495e-06,
      "loss": 0.0005,
      "step": 3738
    },
    {
      "epoch": 155.39,
      "grad_norm": 0.07334764301776886,
      "learning_rate": 1.547004533483611e-06,
      "loss": 0.0006,
      "step": 3739
    },
    {
      "epoch": 155.43,
      "grad_norm": 0.0038644117303192616,
      "learning_rate": 1.544686755065677e-06,
      "loss": 0.0002,
      "step": 3740
    },
    {
      "epoch": 155.47,
      "grad_norm": 0.08011471480131149,
      "learning_rate": 1.5423703969964998e-06,
      "loss": 0.0006,
      "step": 3741
    },
    {
      "epoch": 155.51,
      "grad_norm": 0.04207282140851021,
      "learning_rate": 1.5400554602282465e-06,
      "loss": 0.0004,
      "step": 3742
    },
    {
      "epoch": 155.55,
      "grad_norm": 0.03453357145190239,
      "learning_rate": 1.5377419457124993e-06,
      "loss": 0.0004,
      "step": 3743
    },
    {
      "epoch": 155.59,
      "grad_norm": 0.09280350804328918,
      "learning_rate": 1.5354298544002576e-06,
      "loss": 0.0008,
      "step": 3744
    },
    {
      "epoch": 155.64,
      "grad_norm": 0.04689628630876541,
      "learning_rate": 1.5331191872419349e-06,
      "loss": 0.0004,
      "step": 3745
    },
    {
      "epoch": 155.68,
      "grad_norm": 0.003685594769194722,
      "learning_rate": 1.5308099451873582e-06,
      "loss": 0.0002,
      "step": 3746
    },
    {
      "epoch": 155.72,
      "grad_norm": 0.03940146043896675,
      "learning_rate": 1.5285021291857705e-06,
      "loss": 0.0004,
      "step": 3747
    },
    {
      "epoch": 155.76,
      "grad_norm": 0.34866833686828613,
      "learning_rate": 1.526195740185829e-06,
      "loss": 0.0023,
      "step": 3748
    },
    {
      "epoch": 155.8,
      "grad_norm": 0.10620780289173126,
      "learning_rate": 1.5238907791356005e-06,
      "loss": 0.0009,
      "step": 3749
    },
    {
      "epoch": 155.84,
      "grad_norm": 0.07379370182752609,
      "learning_rate": 1.5215872469825682e-06,
      "loss": 0.0013,
      "step": 3750
    },
    {
      "epoch": 155.89,
      "grad_norm": 0.04798823595046997,
      "learning_rate": 1.5192851446736278e-06,
      "loss": 0.0004,
      "step": 3751
    },
    {
      "epoch": 155.93,
      "grad_norm": 0.04108637571334839,
      "learning_rate": 1.516984473155086e-06,
      "loss": 0.0004,
      "step": 3752
    },
    {
      "epoch": 155.97,
      "grad_norm": 0.03319930657744408,
      "learning_rate": 1.5146852333726642e-06,
      "loss": 0.0003,
      "step": 3753
    },
    {
      "epoch": 156.01,
      "grad_norm": 0.11791758239269257,
      "learning_rate": 1.5123874262714893e-06,
      "loss": 0.0013,
      "step": 3754
    },
    {
      "epoch": 156.05,
      "grad_norm": 0.022522838786244392,
      "learning_rate": 1.510091052796105e-06,
      "loss": 0.0003,
      "step": 3755
    },
    {
      "epoch": 156.09,
      "grad_norm": 0.024653399363160133,
      "learning_rate": 1.5077961138904628e-06,
      "loss": 0.0003,
      "step": 3756
    },
    {
      "epoch": 156.14,
      "grad_norm": 0.05113057792186737,
      "learning_rate": 1.505502610497927e-06,
      "loss": 0.0004,
      "step": 3757
    },
    {
      "epoch": 156.18,
      "grad_norm": 0.14350064098834991,
      "learning_rate": 1.5032105435612693e-06,
      "loss": 0.0007,
      "step": 3758
    },
    {
      "epoch": 156.22,
      "grad_norm": 0.05708557739853859,
      "learning_rate": 1.500919914022671e-06,
      "loss": 0.0006,
      "step": 3759
    },
    {
      "epoch": 156.26,
      "grad_norm": 0.07916730642318726,
      "learning_rate": 1.4986307228237268e-06,
      "loss": 0.0007,
      "step": 3760
    },
    {
      "epoch": 156.3,
      "grad_norm": 0.0603613555431366,
      "learning_rate": 1.4963429709054323e-06,
      "loss": 0.0005,
      "step": 3761
    },
    {
      "epoch": 156.34,
      "grad_norm": 0.031879547983407974,
      "learning_rate": 1.4940566592081973e-06,
      "loss": 0.0003,
      "step": 3762
    },
    {
      "epoch": 156.38,
      "grad_norm": 0.04037948697805405,
      "learning_rate": 1.4917717886718392e-06,
      "loss": 0.0004,
      "step": 3763
    },
    {
      "epoch": 156.43,
      "grad_norm": 0.02845039777457714,
      "learning_rate": 1.4894883602355808e-06,
      "loss": 0.0003,
      "step": 3764
    },
    {
      "epoch": 156.47,
      "grad_norm": 0.03391824662685394,
      "learning_rate": 1.4872063748380544e-06,
      "loss": 0.0004,
      "step": 3765
    },
    {
      "epoch": 156.51,
      "grad_norm": 0.11603379249572754,
      "learning_rate": 1.4849258334172973e-06,
      "loss": 0.0011,
      "step": 3766
    },
    {
      "epoch": 156.55,
      "grad_norm": 0.07559607923030853,
      "learning_rate": 1.4826467369107545e-06,
      "loss": 0.0009,
      "step": 3767
    },
    {
      "epoch": 156.59,
      "grad_norm": 0.05178965628147125,
      "learning_rate": 1.4803690862552755e-06,
      "loss": 0.0005,
      "step": 3768
    },
    {
      "epoch": 156.63,
      "grad_norm": 0.0037730613257735968,
      "learning_rate": 1.478092882387117e-06,
      "loss": 0.0002,
      "step": 3769
    },
    {
      "epoch": 156.68,
      "grad_norm": 0.042709995061159134,
      "learning_rate": 1.4758181262419425e-06,
      "loss": 0.0004,
      "step": 3770
    },
    {
      "epoch": 156.72,
      "grad_norm": 0.03723019361495972,
      "learning_rate": 1.4735448187548147e-06,
      "loss": 0.0004,
      "step": 3771
    },
    {
      "epoch": 156.76,
      "grad_norm": 0.0503137931227684,
      "learning_rate": 1.4712729608602062e-06,
      "loss": 0.0007,
      "step": 3772
    },
    {
      "epoch": 156.8,
      "grad_norm": 0.043475326150655746,
      "learning_rate": 1.4690025534919916e-06,
      "loss": 0.0004,
      "step": 3773
    },
    {
      "epoch": 156.84,
      "grad_norm": 0.021634578704833984,
      "learning_rate": 1.4667335975834495e-06,
      "loss": 0.0003,
      "step": 3774
    },
    {
      "epoch": 156.88,
      "grad_norm": 0.09087492525577545,
      "learning_rate": 1.4644660940672628e-06,
      "loss": 0.0008,
      "step": 3775
    },
    {
      "epoch": 156.92,
      "grad_norm": 0.07225903868675232,
      "learning_rate": 1.4622000438755157e-06,
      "loss": 0.0008,
      "step": 3776
    },
    {
      "epoch": 156.97,
      "grad_norm": 0.06903776526451111,
      "learning_rate": 1.4599354479396966e-06,
      "loss": 0.0006,
      "step": 3777
    },
    {
      "epoch": 157.01,
      "grad_norm": 0.05251191556453705,
      "learning_rate": 1.4576723071906945e-06,
      "loss": 0.0004,
      "step": 3778
    },
    {
      "epoch": 157.05,
      "grad_norm": 0.07727067172527313,
      "learning_rate": 1.4554106225588017e-06,
      "loss": 0.0005,
      "step": 3779
    },
    {
      "epoch": 157.09,
      "grad_norm": 0.0372808538377285,
      "learning_rate": 1.4531503949737107e-06,
      "loss": 0.0004,
      "step": 3780
    },
    {
      "epoch": 157.13,
      "grad_norm": 0.06926334649324417,
      "learning_rate": 1.4508916253645183e-06,
      "loss": 0.0005,
      "step": 3781
    },
    {
      "epoch": 157.17,
      "grad_norm": 0.1044364646077156,
      "learning_rate": 1.4486343146597154e-06,
      "loss": 0.0006,
      "step": 3782
    },
    {
      "epoch": 157.22,
      "grad_norm": 0.05447789281606674,
      "learning_rate": 1.4463784637871991e-06,
      "loss": 0.0005,
      "step": 3783
    },
    {
      "epoch": 157.26,
      "grad_norm": 0.030658302828669548,
      "learning_rate": 1.444124073674264e-06,
      "loss": 0.0003,
      "step": 3784
    },
    {
      "epoch": 157.3,
      "grad_norm": 0.06149066612124443,
      "learning_rate": 1.4418711452476048e-06,
      "loss": 0.0005,
      "step": 3785
    },
    {
      "epoch": 157.34,
      "grad_norm": 0.036055874079465866,
      "learning_rate": 1.439619679433316e-06,
      "loss": 0.0003,
      "step": 3786
    },
    {
      "epoch": 157.38,
      "grad_norm": 0.056517209857702255,
      "learning_rate": 1.4373696771568896e-06,
      "loss": 0.0004,
      "step": 3787
    },
    {
      "epoch": 157.42,
      "grad_norm": 0.037451256066560745,
      "learning_rate": 1.4351211393432162e-06,
      "loss": 0.0004,
      "step": 3788
    },
    {
      "epoch": 157.46,
      "grad_norm": 0.0478711873292923,
      "learning_rate": 1.4328740669165858e-06,
      "loss": 0.0005,
      "step": 3789
    },
    {
      "epoch": 157.51,
      "grad_norm": 0.13994403183460236,
      "learning_rate": 1.4306284608006837e-06,
      "loss": 0.001,
      "step": 3790
    },
    {
      "epoch": 157.55,
      "grad_norm": 0.047356121242046356,
      "learning_rate": 1.4283843219185966e-06,
      "loss": 0.0005,
      "step": 3791
    },
    {
      "epoch": 157.59,
      "grad_norm": 0.05060246214270592,
      "learning_rate": 1.4261416511928012e-06,
      "loss": 0.0004,
      "step": 3792
    },
    {
      "epoch": 157.63,
      "grad_norm": 0.057527631521224976,
      "learning_rate": 1.4239004495451763e-06,
      "loss": 0.0004,
      "step": 3793
    },
    {
      "epoch": 157.67,
      "grad_norm": 0.055015817284584045,
      "learning_rate": 1.421660717896996e-06,
      "loss": 0.0006,
      "step": 3794
    },
    {
      "epoch": 157.71,
      "grad_norm": 0.03637170419096947,
      "learning_rate": 1.4194224571689286e-06,
      "loss": 0.0004,
      "step": 3795
    },
    {
      "epoch": 157.76,
      "grad_norm": 0.1119626984000206,
      "learning_rate": 1.4171856682810386e-06,
      "loss": 0.001,
      "step": 3796
    },
    {
      "epoch": 157.8,
      "grad_norm": 0.09417091310024261,
      "learning_rate": 1.4149503521527862e-06,
      "loss": 0.0008,
      "step": 3797
    },
    {
      "epoch": 157.84,
      "grad_norm": 0.12337389588356018,
      "learning_rate": 1.412716509703026e-06,
      "loss": 0.0011,
      "step": 3798
    },
    {
      "epoch": 157.88,
      "grad_norm": 0.03896244242787361,
      "learning_rate": 1.4104841418500038e-06,
      "loss": 0.0003,
      "step": 3799
    },
    {
      "epoch": 157.92,
      "grad_norm": 0.007127730175852776,
      "learning_rate": 1.4082532495113627e-06,
      "loss": 0.0004,
      "step": 3800
    },
    {
      "epoch": 157.96,
      "grad_norm": 0.07942785322666168,
      "learning_rate": 1.4060238336041388e-06,
      "loss": 0.0008,
      "step": 3801
    },
    {
      "epoch": 158.01,
      "grad_norm": 0.003962188493460417,
      "learning_rate": 1.4037958950447604e-06,
      "loss": 0.0002,
      "step": 3802
    },
    {
      "epoch": 158.05,
      "grad_norm": 0.041380930691957474,
      "learning_rate": 1.401569434749051e-06,
      "loss": 0.0005,
      "step": 3803
    },
    {
      "epoch": 158.09,
      "grad_norm": 0.07599319517612457,
      "learning_rate": 1.3993444536322204e-06,
      "loss": 0.0009,
      "step": 3804
    },
    {
      "epoch": 158.13,
      "grad_norm": 0.05052020028233528,
      "learning_rate": 1.3971209526088764e-06,
      "loss": 0.0005,
      "step": 3805
    },
    {
      "epoch": 158.17,
      "grad_norm": 0.03550953045487404,
      "learning_rate": 1.3948989325930162e-06,
      "loss": 0.0004,
      "step": 3806
    },
    {
      "epoch": 158.21,
      "grad_norm": 0.01891433820128441,
      "learning_rate": 1.3926783944980283e-06,
      "loss": 0.0003,
      "step": 3807
    },
    {
      "epoch": 158.25,
      "grad_norm": 0.03321753069758415,
      "learning_rate": 1.3904593392366916e-06,
      "loss": 0.0003,
      "step": 3808
    },
    {
      "epoch": 158.3,
      "grad_norm": 0.05618257075548172,
      "learning_rate": 1.3882417677211769e-06,
      "loss": 0.0005,
      "step": 3809
    },
    {
      "epoch": 158.34,
      "grad_norm": 0.033958110958337784,
      "learning_rate": 1.3860256808630429e-06,
      "loss": 0.0004,
      "step": 3810
    },
    {
      "epoch": 158.38,
      "grad_norm": 0.03923359513282776,
      "learning_rate": 1.3838110795732396e-06,
      "loss": 0.0003,
      "step": 3811
    },
    {
      "epoch": 158.42,
      "grad_norm": 0.05775700882077217,
      "learning_rate": 1.3815979647621063e-06,
      "loss": 0.0006,
      "step": 3812
    },
    {
      "epoch": 158.46,
      "grad_norm": 0.0037071630358695984,
      "learning_rate": 1.3793863373393711e-06,
      "loss": 0.0002,
      "step": 3813
    },
    {
      "epoch": 158.5,
      "grad_norm": 0.1420128047466278,
      "learning_rate": 1.3771761982141513e-06,
      "loss": 0.0009,
      "step": 3814
    },
    {
      "epoch": 158.55,
      "grad_norm": 0.036974769085645676,
      "learning_rate": 1.3749675482949487e-06,
      "loss": 0.0004,
      "step": 3815
    },
    {
      "epoch": 158.59,
      "grad_norm": 0.05391484498977661,
      "learning_rate": 1.3727603884896578e-06,
      "loss": 0.0004,
      "step": 3816
    },
    {
      "epoch": 158.63,
      "grad_norm": 0.06196398660540581,
      "learning_rate": 1.3705547197055586e-06,
      "loss": 0.0005,
      "step": 3817
    },
    {
      "epoch": 158.67,
      "grad_norm": 0.08397499471902847,
      "learning_rate": 1.3683505428493177e-06,
      "loss": 0.0006,
      "step": 3818
    },
    {
      "epoch": 158.71,
      "grad_norm": 0.05105561763048172,
      "learning_rate": 1.3661478588269889e-06,
      "loss": 0.0004,
      "step": 3819
    },
    {
      "epoch": 158.75,
      "grad_norm": 0.09945247322320938,
      "learning_rate": 1.3639466685440133e-06,
      "loss": 0.0009,
      "step": 3820
    },
    {
      "epoch": 158.79,
      "grad_norm": 0.09281975030899048,
      "learning_rate": 1.3617469729052162e-06,
      "loss": 0.0008,
      "step": 3821
    },
    {
      "epoch": 158.84,
      "grad_norm": 0.029569508507847786,
      "learning_rate": 1.3595487728148099e-06,
      "loss": 0.0003,
      "step": 3822
    },
    {
      "epoch": 158.88,
      "grad_norm": 0.039881885051727295,
      "learning_rate": 1.3573520691763914e-06,
      "loss": 0.0004,
      "step": 3823
    },
    {
      "epoch": 158.92,
      "grad_norm": 0.04921633377671242,
      "learning_rate": 1.3551568628929434e-06,
      "loss": 0.0003,
      "step": 3824
    },
    {
      "epoch": 158.96,
      "grad_norm": 0.04846923425793648,
      "learning_rate": 1.3529631548668298e-06,
      "loss": 0.0004,
      "step": 3825
    },
    {
      "epoch": 159.0,
      "grad_norm": 0.1175399199128151,
      "learning_rate": 1.3507709459998032e-06,
      "loss": 0.0011,
      "step": 3826
    },
    {
      "epoch": 159.04,
      "grad_norm": 0.0038944166153669357,
      "learning_rate": 1.3485802371929968e-06,
      "loss": 0.0002,
      "step": 3827
    },
    {
      "epoch": 159.09,
      "grad_norm": 0.03427647799253464,
      "learning_rate": 1.346391029346929e-06,
      "loss": 0.0003,
      "step": 3828
    },
    {
      "epoch": 159.13,
      "grad_norm": 0.01561632938683033,
      "learning_rate": 1.3442033233614998e-06,
      "loss": 0.0004,
      "step": 3829
    },
    {
      "epoch": 159.17,
      "grad_norm": 0.003474829951301217,
      "learning_rate": 1.3420171201359933e-06,
      "loss": 0.0002,
      "step": 3830
    },
    {
      "epoch": 159.21,
      "grad_norm": 0.050131089985370636,
      "learning_rate": 1.3398324205690744e-06,
      "loss": 0.0004,
      "step": 3831
    },
    {
      "epoch": 159.25,
      "grad_norm": 0.11192022264003754,
      "learning_rate": 1.3376492255587909e-06,
      "loss": 0.0011,
      "step": 3832
    },
    {
      "epoch": 159.29,
      "grad_norm": 0.03668370470404625,
      "learning_rate": 1.3354675360025709e-06,
      "loss": 0.0004,
      "step": 3833
    },
    {
      "epoch": 159.34,
      "grad_norm": 0.03512971103191376,
      "learning_rate": 1.333287352797228e-06,
      "loss": 0.0004,
      "step": 3834
    },
    {
      "epoch": 159.38,
      "grad_norm": 0.41348135471343994,
      "learning_rate": 1.331108676838948e-06,
      "loss": 0.0023,
      "step": 3835
    },
    {
      "epoch": 159.42,
      "grad_norm": 0.02163664996623993,
      "learning_rate": 1.3289315090233056e-06,
      "loss": 0.0003,
      "step": 3836
    },
    {
      "epoch": 159.46,
      "grad_norm": 0.0485423319041729,
      "learning_rate": 1.326755850245251e-06,
      "loss": 0.0004,
      "step": 3837
    },
    {
      "epoch": 159.5,
      "grad_norm": 0.003918634261935949,
      "learning_rate": 1.3245817013991164e-06,
      "loss": 0.0002,
      "step": 3838
    },
    {
      "epoch": 159.54,
      "grad_norm": 0.07005701959133148,
      "learning_rate": 1.3224090633786112e-06,
      "loss": 0.0007,
      "step": 3839
    },
    {
      "epoch": 159.58,
      "grad_norm": 0.094656802713871,
      "learning_rate": 1.3202379370768254e-06,
      "loss": 0.0008,
      "step": 3840
    },
    {
      "epoch": 159.63,
      "grad_norm": 0.10502651333808899,
      "learning_rate": 1.3180683233862268e-06,
      "loss": 0.001,
      "step": 3841
    },
    {
      "epoch": 159.67,
      "grad_norm": 0.045584794133901596,
      "learning_rate": 1.3159002231986617e-06,
      "loss": 0.0004,
      "step": 3842
    },
    {
      "epoch": 159.71,
      "grad_norm": 0.055234070867300034,
      "learning_rate": 1.3137336374053544e-06,
      "loss": 0.0006,
      "step": 3843
    },
    {
      "epoch": 159.75,
      "grad_norm": 0.03379153832793236,
      "learning_rate": 1.3115685668969075e-06,
      "loss": 0.0003,
      "step": 3844
    },
    {
      "epoch": 159.79,
      "grad_norm": 0.0035738316364586353,
      "learning_rate": 1.3094050125632973e-06,
      "loss": 0.0002,
      "step": 3845
    },
    {
      "epoch": 159.83,
      "grad_norm": 0.07502773404121399,
      "learning_rate": 1.3072429752938803e-06,
      "loss": 0.0007,
      "step": 3846
    },
    {
      "epoch": 159.88,
      "grad_norm": 0.06296712905168533,
      "learning_rate": 1.30508245597739e-06,
      "loss": 0.0005,
      "step": 3847
    },
    {
      "epoch": 159.92,
      "grad_norm": 0.04817529022693634,
      "learning_rate": 1.3029234555019315e-06,
      "loss": 0.0003,
      "step": 3848
    },
    {
      "epoch": 159.96,
      "grad_norm": 0.10105907171964645,
      "learning_rate": 1.30076597475499e-06,
      "loss": 0.0008,
      "step": 3849
    },
    {
      "epoch": 160.0,
      "grad_norm": 0.13418295979499817,
      "learning_rate": 1.298610014623423e-06,
      "loss": 0.0008,
      "step": 3850
    },
    {
      "epoch": 160.04,
      "grad_norm": 0.04111272096633911,
      "learning_rate": 1.296455575993466e-06,
      "loss": 0.0004,
      "step": 3851
    },
    {
      "epoch": 160.08,
      "grad_norm": 0.03706098720431328,
      "learning_rate": 1.2943026597507268e-06,
      "loss": 0.0003,
      "step": 3852
    },
    {
      "epoch": 160.12,
      "grad_norm": 0.01956324465572834,
      "learning_rate": 1.2921512667801871e-06,
      "loss": 0.0003,
      "step": 3853
    },
    {
      "epoch": 160.17,
      "grad_norm": 0.03193320333957672,
      "learning_rate": 1.2900013979662046e-06,
      "loss": 0.0005,
      "step": 3854
    },
    {
      "epoch": 160.21,
      "grad_norm": 0.04664044827222824,
      "learning_rate": 1.2878530541925077e-06,
      "loss": 0.0005,
      "step": 3855
    },
    {
      "epoch": 160.25,
      "grad_norm": 0.023383615538477898,
      "learning_rate": 1.2857062363422007e-06,
      "loss": 0.0003,
      "step": 3856
    },
    {
      "epoch": 160.29,
      "grad_norm": 0.017515694722533226,
      "learning_rate": 1.2835609452977604e-06,
      "loss": 0.0003,
      "step": 3857
    },
    {
      "epoch": 160.33,
      "grad_norm": 0.06745463609695435,
      "learning_rate": 1.2814171819410315e-06,
      "loss": 0.0007,
      "step": 3858
    },
    {
      "epoch": 160.37,
      "grad_norm": 0.040468309074640274,
      "learning_rate": 1.2792749471532361e-06,
      "loss": 0.0005,
      "step": 3859
    },
    {
      "epoch": 160.42,
      "grad_norm": 0.0780482292175293,
      "learning_rate": 1.2771342418149658e-06,
      "loss": 0.0007,
      "step": 3860
    },
    {
      "epoch": 160.46,
      "grad_norm": 0.06717100739479065,
      "learning_rate": 1.274995066806184e-06,
      "loss": 0.0005,
      "step": 3861
    },
    {
      "epoch": 160.5,
      "grad_norm": 0.0488230399787426,
      "learning_rate": 1.2728574230062242e-06,
      "loss": 0.0004,
      "step": 3862
    },
    {
      "epoch": 160.54,
      "grad_norm": 0.0324644036591053,
      "learning_rate": 1.2707213112937911e-06,
      "loss": 0.0004,
      "step": 3863
    },
    {
      "epoch": 160.58,
      "grad_norm": 0.11668048799037933,
      "learning_rate": 1.2685867325469603e-06,
      "loss": 0.0008,
      "step": 3864
    },
    {
      "epoch": 160.62,
      "grad_norm": 0.038517918437719345,
      "learning_rate": 1.2664536876431755e-06,
      "loss": 0.001,
      "step": 3865
    },
    {
      "epoch": 160.66,
      "grad_norm": 0.05955545976758003,
      "learning_rate": 1.2643221774592517e-06,
      "loss": 0.0006,
      "step": 3866
    },
    {
      "epoch": 160.71,
      "grad_norm": 0.0034207517746835947,
      "learning_rate": 1.2621922028713719e-06,
      "loss": 0.0002,
      "step": 3867
    },
    {
      "epoch": 160.75,
      "grad_norm": 0.0628741905093193,
      "learning_rate": 1.2600637647550889e-06,
      "loss": 0.0005,
      "step": 3868
    },
    {
      "epoch": 160.79,
      "grad_norm": 0.04774365574121475,
      "learning_rate": 1.2579368639853217e-06,
      "loss": 0.0004,
      "step": 3869
    },
    {
      "epoch": 160.83,
      "grad_norm": 0.04067762568593025,
      "learning_rate": 1.2558115014363592e-06,
      "loss": 0.0005,
      "step": 3870
    },
    {
      "epoch": 160.87,
      "grad_norm": 0.0037744089495390654,
      "learning_rate": 1.2536876779818586e-06,
      "loss": 0.0002,
      "step": 3871
    },
    {
      "epoch": 160.91,
      "grad_norm": 0.027242450043559074,
      "learning_rate": 1.2515653944948424e-06,
      "loss": 0.0003,
      "step": 3872
    },
    {
      "epoch": 160.96,
      "grad_norm": 0.04667213186621666,
      "learning_rate": 1.2494446518477022e-06,
      "loss": 0.0004,
      "step": 3873
    },
    {
      "epoch": 161.0,
      "grad_norm": 0.06814206391572952,
      "learning_rate": 1.2473254509121947e-06,
      "loss": 0.0006,
      "step": 3874
    },
    {
      "epoch": 161.04,
      "grad_norm": 0.0036401841789484024,
      "learning_rate": 1.2452077925594435e-06,
      "loss": 0.0002,
      "step": 3875
    },
    {
      "epoch": 161.08,
      "grad_norm": 0.04175164923071861,
      "learning_rate": 1.2430916776599378e-06,
      "loss": 0.0004,
      "step": 3876
    },
    {
      "epoch": 161.12,
      "grad_norm": 0.06318776309490204,
      "learning_rate": 1.2409771070835324e-06,
      "loss": 0.0005,
      "step": 3877
    },
    {
      "epoch": 161.16,
      "grad_norm": 0.016859199851751328,
      "learning_rate": 1.238864081699449e-06,
      "loss": 0.0003,
      "step": 3878
    },
    {
      "epoch": 161.21,
      "grad_norm": 0.0811244323849678,
      "learning_rate": 1.23675260237627e-06,
      "loss": 0.0006,
      "step": 3879
    },
    {
      "epoch": 161.25,
      "grad_norm": 0.07176660746335983,
      "learning_rate": 1.234642669981946e-06,
      "loss": 0.0005,
      "step": 3880
    },
    {
      "epoch": 161.29,
      "grad_norm": 0.003523362334817648,
      "learning_rate": 1.2325342853837896e-06,
      "loss": 0.0002,
      "step": 3881
    },
    {
      "epoch": 161.33,
      "grad_norm": 0.04309208318591118,
      "learning_rate": 1.2304274494484796e-06,
      "loss": 0.0004,
      "step": 3882
    },
    {
      "epoch": 161.37,
      "grad_norm": 0.022746751084923744,
      "learning_rate": 1.2283221630420556e-06,
      "loss": 0.0003,
      "step": 3883
    },
    {
      "epoch": 161.41,
      "grad_norm": 0.09293018281459808,
      "learning_rate": 1.2262184270299215e-06,
      "loss": 0.0008,
      "step": 3884
    },
    {
      "epoch": 161.45,
      "grad_norm": 0.07261931151151657,
      "learning_rate": 1.2241162422768444e-06,
      "loss": 0.0007,
      "step": 3885
    },
    {
      "epoch": 161.5,
      "grad_norm": 0.08464965969324112,
      "learning_rate": 1.222015609646952e-06,
      "loss": 0.0006,
      "step": 3886
    },
    {
      "epoch": 161.54,
      "grad_norm": 0.031698405742645264,
      "learning_rate": 1.2199165300037358e-06,
      "loss": 0.0004,
      "step": 3887
    },
    {
      "epoch": 161.58,
      "grad_norm": 0.05873829126358032,
      "learning_rate": 1.217819004210049e-06,
      "loss": 0.001,
      "step": 3888
    },
    {
      "epoch": 161.62,
      "grad_norm": 0.08030826598405838,
      "learning_rate": 1.2157230331281028e-06,
      "loss": 0.0007,
      "step": 3889
    },
    {
      "epoch": 161.66,
      "grad_norm": 0.03218742460012436,
      "learning_rate": 1.2136286176194744e-06,
      "loss": 0.0003,
      "step": 3890
    },
    {
      "epoch": 161.7,
      "grad_norm": 0.03117331489920616,
      "learning_rate": 1.2115357585450965e-06,
      "loss": 0.0003,
      "step": 3891
    },
    {
      "epoch": 161.75,
      "grad_norm": 0.04588352516293526,
      "learning_rate": 1.2094444567652652e-06,
      "loss": 0.0004,
      "step": 3892
    },
    {
      "epoch": 161.79,
      "grad_norm": 0.06937912106513977,
      "learning_rate": 1.2073547131396357e-06,
      "loss": 0.0009,
      "step": 3893
    },
    {
      "epoch": 161.83,
      "grad_norm": 0.029151732102036476,
      "learning_rate": 1.205266528527223e-06,
      "loss": 0.0003,
      "step": 3894
    },
    {
      "epoch": 161.87,
      "grad_norm": 0.003639618633314967,
      "learning_rate": 1.203179903786401e-06,
      "loss": 0.0002,
      "step": 3895
    },
    {
      "epoch": 161.91,
      "grad_norm": 0.03800598159432411,
      "learning_rate": 1.2010948397749022e-06,
      "loss": 0.0004,
      "step": 3896
    },
    {
      "epoch": 161.95,
      "grad_norm": 0.12542590498924255,
      "learning_rate": 1.1990113373498174e-06,
      "loss": 0.001,
      "step": 3897
    },
    {
      "epoch": 161.99,
      "grad_norm": 0.07048996537923813,
      "learning_rate": 1.1969293973675961e-06,
      "loss": 0.0006,
      "step": 3898
    },
    {
      "epoch": 162.04,
      "grad_norm": 0.034899044781923294,
      "learning_rate": 1.1948490206840447e-06,
      "loss": 0.0003,
      "step": 3899
    },
    {
      "epoch": 162.08,
      "grad_norm": 0.0036258602049201727,
      "learning_rate": 1.1927702081543279e-06,
      "loss": 0.0002,
      "step": 3900
    },
    {
      "epoch": 162.12,
      "grad_norm": 0.039535991847515106,
      "learning_rate": 1.1906929606329682e-06,
      "loss": 0.0004,
      "step": 3901
    },
    {
      "epoch": 162.16,
      "grad_norm": 0.07766419649124146,
      "learning_rate": 1.188617278973841e-06,
      "loss": 0.0008,
      "step": 3902
    },
    {
      "epoch": 162.2,
      "grad_norm": 0.03655841574072838,
      "learning_rate": 1.1865431640301816e-06,
      "loss": 0.0004,
      "step": 3903
    },
    {
      "epoch": 162.24,
      "grad_norm": 0.05938355252146721,
      "learning_rate": 1.1844706166545811e-06,
      "loss": 0.0005,
      "step": 3904
    },
    {
      "epoch": 162.29,
      "grad_norm": 0.003735973732545972,
      "learning_rate": 1.1823996376989849e-06,
      "loss": 0.0002,
      "step": 3905
    },
    {
      "epoch": 162.33,
      "grad_norm": 0.0629294142127037,
      "learning_rate": 1.1803302280146938e-06,
      "loss": 0.0005,
      "step": 3906
    },
    {
      "epoch": 162.37,
      "grad_norm": 0.06763817369937897,
      "learning_rate": 1.1782623884523647e-06,
      "loss": 0.0006,
      "step": 3907
    },
    {
      "epoch": 162.41,
      "grad_norm": 0.02924053557217121,
      "learning_rate": 1.1761961198620081e-06,
      "loss": 0.0003,
      "step": 3908
    },
    {
      "epoch": 162.45,
      "grad_norm": 0.03045922890305519,
      "learning_rate": 1.174131423092989e-06,
      "loss": 0.0003,
      "step": 3909
    },
    {
      "epoch": 162.49,
      "grad_norm": 0.011655326932668686,
      "learning_rate": 1.1720682989940264e-06,
      "loss": 0.0003,
      "step": 3910
    },
    {
      "epoch": 162.54,
      "grad_norm": 0.04248048737645149,
      "learning_rate": 1.1700067484131932e-06,
      "loss": 0.0005,
      "step": 3911
    },
    {
      "epoch": 162.58,
      "grad_norm": 0.07850593328475952,
      "learning_rate": 1.1679467721979132e-06,
      "loss": 0.0006,
      "step": 3912
    },
    {
      "epoch": 162.62,
      "grad_norm": 0.042509131133556366,
      "learning_rate": 1.1658883711949658e-06,
      "loss": 0.0004,
      "step": 3913
    },
    {
      "epoch": 162.66,
      "grad_norm": 0.03877079114317894,
      "learning_rate": 1.1638315462504817e-06,
      "loss": 0.0003,
      "step": 3914
    },
    {
      "epoch": 162.7,
      "grad_norm": 0.09918113052845001,
      "learning_rate": 1.1617762982099446e-06,
      "loss": 0.0008,
      "step": 3915
    },
    {
      "epoch": 162.74,
      "grad_norm": 0.11280766129493713,
      "learning_rate": 1.159722627918189e-06,
      "loss": 0.0008,
      "step": 3916
    },
    {
      "epoch": 162.78,
      "grad_norm": 0.08087008446455002,
      "learning_rate": 1.1576705362194008e-06,
      "loss": 0.0008,
      "step": 3917
    },
    {
      "epoch": 162.83,
      "grad_norm": 0.06017446890473366,
      "learning_rate": 1.1556200239571175e-06,
      "loss": 0.0004,
      "step": 3918
    },
    {
      "epoch": 162.87,
      "grad_norm": 0.056138478219509125,
      "learning_rate": 1.1535710919742278e-06,
      "loss": 0.0004,
      "step": 3919
    },
    {
      "epoch": 162.91,
      "grad_norm": 0.026779893785715103,
      "learning_rate": 1.1515237411129698e-06,
      "loss": 0.0003,
      "step": 3920
    },
    {
      "epoch": 162.95,
      "grad_norm": 0.03804904595017433,
      "learning_rate": 1.1494779722149334e-06,
      "loss": 0.0008,
      "step": 3921
    },
    {
      "epoch": 162.99,
      "grad_norm": 0.049021508544683456,
      "learning_rate": 1.1474337861210543e-06,
      "loss": 0.0005,
      "step": 3922
    },
    {
      "epoch": 163.03,
      "grad_norm": 0.17837710678577423,
      "learning_rate": 1.145391183671622e-06,
      "loss": 0.0013,
      "step": 3923
    },
    {
      "epoch": 163.08,
      "grad_norm": 0.10805850476026535,
      "learning_rate": 1.1433501657062723e-06,
      "loss": 0.0006,
      "step": 3924
    },
    {
      "epoch": 163.12,
      "grad_norm": 0.055851537734270096,
      "learning_rate": 1.141310733063991e-06,
      "loss": 0.0008,
      "step": 3925
    },
    {
      "epoch": 163.16,
      "grad_norm": 0.039100293070077896,
      "learning_rate": 1.1392728865831127e-06,
      "loss": 0.0005,
      "step": 3926
    },
    {
      "epoch": 163.2,
      "grad_norm": 0.03888829052448273,
      "learning_rate": 1.1372366271013175e-06,
      "loss": 0.0004,
      "step": 3927
    },
    {
      "epoch": 163.24,
      "grad_norm": 0.06370940059423447,
      "learning_rate": 1.135201955455636e-06,
      "loss": 0.0007,
      "step": 3928
    },
    {
      "epoch": 163.28,
      "grad_norm": 0.029066987335681915,
      "learning_rate": 1.133168872482444e-06,
      "loss": 0.0003,
      "step": 3929
    },
    {
      "epoch": 163.32,
      "grad_norm": 0.06522440165281296,
      "learning_rate": 1.1311373790174656e-06,
      "loss": 0.0006,
      "step": 3930
    },
    {
      "epoch": 163.37,
      "grad_norm": 0.07013023644685745,
      "learning_rate": 1.1291074758957715e-06,
      "loss": 0.0005,
      "step": 3931
    },
    {
      "epoch": 163.41,
      "grad_norm": 0.003960114903748035,
      "learning_rate": 1.1270791639517786e-06,
      "loss": 0.0002,
      "step": 3932
    },
    {
      "epoch": 163.45,
      "grad_norm": 0.017684683203697205,
      "learning_rate": 1.1250524440192472e-06,
      "loss": 0.0003,
      "step": 3933
    },
    {
      "epoch": 163.49,
      "grad_norm": 0.07298028469085693,
      "learning_rate": 1.1230273169312878e-06,
      "loss": 0.0005,
      "step": 3934
    },
    {
      "epoch": 163.53,
      "grad_norm": 0.045854292809963226,
      "learning_rate": 1.1210037835203508e-06,
      "loss": 0.0005,
      "step": 3935
    },
    {
      "epoch": 163.57,
      "grad_norm": 0.04437941685318947,
      "learning_rate": 1.118981844618236e-06,
      "loss": 0.0005,
      "step": 3936
    },
    {
      "epoch": 163.62,
      "grad_norm": 0.049391381442546844,
      "learning_rate": 1.1169615010560863e-06,
      "loss": 0.0004,
      "step": 3937
    },
    {
      "epoch": 163.66,
      "grad_norm": 0.08238492906093597,
      "learning_rate": 1.1149427536643882e-06,
      "loss": 0.0006,
      "step": 3938
    },
    {
      "epoch": 163.7,
      "grad_norm": 0.04753141105175018,
      "learning_rate": 1.1129256032729724e-06,
      "loss": 0.0006,
      "step": 3939
    },
    {
      "epoch": 163.74,
      "grad_norm": 0.003516179509460926,
      "learning_rate": 1.1109100507110133e-06,
      "loss": 0.0002,
      "step": 3940
    },
    {
      "epoch": 163.78,
      "grad_norm": 0.09366832673549652,
      "learning_rate": 1.108896096807029e-06,
      "loss": 0.0006,
      "step": 3941
    },
    {
      "epoch": 163.82,
      "grad_norm": 0.009518038481473923,
      "learning_rate": 1.106883742388879e-06,
      "loss": 0.0003,
      "step": 3942
    },
    {
      "epoch": 163.86,
      "grad_norm": 0.0037257838994264603,
      "learning_rate": 1.1048729882837671e-06,
      "loss": 0.0002,
      "step": 3943
    },
    {
      "epoch": 163.91,
      "grad_norm": 0.003727050730958581,
      "learning_rate": 1.1028638353182392e-06,
      "loss": 0.0002,
      "step": 3944
    },
    {
      "epoch": 163.95,
      "grad_norm": 0.05166512355208397,
      "learning_rate": 1.1008562843181796e-06,
      "loss": 0.0004,
      "step": 3945
    },
    {
      "epoch": 163.99,
      "grad_norm": 0.06621058285236359,
      "learning_rate": 1.0988503361088177e-06,
      "loss": 0.0007,
      "step": 3946
    },
    {
      "epoch": 164.03,
      "grad_norm": 0.03952730447053909,
      "learning_rate": 1.0968459915147234e-06,
      "loss": 0.0004,
      "step": 3947
    },
    {
      "epoch": 164.07,
      "grad_norm": 0.03860030323266983,
      "learning_rate": 1.0948432513598073e-06,
      "loss": 0.0004,
      "step": 3948
    },
    {
      "epoch": 164.11,
      "grad_norm": 0.02828367054462433,
      "learning_rate": 1.0928421164673191e-06,
      "loss": 0.0003,
      "step": 3949
    },
    {
      "epoch": 164.16,
      "grad_norm": 0.04038819298148155,
      "learning_rate": 1.0908425876598512e-06,
      "loss": 0.0004,
      "step": 3950
    },
    {
      "epoch": 164.2,
      "grad_norm": 0.060005515813827515,
      "learning_rate": 1.0888446657593337e-06,
      "loss": 0.0005,
      "step": 3951
    },
    {
      "epoch": 164.24,
      "grad_norm": 0.028906403109431267,
      "learning_rate": 1.086848351587037e-06,
      "loss": 0.0004,
      "step": 3952
    },
    {
      "epoch": 164.28,
      "grad_norm": 0.027320697903633118,
      "learning_rate": 1.08485364596357e-06,
      "loss": 0.0003,
      "step": 3953
    },
    {
      "epoch": 164.32,
      "grad_norm": 0.047117333859205246,
      "learning_rate": 1.0828605497088823e-06,
      "loss": 0.0004,
      "step": 3954
    },
    {
      "epoch": 164.36,
      "grad_norm": 0.055534832179546356,
      "learning_rate": 1.0808690636422587e-06,
      "loss": 0.0004,
      "step": 3955
    },
    {
      "epoch": 164.41,
      "grad_norm": 0.06036979705095291,
      "learning_rate": 1.0788791885823236e-06,
      "loss": 0.0005,
      "step": 3956
    },
    {
      "epoch": 164.45,
      "grad_norm": 0.045227982103824615,
      "learning_rate": 1.076890925347041e-06,
      "loss": 0.0003,
      "step": 3957
    },
    {
      "epoch": 164.49,
      "grad_norm": 0.02637363038957119,
      "learning_rate": 1.0749042747537097e-06,
      "loss": 0.0003,
      "step": 3958
    },
    {
      "epoch": 164.53,
      "grad_norm": 0.03608428314328194,
      "learning_rate": 1.0729192376189678e-06,
      "loss": 0.0003,
      "step": 3959
    },
    {
      "epoch": 164.57,
      "grad_norm": 0.0564427450299263,
      "learning_rate": 1.0709358147587883e-06,
      "loss": 0.0005,
      "step": 3960
    },
    {
      "epoch": 164.61,
      "grad_norm": 0.048122256994247437,
      "learning_rate": 1.0689540069884814e-06,
      "loss": 0.0004,
      "step": 3961
    },
    {
      "epoch": 164.65,
      "grad_norm": 0.008580086752772331,
      "learning_rate": 1.0669738151226939e-06,
      "loss": 0.0003,
      "step": 3962
    },
    {
      "epoch": 164.7,
      "grad_norm": 0.09107095003128052,
      "learning_rate": 1.0649952399754077e-06,
      "loss": 0.0008,
      "step": 3963
    },
    {
      "epoch": 164.74,
      "grad_norm": 0.06014939025044441,
      "learning_rate": 1.06301828235994e-06,
      "loss": 0.0007,
      "step": 3964
    },
    {
      "epoch": 164.78,
      "grad_norm": 0.09734958410263062,
      "learning_rate": 1.0610429430889451e-06,
      "loss": 0.001,
      "step": 3965
    },
    {
      "epoch": 164.82,
      "grad_norm": 0.07134442776441574,
      "learning_rate": 1.0590692229744075e-06,
      "loss": 0.0008,
      "step": 3966
    },
    {
      "epoch": 164.86,
      "grad_norm": 0.054769210517406464,
      "learning_rate": 1.05709712282765e-06,
      "loss": 0.0005,
      "step": 3967
    },
    {
      "epoch": 164.9,
      "grad_norm": 0.09119262546300888,
      "learning_rate": 1.0551266434593293e-06,
      "loss": 0.0008,
      "step": 3968
    },
    {
      "epoch": 164.95,
      "grad_norm": 0.07523580640554428,
      "learning_rate": 1.0531577856794346e-06,
      "loss": 0.0005,
      "step": 3969
    },
    {
      "epoch": 164.99,
      "grad_norm": 0.0878002792596817,
      "learning_rate": 1.0511905502972885e-06,
      "loss": 0.0007,
      "step": 3970
    },
    {
      "epoch": 165.03,
      "grad_norm": 0.0036444757133722305,
      "learning_rate": 1.049224938121548e-06,
      "loss": 0.0002,
      "step": 3971
    },
    {
      "epoch": 165.07,
      "grad_norm": 0.030493520200252533,
      "learning_rate": 1.0472609499602017e-06,
      "loss": 0.0004,
      "step": 3972
    },
    {
      "epoch": 165.11,
      "grad_norm": 0.10732890665531158,
      "learning_rate": 1.0452985866205706e-06,
      "loss": 0.0006,
      "step": 3973
    },
    {
      "epoch": 165.15,
      "grad_norm": 0.07765055447816849,
      "learning_rate": 1.0433378489093082e-06,
      "loss": 0.001,
      "step": 3974
    },
    {
      "epoch": 165.19,
      "grad_norm": 0.07441993057727814,
      "learning_rate": 1.041378737632402e-06,
      "loss": 0.0006,
      "step": 3975
    },
    {
      "epoch": 165.24,
      "grad_norm": 0.03216736763715744,
      "learning_rate": 1.0394212535951642e-06,
      "loss": 0.0003,
      "step": 3976
    },
    {
      "epoch": 165.28,
      "grad_norm": 0.020986098796129227,
      "learning_rate": 1.037465397602246e-06,
      "loss": 0.0003,
      "step": 3977
    },
    {
      "epoch": 165.32,
      "grad_norm": 0.0036946076434105635,
      "learning_rate": 1.0355111704576237e-06,
      "loss": 0.0002,
      "step": 3978
    },
    {
      "epoch": 165.36,
      "grad_norm": 0.06908402591943741,
      "learning_rate": 1.0335585729646081e-06,
      "loss": 0.0005,
      "step": 3979
    },
    {
      "epoch": 165.4,
      "grad_norm": 0.022569075226783752,
      "learning_rate": 1.031607605925839e-06,
      "loss": 0.0006,
      "step": 3980
    },
    {
      "epoch": 165.44,
      "grad_norm": 0.02042323723435402,
      "learning_rate": 1.0296582701432823e-06,
      "loss": 0.0003,
      "step": 3981
    },
    {
      "epoch": 165.49,
      "grad_norm": 0.055073950439691544,
      "learning_rate": 1.0277105664182375e-06,
      "loss": 0.0005,
      "step": 3982
    },
    {
      "epoch": 165.53,
      "grad_norm": 0.17527510225772858,
      "learning_rate": 1.025764495551333e-06,
      "loss": 0.0008,
      "step": 3983
    },
    {
      "epoch": 165.57,
      "grad_norm": 0.041397131979465485,
      "learning_rate": 1.023820058342524e-06,
      "loss": 0.0004,
      "step": 3984
    },
    {
      "epoch": 165.61,
      "grad_norm": 0.11211132258176804,
      "learning_rate": 1.0218772555910955e-06,
      "loss": 0.001,
      "step": 3985
    },
    {
      "epoch": 165.65,
      "grad_norm": 0.12953494489192963,
      "learning_rate": 1.0199360880956605e-06,
      "loss": 0.0009,
      "step": 3986
    },
    {
      "epoch": 165.69,
      "grad_norm": 0.022318469360470772,
      "learning_rate": 1.0179965566541593e-06,
      "loss": 0.0003,
      "step": 3987
    },
    {
      "epoch": 165.74,
      "grad_norm": 0.07016045600175858,
      "learning_rate": 1.016058662063862e-06,
      "loss": 0.0006,
      "step": 3988
    },
    {
      "epoch": 165.78,
      "grad_norm": 0.04388971999287605,
      "learning_rate": 1.01412240512136e-06,
      "loss": 0.0003,
      "step": 3989
    },
    {
      "epoch": 165.82,
      "grad_norm": 0.003881517332047224,
      "learning_rate": 1.0121877866225783e-06,
      "loss": 0.0002,
      "step": 3990
    },
    {
      "epoch": 165.86,
      "grad_norm": 0.10601039975881577,
      "learning_rate": 1.0102548073627645e-06,
      "loss": 0.0011,
      "step": 3991
    },
    {
      "epoch": 165.9,
      "grad_norm": 0.03936108574271202,
      "learning_rate": 1.0083234681364934e-06,
      "loss": 0.0004,
      "step": 3992
    },
    {
      "epoch": 165.94,
      "grad_norm": 0.09254423528909683,
      "learning_rate": 1.0063937697376657e-06,
      "loss": 0.0007,
      "step": 3993
    },
    {
      "epoch": 165.98,
      "grad_norm": 0.02649366296827793,
      "learning_rate": 1.0044657129595075e-06,
      "loss": 0.0003,
      "step": 3994
    },
    {
      "epoch": 166.03,
      "grad_norm": 0.08617302030324936,
      "learning_rate": 1.0025392985945703e-06,
      "loss": 0.0006,
      "step": 3995
    },
    {
      "epoch": 166.07,
      "grad_norm": 0.06694622337818146,
      "learning_rate": 1.0006145274347306e-06,
      "loss": 0.0005,
      "step": 3996
    },
    {
      "epoch": 166.11,
      "grad_norm": 0.06463654339313507,
      "learning_rate": 9.986914002711878e-07,
      "loss": 0.0005,
      "step": 3997
    },
    {
      "epoch": 166.15,
      "grad_norm": 0.0034734634682536125,
      "learning_rate": 9.967699178944695e-07,
      "loss": 0.0002,
      "step": 3998
    },
    {
      "epoch": 166.19,
      "grad_norm": 0.03303484618663788,
      "learning_rate": 9.948500810944218e-07,
      "loss": 0.0003,
      "step": 3999
    },
    {
      "epoch": 166.23,
      "grad_norm": 0.003827764419838786,
      "learning_rate": 9.929318906602176e-07,
      "loss": 0.0002,
      "step": 4000
    },
    {
      "epoch": 166.28,
      "grad_norm": 0.08190528303384781,
      "learning_rate": 9.91015347380353e-07,
      "loss": 0.0006,
      "step": 4001
    },
    {
      "epoch": 166.32,
      "grad_norm": 0.07218709588050842,
      "learning_rate": 9.891004520426461e-07,
      "loss": 0.0006,
      "step": 4002
    },
    {
      "epoch": 166.36,
      "grad_norm": 0.039844609797000885,
      "learning_rate": 9.87187205434239e-07,
      "loss": 0.0004,
      "step": 4003
    },
    {
      "epoch": 166.4,
      "grad_norm": 0.036218542605638504,
      "learning_rate": 9.852756083415944e-07,
      "loss": 0.0003,
      "step": 4004
    },
    {
      "epoch": 166.44,
      "grad_norm": 0.036806587129831314,
      "learning_rate": 9.833656615504978e-07,
      "loss": 0.0004,
      "step": 4005
    },
    {
      "epoch": 166.48,
      "grad_norm": 0.053446512669324875,
      "learning_rate": 9.814573658460564e-07,
      "loss": 0.0005,
      "step": 4006
    },
    {
      "epoch": 166.52,
      "grad_norm": 0.10071437805891037,
      "learning_rate": 9.795507220126975e-07,
      "loss": 0.0007,
      "step": 4007
    },
    {
      "epoch": 166.57,
      "grad_norm": 0.10684312134981155,
      "learning_rate": 9.776457308341735e-07,
      "loss": 0.0009,
      "step": 4008
    },
    {
      "epoch": 166.61,
      "grad_norm": 0.07899998873472214,
      "learning_rate": 9.7574239309355e-07,
      "loss": 0.0008,
      "step": 4009
    },
    {
      "epoch": 166.65,
      "grad_norm": 0.0038762292824685574,
      "learning_rate": 9.738407095732195e-07,
      "loss": 0.0002,
      "step": 4010
    },
    {
      "epoch": 166.69,
      "grad_norm": 0.04686220735311508,
      "learning_rate": 9.719406810548914e-07,
      "loss": 0.0006,
      "step": 4011
    },
    {
      "epoch": 166.73,
      "grad_norm": 0.06027340143918991,
      "learning_rate": 9.70042308319597e-07,
      "loss": 0.0006,
      "step": 4012
    },
    {
      "epoch": 166.77,
      "grad_norm": 0.08894751965999603,
      "learning_rate": 9.68145592147684e-07,
      "loss": 0.0005,
      "step": 4013
    },
    {
      "epoch": 166.82,
      "grad_norm": 0.043357446789741516,
      "learning_rate": 9.662505333188221e-07,
      "loss": 0.0004,
      "step": 4014
    },
    {
      "epoch": 166.86,
      "grad_norm": 0.05737946182489395,
      "learning_rate": 9.643571326119982e-07,
      "loss": 0.0005,
      "step": 4015
    },
    {
      "epoch": 166.9,
      "grad_norm": 0.05558323860168457,
      "learning_rate": 9.62465390805517e-07,
      "loss": 0.0004,
      "step": 4016
    },
    {
      "epoch": 166.94,
      "grad_norm": 0.1766708642244339,
      "learning_rate": 9.605753086770031e-07,
      "loss": 0.001,
      "step": 4017
    },
    {
      "epoch": 166.98,
      "grad_norm": 0.09794827550649643,
      "learning_rate": 9.586868870033972e-07,
      "loss": 0.0008,
      "step": 4018
    },
    {
      "epoch": 167.02,
      "grad_norm": 0.04429207369685173,
      "learning_rate": 9.568001265609593e-07,
      "loss": 0.0005,
      "step": 4019
    },
    {
      "epoch": 167.06,
      "grad_norm": 0.03194603696465492,
      "learning_rate": 9.549150281252633e-07,
      "loss": 0.0003,
      "step": 4020
    },
    {
      "epoch": 167.11,
      "grad_norm": 0.05388449504971504,
      "learning_rate": 9.53031592471203e-07,
      "loss": 0.0005,
      "step": 4021
    },
    {
      "epoch": 167.15,
      "grad_norm": 0.033404361456632614,
      "learning_rate": 9.511498203729874e-07,
      "loss": 0.0003,
      "step": 4022
    },
    {
      "epoch": 167.19,
      "grad_norm": 0.03468212112784386,
      "learning_rate": 9.492697126041429e-07,
      "loss": 0.0004,
      "step": 4023
    },
    {
      "epoch": 167.23,
      "grad_norm": 0.04420047253370285,
      "learning_rate": 9.473912699375093e-07,
      "loss": 0.0004,
      "step": 4024
    },
    {
      "epoch": 167.27,
      "grad_norm": 0.06861262768507004,
      "learning_rate": 9.455144931452459e-07,
      "loss": 0.0005,
      "step": 4025
    },
    {
      "epoch": 167.31,
      "grad_norm": 0.10269005596637726,
      "learning_rate": 9.436393829988217e-07,
      "loss": 0.0008,
      "step": 4026
    },
    {
      "epoch": 167.36,
      "grad_norm": 0.056618668138980865,
      "learning_rate": 9.417659402690254e-07,
      "loss": 0.0005,
      "step": 4027
    },
    {
      "epoch": 167.4,
      "grad_norm": 0.020482826977968216,
      "learning_rate": 9.398941657259575e-07,
      "loss": 0.0003,
      "step": 4028
    },
    {
      "epoch": 167.44,
      "grad_norm": 0.050645921379327774,
      "learning_rate": 9.38024060139035e-07,
      "loss": 0.0005,
      "step": 4029
    },
    {
      "epoch": 167.48,
      "grad_norm": 0.01816132664680481,
      "learning_rate": 9.361556242769871e-07,
      "loss": 0.0003,
      "step": 4030
    },
    {
      "epoch": 167.52,
      "grad_norm": 0.1133401170372963,
      "learning_rate": 9.34288858907858e-07,
      "loss": 0.001,
      "step": 4031
    },
    {
      "epoch": 167.56,
      "grad_norm": 0.04021207243204117,
      "learning_rate": 9.324237647990026e-07,
      "loss": 0.0004,
      "step": 4032
    },
    {
      "epoch": 167.61,
      "grad_norm": 0.05134107172489166,
      "learning_rate": 9.305603427170917e-07,
      "loss": 0.0005,
      "step": 4033
    },
    {
      "epoch": 167.65,
      "grad_norm": 0.021953599527478218,
      "learning_rate": 9.286985934281079e-07,
      "loss": 0.0003,
      "step": 4034
    },
    {
      "epoch": 167.69,
      "grad_norm": 0.055184200406074524,
      "learning_rate": 9.26838517697346e-07,
      "loss": 0.0005,
      "step": 4035
    },
    {
      "epoch": 167.73,
      "grad_norm": 0.041520774364471436,
      "learning_rate": 9.249801162894123e-07,
      "loss": 0.0004,
      "step": 4036
    },
    {
      "epoch": 167.77,
      "grad_norm": 0.06938642263412476,
      "learning_rate": 9.231233899682262e-07,
      "loss": 0.0004,
      "step": 4037
    },
    {
      "epoch": 167.81,
      "grad_norm": 0.1060447096824646,
      "learning_rate": 9.212683394970173e-07,
      "loss": 0.001,
      "step": 4038
    },
    {
      "epoch": 167.85,
      "grad_norm": 0.061341580003499985,
      "learning_rate": 9.194149656383267e-07,
      "loss": 0.0007,
      "step": 4039
    },
    {
      "epoch": 167.9,
      "grad_norm": 0.03428668901324272,
      "learning_rate": 9.175632691540065e-07,
      "loss": 0.0003,
      "step": 4040
    },
    {
      "epoch": 167.94,
      "grad_norm": 0.05266396328806877,
      "learning_rate": 9.157132508052208e-07,
      "loss": 0.0004,
      "step": 4041
    },
    {
      "epoch": 167.98,
      "grad_norm": 0.0654296800494194,
      "learning_rate": 9.138649113524389e-07,
      "loss": 0.0004,
      "step": 4042
    },
    {
      "epoch": 168.02,
      "grad_norm": 0.05028291419148445,
      "learning_rate": 9.12018251555446e-07,
      "loss": 0.0004,
      "step": 4043
    },
    {
      "epoch": 168.06,
      "grad_norm": 0.07278407365083694,
      "learning_rate": 9.101732721733325e-07,
      "loss": 0.0006,
      "step": 4044
    },
    {
      "epoch": 168.1,
      "grad_norm": 0.04788242653012276,
      "learning_rate": 9.083299739645007e-07,
      "loss": 0.0004,
      "step": 4045
    },
    {
      "epoch": 168.15,
      "grad_norm": 0.0586898997426033,
      "learning_rate": 9.064883576866612e-07,
      "loss": 0.0006,
      "step": 4046
    },
    {
      "epoch": 168.19,
      "grad_norm": 0.10774116218090057,
      "learning_rate": 9.046484240968317e-07,
      "loss": 0.0008,
      "step": 4047
    },
    {
      "epoch": 168.23,
      "grad_norm": 0.03139718994498253,
      "learning_rate": 9.028101739513406e-07,
      "loss": 0.0004,
      "step": 4048
    },
    {
      "epoch": 168.27,
      "grad_norm": 0.031907569617033005,
      "learning_rate": 9.009736080058223e-07,
      "loss": 0.0003,
      "step": 4049
    },
    {
      "epoch": 168.31,
      "grad_norm": 0.25042590498924255,
      "learning_rate": 8.991387270152202e-07,
      "loss": 0.0015,
      "step": 4050
    },
    {
      "epoch": 168.35,
      "grad_norm": 0.0917837917804718,
      "learning_rate": 8.973055317337848e-07,
      "loss": 0.0006,
      "step": 4051
    },
    {
      "epoch": 168.39,
      "grad_norm": 0.06740909814834595,
      "learning_rate": 8.954740229150732e-07,
      "loss": 0.0006,
      "step": 4052
    },
    {
      "epoch": 168.44,
      "grad_norm": 0.003267846303060651,
      "learning_rate": 8.936442013119489e-07,
      "loss": 0.0002,
      "step": 4053
    },
    {
      "epoch": 168.48,
      "grad_norm": 0.055268410593271255,
      "learning_rate": 8.918160676765824e-07,
      "loss": 0.0005,
      "step": 4054
    },
    {
      "epoch": 168.52,
      "grad_norm": 0.04267216473817825,
      "learning_rate": 8.899896227604509e-07,
      "loss": 0.0004,
      "step": 4055
    },
    {
      "epoch": 168.56,
      "grad_norm": 0.0525636300444603,
      "learning_rate": 8.881648673143367e-07,
      "loss": 0.0004,
      "step": 4056
    },
    {
      "epoch": 168.6,
      "grad_norm": 0.02551715448498726,
      "learning_rate": 8.86341802088328e-07,
      "loss": 0.0003,
      "step": 4057
    },
    {
      "epoch": 168.64,
      "grad_norm": 0.03454701974987984,
      "learning_rate": 8.845204278318182e-07,
      "loss": 0.0004,
      "step": 4058
    },
    {
      "epoch": 168.69,
      "grad_norm": 0.0036081618163734674,
      "learning_rate": 8.82700745293505e-07,
      "loss": 0.0002,
      "step": 4059
    },
    {
      "epoch": 168.73,
      "grad_norm": 0.05200710892677307,
      "learning_rate": 8.808827552213917e-07,
      "loss": 0.0004,
      "step": 4060
    },
    {
      "epoch": 168.77,
      "grad_norm": 0.06130390241742134,
      "learning_rate": 8.79066458362785e-07,
      "loss": 0.0006,
      "step": 4061
    },
    {
      "epoch": 168.81,
      "grad_norm": 0.05809667333960533,
      "learning_rate": 8.772518554642973e-07,
      "loss": 0.0005,
      "step": 4062
    },
    {
      "epoch": 168.85,
      "grad_norm": 0.006380748935043812,
      "learning_rate": 8.754389472718406e-07,
      "loss": 0.0004,
      "step": 4063
    },
    {
      "epoch": 168.89,
      "grad_norm": 0.11533475667238235,
      "learning_rate": 8.736277345306343e-07,
      "loss": 0.0011,
      "step": 4064
    },
    {
      "epoch": 168.94,
      "grad_norm": 0.0036368730943650007,
      "learning_rate": 8.718182179851998e-07,
      "loss": 0.0002,
      "step": 4065
    },
    {
      "epoch": 168.98,
      "grad_norm": 0.04752381145954132,
      "learning_rate": 8.700103983793606e-07,
      "loss": 0.0004,
      "step": 4066
    },
    {
      "epoch": 169.02,
      "grad_norm": 0.04102938249707222,
      "learning_rate": 8.682042764562431e-07,
      "loss": 0.0004,
      "step": 4067
    },
    {
      "epoch": 169.06,
      "grad_norm": 0.04445416480302811,
      "learning_rate": 8.663998529582768e-07,
      "loss": 0.0003,
      "step": 4068
    },
    {
      "epoch": 169.1,
      "grad_norm": 0.11213593930006027,
      "learning_rate": 8.645971286271903e-07,
      "loss": 0.0012,
      "step": 4069
    },
    {
      "epoch": 169.14,
      "grad_norm": 0.0036332670133560896,
      "learning_rate": 8.627961042040183e-07,
      "loss": 0.0002,
      "step": 4070
    },
    {
      "epoch": 169.18,
      "grad_norm": 0.044862836599349976,
      "learning_rate": 8.609967804290903e-07,
      "loss": 0.0004,
      "step": 4071
    },
    {
      "epoch": 169.23,
      "grad_norm": 0.0329042449593544,
      "learning_rate": 8.591991580420422e-07,
      "loss": 0.0004,
      "step": 4072
    },
    {
      "epoch": 169.27,
      "grad_norm": 0.07114008814096451,
      "learning_rate": 8.574032377818087e-07,
      "loss": 0.0006,
      "step": 4073
    },
    {
      "epoch": 169.31,
      "grad_norm": 0.035816892981529236,
      "learning_rate": 8.556090203866246e-07,
      "loss": 0.0004,
      "step": 4074
    },
    {
      "epoch": 169.35,
      "grad_norm": 0.07943009585142136,
      "learning_rate": 8.538165065940263e-07,
      "loss": 0.0006,
      "step": 4075
    },
    {
      "epoch": 169.39,
      "grad_norm": 0.030102267861366272,
      "learning_rate": 8.520256971408453e-07,
      "loss": 0.0003,
      "step": 4076
    },
    {
      "epoch": 169.43,
      "grad_norm": 0.022873353213071823,
      "learning_rate": 8.502365927632178e-07,
      "loss": 0.0003,
      "step": 4077
    },
    {
      "epoch": 169.48,
      "grad_norm": 0.08983666449785233,
      "learning_rate": 8.484491941965767e-07,
      "loss": 0.0006,
      "step": 4078
    },
    {
      "epoch": 169.52,
      "grad_norm": 0.0034798490814864635,
      "learning_rate": 8.466635021756542e-07,
      "loss": 0.0002,
      "step": 4079
    },
    {
      "epoch": 169.56,
      "grad_norm": 0.11480273306369781,
      "learning_rate": 8.448795174344803e-07,
      "loss": 0.0009,
      "step": 4080
    },
    {
      "epoch": 169.6,
      "grad_norm": 0.05339766666293144,
      "learning_rate": 8.430972407063848e-07,
      "loss": 0.0004,
      "step": 4081
    },
    {
      "epoch": 169.64,
      "grad_norm": 0.04909735918045044,
      "learning_rate": 8.41316672723993e-07,
      "loss": 0.0004,
      "step": 4082
    },
    {
      "epoch": 169.68,
      "grad_norm": 0.04751968756318092,
      "learning_rate": 8.395378142192307e-07,
      "loss": 0.0005,
      "step": 4083
    },
    {
      "epoch": 169.72,
      "grad_norm": 0.11872585117816925,
      "learning_rate": 8.377606659233179e-07,
      "loss": 0.001,
      "step": 4084
    },
    {
      "epoch": 169.77,
      "grad_norm": 0.08677495270967484,
      "learning_rate": 8.359852285667752e-07,
      "loss": 0.0008,
      "step": 4085
    },
    {
      "epoch": 169.81,
      "grad_norm": 0.028245747089385986,
      "learning_rate": 8.342115028794151e-07,
      "loss": 0.0003,
      "step": 4086
    },
    {
      "epoch": 169.85,
      "grad_norm": 0.09377709776163101,
      "learning_rate": 8.324394895903509e-07,
      "loss": 0.0008,
      "step": 4087
    },
    {
      "epoch": 169.89,
      "grad_norm": 0.04361523687839508,
      "learning_rate": 8.306691894279894e-07,
      "loss": 0.0004,
      "step": 4088
    },
    {
      "epoch": 169.93,
      "grad_norm": 0.04854331165552139,
      "learning_rate": 8.289006031200352e-07,
      "loss": 0.0005,
      "step": 4089
    },
    {
      "epoch": 169.97,
      "grad_norm": 0.03400571644306183,
      "learning_rate": 8.271337313934869e-07,
      "loss": 0.0004,
      "step": 4090
    },
    {
      "epoch": 170.02,
      "grad_norm": 0.09619200229644775,
      "learning_rate": 8.253685749746388e-07,
      "loss": 0.0006,
      "step": 4091
    },
    {
      "epoch": 170.06,
      "grad_norm": 0.02414151467382908,
      "learning_rate": 8.2360513458908e-07,
      "loss": 0.0003,
      "step": 4092
    },
    {
      "epoch": 170.1,
      "grad_norm": 0.03612836077809334,
      "learning_rate": 8.218434109616935e-07,
      "loss": 0.0003,
      "step": 4093
    },
    {
      "epoch": 170.14,
      "grad_norm": 0.07656986266374588,
      "learning_rate": 8.200834048166584e-07,
      "loss": 0.0006,
      "step": 4094
    },
    {
      "epoch": 170.18,
      "grad_norm": 0.003565598512068391,
      "learning_rate": 8.183251168774476e-07,
      "loss": 0.0002,
      "step": 4095
    },
    {
      "epoch": 170.22,
      "grad_norm": 0.0034497298765927553,
      "learning_rate": 8.16568547866824e-07,
      "loss": 0.0002,
      "step": 4096
    },
    {
      "epoch": 170.26,
      "grad_norm": 0.032389987260103226,
      "learning_rate": 8.148136985068489e-07,
      "loss": 0.0003,
      "step": 4097
    },
    {
      "epoch": 170.31,
      "grad_norm": 0.09217850863933563,
      "learning_rate": 8.130605695188736e-07,
      "loss": 0.0008,
      "step": 4098
    },
    {
      "epoch": 170.35,
      "grad_norm": 0.12056349962949753,
      "learning_rate": 8.113091616235436e-07,
      "loss": 0.0009,
      "step": 4099
    },
    {
      "epoch": 170.39,
      "grad_norm": 0.10296627879142761,
      "learning_rate": 8.095594755407971e-07,
      "loss": 0.0008,
      "step": 4100
    },
    {
      "epoch": 170.43,
      "grad_norm": 0.042143892496824265,
      "learning_rate": 8.078115119898628e-07,
      "loss": 0.0005,
      "step": 4101
    },
    {
      "epoch": 170.47,
      "grad_norm": 0.0038470993749797344,
      "learning_rate": 8.060652716892637e-07,
      "loss": 0.0002,
      "step": 4102
    },
    {
      "epoch": 170.51,
      "grad_norm": 0.050439778715372086,
      "learning_rate": 8.043207553568122e-07,
      "loss": 0.0004,
      "step": 4103
    },
    {
      "epoch": 170.56,
      "grad_norm": 0.05096382647752762,
      "learning_rate": 8.025779637096138e-07,
      "loss": 0.0006,
      "step": 4104
    },
    {
      "epoch": 170.6,
      "grad_norm": 0.03653968498110771,
      "learning_rate": 8.008368974640634e-07,
      "loss": 0.0004,
      "step": 4105
    },
    {
      "epoch": 170.64,
      "grad_norm": 0.0797809585928917,
      "learning_rate": 7.990975573358501e-07,
      "loss": 0.0006,
      "step": 4106
    },
    {
      "epoch": 170.68,
      "grad_norm": 0.057930946350097656,
      "learning_rate": 7.973599440399476e-07,
      "loss": 0.0005,
      "step": 4107
    },
    {
      "epoch": 170.72,
      "grad_norm": 0.02305675856769085,
      "learning_rate": 7.956240582906244e-07,
      "loss": 0.0003,
      "step": 4108
    },
    {
      "epoch": 170.76,
      "grad_norm": 0.1108010858297348,
      "learning_rate": 7.938899008014378e-07,
      "loss": 0.0007,
      "step": 4109
    },
    {
      "epoch": 170.81,
      "grad_norm": 0.08432762324810028,
      "learning_rate": 7.921574722852343e-07,
      "loss": 0.0008,
      "step": 4110
    },
    {
      "epoch": 170.85,
      "grad_norm": 0.06170075386762619,
      "learning_rate": 7.904267734541499e-07,
      "loss": 0.0005,
      "step": 4111
    },
    {
      "epoch": 170.89,
      "grad_norm": 0.028761640191078186,
      "learning_rate": 7.886978050196093e-07,
      "loss": 0.0003,
      "step": 4112
    },
    {
      "epoch": 170.93,
      "grad_norm": 0.03838323429226875,
      "learning_rate": 7.869705676923262e-07,
      "loss": 0.0003,
      "step": 4113
    },
    {
      "epoch": 170.97,
      "grad_norm": 0.02139725722372532,
      "learning_rate": 7.852450621823027e-07,
      "loss": 0.0003,
      "step": 4114
    },
    {
      "epoch": 171.01,
      "grad_norm": 0.0919148400425911,
      "learning_rate": 7.835212891988292e-07,
      "loss": 0.0008,
      "step": 4115
    },
    {
      "epoch": 171.05,
      "grad_norm": 0.0036921375431120396,
      "learning_rate": 7.817992494504844e-07,
      "loss": 0.0002,
      "step": 4116
    },
    {
      "epoch": 171.1,
      "grad_norm": 0.1135275661945343,
      "learning_rate": 7.800789436451318e-07,
      "loss": 0.0009,
      "step": 4117
    },
    {
      "epoch": 171.14,
      "grad_norm": 0.014117189683020115,
      "learning_rate": 7.783603724899258e-07,
      "loss": 0.0002,
      "step": 4118
    },
    {
      "epoch": 171.18,
      "grad_norm": 0.07858632504940033,
      "learning_rate": 7.766435366913044e-07,
      "loss": 0.0006,
      "step": 4119
    },
    {
      "epoch": 171.22,
      "grad_norm": 0.024210261180996895,
      "learning_rate": 7.749284369549954e-07,
      "loss": 0.0003,
      "step": 4120
    },
    {
      "epoch": 171.26,
      "grad_norm": 0.0355600081384182,
      "learning_rate": 7.732150739860106e-07,
      "loss": 0.0003,
      "step": 4121
    },
    {
      "epoch": 171.3,
      "grad_norm": 0.0036088852211833,
      "learning_rate": 7.715034484886491e-07,
      "loss": 0.0002,
      "step": 4122
    },
    {
      "epoch": 171.35,
      "grad_norm": 0.0693928450345993,
      "learning_rate": 7.697935611664964e-07,
      "loss": 0.0006,
      "step": 4123
    },
    {
      "epoch": 171.39,
      "grad_norm": 0.05508949235081673,
      "learning_rate": 7.680854127224213e-07,
      "loss": 0.0005,
      "step": 4124
    },
    {
      "epoch": 171.43,
      "grad_norm": 0.07542850822210312,
      "learning_rate": 7.663790038585794e-07,
      "loss": 0.0005,
      "step": 4125
    },
    {
      "epoch": 171.47,
      "grad_norm": 0.08138500154018402,
      "learning_rate": 7.646743352764113e-07,
      "loss": 0.0005,
      "step": 4126
    },
    {
      "epoch": 171.51,
      "grad_norm": 0.06417471915483475,
      "learning_rate": 7.62971407676642e-07,
      "loss": 0.0005,
      "step": 4127
    },
    {
      "epoch": 171.55,
      "grad_norm": 0.0693909078836441,
      "learning_rate": 7.612702217592816e-07,
      "loss": 0.0005,
      "step": 4128
    },
    {
      "epoch": 171.59,
      "grad_norm": 0.03953566774725914,
      "learning_rate": 7.595707782236211e-07,
      "loss": 0.0004,
      "step": 4129
    },
    {
      "epoch": 171.64,
      "grad_norm": 0.029258854687213898,
      "learning_rate": 7.578730777682386e-07,
      "loss": 0.0003,
      "step": 4130
    },
    {
      "epoch": 171.68,
      "grad_norm": 0.01739952154457569,
      "learning_rate": 7.561771210909946e-07,
      "loss": 0.0004,
      "step": 4131
    },
    {
      "epoch": 171.72,
      "grad_norm": 0.11030816286802292,
      "learning_rate": 7.544829088890326e-07,
      "loss": 0.0009,
      "step": 4132
    },
    {
      "epoch": 171.76,
      "grad_norm": 0.06365571171045303,
      "learning_rate": 7.5279044185878e-07,
      "loss": 0.0004,
      "step": 4133
    },
    {
      "epoch": 171.8,
      "grad_norm": 0.0434078723192215,
      "learning_rate": 7.510997206959453e-07,
      "loss": 0.0004,
      "step": 4134
    },
    {
      "epoch": 171.84,
      "grad_norm": 0.08920574933290482,
      "learning_rate": 7.494107460955207e-07,
      "loss": 0.0006,
      "step": 4135
    },
    {
      "epoch": 171.89,
      "grad_norm": 0.04818645864725113,
      "learning_rate": 7.477235187517795e-07,
      "loss": 0.0005,
      "step": 4136
    },
    {
      "epoch": 171.93,
      "grad_norm": 0.05255040526390076,
      "learning_rate": 7.460380393582772e-07,
      "loss": 0.0004,
      "step": 4137
    },
    {
      "epoch": 171.97,
      "grad_norm": 0.05244899541139603,
      "learning_rate": 7.443543086078508e-07,
      "loss": 0.0005,
      "step": 4138
    },
    {
      "epoch": 172.01,
      "grad_norm": 0.08355491608381271,
      "learning_rate": 7.426723271926195e-07,
      "loss": 0.0009,
      "step": 4139
    },
    {
      "epoch": 172.05,
      "grad_norm": 0.0036363278049975634,
      "learning_rate": 7.409920958039795e-07,
      "loss": 0.0002,
      "step": 4140
    },
    {
      "epoch": 172.09,
      "grad_norm": 0.07858302444219589,
      "learning_rate": 7.39313615132613e-07,
      "loss": 0.0007,
      "step": 4141
    },
    {
      "epoch": 172.14,
      "grad_norm": 0.01882113516330719,
      "learning_rate": 7.376368858684785e-07,
      "loss": 0.0003,
      "step": 4142
    },
    {
      "epoch": 172.18,
      "grad_norm": 0.0035274182446300983,
      "learning_rate": 7.359619087008169e-07,
      "loss": 0.0002,
      "step": 4143
    },
    {
      "epoch": 172.22,
      "grad_norm": 0.049218568950891495,
      "learning_rate": 7.342886843181479e-07,
      "loss": 0.0004,
      "step": 4144
    },
    {
      "epoch": 172.26,
      "grad_norm": 0.027993539348244667,
      "learning_rate": 7.326172134082704e-07,
      "loss": 0.0003,
      "step": 4145
    },
    {
      "epoch": 172.3,
      "grad_norm": 0.0511021614074707,
      "learning_rate": 7.309474966582636e-07,
      "loss": 0.0004,
      "step": 4146
    },
    {
      "epoch": 172.34,
      "grad_norm": 0.055101748555898666,
      "learning_rate": 7.292795347544851e-07,
      "loss": 0.0005,
      "step": 4147
    },
    {
      "epoch": 172.38,
      "grad_norm": 0.1542111486196518,
      "learning_rate": 7.276133283825698e-07,
      "loss": 0.0011,
      "step": 4148
    },
    {
      "epoch": 172.43,
      "grad_norm": 0.03967108950018883,
      "learning_rate": 7.259488782274349e-07,
      "loss": 0.0004,
      "step": 4149
    },
    {
      "epoch": 172.47,
      "grad_norm": 0.03520706295967102,
      "learning_rate": 7.242861849732696e-07,
      "loss": 0.0004,
      "step": 4150
    },
    {
      "epoch": 172.51,
      "grad_norm": 0.04048540070652962,
      "learning_rate": 7.226252493035463e-07,
      "loss": 0.0004,
      "step": 4151
    },
    {
      "epoch": 172.55,
      "grad_norm": 0.0729769691824913,
      "learning_rate": 7.209660719010119e-07,
      "loss": 0.0006,
      "step": 4152
    },
    {
      "epoch": 172.59,
      "grad_norm": 0.02276795171201229,
      "learning_rate": 7.193086534476923e-07,
      "loss": 0.0003,
      "step": 4153
    },
    {
      "epoch": 172.63,
      "grad_norm": 0.13010674715042114,
      "learning_rate": 7.176529946248894e-07,
      "loss": 0.0013,
      "step": 4154
    },
    {
      "epoch": 172.68,
      "grad_norm": 0.10354293882846832,
      "learning_rate": 7.159990961131818e-07,
      "loss": 0.001,
      "step": 4155
    },
    {
      "epoch": 172.72,
      "grad_norm": 0.08793879300355911,
      "learning_rate": 7.143469585924251e-07,
      "loss": 0.0008,
      "step": 4156
    },
    {
      "epoch": 172.76,
      "grad_norm": 0.003759097307920456,
      "learning_rate": 7.126965827417509e-07,
      "loss": 0.0002,
      "step": 4157
    },
    {
      "epoch": 172.8,
      "grad_norm": 0.0758214145898819,
      "learning_rate": 7.110479692395656e-07,
      "loss": 0.0005,
      "step": 4158
    },
    {
      "epoch": 172.84,
      "grad_norm": 0.03540278226137161,
      "learning_rate": 7.09401118763553e-07,
      "loss": 0.0004,
      "step": 4159
    },
    {
      "epoch": 172.88,
      "grad_norm": 0.03714515641331673,
      "learning_rate": 7.077560319906696e-07,
      "loss": 0.0003,
      "step": 4160
    },
    {
      "epoch": 172.92,
      "grad_norm": 0.055035289376974106,
      "learning_rate": 7.061127095971504e-07,
      "loss": 0.0006,
      "step": 4161
    },
    {
      "epoch": 172.97,
      "grad_norm": 0.02681593783199787,
      "learning_rate": 7.044711522585007e-07,
      "loss": 0.0003,
      "step": 4162
    },
    {
      "epoch": 173.01,
      "grad_norm": 0.0690910592675209,
      "learning_rate": 7.02831360649504e-07,
      "loss": 0.0006,
      "step": 4163
    },
    {
      "epoch": 173.05,
      "grad_norm": 0.12423741817474365,
      "learning_rate": 7.011933354442168e-07,
      "loss": 0.0007,
      "step": 4164
    },
    {
      "epoch": 173.09,
      "grad_norm": 0.12998493015766144,
      "learning_rate": 6.995570773159693e-07,
      "loss": 0.0006,
      "step": 4165
    },
    {
      "epoch": 173.13,
      "grad_norm": 0.027554281055927277,
      "learning_rate": 6.979225869373657e-07,
      "loss": 0.0003,
      "step": 4166
    },
    {
      "epoch": 173.17,
      "grad_norm": 0.0035897635389119387,
      "learning_rate": 6.962898649802824e-07,
      "loss": 0.0002,
      "step": 4167
    },
    {
      "epoch": 173.22,
      "grad_norm": 0.02762635424733162,
      "learning_rate": 6.946589121158703e-07,
      "loss": 0.0003,
      "step": 4168
    },
    {
      "epoch": 173.26,
      "grad_norm": 0.0550624281167984,
      "learning_rate": 6.930297290145527e-07,
      "loss": 0.0004,
      "step": 4169
    },
    {
      "epoch": 173.3,
      "grad_norm": 0.07162906229496002,
      "learning_rate": 6.914023163460248e-07,
      "loss": 0.0007,
      "step": 4170
    },
    {
      "epoch": 173.34,
      "grad_norm": 0.043218567967414856,
      "learning_rate": 6.89776674779255e-07,
      "loss": 0.0004,
      "step": 4171
    },
    {
      "epoch": 173.38,
      "grad_norm": 0.046461254358291626,
      "learning_rate": 6.881528049824837e-07,
      "loss": 0.0005,
      "step": 4172
    },
    {
      "epoch": 173.42,
      "grad_norm": 0.07166063040494919,
      "learning_rate": 6.865307076232203e-07,
      "loss": 0.0005,
      "step": 4173
    },
    {
      "epoch": 173.46,
      "grad_norm": 0.053722165524959564,
      "learning_rate": 6.849103833682491e-07,
      "loss": 0.0005,
      "step": 4174
    },
    {
      "epoch": 173.51,
      "grad_norm": 0.02966087870299816,
      "learning_rate": 6.832918328836247e-07,
      "loss": 0.0003,
      "step": 4175
    },
    {
      "epoch": 173.55,
      "grad_norm": 0.023093605414032936,
      "learning_rate": 6.816750568346708e-07,
      "loss": 0.0003,
      "step": 4176
    },
    {
      "epoch": 173.59,
      "grad_norm": 0.05792316794395447,
      "learning_rate": 6.800600558859838e-07,
      "loss": 0.0005,
      "step": 4177
    },
    {
      "epoch": 173.63,
      "grad_norm": 0.04973579943180084,
      "learning_rate": 6.784468307014297e-07,
      "loss": 0.0007,
      "step": 4178
    },
    {
      "epoch": 173.67,
      "grad_norm": 0.0774339959025383,
      "learning_rate": 6.768353819441436e-07,
      "loss": 0.001,
      "step": 4179
    },
    {
      "epoch": 173.71,
      "grad_norm": 0.17368797957897186,
      "learning_rate": 6.752257102765325e-07,
      "loss": 0.001,
      "step": 4180
    },
    {
      "epoch": 173.76,
      "grad_norm": 0.025002669543027878,
      "learning_rate": 6.736178163602703e-07,
      "loss": 0.0003,
      "step": 4181
    },
    {
      "epoch": 173.8,
      "grad_norm": 0.0037285657599568367,
      "learning_rate": 6.720117008563032e-07,
      "loss": 0.0002,
      "step": 4182
    },
    {
      "epoch": 173.84,
      "grad_norm": 0.09426668286323547,
      "learning_rate": 6.704073644248427e-07,
      "loss": 0.0006,
      "step": 4183
    },
    {
      "epoch": 173.88,
      "grad_norm": 0.05959710851311684,
      "learning_rate": 6.688048077253712e-07,
      "loss": 0.0006,
      "step": 4184
    },
    {
      "epoch": 173.92,
      "grad_norm": 0.003413091879338026,
      "learning_rate": 6.6720403141664e-07,
      "loss": 0.0002,
      "step": 4185
    },
    {
      "epoch": 173.96,
      "grad_norm": 0.04691416397690773,
      "learning_rate": 6.656050361566679e-07,
      "loss": 0.0005,
      "step": 4186
    },
    {
      "epoch": 174.01,
      "grad_norm": 0.04025602713227272,
      "learning_rate": 6.640078226027402e-07,
      "loss": 0.0007,
      "step": 4187
    },
    {
      "epoch": 174.05,
      "grad_norm": 0.003555688541382551,
      "learning_rate": 6.624123914114122e-07,
      "loss": 0.0002,
      "step": 4188
    },
    {
      "epoch": 174.09,
      "grad_norm": 0.0418289452791214,
      "learning_rate": 6.608187432385055e-07,
      "loss": 0.0004,
      "step": 4189
    },
    {
      "epoch": 174.13,
      "grad_norm": 0.04071364179253578,
      "learning_rate": 6.592268787391077e-07,
      "loss": 0.0004,
      "step": 4190
    },
    {
      "epoch": 174.17,
      "grad_norm": 0.12834486365318298,
      "learning_rate": 6.576367985675752e-07,
      "loss": 0.0011,
      "step": 4191
    },
    {
      "epoch": 174.21,
      "grad_norm": 0.06422906368970871,
      "learning_rate": 6.560485033775299e-07,
      "loss": 0.0006,
      "step": 4192
    },
    {
      "epoch": 174.25,
      "grad_norm": 0.0034566051326692104,
      "learning_rate": 6.544619938218588e-07,
      "loss": 0.0002,
      "step": 4193
    },
    {
      "epoch": 174.3,
      "grad_norm": 0.05089446157217026,
      "learning_rate": 6.528772705527165e-07,
      "loss": 0.0005,
      "step": 4194
    },
    {
      "epoch": 174.34,
      "grad_norm": 0.058463968336582184,
      "learning_rate": 6.512943342215234e-07,
      "loss": 0.0005,
      "step": 4195
    },
    {
      "epoch": 174.38,
      "grad_norm": 0.04630124941468239,
      "learning_rate": 6.49713185478964e-07,
      "loss": 0.0005,
      "step": 4196
    },
    {
      "epoch": 174.42,
      "grad_norm": 0.06585822254419327,
      "learning_rate": 6.481338249749896e-07,
      "loss": 0.0006,
      "step": 4197
    },
    {
      "epoch": 174.46,
      "grad_norm": 0.03853067383170128,
      "learning_rate": 6.46556253358816e-07,
      "loss": 0.0003,
      "step": 4198
    },
    {
      "epoch": 174.5,
      "grad_norm": 0.037866998463869095,
      "learning_rate": 6.449804712789221e-07,
      "loss": 0.0004,
      "step": 4199
    },
    {
      "epoch": 174.55,
      "grad_norm": 0.003575398586690426,
      "learning_rate": 6.43406479383053e-07,
      "loss": 0.0002,
      "step": 4200
    },
    {
      "epoch": 174.59,
      "grad_norm": 0.04086124151945114,
      "learning_rate": 6.418342783182174e-07,
      "loss": 0.0004,
      "step": 4201
    },
    {
      "epoch": 174.63,
      "grad_norm": 0.09764503687620163,
      "learning_rate": 6.402638687306872e-07,
      "loss": 0.0006,
      "step": 4202
    },
    {
      "epoch": 174.67,
      "grad_norm": 0.036008071154356,
      "learning_rate": 6.386952512660005e-07,
      "loss": 0.0004,
      "step": 4203
    },
    {
      "epoch": 174.71,
      "grad_norm": 0.036320075392723083,
      "learning_rate": 6.371284265689543e-07,
      "loss": 0.0004,
      "step": 4204
    },
    {
      "epoch": 174.75,
      "grad_norm": 0.05829505994915962,
      "learning_rate": 6.355633952836115e-07,
      "loss": 0.0006,
      "step": 4205
    },
    {
      "epoch": 174.79,
      "grad_norm": 0.14727139472961426,
      "learning_rate": 6.340001580532979e-07,
      "loss": 0.0008,
      "step": 4206
    },
    {
      "epoch": 174.84,
      "grad_norm": 0.04872272536158562,
      "learning_rate": 6.324387155206019e-07,
      "loss": 0.0004,
      "step": 4207
    },
    {
      "epoch": 174.88,
      "grad_norm": 0.043843649327754974,
      "learning_rate": 6.308790683273719e-07,
      "loss": 0.0003,
      "step": 4208
    },
    {
      "epoch": 174.92,
      "grad_norm": 0.06986267864704132,
      "learning_rate": 6.293212171147206e-07,
      "loss": 0.0007,
      "step": 4209
    },
    {
      "epoch": 174.96,
      "grad_norm": 0.03900320827960968,
      "learning_rate": 6.277651625230219e-07,
      "loss": 0.0003,
      "step": 4210
    },
    {
      "epoch": 175.0,
      "grad_norm": 0.0034239969681948423,
      "learning_rate": 6.262109051919113e-07,
      "loss": 0.0002,
      "step": 4211
    },
    {
      "epoch": 175.04,
      "grad_norm": 0.07620516419410706,
      "learning_rate": 6.24658445760285e-07,
      "loss": 0.0008,
      "step": 4212
    },
    {
      "epoch": 175.09,
      "grad_norm": 0.0034637567587196827,
      "learning_rate": 6.231077848663008e-07,
      "loss": 0.0002,
      "step": 4213
    },
    {
      "epoch": 175.13,
      "grad_norm": 0.03160463646054268,
      "learning_rate": 6.215589231473762e-07,
      "loss": 0.0003,
      "step": 4214
    },
    {
      "epoch": 175.17,
      "grad_norm": 0.0397832989692688,
      "learning_rate": 6.200118612401918e-07,
      "loss": 0.0004,
      "step": 4215
    },
    {
      "epoch": 175.21,
      "grad_norm": 0.035174962133169174,
      "learning_rate": 6.184665997806832e-07,
      "loss": 0.0003,
      "step": 4216
    },
    {
      "epoch": 175.25,
      "grad_norm": 0.08736594021320343,
      "learning_rate": 6.169231394040503e-07,
      "loss": 0.0006,
      "step": 4217
    },
    {
      "epoch": 175.29,
      "grad_norm": 0.06778044998645782,
      "learning_rate": 6.153814807447517e-07,
      "loss": 0.0005,
      "step": 4218
    },
    {
      "epoch": 175.34,
      "grad_norm": 0.03169640526175499,
      "learning_rate": 6.138416244365047e-07,
      "loss": 0.0004,
      "step": 4219
    },
    {
      "epoch": 175.38,
      "grad_norm": 0.05372963845729828,
      "learning_rate": 6.12303571112286e-07,
      "loss": 0.0004,
      "step": 4220
    },
    {
      "epoch": 175.42,
      "grad_norm": 0.0034864889457821846,
      "learning_rate": 6.107673214043319e-07,
      "loss": 0.0002,
      "step": 4221
    },
    {
      "epoch": 175.46,
      "grad_norm": 0.04102037101984024,
      "learning_rate": 6.092328759441357e-07,
      "loss": 0.0005,
      "step": 4222
    },
    {
      "epoch": 175.5,
      "grad_norm": 0.04798632860183716,
      "learning_rate": 6.077002353624506e-07,
      "loss": 0.0005,
      "step": 4223
    },
    {
      "epoch": 175.54,
      "grad_norm": 0.1007457971572876,
      "learning_rate": 6.06169400289287e-07,
      "loss": 0.0007,
      "step": 4224
    },
    {
      "epoch": 175.58,
      "grad_norm": 0.07260915637016296,
      "learning_rate": 6.04640371353914e-07,
      "loss": 0.0005,
      "step": 4225
    },
    {
      "epoch": 175.63,
      "grad_norm": 0.13267791271209717,
      "learning_rate": 6.031131491848574e-07,
      "loss": 0.0011,
      "step": 4226
    },
    {
      "epoch": 175.67,
      "grad_norm": 0.10053817182779312,
      "learning_rate": 6.015877344098997e-07,
      "loss": 0.0007,
      "step": 4227
    },
    {
      "epoch": 175.71,
      "grad_norm": 0.04287368059158325,
      "learning_rate": 6.000641276560814e-07,
      "loss": 0.0006,
      "step": 4228
    },
    {
      "epoch": 175.75,
      "grad_norm": 0.0033965923357754946,
      "learning_rate": 5.985423295497005e-07,
      "loss": 0.0002,
      "step": 4229
    },
    {
      "epoch": 175.79,
      "grad_norm": 0.05631495639681816,
      "learning_rate": 5.9702234071631e-07,
      "loss": 0.0005,
      "step": 4230
    },
    {
      "epoch": 175.83,
      "grad_norm": 0.0035771015100181103,
      "learning_rate": 5.955041617807205e-07,
      "loss": 0.0002,
      "step": 4231
    },
    {
      "epoch": 175.88,
      "grad_norm": 0.050456322729587555,
      "learning_rate": 5.93987793366998e-07,
      "loss": 0.0004,
      "step": 4232
    },
    {
      "epoch": 175.92,
      "grad_norm": 0.06607525050640106,
      "learning_rate": 5.924732360984636e-07,
      "loss": 0.0007,
      "step": 4233
    },
    {
      "epoch": 175.96,
      "grad_norm": 0.072003573179245,
      "learning_rate": 5.909604905976951e-07,
      "loss": 0.0007,
      "step": 4234
    },
    {
      "epoch": 176.0,
      "grad_norm": 0.050097208470106125,
      "learning_rate": 5.89449557486525e-07,
      "loss": 0.0005,
      "step": 4235
    },
    {
      "epoch": 176.04,
      "grad_norm": 0.0650089904665947,
      "learning_rate": 5.879404373860415e-07,
      "loss": 0.0005,
      "step": 4236
    },
    {
      "epoch": 176.08,
      "grad_norm": 0.05890422314405441,
      "learning_rate": 5.86433130916585e-07,
      "loss": 0.0004,
      "step": 4237
    },
    {
      "epoch": 176.12,
      "grad_norm": 0.1623544991016388,
      "learning_rate": 5.849276386977538e-07,
      "loss": 0.0012,
      "step": 4238
    },
    {
      "epoch": 176.17,
      "grad_norm": 0.06324565410614014,
      "learning_rate": 5.834239613483983e-07,
      "loss": 0.0005,
      "step": 4239
    },
    {
      "epoch": 176.21,
      "grad_norm": 0.03656116500496864,
      "learning_rate": 5.819220994866237e-07,
      "loss": 0.0004,
      "step": 4240
    },
    {
      "epoch": 176.25,
      "grad_norm": 0.06655164808034897,
      "learning_rate": 5.804220537297883e-07,
      "loss": 0.0006,
      "step": 4241
    },
    {
      "epoch": 176.29,
      "grad_norm": 0.054659731686115265,
      "learning_rate": 5.789238246945051e-07,
      "loss": 0.0005,
      "step": 4242
    },
    {
      "epoch": 176.33,
      "grad_norm": 0.04378627985715866,
      "learning_rate": 5.774274129966384e-07,
      "loss": 0.0005,
      "step": 4243
    },
    {
      "epoch": 176.37,
      "grad_norm": 0.02057725004851818,
      "learning_rate": 5.759328192513075e-07,
      "loss": 0.0003,
      "step": 4244
    },
    {
      "epoch": 176.42,
      "grad_norm": 0.0702052041888237,
      "learning_rate": 5.744400440728826e-07,
      "loss": 0.0005,
      "step": 4245
    },
    {
      "epoch": 176.46,
      "grad_norm": 0.06714509427547455,
      "learning_rate": 5.729490880749888e-07,
      "loss": 0.0006,
      "step": 4246
    },
    {
      "epoch": 176.5,
      "grad_norm": 0.05567454919219017,
      "learning_rate": 5.714599518704994e-07,
      "loss": 0.0005,
      "step": 4247
    },
    {
      "epoch": 176.54,
      "grad_norm": 0.06291227787733078,
      "learning_rate": 5.699726360715435e-07,
      "loss": 0.0005,
      "step": 4248
    },
    {
      "epoch": 176.58,
      "grad_norm": 0.055668119341135025,
      "learning_rate": 5.684871412894999e-07,
      "loss": 0.0005,
      "step": 4249
    },
    {
      "epoch": 176.62,
      "grad_norm": 0.02663263864815235,
      "learning_rate": 5.670034681349995e-07,
      "loss": 0.0003,
      "step": 4250
    },
    {
      "epoch": 176.66,
      "grad_norm": 0.0600491538643837,
      "learning_rate": 5.655216172179245e-07,
      "loss": 0.0006,
      "step": 4251
    },
    {
      "epoch": 176.71,
      "grad_norm": 0.04853866249322891,
      "learning_rate": 5.640415891474094e-07,
      "loss": 0.0004,
      "step": 4252
    },
    {
      "epoch": 176.75,
      "grad_norm": 0.00367122795432806,
      "learning_rate": 5.625633845318346e-07,
      "loss": 0.0002,
      "step": 4253
    },
    {
      "epoch": 176.79,
      "grad_norm": 0.04089561849832535,
      "learning_rate": 5.610870039788357e-07,
      "loss": 0.0004,
      "step": 4254
    },
    {
      "epoch": 176.83,
      "grad_norm": 0.0035881465300917625,
      "learning_rate": 5.596124480952975e-07,
      "loss": 0.0002,
      "step": 4255
    },
    {
      "epoch": 176.87,
      "grad_norm": 0.05786658450961113,
      "learning_rate": 5.581397174873532e-07,
      "loss": 0.0004,
      "step": 4256
    },
    {
      "epoch": 176.91,
      "grad_norm": 0.08495038747787476,
      "learning_rate": 5.566688127603876e-07,
      "loss": 0.0007,
      "step": 4257
    },
    {
      "epoch": 176.96,
      "grad_norm": 0.036493491381406784,
      "learning_rate": 5.55199734519034e-07,
      "loss": 0.0003,
      "step": 4258
    },
    {
      "epoch": 177.0,
      "grad_norm": 0.06910626590251923,
      "learning_rate": 5.537324833671753e-07,
      "loss": 0.0007,
      "step": 4259
    },
    {
      "epoch": 177.04,
      "grad_norm": 0.06840959191322327,
      "learning_rate": 5.522670599079416e-07,
      "loss": 0.0007,
      "step": 4260
    },
    {
      "epoch": 177.08,
      "grad_norm": 0.047871921211481094,
      "learning_rate": 5.508034647437144e-07,
      "loss": 0.0005,
      "step": 4261
    },
    {
      "epoch": 177.12,
      "grad_norm": 0.04826747998595238,
      "learning_rate": 5.493416984761218e-07,
      "loss": 0.0004,
      "step": 4262
    },
    {
      "epoch": 177.16,
      "grad_norm": 0.08415322750806808,
      "learning_rate": 5.478817617060406e-07,
      "loss": 0.0008,
      "step": 4263
    },
    {
      "epoch": 177.21,
      "grad_norm": 0.003403408918529749,
      "learning_rate": 5.464236550335961e-07,
      "loss": 0.0002,
      "step": 4264
    },
    {
      "epoch": 177.25,
      "grad_norm": 0.06326594948768616,
      "learning_rate": 5.449673790581611e-07,
      "loss": 0.0005,
      "step": 4265
    },
    {
      "epoch": 177.29,
      "grad_norm": 0.03065609000623226,
      "learning_rate": 5.435129343783547e-07,
      "loss": 0.0003,
      "step": 4266
    },
    {
      "epoch": 177.33,
      "grad_norm": 0.05428009107708931,
      "learning_rate": 5.42060321592045e-07,
      "loss": 0.0004,
      "step": 4267
    },
    {
      "epoch": 177.37,
      "grad_norm": 0.05344760790467262,
      "learning_rate": 5.406095412963464e-07,
      "loss": 0.0005,
      "step": 4268
    },
    {
      "epoch": 177.41,
      "grad_norm": 0.038745585829019547,
      "learning_rate": 5.391605940876199e-07,
      "loss": 0.0004,
      "step": 4269
    },
    {
      "epoch": 177.45,
      "grad_norm": 0.05517459288239479,
      "learning_rate": 5.377134805614714e-07,
      "loss": 0.0004,
      "step": 4270
    },
    {
      "epoch": 177.5,
      "grad_norm": 0.04421629011631012,
      "learning_rate": 5.362682013127562e-07,
      "loss": 0.0003,
      "step": 4271
    },
    {
      "epoch": 177.54,
      "grad_norm": 0.06858768314123154,
      "learning_rate": 5.348247569355736e-07,
      "loss": 0.0006,
      "step": 4272
    },
    {
      "epoch": 177.58,
      "grad_norm": 0.09051399677991867,
      "learning_rate": 5.333831480232687e-07,
      "loss": 0.0006,
      "step": 4273
    },
    {
      "epoch": 177.62,
      "grad_norm": 0.02501581236720085,
      "learning_rate": 5.319433751684328e-07,
      "loss": 0.0003,
      "step": 4274
    },
    {
      "epoch": 177.66,
      "grad_norm": 0.04497339949011803,
      "learning_rate": 5.305054389629022e-07,
      "loss": 0.0004,
      "step": 4275
    },
    {
      "epoch": 177.7,
      "grad_norm": 0.07100312411785126,
      "learning_rate": 5.290693399977581e-07,
      "loss": 0.0005,
      "step": 4276
    },
    {
      "epoch": 177.75,
      "grad_norm": 0.025754155591130257,
      "learning_rate": 5.276350788633267e-07,
      "loss": 0.0003,
      "step": 4277
    },
    {
      "epoch": 177.79,
      "grad_norm": 0.06221631541848183,
      "learning_rate": 5.262026561491779e-07,
      "loss": 0.0005,
      "step": 4278
    },
    {
      "epoch": 177.83,
      "grad_norm": 0.05481186881661415,
      "learning_rate": 5.247720724441285e-07,
      "loss": 0.0006,
      "step": 4279
    },
    {
      "epoch": 177.87,
      "grad_norm": 0.0035741962492465973,
      "learning_rate": 5.233433283362349e-07,
      "loss": 0.0002,
      "step": 4280
    },
    {
      "epoch": 177.91,
      "grad_norm": 0.0667453482747078,
      "learning_rate": 5.219164244128006e-07,
      "loss": 0.0007,
      "step": 4281
    },
    {
      "epoch": 177.95,
      "grad_norm": 0.052494630217552185,
      "learning_rate": 5.204913612603724e-07,
      "loss": 0.0005,
      "step": 4282
    },
    {
      "epoch": 177.99,
      "grad_norm": 0.039853889495134354,
      "learning_rate": 5.190681394647401e-07,
      "loss": 0.0003,
      "step": 4283
    },
    {
      "epoch": 178.04,
      "grad_norm": 0.060269806534051895,
      "learning_rate": 5.176467596109358e-07,
      "loss": 0.0005,
      "step": 4284
    },
    {
      "epoch": 178.08,
      "grad_norm": 0.03806144744157791,
      "learning_rate": 5.162272222832349e-07,
      "loss": 0.0003,
      "step": 4285
    },
    {
      "epoch": 178.12,
      "grad_norm": 0.05321771651506424,
      "learning_rate": 5.148095280651566e-07,
      "loss": 0.001,
      "step": 4286
    },
    {
      "epoch": 178.16,
      "grad_norm": 0.0678272396326065,
      "learning_rate": 5.133936775394604e-07,
      "loss": 0.0005,
      "step": 4287
    },
    {
      "epoch": 178.2,
      "grad_norm": 0.057384978979825974,
      "learning_rate": 5.119796712881498e-07,
      "loss": 0.0004,
      "step": 4288
    },
    {
      "epoch": 178.24,
      "grad_norm": 0.11161001026630402,
      "learning_rate": 5.10567509892469e-07,
      "loss": 0.0007,
      "step": 4289
    },
    {
      "epoch": 178.29,
      "grad_norm": 0.07313038408756256,
      "learning_rate": 5.091571939329049e-07,
      "loss": 0.0007,
      "step": 4290
    },
    {
      "epoch": 178.33,
      "grad_norm": 0.07610154896974564,
      "learning_rate": 5.077487239891838e-07,
      "loss": 0.0006,
      "step": 4291
    },
    {
      "epoch": 178.37,
      "grad_norm": 0.0216195248067379,
      "learning_rate": 5.063421006402747e-07,
      "loss": 0.0003,
      "step": 4292
    },
    {
      "epoch": 178.41,
      "grad_norm": 0.024534232914447784,
      "learning_rate": 5.049373244643879e-07,
      "loss": 0.0003,
      "step": 4293
    },
    {
      "epoch": 178.45,
      "grad_norm": 0.0035688402131199837,
      "learning_rate": 5.035343960389738e-07,
      "loss": 0.0002,
      "step": 4294
    },
    {
      "epoch": 178.49,
      "grad_norm": 0.13498134911060333,
      "learning_rate": 5.021333159407232e-07,
      "loss": 0.0011,
      "step": 4295
    },
    {
      "epoch": 178.54,
      "grad_norm": 0.04991241917014122,
      "learning_rate": 5.007340847455667e-07,
      "loss": 0.0004,
      "step": 4296
    },
    {
      "epoch": 178.58,
      "grad_norm": 0.045963432639837265,
      "learning_rate": 4.993367030286772e-07,
      "loss": 0.0007,
      "step": 4297
    },
    {
      "epoch": 178.62,
      "grad_norm": 0.10961826890707016,
      "learning_rate": 4.979411713644627e-07,
      "loss": 0.0007,
      "step": 4298
    },
    {
      "epoch": 178.66,
      "grad_norm": 0.030619416385889053,
      "learning_rate": 4.965474903265755e-07,
      "loss": 0.0003,
      "step": 4299
    },
    {
      "epoch": 178.7,
      "grad_norm": 0.0431707538664341,
      "learning_rate": 4.951556604879049e-07,
      "loss": 0.0005,
      "step": 4300
    },
    {
      "epoch": 178.74,
      "grad_norm": 0.03132045269012451,
      "learning_rate": 4.937656824205789e-07,
      "loss": 0.0003,
      "step": 4301
    },
    {
      "epoch": 178.78,
      "grad_norm": 0.04100581258535385,
      "learning_rate": 4.923775566959666e-07,
      "loss": 0.0004,
      "step": 4302
    },
    {
      "epoch": 178.83,
      "grad_norm": 0.00370790041051805,
      "learning_rate": 4.90991283884672e-07,
      "loss": 0.0002,
      "step": 4303
    },
    {
      "epoch": 178.87,
      "grad_norm": 0.02897707000374794,
      "learning_rate": 4.896068645565405e-07,
      "loss": 0.0004,
      "step": 4304
    },
    {
      "epoch": 178.91,
      "grad_norm": 0.05056876316666603,
      "learning_rate": 4.882242992806546e-07,
      "loss": 0.0004,
      "step": 4305
    },
    {
      "epoch": 178.95,
      "grad_norm": 0.09620048105716705,
      "learning_rate": 4.868435886253348e-07,
      "loss": 0.0007,
      "step": 4306
    },
    {
      "epoch": 178.99,
      "grad_norm": 0.11846112459897995,
      "learning_rate": 4.854647331581385e-07,
      "loss": 0.001,
      "step": 4307
    },
    {
      "epoch": 179.03,
      "grad_norm": 0.028710266575217247,
      "learning_rate": 4.840877334458615e-07,
      "loss": 0.0003,
      "step": 4308
    },
    {
      "epoch": 179.08,
      "grad_norm": 0.003552165348082781,
      "learning_rate": 4.827125900545365e-07,
      "loss": 0.0002,
      "step": 4309
    },
    {
      "epoch": 179.12,
      "grad_norm": 0.09981991350650787,
      "learning_rate": 4.813393035494329e-07,
      "loss": 0.0009,
      "step": 4310
    },
    {
      "epoch": 179.16,
      "grad_norm": 0.09891799837350845,
      "learning_rate": 4.79967874495057e-07,
      "loss": 0.0007,
      "step": 4311
    },
    {
      "epoch": 179.2,
      "grad_norm": 0.12294452637434006,
      "learning_rate": 4.785983034551523e-07,
      "loss": 0.001,
      "step": 4312
    },
    {
      "epoch": 179.24,
      "grad_norm": 0.003457495244219899,
      "learning_rate": 4.772305909926956e-07,
      "loss": 0.0002,
      "step": 4313
    },
    {
      "epoch": 179.28,
      "grad_norm": 0.06428749114274979,
      "learning_rate": 4.758647376699033e-07,
      "loss": 0.0006,
      "step": 4314
    },
    {
      "epoch": 179.32,
      "grad_norm": 0.003312092972919345,
      "learning_rate": 4.745007440482252e-07,
      "loss": 0.0002,
      "step": 4315
    },
    {
      "epoch": 179.37,
      "grad_norm": 0.027035800740122795,
      "learning_rate": 4.731386106883484e-07,
      "loss": 0.0003,
      "step": 4316
    },
    {
      "epoch": 179.41,
      "grad_norm": 0.08373415470123291,
      "learning_rate": 4.7177833815019447e-07,
      "loss": 0.0008,
      "step": 4317
    },
    {
      "epoch": 179.45,
      "grad_norm": 0.06138353422284126,
      "learning_rate": 4.704199269929199e-07,
      "loss": 0.0005,
      "step": 4318
    },
    {
      "epoch": 179.49,
      "grad_norm": 0.0433669239282608,
      "learning_rate": 4.6906337777491594e-07,
      "loss": 0.0004,
      "step": 4319
    },
    {
      "epoch": 179.53,
      "grad_norm": 0.08599013835191727,
      "learning_rate": 4.677086910538092e-07,
      "loss": 0.0005,
      "step": 4320
    },
    {
      "epoch": 179.57,
      "grad_norm": 0.04759877920150757,
      "learning_rate": 4.663558673864599e-07,
      "loss": 0.0004,
      "step": 4321
    },
    {
      "epoch": 179.62,
      "grad_norm": 0.04095076769590378,
      "learning_rate": 4.650049073289625e-07,
      "loss": 0.0005,
      "step": 4322
    },
    {
      "epoch": 179.66,
      "grad_norm": 0.10456499457359314,
      "learning_rate": 4.636558114366468e-07,
      "loss": 0.0009,
      "step": 4323
    },
    {
      "epoch": 179.7,
      "grad_norm": 0.03167801722884178,
      "learning_rate": 4.6230858026407364e-07,
      "loss": 0.0005,
      "step": 4324
    },
    {
      "epoch": 179.74,
      "grad_norm": 0.05000196769833565,
      "learning_rate": 4.6096321436504e-07,
      "loss": 0.0004,
      "step": 4325
    },
    {
      "epoch": 179.78,
      "grad_norm": 0.05557143688201904,
      "learning_rate": 4.59619714292574e-07,
      "loss": 0.0004,
      "step": 4326
    },
    {
      "epoch": 179.82,
      "grad_norm": 0.00352013879455626,
      "learning_rate": 4.582780805989384e-07,
      "loss": 0.0002,
      "step": 4327
    },
    {
      "epoch": 179.86,
      "grad_norm": 0.07450126856565475,
      "learning_rate": 4.569383138356276e-07,
      "loss": 0.0006,
      "step": 4328
    },
    {
      "epoch": 179.91,
      "grad_norm": 0.0037820448633283377,
      "learning_rate": 4.5560041455337043e-07,
      "loss": 0.0002,
      "step": 4329
    },
    {
      "epoch": 179.95,
      "grad_norm": 0.04751460626721382,
      "learning_rate": 4.542643833021254e-07,
      "loss": 0.0003,
      "step": 4330
    },
    {
      "epoch": 179.99,
      "grad_norm": 0.05738186836242676,
      "learning_rate": 4.5293022063108483e-07,
      "loss": 0.0004,
      "step": 4331
    },
    {
      "epoch": 180.03,
      "grad_norm": 0.0722203403711319,
      "learning_rate": 4.515979270886728e-07,
      "loss": 0.0006,
      "step": 4332
    },
    {
      "epoch": 180.07,
      "grad_norm": 0.003407314419746399,
      "learning_rate": 4.5026750322254564e-07,
      "loss": 0.0002,
      "step": 4333
    },
    {
      "epoch": 180.11,
      "grad_norm": 0.06443465501070023,
      "learning_rate": 4.4893894957958875e-07,
      "loss": 0.0005,
      "step": 4334
    },
    {
      "epoch": 180.16,
      "grad_norm": 0.06326986104249954,
      "learning_rate": 4.4761226670592074e-07,
      "loss": 0.0006,
      "step": 4335
    },
    {
      "epoch": 180.2,
      "grad_norm": 0.023590916767716408,
      "learning_rate": 4.4628745514689154e-07,
      "loss": 0.0003,
      "step": 4336
    },
    {
      "epoch": 180.24,
      "grad_norm": 0.03807733207941055,
      "learning_rate": 4.449645154470805e-07,
      "loss": 0.0004,
      "step": 4337
    },
    {
      "epoch": 180.28,
      "grad_norm": 0.04236958548426628,
      "learning_rate": 4.4364344815029825e-07,
      "loss": 0.0004,
      "step": 4338
    },
    {
      "epoch": 180.32,
      "grad_norm": 0.021543795242905617,
      "learning_rate": 4.423242537995864e-07,
      "loss": 0.0003,
      "step": 4339
    },
    {
      "epoch": 180.36,
      "grad_norm": 0.10602430254220963,
      "learning_rate": 4.410069329372152e-07,
      "loss": 0.0007,
      "step": 4340
    },
    {
      "epoch": 180.41,
      "grad_norm": 0.04485483095049858,
      "learning_rate": 4.3969148610468526e-07,
      "loss": 0.0004,
      "step": 4341
    },
    {
      "epoch": 180.45,
      "grad_norm": 0.028119580820202827,
      "learning_rate": 4.383779138427274e-07,
      "loss": 0.0003,
      "step": 4342
    },
    {
      "epoch": 180.49,
      "grad_norm": 0.029426544904708862,
      "learning_rate": 4.370662166913031e-07,
      "loss": 0.0003,
      "step": 4343
    },
    {
      "epoch": 180.53,
      "grad_norm": 0.05745907500386238,
      "learning_rate": 4.357563951895988e-07,
      "loss": 0.0006,
      "step": 4344
    },
    {
      "epoch": 180.57,
      "grad_norm": 0.03389904648065567,
      "learning_rate": 4.344484498760343e-07,
      "loss": 0.0004,
      "step": 4345
    },
    {
      "epoch": 180.61,
      "grad_norm": 0.026752527803182602,
      "learning_rate": 4.3314238128825625e-07,
      "loss": 0.0003,
      "step": 4346
    },
    {
      "epoch": 180.65,
      "grad_norm": 0.051509078592061996,
      "learning_rate": 4.3183818996313965e-07,
      "loss": 0.0004,
      "step": 4347
    },
    {
      "epoch": 180.7,
      "grad_norm": 0.08533557504415512,
      "learning_rate": 4.305358764367884e-07,
      "loss": 0.0007,
      "step": 4348
    },
    {
      "epoch": 180.74,
      "grad_norm": 0.16771382093429565,
      "learning_rate": 4.2923544124453484e-07,
      "loss": 0.0012,
      "step": 4349
    },
    {
      "epoch": 180.78,
      "grad_norm": 0.05091048777103424,
      "learning_rate": 4.279368849209381e-07,
      "loss": 0.0004,
      "step": 4350
    },
    {
      "epoch": 180.82,
      "grad_norm": 0.04045620560646057,
      "learning_rate": 4.266402079997861e-07,
      "loss": 0.0004,
      "step": 4351
    },
    {
      "epoch": 180.86,
      "grad_norm": 0.06894935667514801,
      "learning_rate": 4.253454110140942e-07,
      "loss": 0.0005,
      "step": 4352
    },
    {
      "epoch": 180.9,
      "grad_norm": 0.03457597643136978,
      "learning_rate": 4.240524944961033e-07,
      "loss": 0.0003,
      "step": 4353
    },
    {
      "epoch": 180.95,
      "grad_norm": 0.059437137097120285,
      "learning_rate": 4.227614589772838e-07,
      "loss": 0.0006,
      "step": 4354
    },
    {
      "epoch": 180.99,
      "grad_norm": 0.06368913501501083,
      "learning_rate": 4.214723049883307e-07,
      "loss": 0.0005,
      "step": 4355
    },
    {
      "epoch": 181.03,
      "grad_norm": 0.16119448840618134,
      "learning_rate": 4.201850330591678e-07,
      "loss": 0.0007,
      "step": 4356
    },
    {
      "epoch": 181.07,
      "grad_norm": 0.0033752182498574257,
      "learning_rate": 4.188996437189424e-07,
      "loss": 0.0002,
      "step": 4357
    },
    {
      "epoch": 181.11,
      "grad_norm": 0.05574720352888107,
      "learning_rate": 4.1761613749603024e-07,
      "loss": 0.0005,
      "step": 4358
    },
    {
      "epoch": 181.15,
      "grad_norm": 0.0035975303035229445,
      "learning_rate": 4.1633451491803203e-07,
      "loss": 0.0002,
      "step": 4359
    },
    {
      "epoch": 181.19,
      "grad_norm": 0.0033801300451159477,
      "learning_rate": 4.150547765117746e-07,
      "loss": 0.0002,
      "step": 4360
    },
    {
      "epoch": 181.24,
      "grad_norm": 0.051161643117666245,
      "learning_rate": 4.1377692280331005e-07,
      "loss": 0.0004,
      "step": 4361
    },
    {
      "epoch": 181.28,
      "grad_norm": 0.08758984506130219,
      "learning_rate": 4.125009543179159e-07,
      "loss": 0.0006,
      "step": 4362
    },
    {
      "epoch": 181.32,
      "grad_norm": 0.08360698819160461,
      "learning_rate": 4.112268715800943e-07,
      "loss": 0.0006,
      "step": 4363
    },
    {
      "epoch": 181.36,
      "grad_norm": 0.07595720142126083,
      "learning_rate": 4.0995467511357246e-07,
      "loss": 0.0008,
      "step": 4364
    },
    {
      "epoch": 181.4,
      "grad_norm": 0.09070982038974762,
      "learning_rate": 4.086843654413031e-07,
      "loss": 0.0005,
      "step": 4365
    },
    {
      "epoch": 181.44,
      "grad_norm": 0.028709638863801956,
      "learning_rate": 4.074159430854624e-07,
      "loss": 0.0003,
      "step": 4366
    },
    {
      "epoch": 181.49,
      "grad_norm": 0.017765024676918983,
      "learning_rate": 4.0614940856744944e-07,
      "loss": 0.0002,
      "step": 4367
    },
    {
      "epoch": 181.53,
      "grad_norm": 0.08836475759744644,
      "learning_rate": 4.0488476240789e-07,
      "loss": 0.0006,
      "step": 4368
    },
    {
      "epoch": 181.57,
      "grad_norm": 0.0756153091788292,
      "learning_rate": 4.036220051266321e-07,
      "loss": 0.0007,
      "step": 4369
    },
    {
      "epoch": 181.61,
      "grad_norm": 0.061985112726688385,
      "learning_rate": 4.0236113724274716e-07,
      "loss": 0.0005,
      "step": 4370
    },
    {
      "epoch": 181.65,
      "grad_norm": 0.06476497650146484,
      "learning_rate": 4.011021592745307e-07,
      "loss": 0.0006,
      "step": 4371
    },
    {
      "epoch": 181.69,
      "grad_norm": 0.054319292306900024,
      "learning_rate": 3.9984507173950136e-07,
      "loss": 0.0006,
      "step": 4372
    },
    {
      "epoch": 181.74,
      "grad_norm": 0.03867991641163826,
      "learning_rate": 3.9858987515439986e-07,
      "loss": 0.0003,
      "step": 4373
    },
    {
      "epoch": 181.78,
      "grad_norm": 0.10687530040740967,
      "learning_rate": 3.9733657003519e-07,
      "loss": 0.0008,
      "step": 4374
    },
    {
      "epoch": 181.82,
      "grad_norm": 0.05362658202648163,
      "learning_rate": 3.960851568970586e-07,
      "loss": 0.0004,
      "step": 4375
    },
    {
      "epoch": 181.86,
      "grad_norm": 0.03469327464699745,
      "learning_rate": 3.9483563625441424e-07,
      "loss": 0.0004,
      "step": 4376
    },
    {
      "epoch": 181.9,
      "grad_norm": 0.003450056305155158,
      "learning_rate": 3.935880086208882e-07,
      "loss": 0.0002,
      "step": 4377
    },
    {
      "epoch": 181.94,
      "grad_norm": 0.045788008719682693,
      "learning_rate": 3.9234227450933136e-07,
      "loss": 0.0004,
      "step": 4378
    },
    {
      "epoch": 181.98,
      "grad_norm": 0.06476867944002151,
      "learning_rate": 3.910984344318192e-07,
      "loss": 0.0008,
      "step": 4379
    },
    {
      "epoch": 182.03,
      "grad_norm": 0.06733347475528717,
      "learning_rate": 3.8985648889964755e-07,
      "loss": 0.0007,
      "step": 4380
    },
    {
      "epoch": 182.07,
      "grad_norm": 0.003519276389852166,
      "learning_rate": 3.886164384233326e-07,
      "loss": 0.0002,
      "step": 4381
    },
    {
      "epoch": 182.11,
      "grad_norm": 0.0033883685246109962,
      "learning_rate": 3.8737828351261273e-07,
      "loss": 0.0002,
      "step": 4382
    },
    {
      "epoch": 182.15,
      "grad_norm": 0.038042377680540085,
      "learning_rate": 3.8614202467644636e-07,
      "loss": 0.0003,
      "step": 4383
    },
    {
      "epoch": 182.19,
      "grad_norm": 0.049696773290634155,
      "learning_rate": 3.8490766242301356e-07,
      "loss": 0.0004,
      "step": 4384
    },
    {
      "epoch": 182.23,
      "grad_norm": 0.003903832519426942,
      "learning_rate": 3.83675197259713e-07,
      "loss": 0.0002,
      "step": 4385
    },
    {
      "epoch": 182.28,
      "grad_norm": 0.08482588827610016,
      "learning_rate": 3.82444629693165e-07,
      "loss": 0.0007,
      "step": 4386
    },
    {
      "epoch": 182.32,
      "grad_norm": 0.050587449222803116,
      "learning_rate": 3.812159602292104e-07,
      "loss": 0.0004,
      "step": 4387
    },
    {
      "epoch": 182.36,
      "grad_norm": 0.021261243149638176,
      "learning_rate": 3.7998918937290686e-07,
      "loss": 0.0003,
      "step": 4388
    },
    {
      "epoch": 182.4,
      "grad_norm": 0.003461672691628337,
      "learning_rate": 3.787643176285355e-07,
      "loss": 0.0002,
      "step": 4389
    },
    {
      "epoch": 182.44,
      "grad_norm": 0.08953465521335602,
      "learning_rate": 3.77541345499593e-07,
      "loss": 0.0009,
      "step": 4390
    },
    {
      "epoch": 182.48,
      "grad_norm": 0.035266388207674026,
      "learning_rate": 3.7632027348879773e-07,
      "loss": 0.0004,
      "step": 4391
    },
    {
      "epoch": 182.52,
      "grad_norm": 0.06193764507770538,
      "learning_rate": 3.7510110209808657e-07,
      "loss": 0.0005,
      "step": 4392
    },
    {
      "epoch": 182.57,
      "grad_norm": 0.03366222232580185,
      "learning_rate": 3.738838318286142e-07,
      "loss": 0.0005,
      "step": 4393
    },
    {
      "epoch": 182.61,
      "grad_norm": 0.04572940245270729,
      "learning_rate": 3.7266846318075535e-07,
      "loss": 0.0006,
      "step": 4394
    },
    {
      "epoch": 182.65,
      "grad_norm": 0.06499326229095459,
      "learning_rate": 3.7145499665410147e-07,
      "loss": 0.0007,
      "step": 4395
    },
    {
      "epoch": 182.69,
      "grad_norm": 0.09464967995882034,
      "learning_rate": 3.70243432747463e-07,
      "loss": 0.0009,
      "step": 4396
    },
    {
      "epoch": 182.73,
      "grad_norm": 0.037491701543331146,
      "learning_rate": 3.6903377195886823e-07,
      "loss": 0.0003,
      "step": 4397
    },
    {
      "epoch": 182.77,
      "grad_norm": 0.0036455411463975906,
      "learning_rate": 3.678260147855628e-07,
      "loss": 0.0002,
      "step": 4398
    },
    {
      "epoch": 182.82,
      "grad_norm": 0.1338804066181183,
      "learning_rate": 3.6662016172401114e-07,
      "loss": 0.0009,
      "step": 4399
    },
    {
      "epoch": 182.86,
      "grad_norm": 0.13390637934207916,
      "learning_rate": 3.6541621326989183e-07,
      "loss": 0.0013,
      "step": 4400
    },
    {
      "epoch": 182.9,
      "grad_norm": 0.003198909107595682,
      "learning_rate": 3.64214169918104e-07,
      "loss": 0.0002,
      "step": 4401
    },
    {
      "epoch": 182.94,
      "grad_norm": 0.02951487898826599,
      "learning_rate": 3.6301403216276176e-07,
      "loss": 0.0003,
      "step": 4402
    },
    {
      "epoch": 182.98,
      "grad_norm": 0.07283924520015717,
      "learning_rate": 3.6181580049719664e-07,
      "loss": 0.0005,
      "step": 4403
    },
    {
      "epoch": 183.02,
      "grad_norm": 0.046203263103961945,
      "learning_rate": 3.606194754139569e-07,
      "loss": 0.0006,
      "step": 4404
    },
    {
      "epoch": 183.06,
      "grad_norm": 0.08342856168746948,
      "learning_rate": 3.5942505740480583e-07,
      "loss": 0.0008,
      "step": 4405
    },
    {
      "epoch": 183.11,
      "grad_norm": 0.11189962923526764,
      "learning_rate": 3.5823254696072343e-07,
      "loss": 0.0008,
      "step": 4406
    },
    {
      "epoch": 183.15,
      "grad_norm": 0.16079916059970856,
      "learning_rate": 3.5704194457190647e-07,
      "loss": 0.0008,
      "step": 4407
    },
    {
      "epoch": 183.19,
      "grad_norm": 0.04172208160161972,
      "learning_rate": 3.5585325072776625e-07,
      "loss": 0.0003,
      "step": 4408
    },
    {
      "epoch": 183.23,
      "grad_norm": 0.07483412325382233,
      "learning_rate": 3.5466646591692966e-07,
      "loss": 0.0008,
      "step": 4409
    },
    {
      "epoch": 183.27,
      "grad_norm": 0.03418327122926712,
      "learning_rate": 3.534815906272404e-07,
      "loss": 0.0004,
      "step": 4410
    },
    {
      "epoch": 183.31,
      "grad_norm": 0.06591980904340744,
      "learning_rate": 3.5229862534575386e-07,
      "loss": 0.0005,
      "step": 4411
    },
    {
      "epoch": 183.36,
      "grad_norm": 0.05261481553316116,
      "learning_rate": 3.511175705587433e-07,
      "loss": 0.0005,
      "step": 4412
    },
    {
      "epoch": 183.4,
      "grad_norm": 0.056237492710351944,
      "learning_rate": 3.4993842675169587e-07,
      "loss": 0.0006,
      "step": 4413
    },
    {
      "epoch": 183.44,
      "grad_norm": 0.003447652794420719,
      "learning_rate": 3.487611944093133e-07,
      "loss": 0.0002,
      "step": 4414
    },
    {
      "epoch": 183.48,
      "grad_norm": 0.08281609416007996,
      "learning_rate": 3.475858740155108e-07,
      "loss": 0.0008,
      "step": 4415
    },
    {
      "epoch": 183.52,
      "grad_norm": 0.03893037140369415,
      "learning_rate": 3.464124660534191e-07,
      "loss": 0.0004,
      "step": 4416
    },
    {
      "epoch": 183.56,
      "grad_norm": 0.18160413205623627,
      "learning_rate": 3.452409710053806e-07,
      "loss": 0.0007,
      "step": 4417
    },
    {
      "epoch": 183.61,
      "grad_norm": 0.028164375573396683,
      "learning_rate": 3.440713893529535e-07,
      "loss": 0.0003,
      "step": 4418
    },
    {
      "epoch": 183.65,
      "grad_norm": 0.02895270101726055,
      "learning_rate": 3.429037215769082e-07,
      "loss": 0.0003,
      "step": 4419
    },
    {
      "epoch": 183.69,
      "grad_norm": 0.09315793216228485,
      "learning_rate": 3.417379681572297e-07,
      "loss": 0.0007,
      "step": 4420
    },
    {
      "epoch": 183.73,
      "grad_norm": 0.03813197463750839,
      "learning_rate": 3.4057412957311407e-07,
      "loss": 0.0003,
      "step": 4421
    },
    {
      "epoch": 183.77,
      "grad_norm": 0.06145633012056351,
      "learning_rate": 3.39412206302972e-07,
      "loss": 0.0005,
      "step": 4422
    },
    {
      "epoch": 183.81,
      "grad_norm": 0.04092457890510559,
      "learning_rate": 3.382521988244264e-07,
      "loss": 0.0004,
      "step": 4423
    },
    {
      "epoch": 183.85,
      "grad_norm": 0.003465474583208561,
      "learning_rate": 3.3709410761431136e-07,
      "loss": 0.0002,
      "step": 4424
    },
    {
      "epoch": 183.9,
      "grad_norm": 0.03322247043251991,
      "learning_rate": 3.359379331486762e-07,
      "loss": 0.0003,
      "step": 4425
    },
    {
      "epoch": 183.94,
      "grad_norm": 0.0038129384629428387,
      "learning_rate": 3.347836759027789e-07,
      "loss": 0.0002,
      "step": 4426
    },
    {
      "epoch": 183.98,
      "grad_norm": 0.04792776703834534,
      "learning_rate": 3.3363133635109233e-07,
      "loss": 0.0003,
      "step": 4427
    },
    {
      "epoch": 184.02,
      "grad_norm": 0.030956782400608063,
      "learning_rate": 3.324809149672992e-07,
      "loss": 0.0003,
      "step": 4428
    },
    {
      "epoch": 184.06,
      "grad_norm": 0.03966711461544037,
      "learning_rate": 3.313324122242945e-07,
      "loss": 0.0004,
      "step": 4429
    },
    {
      "epoch": 184.1,
      "grad_norm": 0.018890149891376495,
      "learning_rate": 3.301858285941845e-07,
      "loss": 0.0003,
      "step": 4430
    },
    {
      "epoch": 184.15,
      "grad_norm": 0.021179109811782837,
      "learning_rate": 3.290411645482855e-07,
      "loss": 0.0003,
      "step": 4431
    },
    {
      "epoch": 184.19,
      "grad_norm": 0.0032135446090251207,
      "learning_rate": 3.278984205571262e-07,
      "loss": 0.0002,
      "step": 4432
    },
    {
      "epoch": 184.23,
      "grad_norm": 0.044550634920597076,
      "learning_rate": 3.2675759709044577e-07,
      "loss": 0.0003,
      "step": 4433
    },
    {
      "epoch": 184.27,
      "grad_norm": 0.038786645978689194,
      "learning_rate": 3.25618694617193e-07,
      "loss": 0.0003,
      "step": 4434
    },
    {
      "epoch": 184.31,
      "grad_norm": 0.059234920889139175,
      "learning_rate": 3.2448171360552837e-07,
      "loss": 0.0006,
      "step": 4435
    },
    {
      "epoch": 184.35,
      "grad_norm": 0.06373316049575806,
      "learning_rate": 3.2334665452282143e-07,
      "loss": 0.0008,
      "step": 4436
    },
    {
      "epoch": 184.39,
      "grad_norm": 0.12147259712219238,
      "learning_rate": 3.222135178356517e-07,
      "loss": 0.0006,
      "step": 4437
    },
    {
      "epoch": 184.44,
      "grad_norm": 0.06052219867706299,
      "learning_rate": 3.2108230400980987e-07,
      "loss": 0.0006,
      "step": 4438
    },
    {
      "epoch": 184.48,
      "grad_norm": 0.03338371589779854,
      "learning_rate": 3.1995301351029407e-07,
      "loss": 0.0004,
      "step": 4439
    },
    {
      "epoch": 184.52,
      "grad_norm": 0.04224841669201851,
      "learning_rate": 3.18825646801314e-07,
      "loss": 0.0004,
      "step": 4440
    },
    {
      "epoch": 184.56,
      "grad_norm": 0.03813169151544571,
      "learning_rate": 3.1770020434628734e-07,
      "loss": 0.0003,
      "step": 4441
    },
    {
      "epoch": 184.6,
      "grad_norm": 0.034197330474853516,
      "learning_rate": 3.1657668660784015e-07,
      "loss": 0.0005,
      "step": 4442
    },
    {
      "epoch": 184.64,
      "grad_norm": 0.0705999806523323,
      "learning_rate": 3.154550940478102e-07,
      "loss": 0.0006,
      "step": 4443
    },
    {
      "epoch": 184.69,
      "grad_norm": 0.09636534750461578,
      "learning_rate": 3.143354271272392e-07,
      "loss": 0.0008,
      "step": 4444
    },
    {
      "epoch": 184.73,
      "grad_norm": 0.03066423535346985,
      "learning_rate": 3.1321768630638073e-07,
      "loss": 0.0003,
      "step": 4445
    },
    {
      "epoch": 184.77,
      "grad_norm": 0.04889223352074623,
      "learning_rate": 3.1210187204469667e-07,
      "loss": 0.0004,
      "step": 4446
    },
    {
      "epoch": 184.81,
      "grad_norm": 0.07104761153459549,
      "learning_rate": 3.109879848008557e-07,
      "loss": 0.0005,
      "step": 4447
    },
    {
      "epoch": 184.85,
      "grad_norm": 0.028338255360722542,
      "learning_rate": 3.098760250327343e-07,
      "loss": 0.0003,
      "step": 4448
    },
    {
      "epoch": 184.89,
      "grad_norm": 0.08925722539424896,
      "learning_rate": 3.0876599319741797e-07,
      "loss": 0.0007,
      "step": 4449
    },
    {
      "epoch": 184.94,
      "grad_norm": 0.1104382798075676,
      "learning_rate": 3.076578897511978e-07,
      "loss": 0.0012,
      "step": 4450
    },
    {
      "epoch": 184.98,
      "grad_norm": 0.003200847189873457,
      "learning_rate": 3.065517151495745e-07,
      "loss": 0.0002,
      "step": 4451
    },
    {
      "epoch": 185.02,
      "grad_norm": 0.03435301035642624,
      "learning_rate": 3.054474698472537e-07,
      "loss": 0.0004,
      "step": 4452
    },
    {
      "epoch": 185.06,
      "grad_norm": 0.044164661318063736,
      "learning_rate": 3.043451542981507e-07,
      "loss": 0.0005,
      "step": 4453
    },
    {
      "epoch": 185.1,
      "grad_norm": 0.11789379268884659,
      "learning_rate": 3.03244768955383e-07,
      "loss": 0.0009,
      "step": 4454
    },
    {
      "epoch": 185.14,
      "grad_norm": 0.025745438411831856,
      "learning_rate": 3.0214631427127883e-07,
      "loss": 0.0003,
      "step": 4455
    },
    {
      "epoch": 185.18,
      "grad_norm": 0.041238825768232346,
      "learning_rate": 3.010497906973714e-07,
      "loss": 0.0003,
      "step": 4456
    },
    {
      "epoch": 185.23,
      "grad_norm": 0.06935503333806992,
      "learning_rate": 2.999551986844001e-07,
      "loss": 0.0008,
      "step": 4457
    },
    {
      "epoch": 185.27,
      "grad_norm": 0.04350985959172249,
      "learning_rate": 2.9886253868231073e-07,
      "loss": 0.0003,
      "step": 4458
    },
    {
      "epoch": 185.31,
      "grad_norm": 0.028735127300024033,
      "learning_rate": 2.977718111402544e-07,
      "loss": 0.0003,
      "step": 4459
    },
    {
      "epoch": 185.35,
      "grad_norm": 0.10899675637483597,
      "learning_rate": 2.966830165065876e-07,
      "loss": 0.0007,
      "step": 4460
    },
    {
      "epoch": 185.39,
      "grad_norm": 0.03409833088517189,
      "learning_rate": 2.9559615522887275e-07,
      "loss": 0.0004,
      "step": 4461
    },
    {
      "epoch": 185.43,
      "grad_norm": 0.0035426514223217964,
      "learning_rate": 2.9451122775387755e-07,
      "loss": 0.0002,
      "step": 4462
    },
    {
      "epoch": 185.48,
      "grad_norm": 0.07250498980283737,
      "learning_rate": 2.9342823452757463e-07,
      "loss": 0.0004,
      "step": 4463
    },
    {
      "epoch": 185.52,
      "grad_norm": 0.022960767149925232,
      "learning_rate": 2.92347175995143e-07,
      "loss": 0.0003,
      "step": 4464
    },
    {
      "epoch": 185.56,
      "grad_norm": 0.05291111767292023,
      "learning_rate": 2.912680526009626e-07,
      "loss": 0.0004,
      "step": 4465
    },
    {
      "epoch": 185.6,
      "grad_norm": 0.11638107150793076,
      "learning_rate": 2.9019086478862144e-07,
      "loss": 0.0009,
      "step": 4466
    },
    {
      "epoch": 185.64,
      "grad_norm": 0.09926187247037888,
      "learning_rate": 2.8911561300091094e-07,
      "loss": 0.0011,
      "step": 4467
    },
    {
      "epoch": 185.68,
      "grad_norm": 0.003443600842729211,
      "learning_rate": 2.8804229767982637e-07,
      "loss": 0.0002,
      "step": 4468
    },
    {
      "epoch": 185.72,
      "grad_norm": 0.07358568161725998,
      "learning_rate": 2.869709192665665e-07,
      "loss": 0.0009,
      "step": 4469
    },
    {
      "epoch": 185.77,
      "grad_norm": 0.04679173603653908,
      "learning_rate": 2.8590147820153513e-07,
      "loss": 0.0004,
      "step": 4470
    },
    {
      "epoch": 185.81,
      "grad_norm": 0.0033466850873082876,
      "learning_rate": 2.8483397492433953e-07,
      "loss": 0.0002,
      "step": 4471
    },
    {
      "epoch": 185.85,
      "grad_norm": 0.07518316805362701,
      "learning_rate": 2.837684098737892e-07,
      "loss": 0.0007,
      "step": 4472
    },
    {
      "epoch": 185.89,
      "grad_norm": 0.003493700874969363,
      "learning_rate": 2.8270478348789764e-07,
      "loss": 0.0002,
      "step": 4473
    },
    {
      "epoch": 185.93,
      "grad_norm": 0.09514742344617844,
      "learning_rate": 2.81643096203883e-07,
      "loss": 0.0008,
      "step": 4474
    },
    {
      "epoch": 185.97,
      "grad_norm": 0.003646743018180132,
      "learning_rate": 2.8058334845816214e-07,
      "loss": 0.0002,
      "step": 4475
    },
    {
      "epoch": 186.02,
      "grad_norm": 0.019979141652584076,
      "learning_rate": 2.795255406863595e-07,
      "loss": 0.0003,
      "step": 4476
    },
    {
      "epoch": 186.06,
      "grad_norm": 0.047480255365371704,
      "learning_rate": 2.784696733232989e-07,
      "loss": 0.0007,
      "step": 4477
    },
    {
      "epoch": 186.1,
      "grad_norm": 0.0638602003455162,
      "learning_rate": 2.7741574680300754e-07,
      "loss": 0.0007,
      "step": 4478
    },
    {
      "epoch": 186.14,
      "grad_norm": 0.0034049612004309893,
      "learning_rate": 2.763637615587161e-07,
      "loss": 0.0002,
      "step": 4479
    },
    {
      "epoch": 186.18,
      "grad_norm": 0.05239824950695038,
      "learning_rate": 2.7531371802285436e-07,
      "loss": 0.0005,
      "step": 4480
    },
    {
      "epoch": 186.22,
      "grad_norm": 0.04611918702721596,
      "learning_rate": 2.7426561662705575e-07,
      "loss": 0.0006,
      "step": 4481
    },
    {
      "epoch": 186.26,
      "grad_norm": 0.06361580640077591,
      "learning_rate": 2.7321945780215576e-07,
      "loss": 0.0005,
      "step": 4482
    },
    {
      "epoch": 186.31,
      "grad_norm": 0.04311923310160637,
      "learning_rate": 2.721752419781903e-07,
      "loss": 0.0005,
      "step": 4483
    },
    {
      "epoch": 186.35,
      "grad_norm": 0.16317689418792725,
      "learning_rate": 2.711329695843978e-07,
      "loss": 0.001,
      "step": 4484
    },
    {
      "epoch": 186.39,
      "grad_norm": 0.024332763627171516,
      "learning_rate": 2.7009264104921606e-07,
      "loss": 0.0003,
      "step": 4485
    },
    {
      "epoch": 186.43,
      "grad_norm": 0.02963087521493435,
      "learning_rate": 2.6905425680028686e-07,
      "loss": 0.0004,
      "step": 4486
    },
    {
      "epoch": 186.47,
      "grad_norm": 0.15545503795146942,
      "learning_rate": 2.6801781726444767e-07,
      "loss": 0.0009,
      "step": 4487
    },
    {
      "epoch": 186.51,
      "grad_norm": 0.0034839939326047897,
      "learning_rate": 2.6698332286774153e-07,
      "loss": 0.0002,
      "step": 4488
    },
    {
      "epoch": 186.56,
      "grad_norm": 0.0459781251847744,
      "learning_rate": 2.6595077403541e-07,
      "loss": 0.0004,
      "step": 4489
    },
    {
      "epoch": 186.6,
      "grad_norm": 0.003386038588359952,
      "learning_rate": 2.6492017119189415e-07,
      "loss": 0.0002,
      "step": 4490
    },
    {
      "epoch": 186.64,
      "grad_norm": 0.07202797383069992,
      "learning_rate": 2.638915147608362e-07,
      "loss": 0.0006,
      "step": 4491
    },
    {
      "epoch": 186.68,
      "grad_norm": 0.02346661686897278,
      "learning_rate": 2.628648051650784e-07,
      "loss": 0.0003,
      "step": 4492
    },
    {
      "epoch": 186.72,
      "grad_norm": 0.05444522574543953,
      "learning_rate": 2.618400428266621e-07,
      "loss": 0.0004,
      "step": 4493
    },
    {
      "epoch": 186.76,
      "grad_norm": 0.050378356128931046,
      "learning_rate": 2.608172281668281e-07,
      "loss": 0.0005,
      "step": 4494
    },
    {
      "epoch": 186.81,
      "grad_norm": 0.06849031150341034,
      "learning_rate": 2.5979636160601673e-07,
      "loss": 0.0006,
      "step": 4495
    },
    {
      "epoch": 186.85,
      "grad_norm": 0.08890235424041748,
      "learning_rate": 2.587774435638679e-07,
      "loss": 0.0006,
      "step": 4496
    },
    {
      "epoch": 186.89,
      "grad_norm": 0.0033622002229094505,
      "learning_rate": 2.577604744592216e-07,
      "loss": 0.0002,
      "step": 4497
    },
    {
      "epoch": 186.93,
      "grad_norm": 0.017601830884814262,
      "learning_rate": 2.5674545471011335e-07,
      "loss": 0.0003,
      "step": 4498
    },
    {
      "epoch": 186.97,
      "grad_norm": 0.046095166355371475,
      "learning_rate": 2.5573238473378003e-07,
      "loss": 0.0004,
      "step": 4499
    },
    {
      "epoch": 187.01,
      "grad_norm": 0.12764433026313782,
      "learning_rate": 2.547212649466568e-07,
      "loss": 0.0009,
      "step": 4500
    },
    {
      "epoch": 187.05,
      "grad_norm": 0.03356315568089485,
      "learning_rate": 2.537120957643768e-07,
      "loss": 0.0005,
      "step": 4501
    },
    {
      "epoch": 187.1,
      "grad_norm": 0.003319127019494772,
      "learning_rate": 2.5270487760177153e-07,
      "loss": 0.0002,
      "step": 4502
    },
    {
      "epoch": 187.14,
      "grad_norm": 0.07508708536624908,
      "learning_rate": 2.5169961087286975e-07,
      "loss": 0.0007,
      "step": 4503
    },
    {
      "epoch": 187.18,
      "grad_norm": 0.07706225663423538,
      "learning_rate": 2.5069629599089874e-07,
      "loss": 0.0005,
      "step": 4504
    },
    {
      "epoch": 187.22,
      "grad_norm": 0.07417774945497513,
      "learning_rate": 2.4969493336828353e-07,
      "loss": 0.0006,
      "step": 4505
    },
    {
      "epoch": 187.26,
      "grad_norm": 0.04726199805736542,
      "learning_rate": 2.4869552341664715e-07,
      "loss": 0.0005,
      "step": 4506
    },
    {
      "epoch": 187.3,
      "grad_norm": 0.023506974801421165,
      "learning_rate": 2.4769806654680874e-07,
      "loss": 0.0003,
      "step": 4507
    },
    {
      "epoch": 187.35,
      "grad_norm": 0.04102605581283569,
      "learning_rate": 2.467025631687847e-07,
      "loss": 0.0003,
      "step": 4508
    },
    {
      "epoch": 187.39,
      "grad_norm": 0.06319355964660645,
      "learning_rate": 2.457090136917889e-07,
      "loss": 0.0006,
      "step": 4509
    },
    {
      "epoch": 187.43,
      "grad_norm": 0.027618179097771645,
      "learning_rate": 2.447174185242324e-07,
      "loss": 0.0003,
      "step": 4510
    },
    {
      "epoch": 187.47,
      "grad_norm": 0.02953634038567543,
      "learning_rate": 2.4372777807372237e-07,
      "loss": 0.0003,
      "step": 4511
    },
    {
      "epoch": 187.51,
      "grad_norm": 0.061542026698589325,
      "learning_rate": 2.4274009274706244e-07,
      "loss": 0.0006,
      "step": 4512
    },
    {
      "epoch": 187.55,
      "grad_norm": 0.04067669063806534,
      "learning_rate": 2.4175436295025333e-07,
      "loss": 0.0004,
      "step": 4513
    },
    {
      "epoch": 187.59,
      "grad_norm": 0.060659244656562805,
      "learning_rate": 2.4077058908849093e-07,
      "loss": 0.0004,
      "step": 4514
    },
    {
      "epoch": 187.64,
      "grad_norm": 0.023097822442650795,
      "learning_rate": 2.397887715661679e-07,
      "loss": 0.0003,
      "step": 4515
    },
    {
      "epoch": 187.68,
      "grad_norm": 0.04880494624376297,
      "learning_rate": 2.388089107868713e-07,
      "loss": 0.0005,
      "step": 4516
    },
    {
      "epoch": 187.72,
      "grad_norm": 0.031009491533041,
      "learning_rate": 2.3783100715338624e-07,
      "loss": 0.0004,
      "step": 4517
    },
    {
      "epoch": 187.76,
      "grad_norm": 0.07341200113296509,
      "learning_rate": 2.368550610676912e-07,
      "loss": 0.0005,
      "step": 4518
    },
    {
      "epoch": 187.8,
      "grad_norm": 0.061918385326862335,
      "learning_rate": 2.3588107293096075e-07,
      "loss": 0.0004,
      "step": 4519
    },
    {
      "epoch": 187.84,
      "grad_norm": 0.08450707793235779,
      "learning_rate": 2.3490904314356412e-07,
      "loss": 0.0006,
      "step": 4520
    },
    {
      "epoch": 187.89,
      "grad_norm": 0.11208568513393402,
      "learning_rate": 2.3393897210506723e-07,
      "loss": 0.001,
      "step": 4521
    },
    {
      "epoch": 187.93,
      "grad_norm": 0.061834607273340225,
      "learning_rate": 2.3297086021422887e-07,
      "loss": 0.0005,
      "step": 4522
    },
    {
      "epoch": 187.97,
      "grad_norm": 0.0035089198499917984,
      "learning_rate": 2.3200470786900298e-07,
      "loss": 0.0002,
      "step": 4523
    },
    {
      "epoch": 188.01,
      "grad_norm": 0.04593964293599129,
      "learning_rate": 2.3104051546654016e-07,
      "loss": 0.0004,
      "step": 4524
    },
    {
      "epoch": 188.05,
      "grad_norm": 0.06622930616140366,
      "learning_rate": 2.3007828340318117e-07,
      "loss": 0.0006,
      "step": 4525
    },
    {
      "epoch": 188.09,
      "grad_norm": 0.03680960088968277,
      "learning_rate": 2.2911801207446403e-07,
      "loss": 0.0004,
      "step": 4526
    },
    {
      "epoch": 188.14,
      "grad_norm": 0.03213830292224884,
      "learning_rate": 2.2815970187512027e-07,
      "loss": 0.0003,
      "step": 4527
    },
    {
      "epoch": 188.18,
      "grad_norm": 0.0032603784929960966,
      "learning_rate": 2.2720335319907472e-07,
      "loss": 0.0002,
      "step": 4528
    },
    {
      "epoch": 188.22,
      "grad_norm": 0.08026333898305893,
      "learning_rate": 2.2624896643944626e-07,
      "loss": 0.0006,
      "step": 4529
    },
    {
      "epoch": 188.26,
      "grad_norm": 0.0540655180811882,
      "learning_rate": 2.2529654198854834e-07,
      "loss": 0.0006,
      "step": 4530
    },
    {
      "epoch": 188.3,
      "grad_norm": 0.028904955834150314,
      "learning_rate": 2.2434608023788496e-07,
      "loss": 0.0003,
      "step": 4531
    },
    {
      "epoch": 188.34,
      "grad_norm": 0.016214819625020027,
      "learning_rate": 2.2339758157815583e-07,
      "loss": 0.0003,
      "step": 4532
    },
    {
      "epoch": 188.38,
      "grad_norm": 0.018070098012685776,
      "learning_rate": 2.2245104639925297e-07,
      "loss": 0.0002,
      "step": 4533
    },
    {
      "epoch": 188.43,
      "grad_norm": 0.06730443984270096,
      "learning_rate": 2.2150647509026068e-07,
      "loss": 0.0005,
      "step": 4534
    },
    {
      "epoch": 188.47,
      "grad_norm": 0.04545215144753456,
      "learning_rate": 2.205638680394573e-07,
      "loss": 0.0004,
      "step": 4535
    },
    {
      "epoch": 188.51,
      "grad_norm": 0.03451061621308327,
      "learning_rate": 2.1962322563431283e-07,
      "loss": 0.0004,
      "step": 4536
    },
    {
      "epoch": 188.55,
      "grad_norm": 0.1068977490067482,
      "learning_rate": 2.1868454826148966e-07,
      "loss": 0.0011,
      "step": 4537
    },
    {
      "epoch": 188.59,
      "grad_norm": 0.05420712009072304,
      "learning_rate": 2.177478363068425e-07,
      "loss": 0.0005,
      "step": 4538
    },
    {
      "epoch": 188.63,
      "grad_norm": 0.0640217661857605,
      "learning_rate": 2.168130901554183e-07,
      "loss": 0.0005,
      "step": 4539
    },
    {
      "epoch": 188.68,
      "grad_norm": 0.07154547423124313,
      "learning_rate": 2.1588031019145638e-07,
      "loss": 0.0005,
      "step": 4540
    },
    {
      "epoch": 188.72,
      "grad_norm": 0.03247871622443199,
      "learning_rate": 2.1494949679838673e-07,
      "loss": 0.0003,
      "step": 4541
    },
    {
      "epoch": 188.76,
      "grad_norm": 0.09208584576845169,
      "learning_rate": 2.1402065035883157e-07,
      "loss": 0.0008,
      "step": 4542
    },
    {
      "epoch": 188.8,
      "grad_norm": 0.1895773559808731,
      "learning_rate": 2.1309377125460495e-07,
      "loss": 0.0012,
      "step": 4543
    },
    {
      "epoch": 188.84,
      "grad_norm": 0.0035979824606329203,
      "learning_rate": 2.1216885986671155e-07,
      "loss": 0.0002,
      "step": 4544
    },
    {
      "epoch": 188.88,
      "grad_norm": 0.07413648068904877,
      "learning_rate": 2.1124591657534776e-07,
      "loss": 0.0006,
      "step": 4545
    },
    {
      "epoch": 188.92,
      "grad_norm": 0.0628839060664177,
      "learning_rate": 2.103249417599007e-07,
      "loss": 0.0006,
      "step": 4546
    },
    {
      "epoch": 188.97,
      "grad_norm": 0.021736906841397285,
      "learning_rate": 2.0940593579894807e-07,
      "loss": 0.0003,
      "step": 4547
    },
    {
      "epoch": 189.01,
      "grad_norm": 0.04286351427435875,
      "learning_rate": 2.0848889907025883e-07,
      "loss": 0.0004,
      "step": 4548
    },
    {
      "epoch": 189.05,
      "grad_norm": 0.029144298285245895,
      "learning_rate": 2.0757383195079194e-07,
      "loss": 0.0003,
      "step": 4549
    },
    {
      "epoch": 189.09,
      "grad_norm": 0.030752014368772507,
      "learning_rate": 2.0666073481669714e-07,
      "loss": 0.0003,
      "step": 4550
    },
    {
      "epoch": 189.13,
      "grad_norm": 0.0234576053917408,
      "learning_rate": 2.0574960804331412e-07,
      "loss": 0.0003,
      "step": 4551
    },
    {
      "epoch": 189.17,
      "grad_norm": 0.10366877168416977,
      "learning_rate": 2.0484045200517222e-07,
      "loss": 0.0009,
      "step": 4552
    },
    {
      "epoch": 189.22,
      "grad_norm": 0.08592542260885239,
      "learning_rate": 2.0393326707599137e-07,
      "loss": 0.0006,
      "step": 4553
    },
    {
      "epoch": 189.26,
      "grad_norm": 0.0033718387130647898,
      "learning_rate": 2.0302805362868105e-07,
      "loss": 0.0002,
      "step": 4554
    },
    {
      "epoch": 189.3,
      "grad_norm": 0.02121073007583618,
      "learning_rate": 2.0212481203534083e-07,
      "loss": 0.0003,
      "step": 4555
    },
    {
      "epoch": 189.34,
      "grad_norm": 0.03565370664000511,
      "learning_rate": 2.0122354266725874e-07,
      "loss": 0.0005,
      "step": 4556
    },
    {
      "epoch": 189.38,
      "grad_norm": 0.03905482590198517,
      "learning_rate": 2.0032424589491228e-07,
      "loss": 0.0003,
      "step": 4557
    },
    {
      "epoch": 189.42,
      "grad_norm": 0.05652276426553726,
      "learning_rate": 1.994269220879691e-07,
      "loss": 0.0004,
      "step": 4558
    },
    {
      "epoch": 189.46,
      "grad_norm": 0.10927025973796844,
      "learning_rate": 1.9853157161528468e-07,
      "loss": 0.0007,
      "step": 4559
    },
    {
      "epoch": 189.51,
      "grad_norm": 0.055205948650836945,
      "learning_rate": 1.9763819484490353e-07,
      "loss": 0.0005,
      "step": 4560
    },
    {
      "epoch": 189.55,
      "grad_norm": 0.029678184539079666,
      "learning_rate": 1.9674679214406023e-07,
      "loss": 0.0003,
      "step": 4561
    },
    {
      "epoch": 189.59,
      "grad_norm": 0.09318513423204422,
      "learning_rate": 1.9585736387917554e-07,
      "loss": 0.0008,
      "step": 4562
    },
    {
      "epoch": 189.63,
      "grad_norm": 0.047376181930303574,
      "learning_rate": 1.9496991041585977e-07,
      "loss": 0.0004,
      "step": 4563
    },
    {
      "epoch": 189.67,
      "grad_norm": 0.053261466324329376,
      "learning_rate": 1.9408443211891227e-07,
      "loss": 0.0005,
      "step": 4564
    },
    {
      "epoch": 189.71,
      "grad_norm": 0.12519778311252594,
      "learning_rate": 1.932009293523196e-07,
      "loss": 0.0011,
      "step": 4565
    },
    {
      "epoch": 189.76,
      "grad_norm": 0.020286928862333298,
      "learning_rate": 1.9231940247925573e-07,
      "loss": 0.0003,
      "step": 4566
    },
    {
      "epoch": 189.8,
      "grad_norm": 0.05130898579955101,
      "learning_rate": 1.9143985186208357e-07,
      "loss": 0.0004,
      "step": 4567
    },
    {
      "epoch": 189.84,
      "grad_norm": 0.04573214799165726,
      "learning_rate": 1.9056227786235337e-07,
      "loss": 0.0004,
      "step": 4568
    },
    {
      "epoch": 189.88,
      "grad_norm": 0.06554970145225525,
      "learning_rate": 1.896866808408032e-07,
      "loss": 0.0005,
      "step": 4569
    },
    {
      "epoch": 189.92,
      "grad_norm": 0.05271447077393532,
      "learning_rate": 1.8881306115735632e-07,
      "loss": 0.0004,
      "step": 4570
    },
    {
      "epoch": 189.96,
      "grad_norm": 0.04453964903950691,
      "learning_rate": 1.8794141917112542e-07,
      "loss": 0.0004,
      "step": 4571
    },
    {
      "epoch": 190.01,
      "grad_norm": 0.0031858140137046576,
      "learning_rate": 1.8707175524040998e-07,
      "loss": 0.0002,
      "step": 4572
    },
    {
      "epoch": 190.05,
      "grad_norm": 0.046937495470047,
      "learning_rate": 1.8620406972269577e-07,
      "loss": 0.0004,
      "step": 4573
    },
    {
      "epoch": 190.09,
      "grad_norm": 0.038650427013635635,
      "learning_rate": 1.853383629746558e-07,
      "loss": 0.0003,
      "step": 4574
    },
    {
      "epoch": 190.13,
      "grad_norm": 0.10731574892997742,
      "learning_rate": 1.8447463535214872e-07,
      "loss": 0.0009,
      "step": 4575
    },
    {
      "epoch": 190.17,
      "grad_norm": 0.1279924362897873,
      "learning_rate": 1.8361288721022053e-07,
      "loss": 0.0008,
      "step": 4576
    },
    {
      "epoch": 190.21,
      "grad_norm": 0.025038747116923332,
      "learning_rate": 1.82753118903104e-07,
      "loss": 0.0003,
      "step": 4577
    },
    {
      "epoch": 190.25,
      "grad_norm": 0.09015356749296188,
      "learning_rate": 1.8189533078421638e-07,
      "loss": 0.001,
      "step": 4578
    },
    {
      "epoch": 190.3,
      "grad_norm": 0.027231434360146523,
      "learning_rate": 1.8103952320616348e-07,
      "loss": 0.0003,
      "step": 4579
    },
    {
      "epoch": 190.34,
      "grad_norm": 0.05712214484810829,
      "learning_rate": 1.801856965207338e-07,
      "loss": 0.0005,
      "step": 4580
    },
    {
      "epoch": 190.38,
      "grad_norm": 0.014050048775970936,
      "learning_rate": 1.793338510789039e-07,
      "loss": 0.0003,
      "step": 4581
    },
    {
      "epoch": 190.42,
      "grad_norm": 0.04759277030825615,
      "learning_rate": 1.7848398723083583e-07,
      "loss": 0.0004,
      "step": 4582
    },
    {
      "epoch": 190.46,
      "grad_norm": 0.05961957946419716,
      "learning_rate": 1.7763610532587573e-07,
      "loss": 0.0004,
      "step": 4583
    },
    {
      "epoch": 190.5,
      "grad_norm": 0.05877555161714554,
      "learning_rate": 1.76790205712557e-07,
      "loss": 0.0005,
      "step": 4584
    },
    {
      "epoch": 190.55,
      "grad_norm": 0.026343148201704025,
      "learning_rate": 1.7594628873859488e-07,
      "loss": 0.0003,
      "step": 4585
    },
    {
      "epoch": 190.59,
      "grad_norm": 0.053962308913469315,
      "learning_rate": 1.7510435475089348e-07,
      "loss": 0.0005,
      "step": 4586
    },
    {
      "epoch": 190.63,
      "grad_norm": 0.0034545287489891052,
      "learning_rate": 1.742644040955399e-07,
      "loss": 0.0002,
      "step": 4587
    },
    {
      "epoch": 190.67,
      "grad_norm": 0.003323584794998169,
      "learning_rate": 1.7342643711780516e-07,
      "loss": 0.0002,
      "step": 4588
    },
    {
      "epoch": 190.71,
      "grad_norm": 0.029348142445087433,
      "learning_rate": 1.7259045416214703e-07,
      "loss": 0.0003,
      "step": 4589
    },
    {
      "epoch": 190.75,
      "grad_norm": 0.048739053308963776,
      "learning_rate": 1.7175645557220567e-07,
      "loss": 0.0003,
      "step": 4590
    },
    {
      "epoch": 190.79,
      "grad_norm": 0.039399199187755585,
      "learning_rate": 1.709244416908068e-07,
      "loss": 0.0003,
      "step": 4591
    },
    {
      "epoch": 190.84,
      "grad_norm": 0.14226438105106354,
      "learning_rate": 1.700944128599602e-07,
      "loss": 0.0011,
      "step": 4592
    },
    {
      "epoch": 190.88,
      "grad_norm": 0.15088839828968048,
      "learning_rate": 1.692663694208585e-07,
      "loss": 0.0008,
      "step": 4593
    },
    {
      "epoch": 190.92,
      "grad_norm": 0.015508956275880337,
      "learning_rate": 1.6844031171388054e-07,
      "loss": 0.0003,
      "step": 4594
    },
    {
      "epoch": 190.96,
      "grad_norm": 0.08574432134628296,
      "learning_rate": 1.6761624007858524e-07,
      "loss": 0.0008,
      "step": 4595
    },
    {
      "epoch": 191.0,
      "grad_norm": 0.08715531975030899,
      "learning_rate": 1.667941548537194e-07,
      "loss": 0.0006,
      "step": 4596
    },
    {
      "epoch": 191.04,
      "grad_norm": 0.04977990314364433,
      "learning_rate": 1.6597405637720997e-07,
      "loss": 0.0004,
      "step": 4597
    },
    {
      "epoch": 191.09,
      "grad_norm": 0.07886974513530731,
      "learning_rate": 1.651559449861684e-07,
      "loss": 0.0007,
      "step": 4598
    },
    {
      "epoch": 191.13,
      "grad_norm": 0.1745070070028305,
      "learning_rate": 1.6433982101689062e-07,
      "loss": 0.0012,
      "step": 4599
    },
    {
      "epoch": 191.17,
      "grad_norm": 0.015784021466970444,
      "learning_rate": 1.6352568480485277e-07,
      "loss": 0.0002,
      "step": 4600
    },
    {
      "epoch": 191.21,
      "grad_norm": 0.02108718268573284,
      "learning_rate": 1.6271353668471657e-07,
      "loss": 0.0003,
      "step": 4601
    },
    {
      "epoch": 191.25,
      "grad_norm": 0.04756865277886391,
      "learning_rate": 1.6190337699032554e-07,
      "loss": 0.0004,
      "step": 4602
    },
    {
      "epoch": 191.29,
      "grad_norm": 0.0781262069940567,
      "learning_rate": 1.6109520605470497e-07,
      "loss": 0.0006,
      "step": 4603
    },
    {
      "epoch": 191.34,
      "grad_norm": 0.02458546869456768,
      "learning_rate": 1.6028902421006464e-07,
      "loss": 0.0003,
      "step": 4604
    },
    {
      "epoch": 191.38,
      "grad_norm": 0.05875549465417862,
      "learning_rate": 1.594848317877934e-07,
      "loss": 0.0005,
      "step": 4605
    },
    {
      "epoch": 191.42,
      "grad_norm": 0.06307608634233475,
      "learning_rate": 1.586826291184662e-07,
      "loss": 0.0004,
      "step": 4606
    },
    {
      "epoch": 191.46,
      "grad_norm": 0.022434204816818237,
      "learning_rate": 1.5788241653183768e-07,
      "loss": 0.0003,
      "step": 4607
    },
    {
      "epoch": 191.5,
      "grad_norm": 0.061443962156772614,
      "learning_rate": 1.5708419435684463e-07,
      "loss": 0.0004,
      "step": 4608
    },
    {
      "epoch": 191.54,
      "grad_norm": 0.08924088627099991,
      "learning_rate": 1.562879629216063e-07,
      "loss": 0.0007,
      "step": 4609
    },
    {
      "epoch": 191.58,
      "grad_norm": 0.035555150359869,
      "learning_rate": 1.5549372255342367e-07,
      "loss": 0.0004,
      "step": 4610
    },
    {
      "epoch": 191.63,
      "grad_norm": 0.08935681730508804,
      "learning_rate": 1.547014735787783e-07,
      "loss": 0.0006,
      "step": 4611
    },
    {
      "epoch": 191.67,
      "grad_norm": 0.0554012693464756,
      "learning_rate": 1.5391121632333473e-07,
      "loss": 0.0007,
      "step": 4612
    },
    {
      "epoch": 191.71,
      "grad_norm": 0.08242487907409668,
      "learning_rate": 1.53122951111937e-07,
      "loss": 0.0007,
      "step": 4613
    },
    {
      "epoch": 191.75,
      "grad_norm": 0.026601912453770638,
      "learning_rate": 1.5233667826861143e-07,
      "loss": 0.0003,
      "step": 4614
    },
    {
      "epoch": 191.79,
      "grad_norm": 0.03816358745098114,
      "learning_rate": 1.5155239811656562e-07,
      "loss": 0.0004,
      "step": 4615
    },
    {
      "epoch": 191.83,
      "grad_norm": 0.0036221418995410204,
      "learning_rate": 1.5077011097818729e-07,
      "loss": 0.0002,
      "step": 4616
    },
    {
      "epoch": 191.88,
      "grad_norm": 0.06601093709468842,
      "learning_rate": 1.4998981717504469e-07,
      "loss": 0.0006,
      "step": 4617
    },
    {
      "epoch": 191.92,
      "grad_norm": 0.044554706662893295,
      "learning_rate": 1.4921151702788683e-07,
      "loss": 0.0004,
      "step": 4618
    },
    {
      "epoch": 191.96,
      "grad_norm": 0.03557157516479492,
      "learning_rate": 1.4843521085664448e-07,
      "loss": 0.0005,
      "step": 4619
    },
    {
      "epoch": 192.0,
      "grad_norm": 0.14972402155399323,
      "learning_rate": 1.4766089898042678e-07,
      "loss": 0.0007,
      "step": 4620
    },
    {
      "epoch": 192.04,
      "grad_norm": 0.033036936074495316,
      "learning_rate": 1.468885817175253e-07,
      "loss": 0.0003,
      "step": 4621
    },
    {
      "epoch": 192.08,
      "grad_norm": 0.042480431497097015,
      "learning_rate": 1.4611825938540936e-07,
      "loss": 0.0004,
      "step": 4622
    },
    {
      "epoch": 192.12,
      "grad_norm": 0.024286864325404167,
      "learning_rate": 1.4534993230073013e-07,
      "loss": 0.0003,
      "step": 4623
    },
    {
      "epoch": 192.17,
      "grad_norm": 0.04995991289615631,
      "learning_rate": 1.4458360077931721e-07,
      "loss": 0.0006,
      "step": 4624
    },
    {
      "epoch": 192.21,
      "grad_norm": 0.08556213229894638,
      "learning_rate": 1.4381926513618139e-07,
      "loss": 0.0006,
      "step": 4625
    },
    {
      "epoch": 192.25,
      "grad_norm": 0.04247906059026718,
      "learning_rate": 1.4305692568551134e-07,
      "loss": 0.0004,
      "step": 4626
    },
    {
      "epoch": 192.29,
      "grad_norm": 0.043592557311058044,
      "learning_rate": 1.4229658274067694e-07,
      "loss": 0.0003,
      "step": 4627
    },
    {
      "epoch": 192.33,
      "grad_norm": 0.018578359857201576,
      "learning_rate": 1.4153823661422485e-07,
      "loss": 0.0003,
      "step": 4628
    },
    {
      "epoch": 192.37,
      "grad_norm": 0.0032822364009916782,
      "learning_rate": 1.4078188761788402e-07,
      "loss": 0.0002,
      "step": 4629
    },
    {
      "epoch": 192.42,
      "grad_norm": 0.0032645314931869507,
      "learning_rate": 1.4002753606256082e-07,
      "loss": 0.0002,
      "step": 4630
    },
    {
      "epoch": 192.46,
      "grad_norm": 0.04774810001254082,
      "learning_rate": 1.3927518225833992e-07,
      "loss": 0.0004,
      "step": 4631
    },
    {
      "epoch": 192.5,
      "grad_norm": 0.09807094931602478,
      "learning_rate": 1.3852482651448618e-07,
      "loss": 0.001,
      "step": 4632
    },
    {
      "epoch": 192.54,
      "grad_norm": 0.032861195504665375,
      "learning_rate": 1.3777646913944175e-07,
      "loss": 0.0003,
      "step": 4633
    },
    {
      "epoch": 192.58,
      "grad_norm": 0.003378249006345868,
      "learning_rate": 1.3703011044082893e-07,
      "loss": 0.0002,
      "step": 4634
    },
    {
      "epoch": 192.62,
      "grad_norm": 0.00323776388540864,
      "learning_rate": 1.362857507254478e-07,
      "loss": 0.0002,
      "step": 4635
    },
    {
      "epoch": 192.66,
      "grad_norm": 0.09145743399858475,
      "learning_rate": 1.3554339029927532e-07,
      "loss": 0.0009,
      "step": 4636
    },
    {
      "epoch": 192.71,
      "grad_norm": 0.056898344308137894,
      "learning_rate": 1.3480302946746903e-07,
      "loss": 0.0005,
      "step": 4637
    },
    {
      "epoch": 192.75,
      "grad_norm": 0.08104156702756882,
      "learning_rate": 1.3406466853436274e-07,
      "loss": 0.0008,
      "step": 4638
    },
    {
      "epoch": 192.79,
      "grad_norm": 0.0033043099101632833,
      "learning_rate": 1.333283078034686e-07,
      "loss": 0.0002,
      "step": 4639
    },
    {
      "epoch": 192.83,
      "grad_norm": 0.1271791309118271,
      "learning_rate": 1.3259394757747678e-07,
      "loss": 0.001,
      "step": 4640
    },
    {
      "epoch": 192.87,
      "grad_norm": 0.03854596987366676,
      "learning_rate": 1.3186158815825467e-07,
      "loss": 0.0003,
      "step": 4641
    },
    {
      "epoch": 192.91,
      "grad_norm": 0.06878644227981567,
      "learning_rate": 1.3113122984684869e-07,
      "loss": 0.0005,
      "step": 4642
    },
    {
      "epoch": 192.96,
      "grad_norm": 0.04265411198139191,
      "learning_rate": 1.3040287294348032e-07,
      "loss": 0.0003,
      "step": 4643
    },
    {
      "epoch": 193.0,
      "grad_norm": 0.170793816447258,
      "learning_rate": 1.2967651774755065e-07,
      "loss": 0.0011,
      "step": 4644
    },
    {
      "epoch": 193.04,
      "grad_norm": 0.044725172221660614,
      "learning_rate": 1.2895216455763582e-07,
      "loss": 0.0004,
      "step": 4645
    },
    {
      "epoch": 193.08,
      "grad_norm": 0.02593834511935711,
      "learning_rate": 1.2822981367149102e-07,
      "loss": 0.0003,
      "step": 4646
    },
    {
      "epoch": 193.12,
      "grad_norm": 0.0468556210398674,
      "learning_rate": 1.2750946538604702e-07,
      "loss": 0.0004,
      "step": 4647
    },
    {
      "epoch": 193.16,
      "grad_norm": 0.3975231349468231,
      "learning_rate": 1.26791119997412e-07,
      "loss": 0.0019,
      "step": 4648
    },
    {
      "epoch": 193.21,
      "grad_norm": 0.038316335529088974,
      "learning_rate": 1.2607477780086974e-07,
      "loss": 0.0003,
      "step": 4649
    },
    {
      "epoch": 193.25,
      "grad_norm": 0.06182920187711716,
      "learning_rate": 1.253604390908819e-07,
      "loss": 0.0005,
      "step": 4650
    },
    {
      "epoch": 193.29,
      "grad_norm": 0.06954608857631683,
      "learning_rate": 1.2464810416108698e-07,
      "loss": 0.0006,
      "step": 4651
    },
    {
      "epoch": 193.33,
      "grad_norm": 0.05385799705982208,
      "learning_rate": 1.2393777330429791e-07,
      "loss": 0.0004,
      "step": 4652
    },
    {
      "epoch": 193.37,
      "grad_norm": 0.03104325756430626,
      "learning_rate": 1.2322944681250503e-07,
      "loss": 0.0004,
      "step": 4653
    },
    {
      "epoch": 193.41,
      "grad_norm": 0.07202994078397751,
      "learning_rate": 1.225231249768749e-07,
      "loss": 0.0006,
      "step": 4654
    },
    {
      "epoch": 193.45,
      "grad_norm": 0.07120415568351746,
      "learning_rate": 1.2181880808775026e-07,
      "loss": 0.0006,
      "step": 4655
    },
    {
      "epoch": 193.5,
      "grad_norm": 0.024042995646595955,
      "learning_rate": 1.2111649643464785e-07,
      "loss": 0.0003,
      "step": 4656
    },
    {
      "epoch": 193.54,
      "grad_norm": 0.04174679517745972,
      "learning_rate": 1.2041619030626283e-07,
      "loss": 0.0004,
      "step": 4657
    },
    {
      "epoch": 193.58,
      "grad_norm": 0.08568542450666428,
      "learning_rate": 1.1971788999046385e-07,
      "loss": 0.0007,
      "step": 4658
    },
    {
      "epoch": 193.62,
      "grad_norm": 0.0034152455627918243,
      "learning_rate": 1.1902159577429629e-07,
      "loss": 0.0002,
      "step": 4659
    },
    {
      "epoch": 193.66,
      "grad_norm": 0.04196679964661598,
      "learning_rate": 1.1832730794397951e-07,
      "loss": 0.0003,
      "step": 4660
    },
    {
      "epoch": 193.7,
      "grad_norm": 0.05204809084534645,
      "learning_rate": 1.1763502678490968e-07,
      "loss": 0.0004,
      "step": 4661
    },
    {
      "epoch": 193.75,
      "grad_norm": 0.031892284750938416,
      "learning_rate": 1.1694475258165749e-07,
      "loss": 0.0003,
      "step": 4662
    },
    {
      "epoch": 193.79,
      "grad_norm": 0.02556130662560463,
      "learning_rate": 1.1625648561796765e-07,
      "loss": 0.0003,
      "step": 4663
    },
    {
      "epoch": 193.83,
      "grad_norm": 0.030771447345614433,
      "learning_rate": 1.1557022617676217e-07,
      "loss": 0.0003,
      "step": 4664
    },
    {
      "epoch": 193.87,
      "grad_norm": 0.07274086773395538,
      "learning_rate": 1.1488597454013539e-07,
      "loss": 0.0005,
      "step": 4665
    },
    {
      "epoch": 193.91,
      "grad_norm": 0.08410806208848953,
      "learning_rate": 1.1420373098935733e-07,
      "loss": 0.0006,
      "step": 4666
    },
    {
      "epoch": 193.95,
      "grad_norm": 0.04657335206866264,
      "learning_rate": 1.135234958048731e-07,
      "loss": 0.0012,
      "step": 4667
    },
    {
      "epoch": 193.99,
      "grad_norm": 0.0368887223303318,
      "learning_rate": 1.1284526926630124e-07,
      "loss": 0.0003,
      "step": 4668
    },
    {
      "epoch": 194.04,
      "grad_norm": 0.053351666778326035,
      "learning_rate": 1.1216905165243542e-07,
      "loss": 0.0005,
      "step": 4669
    },
    {
      "epoch": 194.08,
      "grad_norm": 0.0034016529098153114,
      "learning_rate": 1.1149484324124326e-07,
      "loss": 0.0002,
      "step": 4670
    },
    {
      "epoch": 194.12,
      "grad_norm": 0.03060990571975708,
      "learning_rate": 1.1082264430986533e-07,
      "loss": 0.0003,
      "step": 4671
    },
    {
      "epoch": 194.16,
      "grad_norm": 0.046749718487262726,
      "learning_rate": 1.1015245513461837e-07,
      "loss": 0.0004,
      "step": 4672
    },
    {
      "epoch": 194.2,
      "grad_norm": 0.035473112016916275,
      "learning_rate": 1.0948427599099143e-07,
      "loss": 0.0004,
      "step": 4673
    },
    {
      "epoch": 194.24,
      "grad_norm": 0.092288076877594,
      "learning_rate": 1.088181071536476e-07,
      "loss": 0.0008,
      "step": 4674
    },
    {
      "epoch": 194.29,
      "grad_norm": 0.049582406878471375,
      "learning_rate": 1.0815394889642339e-07,
      "loss": 0.0003,
      "step": 4675
    },
    {
      "epoch": 194.33,
      "grad_norm": 0.05207466706633568,
      "learning_rate": 1.0749180149233041e-07,
      "loss": 0.0005,
      "step": 4676
    },
    {
      "epoch": 194.37,
      "grad_norm": 0.1119236946105957,
      "learning_rate": 1.0683166521355093e-07,
      "loss": 0.0007,
      "step": 4677
    },
    {
      "epoch": 194.41,
      "grad_norm": 0.03810373693704605,
      "learning_rate": 1.0617354033144289e-07,
      "loss": 0.0003,
      "step": 4678
    },
    {
      "epoch": 194.45,
      "grad_norm": 0.09454295784235,
      "learning_rate": 1.0551742711653656e-07,
      "loss": 0.0005,
      "step": 4679
    },
    {
      "epoch": 194.49,
      "grad_norm": 0.07865139842033386,
      "learning_rate": 1.0486332583853565e-07,
      "loss": 0.0004,
      "step": 4680
    },
    {
      "epoch": 194.54,
      "grad_norm": 0.059514280408620834,
      "learning_rate": 1.0421123676631562e-07,
      "loss": 0.0006,
      "step": 4681
    },
    {
      "epoch": 194.58,
      "grad_norm": 0.05050773173570633,
      "learning_rate": 1.0356116016792594e-07,
      "loss": 0.0006,
      "step": 4682
    },
    {
      "epoch": 194.62,
      "grad_norm": 0.06898639351129532,
      "learning_rate": 1.0291309631058899e-07,
      "loss": 0.0006,
      "step": 4683
    },
    {
      "epoch": 194.66,
      "grad_norm": 0.026973087340593338,
      "learning_rate": 1.0226704546069832e-07,
      "loss": 0.0003,
      "step": 4684
    },
    {
      "epoch": 194.7,
      "grad_norm": 0.08217272907495499,
      "learning_rate": 1.0162300788382263e-07,
      "loss": 0.001,
      "step": 4685
    },
    {
      "epoch": 194.74,
      "grad_norm": 0.11811311542987823,
      "learning_rate": 1.0098098384469957e-07,
      "loss": 0.0006,
      "step": 4686
    },
    {
      "epoch": 194.78,
      "grad_norm": 0.08268722146749496,
      "learning_rate": 1.0034097360724249e-07,
      "loss": 0.0006,
      "step": 4687
    },
    {
      "epoch": 194.83,
      "grad_norm": 0.053175561130046844,
      "learning_rate": 9.970297743453484e-08,
      "loss": 0.0003,
      "step": 4688
    },
    {
      "epoch": 194.87,
      "grad_norm": 0.05191212520003319,
      "learning_rate": 9.906699558883238e-08,
      "loss": 0.0005,
      "step": 4689
    },
    {
      "epoch": 194.91,
      "grad_norm": 0.1071222797036171,
      "learning_rate": 9.843302833156377e-08,
      "loss": 0.0007,
      "step": 4690
    },
    {
      "epoch": 194.95,
      "grad_norm": 0.02409466542303562,
      "learning_rate": 9.780107592332944e-08,
      "loss": 0.0003,
      "step": 4691
    },
    {
      "epoch": 194.99,
      "grad_norm": 0.04528724402189255,
      "learning_rate": 9.717113862389993e-08,
      "loss": 0.0004,
      "step": 4692
    },
    {
      "epoch": 195.03,
      "grad_norm": 0.06789017468690872,
      "learning_rate": 9.654321669221978e-08,
      "loss": 0.0005,
      "step": 4693
    },
    {
      "epoch": 195.08,
      "grad_norm": 0.0032665685284882784,
      "learning_rate": 9.591731038640306e-08,
      "loss": 0.0002,
      "step": 4694
    },
    {
      "epoch": 195.12,
      "grad_norm": 0.040042944252491,
      "learning_rate": 9.529341996373675e-08,
      "loss": 0.0003,
      "step": 4695
    },
    {
      "epoch": 195.16,
      "grad_norm": 0.15913844108581543,
      "learning_rate": 9.467154568067849e-08,
      "loss": 0.0008,
      "step": 4696
    },
    {
      "epoch": 195.2,
      "grad_norm": 0.06970717012882233,
      "learning_rate": 9.405168779285712e-08,
      "loss": 0.0006,
      "step": 4697
    },
    {
      "epoch": 195.24,
      "grad_norm": 0.053441017866134644,
      "learning_rate": 9.343384655507326e-08,
      "loss": 0.0006,
      "step": 4698
    },
    {
      "epoch": 195.28,
      "grad_norm": 0.050596557557582855,
      "learning_rate": 9.281802222129766e-08,
      "loss": 0.0005,
      "step": 4699
    },
    {
      "epoch": 195.32,
      "grad_norm": 0.03262658044695854,
      "learning_rate": 9.22042150446728e-08,
      "loss": 0.0004,
      "step": 4700
    },
    {
      "epoch": 195.37,
      "grad_norm": 0.03962549567222595,
      "learning_rate": 9.159242527751245e-08,
      "loss": 0.0004,
      "step": 4701
    },
    {
      "epoch": 195.41,
      "grad_norm": 0.04461977630853653,
      "learning_rate": 9.098265317129817e-08,
      "loss": 0.0003,
      "step": 4702
    },
    {
      "epoch": 195.45,
      "grad_norm": 0.07266664505004883,
      "learning_rate": 9.037489897668561e-08,
      "loss": 0.0007,
      "step": 4703
    },
    {
      "epoch": 195.49,
      "grad_norm": 0.06540629267692566,
      "learning_rate": 8.976916294349935e-08,
      "loss": 0.0004,
      "step": 4704
    },
    {
      "epoch": 195.53,
      "grad_norm": 0.05682792142033577,
      "learning_rate": 8.916544532073413e-08,
      "loss": 0.0005,
      "step": 4705
    },
    {
      "epoch": 195.57,
      "grad_norm": 0.04808447137475014,
      "learning_rate": 8.856374635655696e-08,
      "loss": 0.0005,
      "step": 4706
    },
    {
      "epoch": 195.62,
      "grad_norm": 0.08199890702962875,
      "learning_rate": 8.796406629830168e-08,
      "loss": 0.0007,
      "step": 4707
    },
    {
      "epoch": 195.66,
      "grad_norm": 0.01645551808178425,
      "learning_rate": 8.736640539247498e-08,
      "loss": 0.0003,
      "step": 4708
    },
    {
      "epoch": 195.7,
      "grad_norm": 0.08623120188713074,
      "learning_rate": 8.677076388475314e-08,
      "loss": 0.0008,
      "step": 4709
    },
    {
      "epoch": 195.74,
      "grad_norm": 0.045988697558641434,
      "learning_rate": 8.617714201998084e-08,
      "loss": 0.0004,
      "step": 4710
    },
    {
      "epoch": 195.78,
      "grad_norm": 0.04944803565740585,
      "learning_rate": 8.5585540042174e-08,
      "loss": 0.0005,
      "step": 4711
    },
    {
      "epoch": 195.82,
      "grad_norm": 0.02821500226855278,
      "learning_rate": 8.499595819451811e-08,
      "loss": 0.0003,
      "step": 4712
    },
    {
      "epoch": 195.86,
      "grad_norm": 0.0034249236341565847,
      "learning_rate": 8.440839671936819e-08,
      "loss": 0.0002,
      "step": 4713
    },
    {
      "epoch": 195.91,
      "grad_norm": 0.04669228568673134,
      "learning_rate": 8.382285585824879e-08,
      "loss": 0.0006,
      "step": 4714
    },
    {
      "epoch": 195.95,
      "grad_norm": 0.024590857326984406,
      "learning_rate": 8.323933585185184e-08,
      "loss": 0.0003,
      "step": 4715
    },
    {
      "epoch": 195.99,
      "grad_norm": 0.04841051623225212,
      "learning_rate": 8.265783694004214e-08,
      "loss": 0.0004,
      "step": 4716
    },
    {
      "epoch": 196.03,
      "grad_norm": 0.020963143557310104,
      "learning_rate": 8.207835936185182e-08,
      "loss": 0.0003,
      "step": 4717
    },
    {
      "epoch": 196.07,
      "grad_norm": 0.045973293483257294,
      "learning_rate": 8.150090335548144e-08,
      "loss": 0.0003,
      "step": 4718
    },
    {
      "epoch": 196.11,
      "grad_norm": 0.05154966190457344,
      "learning_rate": 8.092546915830168e-08,
      "loss": 0.0007,
      "step": 4719
    },
    {
      "epoch": 196.16,
      "grad_norm": 0.0031886317301541567,
      "learning_rate": 8.035205700685167e-08,
      "loss": 0.0002,
      "step": 4720
    },
    {
      "epoch": 196.2,
      "grad_norm": 0.03374527394771576,
      "learning_rate": 7.978066713684008e-08,
      "loss": 0.0003,
      "step": 4721
    },
    {
      "epoch": 196.24,
      "grad_norm": 0.04092267155647278,
      "learning_rate": 7.92112997831429e-08,
      "loss": 0.0004,
      "step": 4722
    },
    {
      "epoch": 196.28,
      "grad_norm": 0.07248379290103912,
      "learning_rate": 7.864395517980627e-08,
      "loss": 0.0006,
      "step": 4723
    },
    {
      "epoch": 196.32,
      "grad_norm": 0.045836225152015686,
      "learning_rate": 7.80786335600442e-08,
      "loss": 0.0004,
      "step": 4724
    },
    {
      "epoch": 196.36,
      "grad_norm": 0.07311774045228958,
      "learning_rate": 7.7515335156238e-08,
      "loss": 0.0006,
      "step": 4725
    },
    {
      "epoch": 196.41,
      "grad_norm": 0.04401194676756859,
      "learning_rate": 7.695406019993912e-08,
      "loss": 0.0004,
      "step": 4726
    },
    {
      "epoch": 196.45,
      "grad_norm": 0.023081764578819275,
      "learning_rate": 7.639480892186634e-08,
      "loss": 0.0003,
      "step": 4727
    },
    {
      "epoch": 196.49,
      "grad_norm": 0.041748810559511185,
      "learning_rate": 7.58375815519069e-08,
      "loss": 0.0004,
      "step": 4728
    },
    {
      "epoch": 196.53,
      "grad_norm": 0.07320411503314972,
      "learning_rate": 7.528237831911588e-08,
      "loss": 0.0006,
      "step": 4729
    },
    {
      "epoch": 196.57,
      "grad_norm": 0.09873045235872269,
      "learning_rate": 7.47291994517163e-08,
      "loss": 0.0009,
      "step": 4730
    },
    {
      "epoch": 196.61,
      "grad_norm": 0.07442019134759903,
      "learning_rate": 7.417804517709903e-08,
      "loss": 0.0008,
      "step": 4731
    },
    {
      "epoch": 196.65,
      "grad_norm": 0.09433631598949432,
      "learning_rate": 7.362891572182284e-08,
      "loss": 0.0011,
      "step": 4732
    },
    {
      "epoch": 196.7,
      "grad_norm": 0.0036561954766511917,
      "learning_rate": 7.308181131161385e-08,
      "loss": 0.0002,
      "step": 4733
    },
    {
      "epoch": 196.74,
      "grad_norm": 0.003387393895536661,
      "learning_rate": 7.253673217136659e-08,
      "loss": 0.0002,
      "step": 4734
    },
    {
      "epoch": 196.78,
      "grad_norm": 0.06084393337368965,
      "learning_rate": 7.199367852514239e-08,
      "loss": 0.0005,
      "step": 4735
    },
    {
      "epoch": 196.82,
      "grad_norm": 0.09039139747619629,
      "learning_rate": 7.145265059616934e-08,
      "loss": 0.0009,
      "step": 4736
    },
    {
      "epoch": 196.86,
      "grad_norm": 0.02283559925854206,
      "learning_rate": 7.091364860684403e-08,
      "loss": 0.0003,
      "step": 4737
    },
    {
      "epoch": 196.9,
      "grad_norm": 0.021562160924077034,
      "learning_rate": 7.037667277873029e-08,
      "loss": 0.0003,
      "step": 4738
    },
    {
      "epoch": 196.95,
      "grad_norm": 0.033629268407821655,
      "learning_rate": 6.984172333255824e-08,
      "loss": 0.0003,
      "step": 4739
    },
    {
      "epoch": 196.99,
      "grad_norm": 0.003271044697612524,
      "learning_rate": 6.930880048822531e-08,
      "loss": 0.0002,
      "step": 4740
    },
    {
      "epoch": 197.03,
      "grad_norm": 0.003636828390881419,
      "learning_rate": 6.87779044647957e-08,
      "loss": 0.0002,
      "step": 4741
    },
    {
      "epoch": 197.07,
      "grad_norm": 0.04885892570018768,
      "learning_rate": 6.824903548050098e-08,
      "loss": 0.0005,
      "step": 4742
    },
    {
      "epoch": 197.11,
      "grad_norm": 0.03977024182677269,
      "learning_rate": 6.772219375273947e-08,
      "loss": 0.0003,
      "step": 4743
    },
    {
      "epoch": 197.15,
      "grad_norm": 0.16176311671733856,
      "learning_rate": 6.71973794980757e-08,
      "loss": 0.0012,
      "step": 4744
    },
    {
      "epoch": 197.19,
      "grad_norm": 0.0616198293864727,
      "learning_rate": 6.667459293224155e-08,
      "loss": 0.0004,
      "step": 4745
    },
    {
      "epoch": 197.24,
      "grad_norm": 0.03904493898153305,
      "learning_rate": 6.615383427013345e-08,
      "loss": 0.0004,
      "step": 4746
    },
    {
      "epoch": 197.28,
      "grad_norm": 0.0034216041676700115,
      "learning_rate": 6.563510372581682e-08,
      "loss": 0.0002,
      "step": 4747
    },
    {
      "epoch": 197.32,
      "grad_norm": 0.0033297499176114798,
      "learning_rate": 6.511840151252169e-08,
      "loss": 0.0002,
      "step": 4748
    },
    {
      "epoch": 197.36,
      "grad_norm": 0.0867147371172905,
      "learning_rate": 6.460372784264479e-08,
      "loss": 0.0006,
      "step": 4749
    },
    {
      "epoch": 197.4,
      "grad_norm": 0.045613404363393784,
      "learning_rate": 6.409108292774912e-08,
      "loss": 0.0003,
      "step": 4750
    },
    {
      "epoch": 197.44,
      "grad_norm": 0.003666400210931897,
      "learning_rate": 6.358046697856446e-08,
      "loss": 0.0002,
      "step": 4751
    },
    {
      "epoch": 197.49,
      "grad_norm": 0.06259050965309143,
      "learning_rate": 6.307188020498401e-08,
      "loss": 0.0006,
      "step": 4752
    },
    {
      "epoch": 197.53,
      "grad_norm": 0.020158126950263977,
      "learning_rate": 6.256532281606997e-08,
      "loss": 0.0004,
      "step": 4753
    },
    {
      "epoch": 197.57,
      "grad_norm": 0.042516645044088364,
      "learning_rate": 6.206079502004803e-08,
      "loss": 0.0003,
      "step": 4754
    },
    {
      "epoch": 197.61,
      "grad_norm": 0.05014277622103691,
      "learning_rate": 6.15582970243117e-08,
      "loss": 0.0005,
      "step": 4755
    },
    {
      "epoch": 197.65,
      "grad_norm": 0.0684039518237114,
      "learning_rate": 6.105782903541746e-08,
      "loss": 0.0006,
      "step": 4756
    },
    {
      "epoch": 197.69,
      "grad_norm": 0.029150957241654396,
      "learning_rate": 6.055939125909016e-08,
      "loss": 0.0003,
      "step": 4757
    },
    {
      "epoch": 197.74,
      "grad_norm": 0.10475950688123703,
      "learning_rate": 6.006298390021814e-08,
      "loss": 0.0007,
      "step": 4758
    },
    {
      "epoch": 197.78,
      "grad_norm": 0.04778231307864189,
      "learning_rate": 5.95686071628554e-08,
      "loss": 0.0004,
      "step": 4759
    },
    {
      "epoch": 197.82,
      "grad_norm": 0.04832446575164795,
      "learning_rate": 5.907626125022159e-08,
      "loss": 0.0006,
      "step": 4760
    },
    {
      "epoch": 197.86,
      "grad_norm": 0.10037324577569962,
      "learning_rate": 5.858594636470205e-08,
      "loss": 0.0008,
      "step": 4761
    },
    {
      "epoch": 197.9,
      "grad_norm": 0.03680458664894104,
      "learning_rate": 5.809766270784667e-08,
      "loss": 0.0004,
      "step": 4762
    },
    {
      "epoch": 197.94,
      "grad_norm": 0.03184894844889641,
      "learning_rate": 5.761141048036989e-08,
      "loss": 0.0003,
      "step": 4763
    },
    {
      "epoch": 197.98,
      "grad_norm": 0.052101850509643555,
      "learning_rate": 5.712718988215182e-08,
      "loss": 0.0005,
      "step": 4764
    },
    {
      "epoch": 198.03,
      "grad_norm": 0.044745273888111115,
      "learning_rate": 5.6645001112237694e-08,
      "loss": 0.0004,
      "step": 4765
    },
    {
      "epoch": 198.07,
      "grad_norm": 0.043630510568618774,
      "learning_rate": 5.616484436883618e-08,
      "loss": 0.0003,
      "step": 4766
    },
    {
      "epoch": 198.11,
      "grad_norm": 0.028516234830021858,
      "learning_rate": 5.568671984932272e-08,
      "loss": 0.0003,
      "step": 4767
    },
    {
      "epoch": 198.15,
      "grad_norm": 0.02299662120640278,
      "learning_rate": 5.521062775023567e-08,
      "loss": 0.0003,
      "step": 4768
    },
    {
      "epoch": 198.19,
      "grad_norm": 0.0033049474004656076,
      "learning_rate": 5.4736568267278464e-08,
      "loss": 0.0002,
      "step": 4769
    },
    {
      "epoch": 198.23,
      "grad_norm": 0.02809683233499527,
      "learning_rate": 5.426454159531913e-08,
      "loss": 0.0003,
      "step": 4770
    },
    {
      "epoch": 198.28,
      "grad_norm": 0.10478243976831436,
      "learning_rate": 5.379454792838967e-08,
      "loss": 0.0009,
      "step": 4771
    },
    {
      "epoch": 198.32,
      "grad_norm": 0.056589506566524506,
      "learning_rate": 5.3326587459687774e-08,
      "loss": 0.0005,
      "step": 4772
    },
    {
      "epoch": 198.36,
      "grad_norm": 0.03314366936683655,
      "learning_rate": 5.2860660381572894e-08,
      "loss": 0.0003,
      "step": 4773
    },
    {
      "epoch": 198.4,
      "grad_norm": 0.024352990090847015,
      "learning_rate": 5.239676688557072e-08,
      "loss": 0.0003,
      "step": 4774
    },
    {
      "epoch": 198.44,
      "grad_norm": 0.030254017561674118,
      "learning_rate": 5.1934907162370374e-08,
      "loss": 0.0004,
      "step": 4775
    },
    {
      "epoch": 198.48,
      "grad_norm": 0.003451648633927107,
      "learning_rate": 5.1475081401825553e-08,
      "loss": 0.0002,
      "step": 4776
    },
    {
      "epoch": 198.52,
      "grad_norm": 0.039756275713443756,
      "learning_rate": 5.101728979295173e-08,
      "loss": 0.0007,
      "step": 4777
    },
    {
      "epoch": 198.57,
      "grad_norm": 0.06304215639829636,
      "learning_rate": 5.05615325239317e-08,
      "loss": 0.0006,
      "step": 4778
    },
    {
      "epoch": 198.61,
      "grad_norm": 0.06666299700737,
      "learning_rate": 5.0107809782108384e-08,
      "loss": 0.0006,
      "step": 4779
    },
    {
      "epoch": 198.65,
      "grad_norm": 0.05020519718527794,
      "learning_rate": 4.9656121753990924e-08,
      "loss": 0.0005,
      "step": 4780
    },
    {
      "epoch": 198.69,
      "grad_norm": 0.08772388845682144,
      "learning_rate": 4.9206468625250804e-08,
      "loss": 0.0008,
      "step": 4781
    },
    {
      "epoch": 198.73,
      "grad_norm": 0.035984303802251816,
      "learning_rate": 4.875885058072349e-08,
      "loss": 0.0003,
      "step": 4782
    },
    {
      "epoch": 198.77,
      "grad_norm": 0.03965543583035469,
      "learning_rate": 4.8313267804407926e-08,
      "loss": 0.0004,
      "step": 4783
    },
    {
      "epoch": 198.82,
      "grad_norm": 0.04956541210412979,
      "learning_rate": 4.7869720479466475e-08,
      "loss": 0.0005,
      "step": 4784
    },
    {
      "epoch": 198.86,
      "grad_norm": 0.08053681999444962,
      "learning_rate": 4.742820878822496e-08,
      "loss": 0.0006,
      "step": 4785
    },
    {
      "epoch": 198.9,
      "grad_norm": 0.06492030620574951,
      "learning_rate": 4.698873291217154e-08,
      "loss": 0.0006,
      "step": 4786
    },
    {
      "epoch": 198.94,
      "grad_norm": 0.035671260207891464,
      "learning_rate": 4.6551293031958376e-08,
      "loss": 0.0003,
      "step": 4787
    },
    {
      "epoch": 198.98,
      "grad_norm": 0.0695185661315918,
      "learning_rate": 4.611588932740107e-08,
      "loss": 0.0006,
      "step": 4788
    },
    {
      "epoch": 199.02,
      "grad_norm": 0.0035846217069774866,
      "learning_rate": 4.568252197747647e-08,
      "loss": 0.0002,
      "step": 4789
    },
    {
      "epoch": 199.06,
      "grad_norm": 0.05363362282514572,
      "learning_rate": 4.52511911603265e-08,
      "loss": 0.0004,
      "step": 4790
    },
    {
      "epoch": 199.11,
      "grad_norm": 0.03964477404952049,
      "learning_rate": 4.482189705325435e-08,
      "loss": 0.0003,
      "step": 4791
    },
    {
      "epoch": 199.15,
      "grad_norm": 0.07516462355852127,
      "learning_rate": 4.439463983272663e-08,
      "loss": 0.001,
      "step": 4792
    },
    {
      "epoch": 199.19,
      "grad_norm": 0.056569620966911316,
      "learning_rate": 4.3969419674373407e-08,
      "loss": 0.0004,
      "step": 4793
    },
    {
      "epoch": 199.23,
      "grad_norm": 0.18481677770614624,
      "learning_rate": 4.354623675298597e-08,
      "loss": 0.0011,
      "step": 4794
    },
    {
      "epoch": 199.27,
      "grad_norm": 0.057552535086870193,
      "learning_rate": 4.312509124251907e-08,
      "loss": 0.0005,
      "step": 4795
    },
    {
      "epoch": 199.31,
      "grad_norm": 0.05272912606596947,
      "learning_rate": 4.270598331608977e-08,
      "loss": 0.0005,
      "step": 4796
    },
    {
      "epoch": 199.36,
      "grad_norm": 0.05411137640476227,
      "learning_rate": 4.228891314597694e-08,
      "loss": 0.0004,
      "step": 4797
    },
    {
      "epoch": 199.4,
      "grad_norm": 0.05113930627703667,
      "learning_rate": 4.187388090362288e-08,
      "loss": 0.0003,
      "step": 4798
    },
    {
      "epoch": 199.44,
      "grad_norm": 0.025669174268841743,
      "learning_rate": 4.146088675963167e-08,
      "loss": 0.0003,
      "step": 4799
    },
    {
      "epoch": 199.48,
      "grad_norm": 0.020582087337970734,
      "learning_rate": 4.104993088376974e-08,
      "loss": 0.0003,
      "step": 4800
    },
    {
      "epoch": 199.52,
      "grad_norm": 0.057547323405742645,
      "learning_rate": 4.0641013444965294e-08,
      "loss": 0.0005,
      "step": 4801
    },
    {
      "epoch": 199.56,
      "grad_norm": 0.04415301978588104,
      "learning_rate": 4.0234134611308315e-08,
      "loss": 0.0004,
      "step": 4802
    },
    {
      "epoch": 199.61,
      "grad_norm": 0.16007673740386963,
      "learning_rate": 3.9829294550052244e-08,
      "loss": 0.0011,
      "step": 4803
    },
    {
      "epoch": 199.65,
      "grad_norm": 0.05221748352050781,
      "learning_rate": 3.9426493427611177e-08,
      "loss": 0.0004,
      "step": 4804
    },
    {
      "epoch": 199.69,
      "grad_norm": 0.031424008309841156,
      "learning_rate": 3.902573140956101e-08,
      "loss": 0.0003,
      "step": 4805
    },
    {
      "epoch": 199.73,
      "grad_norm": 0.003294882131740451,
      "learning_rate": 3.862700866064051e-08,
      "loss": 0.0002,
      "step": 4806
    },
    {
      "epoch": 199.77,
      "grad_norm": 0.003214283613488078,
      "learning_rate": 3.823032534474969e-08,
      "loss": 0.0002,
      "step": 4807
    },
    {
      "epoch": 199.81,
      "grad_norm": 0.03850501403212547,
      "learning_rate": 3.7835681624949216e-08,
      "loss": 0.0004,
      "step": 4808
    },
    {
      "epoch": 199.85,
      "grad_norm": 0.013773230835795403,
      "learning_rate": 3.744307766346267e-08,
      "loss": 0.0002,
      "step": 4809
    },
    {
      "epoch": 199.9,
      "grad_norm": 0.003660533344373107,
      "learning_rate": 3.705251362167484e-08,
      "loss": 0.0002,
      "step": 4810
    },
    {
      "epoch": 199.94,
      "grad_norm": 0.05147537216544151,
      "learning_rate": 3.666398966013229e-08,
      "loss": 0.0004,
      "step": 4811
    },
    {
      "epoch": 199.98,
      "grad_norm": 0.050710029900074005,
      "learning_rate": 3.6277505938541735e-08,
      "loss": 0.0008,
      "step": 4812
    },
    {
      "epoch": 200.02,
      "grad_norm": 0.05676877871155739,
      "learning_rate": 3.589306261577219e-08,
      "loss": 0.0005,
      "step": 4813
    },
    {
      "epoch": 200.06,
      "grad_norm": 0.0225780438631773,
      "learning_rate": 3.5510659849853355e-08,
      "loss": 0.0003,
      "step": 4814
    },
    {
      "epoch": 200.1,
      "grad_norm": 0.03546389564871788,
      "learning_rate": 3.513029779797783e-08,
      "loss": 0.0004,
      "step": 4815
    },
    {
      "epoch": 200.15,
      "grad_norm": 0.1436503678560257,
      "learning_rate": 3.475197661649665e-08,
      "loss": 0.0008,
      "step": 4816
    },
    {
      "epoch": 200.19,
      "grad_norm": 0.03297566622495651,
      "learning_rate": 3.437569646092487e-08,
      "loss": 0.0003,
      "step": 4817
    },
    {
      "epoch": 200.23,
      "grad_norm": 0.03598206490278244,
      "learning_rate": 3.400145748593542e-08,
      "loss": 0.0004,
      "step": 4818
    },
    {
      "epoch": 200.27,
      "grad_norm": 0.048618461936712265,
      "learning_rate": 3.362925984536525e-08,
      "loss": 0.0005,
      "step": 4819
    },
    {
      "epoch": 200.31,
      "grad_norm": 0.08180452138185501,
      "learning_rate": 3.325910369220975e-08,
      "loss": 0.0006,
      "step": 4820
    },
    {
      "epoch": 200.35,
      "grad_norm": 0.03502446040511131,
      "learning_rate": 3.289098917862721e-08,
      "loss": 0.0004,
      "step": 4821
    },
    {
      "epoch": 200.39,
      "grad_norm": 0.14304278790950775,
      "learning_rate": 3.2524916455934365e-08,
      "loss": 0.0012,
      "step": 4822
    },
    {
      "epoch": 200.44,
      "grad_norm": 0.03357439860701561,
      "learning_rate": 3.2160885674610284e-08,
      "loss": 0.0003,
      "step": 4823
    },
    {
      "epoch": 200.48,
      "grad_norm": 0.0033257927279919386,
      "learning_rate": 3.179889698429473e-08,
      "loss": 0.0002,
      "step": 4824
    },
    {
      "epoch": 200.52,
      "grad_norm": 0.052050087600946426,
      "learning_rate": 3.143895053378698e-08,
      "loss": 0.0006,
      "step": 4825
    },
    {
      "epoch": 200.56,
      "grad_norm": 0.0588911697268486,
      "learning_rate": 3.108104647104815e-08,
      "loss": 0.0005,
      "step": 4826
    },
    {
      "epoch": 200.6,
      "grad_norm": 0.06161995604634285,
      "learning_rate": 3.0725184943198316e-08,
      "loss": 0.0004,
      "step": 4827
    },
    {
      "epoch": 200.64,
      "grad_norm": 0.05983975529670715,
      "learning_rate": 3.037136609651881e-08,
      "loss": 0.0006,
      "step": 4828
    },
    {
      "epoch": 200.69,
      "grad_norm": 0.05849035829305649,
      "learning_rate": 3.001959007645161e-08,
      "loss": 0.0005,
      "step": 4829
    },
    {
      "epoch": 200.73,
      "grad_norm": 0.0033254872541874647,
      "learning_rate": 2.966985702759828e-08,
      "loss": 0.0002,
      "step": 4830
    },
    {
      "epoch": 200.77,
      "grad_norm": 0.06738661229610443,
      "learning_rate": 2.9322167093721598e-08,
      "loss": 0.0006,
      "step": 4831
    },
    {
      "epoch": 200.81,
      "grad_norm": 0.05681668221950531,
      "learning_rate": 2.8976520417742794e-08,
      "loss": 0.0005,
      "step": 4832
    },
    {
      "epoch": 200.85,
      "grad_norm": 0.04762089252471924,
      "learning_rate": 2.8632917141743766e-08,
      "loss": 0.0004,
      "step": 4833
    },
    {
      "epoch": 200.89,
      "grad_norm": 0.10926763713359833,
      "learning_rate": 2.82913574069682e-08,
      "loss": 0.0008,
      "step": 4834
    },
    {
      "epoch": 200.94,
      "grad_norm": 0.055369168519973755,
      "learning_rate": 2.7951841353817676e-08,
      "loss": 0.0004,
      "step": 4835
    },
    {
      "epoch": 200.98,
      "grad_norm": 0.15498165786266327,
      "learning_rate": 2.7614369121854444e-08,
      "loss": 0.0008,
      "step": 4836
    },
    {
      "epoch": 201.02,
      "grad_norm": 0.06607407331466675,
      "learning_rate": 2.7278940849800316e-08,
      "loss": 0.0006,
      "step": 4837
    },
    {
      "epoch": 201.06,
      "grad_norm": 0.047675903886556625,
      "learning_rate": 2.6945556675537776e-08,
      "loss": 0.0004,
      "step": 4838
    },
    {
      "epoch": 201.1,
      "grad_norm": 0.003389863297343254,
      "learning_rate": 2.661421673610831e-08,
      "loss": 0.0002,
      "step": 4839
    },
    {
      "epoch": 201.14,
      "grad_norm": 0.0855761170387268,
      "learning_rate": 2.6284921167712975e-08,
      "loss": 0.001,
      "step": 4840
    },
    {
      "epoch": 201.18,
      "grad_norm": 0.08907513320446014,
      "learning_rate": 2.595767010571293e-08,
      "loss": 0.0007,
      "step": 4841
    },
    {
      "epoch": 201.23,
      "grad_norm": 0.0034917884040623903,
      "learning_rate": 2.5632463684628904e-08,
      "loss": 0.0002,
      "step": 4842
    },
    {
      "epoch": 201.27,
      "grad_norm": 0.0036305526737123728,
      "learning_rate": 2.530930203814064e-08,
      "loss": 0.0002,
      "step": 4843
    },
    {
      "epoch": 201.31,
      "grad_norm": 0.10976672172546387,
      "learning_rate": 2.4988185299087973e-08,
      "loss": 0.0009,
      "step": 4844
    },
    {
      "epoch": 201.35,
      "grad_norm": 0.12615932524204254,
      "learning_rate": 2.4669113599469774e-08,
      "loss": 0.0008,
      "step": 4845
    },
    {
      "epoch": 201.39,
      "grad_norm": 0.024467574432492256,
      "learning_rate": 2.43520870704439e-08,
      "loss": 0.0003,
      "step": 4846
    },
    {
      "epoch": 201.43,
      "grad_norm": 0.0700407475233078,
      "learning_rate": 2.4037105842328324e-08,
      "loss": 0.0005,
      "step": 4847
    },
    {
      "epoch": 201.48,
      "grad_norm": 0.047567322850227356,
      "learning_rate": 2.3724170044600036e-08,
      "loss": 0.0006,
      "step": 4848
    },
    {
      "epoch": 201.52,
      "grad_norm": 0.04945629462599754,
      "learning_rate": 2.3413279805895583e-08,
      "loss": 0.0004,
      "step": 4849
    },
    {
      "epoch": 201.56,
      "grad_norm": 0.054218489676713943,
      "learning_rate": 2.3104435254008852e-08,
      "loss": 0.0005,
      "step": 4850
    },
    {
      "epoch": 201.6,
      "grad_norm": 0.035910170525312424,
      "learning_rate": 2.2797636515895505e-08,
      "loss": 0.0003,
      "step": 4851
    },
    {
      "epoch": 201.64,
      "grad_norm": 0.04623835161328316,
      "learning_rate": 2.2492883717668557e-08,
      "loss": 0.0004,
      "step": 4852
    },
    {
      "epoch": 201.68,
      "grad_norm": 0.0034256933722645044,
      "learning_rate": 2.219017698460002e-08,
      "loss": 0.0002,
      "step": 4853
    },
    {
      "epoch": 201.72,
      "grad_norm": 0.0036439518444240093,
      "learning_rate": 2.1889516441121472e-08,
      "loss": 0.0002,
      "step": 4854
    },
    {
      "epoch": 201.77,
      "grad_norm": 0.11907601356506348,
      "learning_rate": 2.159090221082294e-08,
      "loss": 0.0009,
      "step": 4855
    },
    {
      "epoch": 201.81,
      "grad_norm": 0.21498894691467285,
      "learning_rate": 2.1294334416453456e-08,
      "loss": 0.0011,
      "step": 4856
    },
    {
      "epoch": 201.85,
      "grad_norm": 0.03837033733725548,
      "learning_rate": 2.0999813179921057e-08,
      "loss": 0.0003,
      "step": 4857
    },
    {
      "epoch": 201.89,
      "grad_norm": 0.01874181628227234,
      "learning_rate": 2.070733862229224e-08,
      "loss": 0.0004,
      "step": 4858
    },
    {
      "epoch": 201.93,
      "grad_norm": 0.03957494720816612,
      "learning_rate": 2.0416910863792495e-08,
      "loss": 0.0004,
      "step": 4859
    },
    {
      "epoch": 201.97,
      "grad_norm": 0.03673369437456131,
      "learning_rate": 2.012853002380466e-08,
      "loss": 0.0003,
      "step": 4860
    },
    {
      "epoch": 202.02,
      "grad_norm": 0.003478775732219219,
      "learning_rate": 1.98421962208728e-08,
      "loss": 0.0002,
      "step": 4861
    },
    {
      "epoch": 202.06,
      "grad_norm": 0.04914408177137375,
      "learning_rate": 1.9557909572696098e-08,
      "loss": 0.0004,
      "step": 4862
    },
    {
      "epoch": 202.1,
      "grad_norm": 0.09734827280044556,
      "learning_rate": 1.9275670196135522e-08,
      "loss": 0.0007,
      "step": 4863
    },
    {
      "epoch": 202.14,
      "grad_norm": 0.030330510810017586,
      "learning_rate": 1.899547820720882e-08,
      "loss": 0.0003,
      "step": 4864
    },
    {
      "epoch": 202.18,
      "grad_norm": 0.10524062067270279,
      "learning_rate": 1.8717333721091634e-08,
      "loss": 0.0011,
      "step": 4865
    },
    {
      "epoch": 202.22,
      "grad_norm": 0.003408898366615176,
      "learning_rate": 1.8441236852119184e-08,
      "loss": 0.0002,
      "step": 4866
    },
    {
      "epoch": 202.26,
      "grad_norm": 0.050914216786623,
      "learning_rate": 1.8167187713784563e-08,
      "loss": 0.0004,
      "step": 4867
    },
    {
      "epoch": 202.31,
      "grad_norm": 0.05960596352815628,
      "learning_rate": 1.7895186418738773e-08,
      "loss": 0.0005,
      "step": 4868
    },
    {
      "epoch": 202.35,
      "grad_norm": 0.028173208236694336,
      "learning_rate": 1.7625233078790716e-08,
      "loss": 0.0003,
      "step": 4869
    },
    {
      "epoch": 202.39,
      "grad_norm": 0.05280066281557083,
      "learning_rate": 1.735732780490884e-08,
      "loss": 0.0005,
      "step": 4870
    },
    {
      "epoch": 202.43,
      "grad_norm": 0.03964651748538017,
      "learning_rate": 1.7091470707218948e-08,
      "loss": 0.0004,
      "step": 4871
    },
    {
      "epoch": 202.47,
      "grad_norm": 0.059154219925403595,
      "learning_rate": 1.6827661895004176e-08,
      "loss": 0.0005,
      "step": 4872
    },
    {
      "epoch": 202.51,
      "grad_norm": 0.04697166010737419,
      "learning_rate": 1.6565901476707224e-08,
      "loss": 0.0004,
      "step": 4873
    },
    {
      "epoch": 202.56,
      "grad_norm": 0.06478413939476013,
      "learning_rate": 1.630618955992702e-08,
      "loss": 0.0006,
      "step": 4874
    },
    {
      "epoch": 202.6,
      "grad_norm": 0.06448900699615479,
      "learning_rate": 1.6048526251421502e-08,
      "loss": 0.0004,
      "step": 4875
    },
    {
      "epoch": 202.64,
      "grad_norm": 0.0716291293501854,
      "learning_rate": 1.5792911657107057e-08,
      "loss": 0.0006,
      "step": 4876
    },
    {
      "epoch": 202.68,
      "grad_norm": 0.04321103170514107,
      "learning_rate": 1.55393458820563e-08,
      "loss": 0.0004,
      "step": 4877
    },
    {
      "epoch": 202.72,
      "grad_norm": 0.07112472504377365,
      "learning_rate": 1.5287829030500855e-08,
      "loss": 0.0008,
      "step": 4878
    },
    {
      "epoch": 202.76,
      "grad_norm": 0.0033037567045539618,
      "learning_rate": 1.503836120583024e-08,
      "loss": 0.0002,
      "step": 4879
    },
    {
      "epoch": 202.81,
      "grad_norm": 0.021850764751434326,
      "learning_rate": 1.4790942510590767e-08,
      "loss": 0.0003,
      "step": 4880
    },
    {
      "epoch": 202.85,
      "grad_norm": 0.048300791531801224,
      "learning_rate": 1.4545573046486627e-08,
      "loss": 0.0004,
      "step": 4881
    },
    {
      "epoch": 202.89,
      "grad_norm": 0.0034352701622992754,
      "learning_rate": 1.4302252914380476e-08,
      "loss": 0.0002,
      "step": 4882
    },
    {
      "epoch": 202.93,
      "grad_norm": 0.021307479590177536,
      "learning_rate": 1.4060982214292306e-08,
      "loss": 0.0003,
      "step": 4883
    },
    {
      "epoch": 202.97,
      "grad_norm": 0.07083643227815628,
      "learning_rate": 1.382176104539834e-08,
      "loss": 0.0005,
      "step": 4884
    },
    {
      "epoch": 203.01,
      "grad_norm": 0.05547258257865906,
      "learning_rate": 1.3584589506034362e-08,
      "loss": 0.0005,
      "step": 4885
    },
    {
      "epoch": 203.05,
      "grad_norm": 0.06334733963012695,
      "learning_rate": 1.3349467693692387e-08,
      "loss": 0.0005,
      "step": 4886
    },
    {
      "epoch": 203.1,
      "grad_norm": 0.037083275616168976,
      "learning_rate": 1.3116395705021767e-08,
      "loss": 0.0004,
      "step": 4887
    },
    {
      "epoch": 203.14,
      "grad_norm": 0.10649707913398743,
      "learning_rate": 1.2885373635829756e-08,
      "loss": 0.0009,
      "step": 4888
    },
    {
      "epoch": 203.18,
      "grad_norm": 0.059506773948669434,
      "learning_rate": 1.2656401581080947e-08,
      "loss": 0.0005,
      "step": 4889
    },
    {
      "epoch": 203.22,
      "grad_norm": 0.04357036575675011,
      "learning_rate": 1.2429479634897268e-08,
      "loss": 0.0003,
      "step": 4890
    },
    {
      "epoch": 203.26,
      "grad_norm": 0.07450999319553375,
      "learning_rate": 1.220460789055744e-08,
      "loss": 0.0007,
      "step": 4891
    },
    {
      "epoch": 203.3,
      "grad_norm": 0.02361644245684147,
      "learning_rate": 1.1981786440497523e-08,
      "loss": 0.0003,
      "step": 4892
    },
    {
      "epoch": 203.35,
      "grad_norm": 0.04520363733172417,
      "learning_rate": 1.1761015376310914e-08,
      "loss": 0.0006,
      "step": 4893
    },
    {
      "epoch": 203.39,
      "grad_norm": 0.08928855508565903,
      "learning_rate": 1.1542294788749463e-08,
      "loss": 0.0005,
      "step": 4894
    },
    {
      "epoch": 203.43,
      "grad_norm": 0.0749121755361557,
      "learning_rate": 1.132562476771959e-08,
      "loss": 0.0007,
      "step": 4895
    },
    {
      "epoch": 203.47,
      "grad_norm": 0.15699170529842377,
      "learning_rate": 1.1111005402286712e-08,
      "loss": 0.0007,
      "step": 4896
    },
    {
      "epoch": 203.51,
      "grad_norm": 0.01923554576933384,
      "learning_rate": 1.0898436780672483e-08,
      "loss": 0.0003,
      "step": 4897
    },
    {
      "epoch": 203.55,
      "grad_norm": 0.06607562303543091,
      "learning_rate": 1.0687918990256451e-08,
      "loss": 0.0005,
      "step": 4898
    },
    {
      "epoch": 203.59,
      "grad_norm": 0.007283671759068966,
      "learning_rate": 1.0479452117574396e-08,
      "loss": 0.0004,
      "step": 4899
    },
    {
      "epoch": 203.64,
      "grad_norm": 0.040682971477508545,
      "learning_rate": 1.0273036248318325e-08,
      "loss": 0.0004,
      "step": 4900
    },
    {
      "epoch": 203.68,
      "grad_norm": 0.0035561041440814734,
      "learning_rate": 1.0068671467338698e-08,
      "loss": 0.0002,
      "step": 4901
    },
    {
      "epoch": 203.72,
      "grad_norm": 0.03645624220371246,
      "learning_rate": 9.866357858642206e-09,
      "loss": 0.0003,
      "step": 4902
    },
    {
      "epoch": 203.76,
      "grad_norm": 0.043900880962610245,
      "learning_rate": 9.666095505392325e-09,
      "loss": 0.0005,
      "step": 4903
    },
    {
      "epoch": 203.8,
      "grad_norm": 0.033255018293857574,
      "learning_rate": 9.467884489908763e-09,
      "loss": 0.0005,
      "step": 4904
    },
    {
      "epoch": 203.84,
      "grad_norm": 0.03854905068874359,
      "learning_rate": 9.27172489366912e-09,
      "loss": 0.0003,
      "step": 4905
    },
    {
      "epoch": 203.89,
      "grad_norm": 0.003484841203317046,
      "learning_rate": 9.077616797307232e-09,
      "loss": 0.0002,
      "step": 4906
    },
    {
      "epoch": 203.93,
      "grad_norm": 0.045670535415410995,
      "learning_rate": 8.88556028061316e-09,
      "loss": 0.0003,
      "step": 4907
    },
    {
      "epoch": 203.97,
      "grad_norm": 0.04706059768795967,
      "learning_rate": 8.695555422534863e-09,
      "loss": 0.0003,
      "step": 4908
    },
    {
      "epoch": 204.01,
      "grad_norm": 0.08102821558713913,
      "learning_rate": 8.507602301175422e-09,
      "loss": 0.0007,
      "step": 4909
    },
    {
      "epoch": 204.05,
      "grad_norm": 0.003262300044298172,
      "learning_rate": 8.321700993795812e-09,
      "loss": 0.0002,
      "step": 4910
    },
    {
      "epoch": 204.09,
      "grad_norm": 0.13575616478919983,
      "learning_rate": 8.137851576812128e-09,
      "loss": 0.0008,
      "step": 4911
    },
    {
      "epoch": 204.14,
      "grad_norm": 0.07532718777656555,
      "learning_rate": 7.956054125798917e-09,
      "loss": 0.0006,
      "step": 4912
    },
    {
      "epoch": 204.18,
      "grad_norm": 0.06277936697006226,
      "learning_rate": 7.776308715485847e-09,
      "loss": 0.0006,
      "step": 4913
    },
    {
      "epoch": 204.22,
      "grad_norm": 0.07226244360208511,
      "learning_rate": 7.59861541975937e-09,
      "loss": 0.0006,
      "step": 4914
    },
    {
      "epoch": 204.26,
      "grad_norm": 0.038702767342329025,
      "learning_rate": 7.422974311662723e-09,
      "loss": 0.0003,
      "step": 4915
    },
    {
      "epoch": 204.3,
      "grad_norm": 0.0034884989727288485,
      "learning_rate": 7.249385463395375e-09,
      "loss": 0.0002,
      "step": 4916
    },
    {
      "epoch": 204.34,
      "grad_norm": 0.03783077746629715,
      "learning_rate": 7.0778489463124714e-09,
      "loss": 0.0006,
      "step": 4917
    },
    {
      "epoch": 204.38,
      "grad_norm": 0.04420904815196991,
      "learning_rate": 6.90836483092705e-09,
      "loss": 0.0004,
      "step": 4918
    },
    {
      "epoch": 204.43,
      "grad_norm": 0.1153460443019867,
      "learning_rate": 6.740933186907828e-09,
      "loss": 0.0006,
      "step": 4919
    },
    {
      "epoch": 204.47,
      "grad_norm": 0.0034751873463392258,
      "learning_rate": 6.575554083078084e-09,
      "loss": 0.0002,
      "step": 4920
    },
    {
      "epoch": 204.51,
      "grad_norm": 0.02154458686709404,
      "learning_rate": 6.412227587420661e-09,
      "loss": 0.0003,
      "step": 4921
    },
    {
      "epoch": 204.55,
      "grad_norm": 0.08329223841428757,
      "learning_rate": 6.250953767071855e-09,
      "loss": 0.0007,
      "step": 4922
    },
    {
      "epoch": 204.59,
      "grad_norm": 0.0539223812520504,
      "learning_rate": 6.091732688325303e-09,
      "loss": 0.0004,
      "step": 4923
    },
    {
      "epoch": 204.63,
      "grad_norm": 0.1377256214618683,
      "learning_rate": 5.934564416631427e-09,
      "loss": 0.001,
      "step": 4924
    },
    {
      "epoch": 204.68,
      "grad_norm": 0.02527226321399212,
      "learning_rate": 5.779449016595773e-09,
      "loss": 0.0003,
      "step": 4925
    },
    {
      "epoch": 204.72,
      "grad_norm": 0.060246482491493225,
      "learning_rate": 5.626386551980112e-09,
      "loss": 0.0005,
      "step": 4926
    },
    {
      "epoch": 204.76,
      "grad_norm": 0.029818879440426826,
      "learning_rate": 5.475377085703559e-09,
      "loss": 0.0003,
      "step": 4927
    },
    {
      "epoch": 204.8,
      "grad_norm": 0.0298964511603117,
      "learning_rate": 5.3264206798392395e-09,
      "loss": 0.0003,
      "step": 4928
    },
    {
      "epoch": 204.84,
      "grad_norm": 0.07127923518419266,
      "learning_rate": 5.179517395618172e-09,
      "loss": 0.0006,
      "step": 4929
    },
    {
      "epoch": 204.88,
      "grad_norm": 0.05670250207185745,
      "learning_rate": 5.034667293427053e-09,
      "loss": 0.0005,
      "step": 4930
    },
    {
      "epoch": 204.92,
      "grad_norm": 0.08419836312532425,
      "learning_rate": 4.8918704328071446e-09,
      "loss": 0.0006,
      "step": 4931
    },
    {
      "epoch": 204.97,
      "grad_norm": 0.027748167514801025,
      "learning_rate": 4.751126872458156e-09,
      "loss": 0.0003,
      "step": 4932
    },
    {
      "epoch": 205.01,
      "grad_norm": 0.05998150631785393,
      "learning_rate": 4.612436670233811e-09,
      "loss": 0.0006,
      "step": 4933
    },
    {
      "epoch": 205.05,
      "grad_norm": 0.03572060540318489,
      "learning_rate": 4.475799883144061e-09,
      "loss": 0.0003,
      "step": 4934
    },
    {
      "epoch": 205.09,
      "grad_norm": 0.069346584379673,
      "learning_rate": 4.341216567355644e-09,
      "loss": 0.0006,
      "step": 4935
    },
    {
      "epoch": 205.13,
      "grad_norm": 0.03903647139668465,
      "learning_rate": 4.208686778190974e-09,
      "loss": 0.0003,
      "step": 4936
    },
    {
      "epoch": 205.17,
      "grad_norm": 0.14059914648532867,
      "learning_rate": 4.078210570127028e-09,
      "loss": 0.0008,
      "step": 4937
    },
    {
      "epoch": 205.22,
      "grad_norm": 0.0636400654911995,
      "learning_rate": 3.949787996798682e-09,
      "loss": 0.0005,
      "step": 4938
    },
    {
      "epoch": 205.26,
      "grad_norm": 0.0033600598108023405,
      "learning_rate": 3.8234191109948195e-09,
      "loss": 0.0002,
      "step": 4939
    },
    {
      "epoch": 205.3,
      "grad_norm": 0.019472138956189156,
      "learning_rate": 3.6991039646616657e-09,
      "loss": 0.0002,
      "step": 4940
    },
    {
      "epoch": 205.34,
      "grad_norm": 0.0298497062176466,
      "learning_rate": 3.5768426089000107e-09,
      "loss": 0.0003,
      "step": 4941
    },
    {
      "epoch": 205.38,
      "grad_norm": 0.050257403403520584,
      "learning_rate": 3.4566350939668757e-09,
      "loss": 0.0005,
      "step": 4942
    },
    {
      "epoch": 205.42,
      "grad_norm": 0.011353341862559319,
      "learning_rate": 3.3384814692749567e-09,
      "loss": 0.0002,
      "step": 4943
    },
    {
      "epoch": 205.46,
      "grad_norm": 0.06745294481515884,
      "learning_rate": 3.2223817833931803e-09,
      "loss": 0.0007,
      "step": 4944
    },
    {
      "epoch": 205.51,
      "grad_norm": 0.03908671438694,
      "learning_rate": 3.1083360840455935e-09,
      "loss": 0.0003,
      "step": 4945
    },
    {
      "epoch": 205.55,
      "grad_norm": 0.02110442705452442,
      "learning_rate": 2.9963444181113633e-09,
      "loss": 0.0003,
      "step": 4946
    },
    {
      "epoch": 205.59,
      "grad_norm": 0.027494307607412338,
      "learning_rate": 2.8864068316269976e-09,
      "loss": 0.0004,
      "step": 4947
    },
    {
      "epoch": 205.63,
      "grad_norm": 0.05829061195254326,
      "learning_rate": 2.7785233697835702e-09,
      "loss": 0.0005,
      "step": 4948
    },
    {
      "epoch": 205.67,
      "grad_norm": 0.048576805740594864,
      "learning_rate": 2.6726940769272736e-09,
      "loss": 0.0005,
      "step": 4949
    },
    {
      "epoch": 205.71,
      "grad_norm": 0.07604803144931793,
      "learning_rate": 2.568918996560532e-09,
      "loss": 0.0006,
      "step": 4950
    },
    {
      "epoch": 205.76,
      "grad_norm": 0.07687452435493469,
      "learning_rate": 2.4671981713420003e-09,
      "loss": 0.0007,
      "step": 4951
    },
    {
      "epoch": 205.8,
      "grad_norm": 0.034442078322172165,
      "learning_rate": 2.367531643085452e-09,
      "loss": 0.0004,
      "step": 4952
    },
    {
      "epoch": 205.84,
      "grad_norm": 0.01675284281373024,
      "learning_rate": 2.2699194527592282e-09,
      "loss": 0.0003,
      "step": 4953
    },
    {
      "epoch": 205.88,
      "grad_norm": 0.11409660428762436,
      "learning_rate": 2.1743616404878986e-09,
      "loss": 0.0011,
      "step": 4954
    },
    {
      "epoch": 205.92,
      "grad_norm": 0.07963535189628601,
      "learning_rate": 2.0808582455528194e-09,
      "loss": 0.0006,
      "step": 4955
    },
    {
      "epoch": 205.96,
      "grad_norm": 0.036232609301805496,
      "learning_rate": 1.989409306388801e-09,
      "loss": 0.0004,
      "step": 4956
    },
    {
      "epoch": 206.01,
      "grad_norm": 0.024488529190421104,
      "learning_rate": 1.900014860586885e-09,
      "loss": 0.0003,
      "step": 4957
    },
    {
      "epoch": 206.05,
      "grad_norm": 0.04013488069176674,
      "learning_rate": 1.8126749448943437e-09,
      "loss": 0.0003,
      "step": 4958
    },
    {
      "epoch": 206.09,
      "grad_norm": 0.06590515375137329,
      "learning_rate": 1.7273895952130138e-09,
      "loss": 0.0005,
      "step": 4959
    },
    {
      "epoch": 206.13,
      "grad_norm": 0.036327287554740906,
      "learning_rate": 1.6441588466009627e-09,
      "loss": 0.0003,
      "step": 4960
    },
    {
      "epoch": 206.17,
      "grad_norm": 0.057823002338409424,
      "learning_rate": 1.5629827332702686e-09,
      "loss": 0.0006,
      "step": 4961
    },
    {
      "epoch": 206.21,
      "grad_norm": 0.09882675856351852,
      "learning_rate": 1.4838612885903492e-09,
      "loss": 0.0009,
      "step": 4962
    },
    {
      "epoch": 206.25,
      "grad_norm": 0.04549459367990494,
      "learning_rate": 1.406794545084078e-09,
      "loss": 0.0004,
      "step": 4963
    },
    {
      "epoch": 206.3,
      "grad_norm": 0.02645631693303585,
      "learning_rate": 1.3317825344316692e-09,
      "loss": 0.0003,
      "step": 4964
    },
    {
      "epoch": 206.34,
      "grad_norm": 0.028188001364469528,
      "learning_rate": 1.2588252874673469e-09,
      "loss": 0.0004,
      "step": 4965
    },
    {
      "epoch": 206.38,
      "grad_norm": 0.0474790520966053,
      "learning_rate": 1.187922834180455e-09,
      "loss": 0.0004,
      "step": 4966
    },
    {
      "epoch": 206.42,
      "grad_norm": 0.0515289306640625,
      "learning_rate": 1.1190752037171238e-09,
      "loss": 0.0007,
      "step": 4967
    },
    {
      "epoch": 206.46,
      "grad_norm": 0.06383943557739258,
      "learning_rate": 1.0522824243774932e-09,
      "loss": 0.0005,
      "step": 4968
    },
    {
      "epoch": 206.5,
      "grad_norm": 0.15343070030212402,
      "learning_rate": 9.87544523618489e-10,
      "loss": 0.0009,
      "step": 4969
    },
    {
      "epoch": 206.55,
      "grad_norm": 0.05219855159521103,
      "learning_rate": 9.248615280499362e-10,
      "loss": 0.0004,
      "step": 4970
    },
    {
      "epoch": 206.59,
      "grad_norm": 0.05157869681715965,
      "learning_rate": 8.642334634395566e-10,
      "loss": 0.0003,
      "step": 4971
    },
    {
      "epoch": 206.63,
      "grad_norm": 0.02600965090095997,
      "learning_rate": 8.056603547090813e-10,
      "loss": 0.0003,
      "step": 4972
    },
    {
      "epoch": 206.67,
      "grad_norm": 0.03964587673544884,
      "learning_rate": 7.49142225935362e-10,
      "loss": 0.0003,
      "step": 4973
    },
    {
      "epoch": 206.71,
      "grad_norm": 0.08426570147275925,
      "learning_rate": 6.946791003509256e-10,
      "loss": 0.0006,
      "step": 4974
    },
    {
      "epoch": 206.75,
      "grad_norm": 0.04826999828219414,
      "learning_rate": 6.422710003439747e-10,
      "loss": 0.0006,
      "step": 4975
    },
    {
      "epoch": 206.79,
      "grad_norm": 0.0034628380089998245,
      "learning_rate": 5.919179474567216e-10,
      "loss": 0.0002,
      "step": 4976
    },
    {
      "epoch": 206.84,
      "grad_norm": 0.07847396284341812,
      "learning_rate": 5.436199623881644e-10,
      "loss": 0.0007,
      "step": 4977
    },
    {
      "epoch": 206.88,
      "grad_norm": 0.07993406057357788,
      "learning_rate": 4.973770649918664e-10,
      "loss": 0.0005,
      "step": 4978
    },
    {
      "epoch": 206.92,
      "grad_norm": 0.0342685841023922,
      "learning_rate": 4.5318927427540073e-10,
      "loss": 0.0004,
      "step": 4979
    },
    {
      "epoch": 206.96,
      "grad_norm": 0.07453399151563644,
      "learning_rate": 4.1105660840368154e-10,
      "loss": 0.0006,
      "step": 4980
    },
    {
      "epoch": 207.0,
      "grad_norm": 0.0036202454939484596,
      "learning_rate": 3.7097908469563295e-10,
      "loss": 0.0002,
      "step": 4981
    },
    {
      "epoch": 207.04,
      "grad_norm": 0.0037724014837294817,
      "learning_rate": 3.329567196258543e-10,
      "loss": 0.0004,
      "step": 4982
    },
    {
      "epoch": 207.09,
      "grad_norm": 0.1007915511727333,
      "learning_rate": 2.969895288229552e-10,
      "loss": 0.0008,
      "step": 4983
    },
    {
      "epoch": 207.13,
      "grad_norm": 0.046362005174160004,
      "learning_rate": 2.630775270728858e-10,
      "loss": 0.0004,
      "step": 4984
    },
    {
      "epoch": 207.17,
      "grad_norm": 0.03012751042842865,
      "learning_rate": 2.3122072831505137e-10,
      "loss": 0.0003,
      "step": 4985
    },
    {
      "epoch": 207.21,
      "grad_norm": 0.06437365710735321,
      "learning_rate": 2.0141914564453247e-10,
      "loss": 0.0006,
      "step": 4986
    },
    {
      "epoch": 207.25,
      "grad_norm": 0.03817212954163551,
      "learning_rate": 1.7367279131152991e-10,
      "loss": 0.0003,
      "step": 4987
    },
    {
      "epoch": 207.29,
      "grad_norm": 0.01519866194576025,
      "learning_rate": 1.4798167672192e-10,
      "loss": 0.0003,
      "step": 4988
    },
    {
      "epoch": 207.34,
      "grad_norm": 0.07744058966636658,
      "learning_rate": 1.2434581243614409e-10,
      "loss": 0.0005,
      "step": 4989
    },
    {
      "epoch": 207.38,
      "grad_norm": 0.02914092130959034,
      "learning_rate": 1.0276520816976388e-10,
      "loss": 0.0003,
      "step": 4990
    },
    {
      "epoch": 207.42,
      "grad_norm": 0.041843440383672714,
      "learning_rate": 8.323987279401647e-11,
      "loss": 0.0004,
      "step": 4991
    },
    {
      "epoch": 207.46,
      "grad_norm": 0.0342608205974102,
      "learning_rate": 6.57698143352592e-11,
      "loss": 0.0004,
      "step": 4992
    },
    {
      "epoch": 207.5,
      "grad_norm": 0.0031600934453308582,
      "learning_rate": 5.035503997385949e-11,
      "loss": 0.0002,
      "step": 4993
    },
    {
      "epoch": 207.54,
      "grad_norm": 0.05042959004640579,
      "learning_rate": 3.699555604752547e-11,
      "loss": 0.0003,
      "step": 4994
    },
    {
      "epoch": 207.58,
      "grad_norm": 0.04316791892051697,
      "learning_rate": 2.5691368046865118e-11,
      "loss": 0.0004,
      "step": 4995
    },
    {
      "epoch": 207.63,
      "grad_norm": 0.06368672847747803,
      "learning_rate": 1.6442480619272007e-11,
      "loss": 0.0006,
      "step": 4996
    },
    {
      "epoch": 207.67,
      "grad_norm": 0.046983588486909866,
      "learning_rate": 9.248897566149773e-12,
      "loss": 0.0004,
      "step": 4997
    },
    {
      "epoch": 207.71,
      "grad_norm": 0.14691868424415588,
      "learning_rate": 4.1106218445774445e-12,
      "loss": 0.0009,
      "step": 4998
    },
    {
      "epoch": 207.75,
      "grad_norm": 0.06551156938076019,
      "learning_rate": 1.0276555667543264e-12,
      "loss": 0.0005,
      "step": 4999
    },
    {
      "epoch": 207.79,
      "grad_norm": 0.0504501610994339,
      "learning_rate": 0.0,
      "loss": 0.0004,
      "step": 5000
    },
    {
      "epoch": 207.79,
      "step": 5000,
      "total_flos": 1.313377204409303e+17,
      "train_loss": 0.0363056039992487,
      "train_runtime": 12578.8244,
      "train_samples_per_second": 25.44,
      "train_steps_per_second": 0.397
    }
  ],
  "logging_steps": 1.0,
  "max_steps": 5000,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 209,
  "save_steps": 500,
  "total_flos": 1.313377204409303e+17,
  "train_batch_size": 2,
  "trial_name": null,
  "trial_params": null
}
